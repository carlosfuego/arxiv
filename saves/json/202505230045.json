[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.15793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15793v2",
                "updated": "2025-05-22T04:48:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    48,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T17:47:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    47,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving"
                },
                "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Bo Leng"
                    },
                    {
                        "name": "Zhuoren Li"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Guizhe Jin"
                    },
                    {
                        "name": "Ran Yu"
                    },
                    {
                        "name": "Huanxi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Huanxi Wen"
                },
                "author": "Huanxi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15781v1",
                "updated": "2025-05-21T17:32:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:32:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "dKV-Cache: The Cache for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dKV-Cache: The Cache for Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs."
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at https://github.com/horseee/dKV-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15684v1",
                "updated": "2025-05-21T15:58:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy"
                },
                "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption."
                },
                "authors": [
                    {
                        "name": "Gengyang Li"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v1",
                "updated": "2025-05-21T15:58:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability"
                },
                "summary": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Jiaying Zheng"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Jin Dong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15558v1",
                "updated": "2025-05-21T14:17:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:17:06Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "title": "Robo-DM: Data Management For Large Robot Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robo-DM: Data Management For Large Robot Datasets"
                },
                "summary": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "Letian Fu"
                    },
                    {
                        "name": "David Huang"
                    },
                    {
                        "name": "Yanxiang Zhang"
                    },
                    {
                        "name": "Lawrence Yunliang Chen"
                    },
                    {
                        "name": "Huang Huang"
                    },
                    {
                        "name": "Kush Hari"
                    },
                    {
                        "name": "Ashwin Balakrishna"
                    },
                    {
                        "name": "Ted Xiao"
                    },
                    {
                        "name": "Pannag R Sanketi"
                    },
                    {
                        "name": "John Kubiatowicz"
                    },
                    {
                        "name": "Ken Goldberg"
                    }
                ],
                "author_detail": {
                    "name": "Ken Goldberg"
                },
                "author": "Ken Goldberg",
                "arxiv_comment": "Best paper finalist of IEEE ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v4",
                "updated": "2025-05-21T14:05:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    5,
                    56,
                    2,
                    141,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15535v1",
                "updated": "2025-05-21T13:56:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:56:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead"
                },
                "summary": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations."
                },
                "authors": [
                    {
                        "name": "Michał Wichrowski"
                    },
                    {
                        "name": "Mohsen Rezaee-Hajidehi"
                    },
                    {
                        "name": "Jože Korelc"
                    },
                    {
                        "name": "Martin Kronbichler"
                    },
                    {
                        "name": "Stanisław Stupkiewicz"
                    }
                ],
                "author_detail": {
                    "name": "Stanisław Stupkiewicz"
                },
                "author": "Stanisław Stupkiewicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65M60, 74B20, 74S05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15531v1",
                "updated": "2025-05-21T13:52:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:52:45Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "title": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency"
                },
                "summary": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chaofan Ma"
                },
                "author": "Chaofan Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v2",
                "updated": "2025-05-22T06:44:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    44,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v2",
                "updated": "2025-05-21T10:38:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v2",
                "updated": "2025-05-21T10:38:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    1,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Accepted by ICML 2025. Code is available:\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v2",
                "updated": "2025-05-21T10:37:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    37,
                    50,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v1",
                "updated": "2025-05-21T10:20:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15269v1",
                "updated": "2025-05-21T08:47:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T08:47:15Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "title": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval"
                },
                "summary": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v2",
                "updated": "2025-05-21T06:45:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    6,
                    45,
                    58,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v2",
                "updated": "2025-05-21T01:34:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    1,
                    34,
                    19,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14992v1",
                "updated": "2025-05-21T00:40:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    0,
                    40,
                    5,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T00:40:05Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    0,
                    40,
                    5,
                    2,
                    141,
                    0
                ],
                "title": "Effective and Efficient Schema-aware Information Extraction Using\n  On-Device Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and Efficient Schema-aware Information Extraction Using\n  On-Device Large Language Models"
                },
                "summary": "Information extraction (IE) plays a crucial role in natural language\nprocessing (NLP) by converting unstructured text into structured knowledge.\nDeploying computationally intensive large language models (LLMs) on\nresource-constrained devices for information extraction is challenging,\nparticularly due to issues like hallucinations, limited context length, and\nhigh latency-especially when handling diverse extraction schemas. To address\nthese challenges, we propose a two-stage information extraction approach\nadapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching\n(DLISC), which enhances both schema identification and schema-aware extraction\nin terms of effectiveness and efficiency. In particular, DLISC adopts an\nIdentification LoRA module for retrieving the most relevant schemas to a given\nquery, and an Extraction LoRA module for performing information extraction\nbased on the previously selected schemas. To accelerate extraction inference,\nIncremental Schema Caching is incorporated to reduce redundant computation,\nsubstantially improving efficiency. Extensive experiments across multiple\ninformation extraction datasets demonstrate notable improvements in both\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information extraction (IE) plays a crucial role in natural language\nprocessing (NLP) by converting unstructured text into structured knowledge.\nDeploying computationally intensive large language models (LLMs) on\nresource-constrained devices for information extraction is challenging,\nparticularly due to issues like hallucinations, limited context length, and\nhigh latency-especially when handling diverse extraction schemas. To address\nthese challenges, we propose a two-stage information extraction approach\nadapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching\n(DLISC), which enhances both schema identification and schema-aware extraction\nin terms of effectiveness and efficiency. In particular, DLISC adopts an\nIdentification LoRA module for retrieving the most relevant schemas to a given\nquery, and an Extraction LoRA module for performing information extraction\nbased on the previously selected schemas. To accelerate extraction inference,\nIncremental Schema Caching is incorporated to reduce redundant computation,\nsubstantially improving efficiency. Extensive experiments across multiple\ninformation extraction datasets demonstrate notable improvements in both\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Zhihao Wen"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v1",
                "updated": "2025-05-20T23:12:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v3",
                "updated": "2025-05-20T18:49:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    18,
                    49,
                    30,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v3",
                "updated": "2025-05-20T17:50:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    50,
                    11,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v4",
                "updated": "2025-05-20T16:29:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Konstantina Mellou"
                    },
                    {
                        "name": "Marco Molinaro"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14427v1",
                "updated": "2025-05-20T14:38:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:38:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out"
                },
                "summary": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM."
                },
                "authors": [
                    {
                        "name": "Thomas Sandholm"
                    },
                    {
                        "name": "Sayandev Mukherjee"
                    },
                    {
                        "name": "Lin Cheng"
                    },
                    {
                        "name": "Bernardo A. Huberman"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo A. Huberman"
                },
                "author": "Bernardo A. Huberman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14398v1",
                "updated": "2025-05-20T14:14:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:14:38Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation"
                },
                "summary": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Samuel Madden"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "arxiv_comment": "Data and code are available at https://peterbaile.github.io/lag/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14085v1",
                "updated": "2025-05-20T08:46:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T08:46:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration"
                },
                "summary": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity."
                },
                "authors": [
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "arxiv_comment": "14 pages, 7 figures including subplots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13002v2",
                "updated": "2025-05-20T07:34:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    34,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T11:41:21Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    11,
                    41,
                    21,
                    0,
                    139,
                    0
                ],
                "title": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures"
                },
                "summary": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase."
                },
                "authors": [
                    {
                        "name": "Dongjae Lee"
                    },
                    {
                        "name": "Bongjoon Hyun"
                    },
                    {
                        "name": "Youngjin Kwon"
                    },
                    {
                        "name": "Minsoo Rhu"
                    }
                ],
                "author_detail": {
                    "name": "Minsoo Rhu"
                },
                "author": "Minsoo Rhu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14010v1",
                "updated": "2025-05-20T07:04:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T07:04:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache"
                },
                "summary": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md."
                },
                "authors": [
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Pengwen Dai"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Dianjie Lu"
                    },
                    {
                        "name": "Guijuan Zhang"
                    },
                    {
                        "name": "Youshan Zhang"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Zheng"
                },
                "author": "Zhuoran Zheng",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v3",
                "updated": "2025-05-20T04:52:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    4,
                    52,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Akash Das"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v1",
                "updated": "2025-05-20T03:21:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09561v2",
                "updated": "2025-05-19T20:37:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    20,
                    37,
                    41,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-14T17:00:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Long-Context Diffusion Policies via Past-Token Prediction"
                },
                "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x."
                },
                "authors": [
                    {
                        "name": "Marcel Torne"
                    },
                    {
                        "name": "Andy Tang"
                    },
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Videos are available at https://long-context-dp.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12463v2",
                "updated": "2025-05-19T19:09:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    19,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2024-05-21T02:39:45Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    2,
                    39,
                    45,
                    1,
                    142,
                    0
                ],
                "title": "Stochastic Learning of Computational Resource Usage as Graph Structured\n  Multimarginal Schrödinger Bridge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Learning of Computational Resource Usage as Graph Structured\n  Multimarginal Schrödinger Bridge"
                },
                "summary": "We propose to learn the time-varying stochastic computational resource usage\nof software as a graph structured Schr\\\"odinger bridge problem. In general,\nlearning the computational resource usage from data is challenging because\nresources such as the number of CPU instructions and the number of last level\ncache requests are both time-varying and statistically correlated. Our proposed\nmethod enables learning the joint time-varying stochasticity in computational\nresource usage from the measured profile snapshots in a nonparametric manner.\nThe method can be used to predict the most-likely time-varying distribution of\ncomputational resource availability at a desired time. We provide detailed\nalgorithms for stochastic learning in both single and multi-core cases, discuss\nthe convergence guarantees, computational complexities, and demonstrate their\npractical use in two case studies: a single-core nonlinear model predictive\ncontroller, and a synthetic multi-core software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to learn the time-varying stochastic computational resource usage\nof software as a graph structured Schr\\\"odinger bridge problem. In general,\nlearning the computational resource usage from data is challenging because\nresources such as the number of CPU instructions and the number of last level\ncache requests are both time-varying and statistically correlated. Our proposed\nmethod enables learning the joint time-varying stochasticity in computational\nresource usage from the measured profile snapshots in a nonparametric manner.\nThe method can be used to predict the most-likely time-varying distribution of\ncomputational resource availability at a desired time. We provide detailed\nalgorithms for stochastic learning in both single and multi-core cases, discuss\nthe convergence guarantees, computational complexities, and demonstrate their\npractical use in two case studies: a single-core nonlinear model predictive\ncontroller, and a synthetic multi-core software."
                },
                "authors": [
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10951v2",
                "updated": "2025-05-19T17:51:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    26,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-16T07:39:41Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    39,
                    41,
                    4,
                    136,
                    0
                ],
                "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache"
                },
                "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT)."
                },
                "authors": [
                    {
                        "name": "Qiuyu Zhu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13140v1",
                "updated": "2025-05-19T14:09:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:09:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow"
                },
                "summary": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available."
                },
                "authors": [
                    {
                        "name": "Takahiro Maeda"
                    },
                    {
                        "name": "Jinkun Cao"
                    },
                    {
                        "name": "Norimichi Ukita"
                    },
                    {
                        "name": "Kris Kitani"
                    }
                ],
                "author_detail": {
                    "name": "Kris Kitani"
                },
                "author": "Kris Kitani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v1",
                "updated": "2025-05-19T13:36:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Danning Ke"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13094v1",
                "updated": "2025-05-19T13:25:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:25:51Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation"
                },
                "summary": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/."
                },
                "authors": [
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Runxuan Yang"
                    },
                    {
                        "name": "Xiaolin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolin Hu"
                },
                "author": "Xiaolin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12946v1",
                "updated": "2025-05-19T10:34:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    34,
                    54,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T10:34:54Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    34,
                    54,
                    0,
                    139,
                    0
                ],
                "title": "6G-Enabled Smart Railways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G-Enabled Smart Railways"
                },
                "summary": "Smart railways integrate advanced information technologies into railway\noperating systems to improve efficiency and reliability. Although the\ndevelopment of 5G has enhanced railway services, future smart railways require\nultra-high speeds, ultra-low latency, ultra-high security, full coverage, and\nultra-high positioning accuracy, which 5G cannot fully meet. Therefore, 6G is\nenvisioned to provide green and efficient all-day operations, strong\ninformation security, fully automatic driving, and low-cost intelligent\nmaintenance. To achieve these requirements, we propose an integrated network\narchitecture leveraging communications, computing, edge intelligence, and\ncaching in railway systems. We have conducted in-depth investigations on key\nenabling technologies for reliable transmissions and wireless coverage. For\nhigh-speed mobile scenarios, we propose an AI-enabled cross-domain channel\nmodeling and orthogonal time-frequency space-time spread multiple access\nmechanism to alleviate the conflict between limited spectrum availability and\nmassive user access. The roles of blockchain, edge intelligence, and privacy\ntechnologies in endogenously secure rail communications are also evaluated. We\nfurther explore the application of emerging paradigms such as integrated\nsensing and communications, AI-assisted Internet of Things, semantic\ncommunications, and digital twin networks for railway maintenance, monitoring,\nprediction, and accident warning. Finally, possible future research and\ndevelopment directions are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart railways integrate advanced information technologies into railway\noperating systems to improve efficiency and reliability. Although the\ndevelopment of 5G has enhanced railway services, future smart railways require\nultra-high speeds, ultra-low latency, ultra-high security, full coverage, and\nultra-high positioning accuracy, which 5G cannot fully meet. Therefore, 6G is\nenvisioned to provide green and efficient all-day operations, strong\ninformation security, fully automatic driving, and low-cost intelligent\nmaintenance. To achieve these requirements, we propose an integrated network\narchitecture leveraging communications, computing, edge intelligence, and\ncaching in railway systems. We have conducted in-depth investigations on key\nenabling technologies for reliable transmissions and wireless coverage. For\nhigh-speed mobile scenarios, we propose an AI-enabled cross-domain channel\nmodeling and orthogonal time-frequency space-time spread multiple access\nmechanism to alleviate the conflict between limited spectrum availability and\nmassive user access. The roles of blockchain, edge intelligence, and privacy\ntechnologies in endogenously secure rail communications are also evaluated. We\nfurther explore the application of emerging paradigms such as integrated\nsensing and communications, AI-assisted Internet of Things, semantic\ncommunications, and digital twin networks for railway maintenance, monitoring,\nprediction, and accident warning. Finally, possible future research and\ndevelopment directions are discussed."
                },
                "authors": [
                    {
                        "name": "Bo Ai"
                    },
                    {
                        "name": "Yunlong Lu"
                    },
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Ruisi He"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Guoyu Ma"
                    },
                    {
                        "name": "Yong Niu"
                    },
                    {
                        "name": "Zhangdui Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhangdui Zhong"
                },
                "author": "Zhangdui Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v1",
                "updated": "2025-05-19T10:29:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09928v2",
                "updated": "2025-05-19T10:13:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    13,
                    31,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-15T03:25:41Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    25,
                    41,
                    3,
                    135,
                    0
                ],
                "title": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles"
                },
                "summary": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems."
                },
                "authors": [
                    {
                        "name": "Xingchen Sun"
                    },
                    {
                        "name": "Runhua Xu"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Li Duan"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12742v1",
                "updated": "2025-05-19T05:56:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    56,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T05:56:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    56,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian\n  Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian\n  Conditioning"
                },
                "summary": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x."
                },
                "authors": [
                    {
                        "name": "Jinhua Zhang"
                    },
                    {
                        "name": "Wei Long"
                    },
                    {
                        "name": "Minghao Han"
                    },
                    {
                        "name": "Weiyi You"
                    },
                    {
                        "name": "Shuhang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Gu"
                },
                "author": "Shuhang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v1",
                "updated": "2025-05-19T05:39:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v2",
                "updated": "2025-05-19T02:21:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    21,
                    16,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Frequency-domain compression has proven effective in reducing redundancies\nfor spatial signals. In this work, we propose FreqKV, a novel frequency domain\nkey-value (KV) compression technique that enables efficient context window\nextension for decoder-only large language models (LLMs). Our approach is\nmotivated by a key observation that, in the frequency domain, the energy\ndistribution of the KV cache is predominantly concentrated in low-frequency\ncomponents. By discarding high-frequency components, we achieve efficient\ncompression of the KV cache with minimal information loss. FreqKV iteratively\ncompresses the increasing KV cache to a fixed size in the frequency domain,\nallowing models to process lengthy contexts efficiently. Introducing no\nadditional parameters or architectural modifications, FreqKV is applicable to\nboth fine-tuning and inference. With minimal fine-tuning, LLMs can learn to\nleverage the limited cache that is compressed in the frequency domain and\nextend the context window. Experiments on a range of long context language\nmodeling and understanding tasks demonstrate the efficiency and effectiveness\nof the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-domain compression has proven effective in reducing redundancies\nfor spatial signals. In this work, we propose FreqKV, a novel frequency domain\nkey-value (KV) compression technique that enables efficient context window\nextension for decoder-only large language models (LLMs). Our approach is\nmotivated by a key observation that, in the frequency domain, the energy\ndistribution of the KV cache is predominantly concentrated in low-frequency\ncomponents. By discarding high-frequency components, we achieve efficient\ncompression of the KV cache with minimal information loss. FreqKV iteratively\ncompresses the increasing KV cache to a fixed size in the frequency domain,\nallowing models to process lengthy contexts efficiently. Introducing no\nadditional parameters or architectural modifications, FreqKV is applicable to\nboth fine-tuning and inference. With minimal fine-tuning, LLMs can learn to\nleverage the limited cache that is compressed in the frequency domain and\nextend the context window. Experiments on a range of long context language\nmodeling and understanding tasks demonstrate the efficiency and effectiveness\nof the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12594v1",
                "updated": "2025-05-19T01:14:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    1,
                    14,
                    57,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T01:14:57Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    1,
                    14,
                    57,
                    0,
                    139,
                    0
                ],
                "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection"
                },
                "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."
                },
                "authors": [
                    {
                        "name": "Tiankai Yang"
                    },
                    {
                        "name": "Junjun Liu"
                    },
                    {
                        "name": "Wingchun Siu"
                    },
                    {
                        "name": "Jiahang Wang"
                    },
                    {
                        "name": "Zhuangzhuang Qian"
                    },
                    {
                        "name": "Chanjuan Song"
                    },
                    {
                        "name": "Cheng Cheng"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v1",
                "updated": "2025-05-18T12:37:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18394v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18394v7",
                "updated": "2025-05-18T03:12:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    18,
                    3,
                    12,
                    25,
                    6,
                    138,
                    0
                ],
                "published": "2025-02-25T17:43:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    43,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "SPECTRE: An FFT-Based Efficient Drop-In Replacement to Self-Attention\n  for Long Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPECTRE: An FFT-Based Efficient Drop-In Replacement to Self-Attention\n  for Long Contexts"
                },
                "summary": "Long-context transformers face significant efficiency challenges due to the\nquadratic cost of self-attention. However, many modern applications-from\nmulti-turn dialogue to high-resolution vision-require contexts spanning tens of\nthousands of tokens. We introduce SPECTRE, a method that replaces each\nattention head with a fast real FFT, a content-adaptive spectral gate, and an\ninverse FFT, reducing per-layer complexity from $\\mathcal{O}(L^{2})$ to\n$O(L\\log L)$ while preserving the surrounding architecture. We extend this\nefficiency to autoregressive generation through our Prefix-FFT cache and\nenhance local feature representation with an optional wavelet module that adds\nnegligible computational overhead. Our experiments demonstrate that SPECTRE\noperates up to 7$\\times$ faster than FlashAttention-2 on 128k-token contexts\nwhile matching or exceeding baseline performance on PG-19 language modeling and\nImageNet-1k classification tasks. SPECTRE achieves these improvements by adding\nfewer than 6\\% parameters to the base model, making hundred-kilotoken context\nprocessing feasible on commodity GPUs without specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context transformers face significant efficiency challenges due to the\nquadratic cost of self-attention. However, many modern applications-from\nmulti-turn dialogue to high-resolution vision-require contexts spanning tens of\nthousands of tokens. We introduce SPECTRE, a method that replaces each\nattention head with a fast real FFT, a content-adaptive spectral gate, and an\ninverse FFT, reducing per-layer complexity from $\\mathcal{O}(L^{2})$ to\n$O(L\\log L)$ while preserving the surrounding architecture. We extend this\nefficiency to autoregressive generation through our Prefix-FFT cache and\nenhance local feature representation with an optional wavelet module that adds\nnegligible computational overhead. Our experiments demonstrate that SPECTRE\noperates up to 7$\\times$ faster than FlashAttention-2 on 128k-token contexts\nwhile matching or exceeding baseline performance on PG-19 language modeling and\nImageNet-1k classification tasks. SPECTRE achieves these improvements by adding\nfewer than 6\\% parameters to the base model, making hundred-kilotoken context\nprocessing feasible on commodity GPUs without specialized hardware."
                },
                "authors": [
                    {
                        "name": "Jacob Fein-Ashley"
                    },
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18394v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18394v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v3",
                "updated": "2025-05-17T23:26:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    23,
                    26,
                    8,
                    5,
                    137,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v3",
                "updated": "2025-05-17T21:15:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    21,
                    15,
                    2,
                    5,
                    137,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T. Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v2",
                "updated": "2025-05-17T12:22:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    12,
                    22,
                    59,
                    5,
                    137,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14709v1",
                "updated": "2025-05-17T05:00:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    5,
                    0,
                    39,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T05:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    5,
                    0,
                    39,
                    5,
                    137,
                    0
                ],
                "title": "FastCar: Cache Attentive Replay for Fast Auto-Regressive Video\n  Generation on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCar: Cache Attentive Replay for Fast Auto-Regressive Video\n  Generation on the Edge"
                },
                "summary": "Auto-regressive (AR) models, initially successful in language generation,\nhave recently shown promise in visual generation tasks due to their superior\nsampling efficiency. Unlike image generation, video generation requires a\nsubstantially larger number of tokens to produce coherent temporal frames,\nresulting in significant overhead during the decoding phase. Our key\nobservations are: (i) MLP modules in the decode phase dominate the inference\nlatency, and (ii) there exists high temporal redundancy in MLP outputs of\nadjacent frames. In this paper, we propose the \\textbf{FastCar} framework to\naccelerate the decode phase for the AR video generation by exploring the\ntemporal redundancy. The Temporal Attention Score (TAS) is proposed to\ndetermine whether to apply the replay strategy (\\textit{i.e.}, reusing cached\nMLP outputs from the previous frame to reduce redundant computations) with\ndetailed theoretical analysis and justification. Also, we develop a hardware\naccelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to\nenable better resource utilization and faster inference. Experimental results\ndemonstrate the effectiveness of our method, which outperforms traditional\nsparse attention approaches with more than 2.1x decoding speedup and higher\nenergy efficiency on the edge. Furthermore, by combining FastCar and sparse\nattention, FastCar can boost the performance of sparse attention with\nalleviated drifting, demonstrating our unique advantages for high-resolution\nand long-duration video generation. Code:\nhttps://github.com/shawnricecake/fast-car",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive (AR) models, initially successful in language generation,\nhave recently shown promise in visual generation tasks due to their superior\nsampling efficiency. Unlike image generation, video generation requires a\nsubstantially larger number of tokens to produce coherent temporal frames,\nresulting in significant overhead during the decoding phase. Our key\nobservations are: (i) MLP modules in the decode phase dominate the inference\nlatency, and (ii) there exists high temporal redundancy in MLP outputs of\nadjacent frames. In this paper, we propose the \\textbf{FastCar} framework to\naccelerate the decode phase for the AR video generation by exploring the\ntemporal redundancy. The Temporal Attention Score (TAS) is proposed to\ndetermine whether to apply the replay strategy (\\textit{i.e.}, reusing cached\nMLP outputs from the previous frame to reduce redundant computations) with\ndetailed theoretical analysis and justification. Also, we develop a hardware\naccelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to\nenable better resource utilization and faster inference. Experimental results\ndemonstrate the effectiveness of our method, which outperforms traditional\nsparse attention approaches with more than 2.1x decoding speedup and higher\nenergy efficiency on the edge. Furthermore, by combining FastCar and sparse\nattention, FastCar can boost the performance of sparse attention with\nalleviated drifting, demonstrating our unique advantages for high-resolution\nand long-duration video generation. Code:\nhttps://github.com/shawnricecake/fast-car"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Weize Ma"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Enhao Tang"
                    },
                    {
                        "name": "Yanyue Xie"
                    },
                    {
                        "name": "Zhengang Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Quanyi Wang"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Jun Lin"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Preprint Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11820v1",
                "updated": "2025-05-17T04:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T04:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "Chain-of-Model Learning for Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Model Learning for Language Model"
                },
                "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."
                },
                "authors": [
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Cen LU"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Tao Qin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11783v1",
                "updated": "2025-05-17T01:31:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    1,
                    31,
                    21,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T01:31:21Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    1,
                    31,
                    21,
                    5,
                    137,
                    0
                ],
                "title": "Efficient Vector Search on Disaggregated Memory with d-HNSW",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Vector Search on Disaggregated Memory with d-HNSW"
                },
                "summary": "Efficient vector query processing is critical to enable AI applications at\nscale. Recent solutions struggle with growing vector datasets that exceed\nsingle-machine memory capacity, forcing unnecessary data movement and resource\nunderutilization in monolithic architectures. We present d-HNSW, the first\ndisaggregated vector similarity search engine for RDMA-based remote memory\nsystems that achieves high performance while supporting fast data indexing with\nlow network communication overhead. The core of d-HNSW is a novel\ndisaggregation of the graph-based vector indexing data structure HNSW. It\nexploits the characteristics of greedy searching in HNSW to efficiently\ncoordinate data transfers from the memory pool to the compute pool while\nserving data requests. Specifically, it leverages three ideas: (i)\nRepresentative index caching, a lightweight index constructed from a sampled\nsubset of data, is cached in the compute pool to reduce frequent access to\ncritical components of the hierarchical graph-based index, (ii) RDMA-friendly\ndata layout design to reduce the networking round trips incurred by vector\nquery and insertion and (iii) batched query-aware data loading to reduce\nbandwidth usage on data transfer between pools, addressing the limited cache\ncapacity in compute nodes. We evaluate d-HNSW with extensive benchmarking\ndatasets. The experimental results show that d-HNSW outperforms Naive d-HNSW\nimplementation by up to 117x in latency while maintaining recall as 0.87 in\ndataset SIFT1M@1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient vector query processing is critical to enable AI applications at\nscale. Recent solutions struggle with growing vector datasets that exceed\nsingle-machine memory capacity, forcing unnecessary data movement and resource\nunderutilization in monolithic architectures. We present d-HNSW, the first\ndisaggregated vector similarity search engine for RDMA-based remote memory\nsystems that achieves high performance while supporting fast data indexing with\nlow network communication overhead. The core of d-HNSW is a novel\ndisaggregation of the graph-based vector indexing data structure HNSW. It\nexploits the characteristics of greedy searching in HNSW to efficiently\ncoordinate data transfers from the memory pool to the compute pool while\nserving data requests. Specifically, it leverages three ideas: (i)\nRepresentative index caching, a lightweight index constructed from a sampled\nsubset of data, is cached in the compute pool to reduce frequent access to\ncritical components of the hierarchical graph-based index, (ii) RDMA-friendly\ndata layout design to reduce the networking round trips incurred by vector\nquery and insertion and (iii) batched query-aware data loading to reduce\nbandwidth usage on data transfer between pools, addressing the limited cache\ncapacity in compute nodes. We evaluate d-HNSW with extensive benchmarking\ndatasets. The experimental results show that d-HNSW outperforms Naive d-HNSW\nimplementation by up to 117x in latency while maintaining recall as 0.87 in\ndataset SIFT1M@1."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_comment": "To appear in HotStorage 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v1",
                "updated": "2025-05-16T21:04:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11625v1",
                "updated": "2025-05-16T18:41:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    41,
                    33,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T18:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    41,
                    33,
                    4,
                    136,
                    0
                ],
                "title": "Nearest Neighbor Multivariate Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearest Neighbor Multivariate Time Series Forecasting"
                },
                "summary": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models."
                },
                "authors": [
                    {
                        "name": "Huiliang Zhang"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Benoit Boulet"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Boulet"
                },
                "author": "Benoit Boulet",
                "arxiv_doi": "10.1109/TNNLS.2024.3490603",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNNLS.2024.3490603",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.11625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Neural Netw. Learn. Syst., early access, 14 Nov. 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11302v1",
                "updated": "2025-05-16T14:30:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    30,
                    46,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T14:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    30,
                    46,
                    4,
                    136,
                    0
                ],
                "title": "Depth first representations of $k^2$-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth first representations of $k^2$-trees"
                },
                "summary": "The $k^2$-tree is a compact data structure designed to efficiently store\nsparse binary matrices by leveraging both sparsity and clustering of nonzero\nelements. This representation supports efficiently navigational operations and\ncomplex binary operations, such as matrix-matrix multiplication, while\nmaintaining space efficiency. The standard $k^2$-tree follows a level-by-level\nrepresentation, which, while effective, prevents further compression of\nidentical subtrees and it si not cache friendly when accessing individual\nsubtrees. In this work, we introduce some novel depth-first representations of\nthe $k^2$-tree and propose an efficient linear-time algorithm to identify and\ncompress identical subtrees within these structures. Our experimental results\nshow that the use of a depth-first representations is a strategy worth\npursuing: for the adjacency matrix of web graphs exploiting the presence of\nidentical subtrees does improve the compression ratio, and for some matrices\ndepth-first representations turns out to be faster than the standard $k^2$-tree\nin computing the matrix-matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k^2$-tree is a compact data structure designed to efficiently store\nsparse binary matrices by leveraging both sparsity and clustering of nonzero\nelements. This representation supports efficiently navigational operations and\ncomplex binary operations, such as matrix-matrix multiplication, while\nmaintaining space efficiency. The standard $k^2$-tree follows a level-by-level\nrepresentation, which, while effective, prevents further compression of\nidentical subtrees and it si not cache friendly when accessing individual\nsubtrees. In this work, we introduce some novel depth-first representations of\nthe $k^2$-tree and propose an efficient linear-time algorithm to identify and\ncompress identical subtrees within these structures. Our experimental results\nshow that the use of a depth-first representations is a strategy worth\npursuing: for the adjacency matrix of web graphs exploiting the presence of\nidentical subtrees does improve the compression ratio, and for some matrices\ndepth-first representations turns out to be faster than the standard $k^2$-tree\nin computing the matrix-matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Gabriel Carmona"
                    },
                    {
                        "name": "Giovanni Manzini"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Manzini"
                },
                "author": "Giovanni Manzini",
                "arxiv_comment": "extended submission for SPIRE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11271v1",
                "updated": "2025-05-16T14:04:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    4,
                    31,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T14:04:31Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    4,
                    31,
                    4,
                    136,
                    0
                ],
                "title": "Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants."
                },
                "authors": [
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Spyros Mastorakis"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Victor Rühle"
                    }
                ],
                "author_detail": {
                    "name": "Victor Rühle"
                },
                "author": "Victor Rühle",
                "arxiv_comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.10272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.10272v2",
                "updated": "2025-05-16T13:56:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    13,
                    56,
                    7,
                    4,
                    136,
                    0
                ],
                "published": "2022-09-21T11:24:10Z",
                "published_parsed": [
                    2022,
                    9,
                    21,
                    11,
                    24,
                    10,
                    2,
                    264,
                    0
                ],
                "title": "Evaluating Continuous Basic Graph Patterns over Dynamic Link Data Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Continuous Basic Graph Patterns over Dynamic Link Data Graphs"
                },
                "summary": "In this paper, we investigate the problem of evaluating Basic Graph Patterns\n(BGP, for short, a subclass of SPARQL queries) over dynamic Linked Data graphs;\ni.e., Linked Data graphs that are continuously updated. We consider a setting\nwhere the updates are continuously received through a stream of messages and\nsupport both insertions and deletions of triples (updates are straightforwardly\nhandled as a combination of deletions and insertions). In this context, we\npropose a set of in-memory algorithms minimizing the cached data to efficiently\nand continuously answer BGP queries. The queries are typically submitted into a\nsystem and continuously result in the delta answers while the update messages\nare processed.\n  To efficiently and continuously evaluate the submitted query over the\nstreaming data, as well as to minimize the amount of cached data, we propose an\napproach where the submitted query is decomposed into simpler subqueries and\nthe query evaluation is achieved by combining the intermediate answers of the\nsubqueries. Using this approach, the proposed algorithms compute the delta\nanswers of a BGP query in polynomial time and space. Note that for certain\nsubclasses of BGP queries, we show that the evaluation can be achieved in\nconstant or linear time and space. Consolidating all the historical delta\nanswers, the algorithms ensure that the answer to each query is constructed at\nany given time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the problem of evaluating Basic Graph Patterns\n(BGP, for short, a subclass of SPARQL queries) over dynamic Linked Data graphs;\ni.e., Linked Data graphs that are continuously updated. We consider a setting\nwhere the updates are continuously received through a stream of messages and\nsupport both insertions and deletions of triples (updates are straightforwardly\nhandled as a combination of deletions and insertions). In this context, we\npropose a set of in-memory algorithms minimizing the cached data to efficiently\nand continuously answer BGP queries. The queries are typically submitted into a\nsystem and continuously result in the delta answers while the update messages\nare processed.\n  To efficiently and continuously evaluate the submitted query over the\nstreaming data, as well as to minimize the amount of cached data, we propose an\napproach where the submitted query is decomposed into simpler subqueries and\nthe query evaluation is achieved by combining the intermediate answers of the\nsubqueries. Using this approach, the proposed algorithms compute the delta\nanswers of a BGP query in polynomial time and space. Note that for certain\nsubclasses of BGP queries, we show that the evaluation can be achieved in\nconstant or linear time and space. Consolidating all the historical delta\nanswers, the algorithms ensure that the answer to each query is constructed at\nany given time."
                },
                "authors": [
                    {
                        "name": "Manolis Gergatsoulis"
                    },
                    {
                        "name": "Matthew Damigos"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Damigos"
                },
                "author": "Matthew Damigos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.10272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.10272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v2",
                "updated": "2025-05-16T12:42:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    42,
                    48,
                    4,
                    136,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant\n  KV Cache Reuse"
                },
                "summary": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Mingzhe Huang"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Yin Tang"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v3",
                "updated": "2025-05-16T12:32:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    32,
                    36,
                    4,
                    136,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v2",
                "updated": "2025-05-16T09:40:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    9,
                    40,
                    1,
                    4,
                    136,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10938v1",
                "updated": "2025-05-16T07:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    23,
                    12,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T07:23:12Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    23,
                    12,
                    4,
                    136,
                    0
                ],
                "title": "Accurate KV Cache Quantization with Outlier Tokens Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate KV Cache Quantization with Outlier Tokens Tracing"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v3",
                "updated": "2025-05-16T03:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    34,
                    33,
                    4,
                    136,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. Code is available at:\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. Code is available at:\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Accepted by TMLR, 23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10806v1",
                "updated": "2025-05-16T03:01:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    1,
                    47,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T03:01:47Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    1,
                    47,
                    4,
                    136,
                    0
                ],
                "title": "RapidGNN: Communication Efficient Large-Scale Distributed Training of\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Communication Efficient Large-Scale Distributed Training of\n  Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)\nperformance in diverse domains. However, training GNNs on large-scale graphs\nposes significant challenges due to high memory demands and significant\ncommunication overhead in distributed settings. Traditional sampling-based\napproaches mitigate computation load to some extent but often fail to address\ncommunication inefficiencies inherent in distributed environments. This paper\npresents RapidGNN that introduces a deterministic sampling strategy to\nprecompute mini-batches. By leveraging the sampling strategy, RapidGNN\naccurately anticipates feature access patterns, enabling optimal cache\nconstruction and timely prefetching of remote features. This reduces the\nfrequency and latency of remote data transfers without compromising the\nstochastic nature of training. Evaluations on Reddit and OGBN-Products datasets\ndemonstrate that RapidGNN achieves significant reductions in training time and\nremote feature fetches, outperforming existing models in both communication\nefficiency and throughput. Our findings highlight RapidGNN's potential for\nscalable, high-performance GNN training across large, real-world graph datasets\nalong with improving energy efficiency. Our model improves end-to-end training\nthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x in\nsome settings), while cutting remote feature fetches by over 4x. It also\nreduces energy consumption up to 23%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)\nperformance in diverse domains. However, training GNNs on large-scale graphs\nposes significant challenges due to high memory demands and significant\ncommunication overhead in distributed settings. Traditional sampling-based\napproaches mitigate computation load to some extent but often fail to address\ncommunication inefficiencies inherent in distributed environments. This paper\npresents RapidGNN that introduces a deterministic sampling strategy to\nprecompute mini-batches. By leveraging the sampling strategy, RapidGNN\naccurately anticipates feature access patterns, enabling optimal cache\nconstruction and timely prefetching of remote features. This reduces the\nfrequency and latency of remote data transfers without compromising the\nstochastic nature of training. Evaluations on Reddit and OGBN-Products datasets\ndemonstrate that RapidGNN achieves significant reductions in training time and\nremote feature fetches, outperforming existing models in both communication\nefficiency and throughput. Our findings highlight RapidGNN's potential for\nscalable, high-performance GNN training across large, real-world graph datasets\nalong with improving energy efficiency. Our model improves end-to-end training\nthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x in\nsome settings), while cutting remote feature fetches by over 4x. It also\nreduces energy consumption up to 23%."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v2",
                "updated": "2025-05-16T00:56:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    0,
                    56,
                    30,
                    4,
                    136,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "Accepted to MLSys 2025,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10560v1",
                "updated": "2025-05-15T17:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "title": "Approximation-First Timeseries Monitoring Query At Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximation-First Timeseries Monitoring Query At Scale"
                },
                "summary": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch."
                },
                "authors": [
                    {
                        "name": "Zeying Zhu"
                    },
                    {
                        "name": "Jonathan Chamberlain"
                    },
                    {
                        "name": "Kenny Wu"
                    },
                    {
                        "name": "David Starobinski"
                    },
                    {
                        "name": "Zaoxing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zaoxing Liu"
                },
                "author": "Zaoxing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v4",
                "updated": "2025-05-15T17:18:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    18,
                    12,
                    3,
                    135,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11554v1",
                "updated": "2025-05-15T16:40:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    14,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:40:14Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    14,
                    3,
                    135,
                    0
                ],
                "title": "Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for\n  Multicore Real-Time Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for\n  Multicore Real-Time Systems"
                },
                "summary": "Memory bandwidth regulation and cache partitioning are widely used techniques\nfor achieving predictable timing in real-time computing systems. Combined with\npartitioned scheduling, these methods require careful co-allocation of tasks\nand resources to cores, as task execution times strongly depend on available\nallocated resources. To address this challenge, this paper presents a 0-1\nlinear program for task-resource co-allocation, along with a multi-objective\nheuristic designed to minimize resource usage while guaranteeing schedulability\nunder a preemptive EDF scheduling policy. Our heuristic employs a multi-layer\nframework, where an outer layer explores resource allocations using\nPareto-pruned search, and an inner layer optimizes task allocation by solving a\nknapsack problem using dynamic programming. To evaluate the performance of the\nproposed optimization algorithm, we profile real-world benchmarks on an\nembedded AMD UltraScale+ ZCU102 platform, with fine-grained resource\npartitioning enabled by the Jailhouse hypervisor, leveraging cache set\npartitioning and MemGuard for memory bandwidth regulation. Experiments based on\nthe benchmarking results show that the proposed 0-1 linear program outperforms\nexisting mixed-integer programs by finding more optimal solutions within the\nsame time limit. Moreover, the proposed multi-objective multi-layer heuristic\nperforms consistently better than the state-of-the-art multi-resource-task\nco-allocation algorithm in terms of schedulability, resource usage, number of\nnon-dominated solutions, and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory bandwidth regulation and cache partitioning are widely used techniques\nfor achieving predictable timing in real-time computing systems. Combined with\npartitioned scheduling, these methods require careful co-allocation of tasks\nand resources to cores, as task execution times strongly depend on available\nallocated resources. To address this challenge, this paper presents a 0-1\nlinear program for task-resource co-allocation, along with a multi-objective\nheuristic designed to minimize resource usage while guaranteeing schedulability\nunder a preemptive EDF scheduling policy. Our heuristic employs a multi-layer\nframework, where an outer layer explores resource allocations using\nPareto-pruned search, and an inner layer optimizes task allocation by solving a\nknapsack problem using dynamic programming. To evaluate the performance of the\nproposed optimization algorithm, we profile real-world benchmarks on an\nembedded AMD UltraScale+ ZCU102 platform, with fine-grained resource\npartitioning enabled by the Jailhouse hypervisor, leveraging cache set\npartitioning and MemGuard for memory bandwidth regulation. Experiments based on\nthe benchmarking results show that the proposed 0-1 linear program outperforms\nexisting mixed-integer programs by finding more optimal solutions within the\nsame time limit. Moreover, the proposed multi-objective multi-layer heuristic\nperforms consistently better than the state-of-the-art multi-resource-task\nco-allocation algorithm in terms of schedulability, resource usage, number of\nnon-dominated solutions, and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Binqi Sun"
                    },
                    {
                        "name": "Zhihang Wei"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Debayan Roy"
                    },
                    {
                        "name": "Mirco Theile"
                    },
                    {
                        "name": "Tomasz Kloda"
                    },
                    {
                        "name": "Rodolfo Pellizzoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_doi": "10.4230/LIPIcs.ECRTS.2025.7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ECRTS.2025.7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.11554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in the 37th Euromicro Conference on Real-Time Systems (ECRTS\n  2025)",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v7",
                "updated": "2025-05-15T13:48:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    48,
                    40,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v3",
                "updated": "2025-05-15T03:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    29,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v2",
                "updated": "2025-05-15T03:27:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    27,
                    28,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Zongming Guo"
                    },
                    {
                        "name": "Xinggong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggong Zhang"
                },
                "author": "Xinggong Zhang",
                "arxiv_doi": "10.1145/3735358.3735383",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3735358.3735383",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.16112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages (excluding references), 10 figures, to appear in APNET 2025",
                "arxiv_journal_ref": "Proc. 9th Asia-Pacific Workshop on Networking (APNET), Aug 2025,\n  Paper No. 24",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v2",
                "updated": "2025-05-14T16:04:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    4,
                    57,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10584v1",
                "updated": "2025-05-14T13:39:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    39,
                    53,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:39:53Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    39,
                    53,
                    2,
                    134,
                    0
                ],
                "title": "Aquarius: A Family of Industry-Level Video Generation Models for\n  Marketing Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aquarius: A Family of Industry-Level Video Generation Models for\n  Marketing Scenarios"
                },
                "summary": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates."
                },
                "authors": [
                    {
                        "name": "Huafeng Shi"
                    },
                    {
                        "name": "Jianzhong Liang"
                    },
                    {
                        "name": "Rongchang Xie"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v7",
                "updated": "2025-05-14T04:38:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    38,
                    42,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v2",
                "updated": "2025-05-14T04:22:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    22,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1145/3695053.3731019",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731019",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 14 figures, and 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v1",
                "updated": "2025-05-14T02:29:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v1",
                "updated": "2025-05-14T00:41:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-cache: Efficient Robot Trajectory Retrieval System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-cache: Efficient Robot Trajectory Retrieval System"
                },
                "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08958v1",
                "updated": "2025-05-13T20:51:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T20:51:59Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "title": "Adaptive Entanglement Generation for Quantum Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Entanglement Generation for Quantum Routing"
                },
                "summary": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%."
                },
                "authors": [
                    {
                        "name": "Tasdiqul Islam"
                    },
                    {
                        "name": "Md Arifuzzaman"
                    },
                    {
                        "name": "Engin Arslan"
                    }
                ],
                "author_detail": {
                    "name": "Engin Arslan"
                },
                "author": "Engin Arslan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v5",
                "updated": "2025-05-13T17:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    43,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08587v1",
                "updated": "2025-05-13T13:58:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:58:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications"
                },
                "summary": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library."
                },
                "authors": [
                    {
                        "name": "Nicolás A. Barnafi"
                    },
                    {
                        "name": "Massimiliano Lupo Pasini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Lupo Pasini"
                },
                "author": "Massimiliano Lupo Pasini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65N12, 65N22, 65K10, 65F10, 65F99, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v2",
                "updated": "2025-05-13T09:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    36,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08261v1",
                "updated": "2025-05-13T06:24:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration"
                },
                "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."
                },
                "authors": [
                    {
                        "name": "Rishabh Agrawal"
                    },
                    {
                        "name": "Himanshu Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Himanshu Kumar"
                },
                "author": "Himanshu Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07692v1",
                "updated": "2025-05-12T15:58:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:58:39Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "title": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments"
                },
                "summary": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB."
                },
                "authors": [
                    {
                        "name": "Rong Kang"
                    },
                    {
                        "name": "Yanbin Chen"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Fuxin Jiang"
                    },
                    {
                        "name": "Qingshuo Li"
                    },
                    {
                        "name": "Miao Ma"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Guangliang Zhao"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "SIGMOD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07680v1",
                "updated": "2025-05-12T15:46:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:46:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method."
                },
                "authors": [
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v1",
                "updated": "2025-05-12T08:44:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07274v1",
                "updated": "2025-05-12T06:53:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T06:53:24Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains"
                },
                "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07239v1",
                "updated": "2025-05-12T05:29:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T05:29:30Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "title": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity"
                },
                "summary": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction."
                },
                "authors": [
                    {
                        "name": "Guang Yan"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Zimu Guo"
                    },
                    {
                        "name": "Lutan Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_doi": "10.1109/SP61157.2025.00182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP61157.2025.00182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07203v1",
                "updated": "2025-05-12T03:22:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T03:22:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications"
                },
                "summary": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency."
                },
                "authors": [
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yiming Cheng"
                    },
                    {
                        "name": "Qing Lan"
                    },
                    {
                        "name": "Hejian Sang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06901v1",
                "updated": "2025-05-11T08:44:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "published": "2025-05-11T08:44:31Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "title": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression"
                },
                "summary": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models."
                },
                "authors": [
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Junyao Zhang"
                    },
                    {
                        "name": "Changchun Zhou"
                    },
                    {
                        "name": "Edward Hanson"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06625v1",
                "updated": "2025-05-10T12:16:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T12:16:50Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs"
                },
                "summary": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average)."
                },
                "authors": [
                    {
                        "name": "Tianhao Cai"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Limin Xiao"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Xiaojian Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojian Liao"
                },
                "author": "Xiaojian Liao",
                "arxiv_comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06556v1",
                "updated": "2025-05-10T07:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T07:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "title": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store"
                },
                "summary": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving."
                },
                "authors": [
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Shiyu Yang"
                    },
                    {
                        "name": "Weibo Chen"
                    },
                    {
                        "name": "Kunming Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Junwei Chen"
                    },
                    {
                        "name": "Yuan Su"
                    },
                    {
                        "name": "Xiaoxia Duan"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Ruoyi Ruan"
                    },
                    {
                        "name": "Xuemin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Lin"
                },
                "author": "Xuemin Lin",
                "arxiv_comment": "Accepted by ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v1",
                "updated": "2025-05-09T21:05:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06095v3",
                "updated": "2025-05-09T07:26:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    26,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-10T08:26:27Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    8,
                    26,
                    27,
                    0,
                    162,
                    0
                ],
                "title": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development"
                },
                "summary": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity."
                },
                "authors": [
                    {
                        "name": "Pawel K. Radtke"
                    },
                    {
                        "name": "Cristian G. Barrera-Hinojosa"
                    },
                    {
                        "name": "Mladen Ivkovic"
                    },
                    {
                        "name": "Tobias Weinzierl"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weinzierl"
                },
                "author": "Tobias Weinzierl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05829v1",
                "updated": "2025-05-09T06:56:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:56:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition"
                },
                "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Keyi Li"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Yufei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ma"
                },
                "author": "Yufei Ma",
                "arxiv_comment": "accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05772v1",
                "updated": "2025-05-09T04:17:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM"
                },
                "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
                },
                "authors": [
                    {
                        "name": "Zehao Fan"
                    },
                    {
                        "name": "Garrett Gagnon"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Liu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liu Liu"
                },
                "author": "Liu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v3",
                "updated": "2025-05-09T00:31:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    31,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05251v1",
                "updated": "2025-05-08T13:56:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:56:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity"
                },
                "summary": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity."
                },
                "authors": [
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "arxiv_comment": "13 pages, 8 figures, submitted to IEEE journals for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v1",
                "updated": "2025-05-08T11:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03762v2",
                "updated": "2025-05-08T09:05:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    5,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-20T17:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    17,
                    48,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture"
                },
                "summary": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Côme Allart"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Zexin Fu"
                    },
                    {
                        "name": "Filippo Grillotti"
                    },
                    {
                        "name": "Fabio De Ambroggi"
                    },
                    {
                        "name": "Elio Guidetti"
                    },
                    {
                        "name": "Jean-Baptiste Rigaud"
                    },
                    {
                        "name": "Olivier Potin"
                    },
                    {
                        "name": "Jean Roch Coulon"
                    },
                    {
                        "name": "César Fuguet"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12110v2",
                "updated": "2025-05-08T07:55:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    55,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-17T21:43:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    43,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "CacheSquash: Making caches speculation-aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheSquash: Making caches speculation-aware"
                },
                "summary": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads."
                },
                "authors": [
                    {
                        "name": "Hossam ElAtali"
                    },
                    {
                        "name": "N. Asokan"
                    }
                ],
                "author_detail": {
                    "name": "N. Asokan"
                },
                "author": "N. Asokan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04896v1",
                "updated": "2025-05-08T02:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T02:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory"
                },
                "summary": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
                },
                "authors": [
                    {
                        "name": "MD Mahady Hassan"
                    },
                    {
                        "name": "Shanto Roy"
                    },
                    {
                        "name": "Reza Rahaeimehr"
                    }
                ],
                "author_detail": {
                    "name": "Reza Rahaeimehr"
                },
                "author": "Reza Rahaeimehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04556v1",
                "updated": "2025-05-07T16:44:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:44:21Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "title": "Comparing CPU and GPU compute of PERMANOVA on MI300A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing CPU and GPU compute of PERMANOVA on MI300A"
                },
                "summary": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise."
                },
                "authors": [
                    {
                        "name": "Igor Sfiligoi"
                    }
                ],
                "author_detail": {
                    "name": "Igor Sfiligoi"
                },
                "author": "Igor Sfiligoi",
                "arxiv_comment": "7 pages, 1 figure, Accepted at PEARC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04466v1",
                "updated": "2025-05-07T14:37:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:37:13Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption"
                },
                "summary": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
                },
                "authors": [
                    {
                        "name": "Mohammad Waquas Usmani"
                    },
                    {
                        "name": "Susmit Shannigrahi"
                    },
                    {
                        "name": "Michael Zink"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zink"
                },
                "author": "Michael Zink",
                "arxiv_comment": "8 pages plus references, 10 figures, some with subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.15818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15818v1",
                "updated": "2025-05-21T17:59:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    59,
                    56,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:59:56Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    59,
                    56,
                    2,
                    141,
                    0
                ],
                "title": "InstructSAM: A Training-Free Framework for Instruction-Oriented Remote\n  Sensing Object Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructSAM: A Training-Free Framework for Instruction-Oriented Remote\n  Sensing Object Recognition"
                },
                "summary": "Language-Guided object recognition in remote sensing imagery is crucial for\nlarge-scale mapping and automated data annotation. However, existing\nopen-vocabulary and visual grounding methods rely on explicit category cues,\nlimiting their ability to handle complex or implicit queries that require\nadvanced reasoning. To address this issue, we introduce a new suite of tasks,\nincluding Instruction-Oriented Object Counting, Detection, and Segmentation\n(InstructCDS), covering open-vocabulary, open-ended, and open-subclass\nscenarios. We further present EarthInstruct, the first InstructCDS benchmark\nfor earth observation. It is constructed from two diverse remote sensing\ndatasets with varying spatial resolutions and annotation rules across 20\ncategories, necessitating models to interpret dataset-specific instructions.\nGiven the scarcity of semantically rich labeled data in remote sensing, we\npropose InstructSAM, a training-free framework for instruction-driven object\nrecognition. InstructSAM leverages large vision-language models to interpret\nuser instructions and estimate object counts, employs SAM2 for mask proposal,\nand formulates mask-label assignment as a binary integer programming problem.\nBy integrating semantic similarity with counting constraints, InstructSAM\nefficiently assigns categories to predicted masks without relying on confidence\nthresholds. Experiments demonstrate that InstructSAM matches or surpasses\nspecialized baselines across multiple tasks while maintaining near-constant\ninference time regardless of object count, reducing output tokens by 89% and\noverall runtime by over 32% compared to direct generation approaches. We\nbelieve the contributions of the proposed tasks, benchmark, and effective\napproach will advance future research in developing versatile object\nrecognition systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Guided object recognition in remote sensing imagery is crucial for\nlarge-scale mapping and automated data annotation. However, existing\nopen-vocabulary and visual grounding methods rely on explicit category cues,\nlimiting their ability to handle complex or implicit queries that require\nadvanced reasoning. To address this issue, we introduce a new suite of tasks,\nincluding Instruction-Oriented Object Counting, Detection, and Segmentation\n(InstructCDS), covering open-vocabulary, open-ended, and open-subclass\nscenarios. We further present EarthInstruct, the first InstructCDS benchmark\nfor earth observation. It is constructed from two diverse remote sensing\ndatasets with varying spatial resolutions and annotation rules across 20\ncategories, necessitating models to interpret dataset-specific instructions.\nGiven the scarcity of semantically rich labeled data in remote sensing, we\npropose InstructSAM, a training-free framework for instruction-driven object\nrecognition. InstructSAM leverages large vision-language models to interpret\nuser instructions and estimate object counts, employs SAM2 for mask proposal,\nand formulates mask-label assignment as a binary integer programming problem.\nBy integrating semantic similarity with counting constraints, InstructSAM\nefficiently assigns categories to predicted masks without relying on confidence\nthresholds. Experiments demonstrate that InstructSAM matches or surpasses\nspecialized baselines across multiple tasks while maintaining near-constant\ninference time regardless of object count, reducing output tokens by 89% and\noverall runtime by over 32% compared to direct generation approaches. We\nbelieve the contributions of the proposed tasks, benchmark, and effective\napproach will advance future research in developing versatile object\nrecognition systems."
                },
                "authors": [
                    {
                        "name": "Yijie Zheng"
                    },
                    {
                        "name": "Weijie Wu"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Xuehui Wang"
                    },
                    {
                        "name": "Xu Zhou"
                    },
                    {
                        "name": "Aiai Ren"
                    },
                    {
                        "name": "Jun Shen"
                    },
                    {
                        "name": "Long Zhao"
                    },
                    {
                        "name": "Guoqing Li"
                    },
                    {
                        "name": "Xue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xue Yang"
                },
                "author": "Xue Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15817v1",
                "updated": "2025-05-21T17:59:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    59,
                    54,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:59:54Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    59,
                    54,
                    2,
                    141,
                    0
                ],
                "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason via Mixture-of-Thought for Logical Reasoning"
                },
                "summary": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference."
                },
                "authors": [
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "Simeng Han"
                    },
                    {
                        "name": "R. Thomas McCoy"
                    },
                    {
                        "name": "Heng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Huang"
                },
                "author": "Heng Huang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15807v1",
                "updated": "2025-05-21T17:59:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    59,
                    1,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:59:01Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    59,
                    1,
                    2,
                    141,
                    0
                ],
                "title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context\n  Retrieval Augmentation"
                },
                "summary": "Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels."
                },
                "authors": [
                    {
                        "name": "Patrick Kahardipraja"
                    },
                    {
                        "name": "Reduan Achtibat"
                    },
                    {
                        "name": "Thomas Wiegand"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lapuschkin"
                },
                "author": "Sebastian Lapuschkin",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15805v1",
                "updated": "2025-05-21T17:58:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    58,
                    11,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:58:11Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    58,
                    11,
                    2,
                    141,
                    0
                ],
                "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering"
                },
                "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security."
                },
                "authors": [
                    {
                        "name": "Hwan Chang"
                    },
                    {
                        "name": "Yumin Kim"
                    },
                    {
                        "name": "Yonghyun Jun"
                    },
                    {
                        "name": "Hwanhee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwanhee Lee"
                },
                "author": "Hwanhee Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13247v2",
                "updated": "2025-05-21T17:58:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    58,
                    4,
                    2,
                    141,
                    0
                ],
                "published": "2024-08-23T17:42:06Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    42,
                    6,
                    4,
                    236,
                    0
                ],
                "title": "An In-Depth Investigation of Data Collection in LLM App Ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An In-Depth Investigation of Data Collection in LLM App Ecosystems"
                },
                "summary": "LLM app (tool) ecosystems are rapidly evolving to support sophisticated use\ncases that often require extensive user data collection. Given that LLM apps\nare developed by third parties and anecdotal evidence indicating inconsistent\nenforcement of policies by LLM platforms, sharing user data with these apps\npresents significant privacy risks. In this paper, we aim to bring transparency\nin data practices of LLM app ecosystems. We examine OpenAI's GPT app ecosystem\nas a case study. We propose an LLM-based framework to analyze the natural\nlanguage specifications of GPT Actions (custom tools) and assess their data\ncollection practices. Our analysis reveals that Actions collect excessive data\nacross 24 categories and 145 data types, with third-party Actions collecting\n6.03% more data on average. We find that several Actions violate OpenAI's\npolicies by collecting sensitive information, such as passwords, which is\nexplicitly prohibited by OpenAI. Lastly, we develop an LLM-based privacy policy\nanalysis framework to automatically check the consistency of data collection by\nActions with disclosures in their privacy policies. Our measurements indicate\nthat the disclosures for most of the collected data types are omitted, with\nonly 5.8% of Actions clearly disclosing their data collection practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM app (tool) ecosystems are rapidly evolving to support sophisticated use\ncases that often require extensive user data collection. Given that LLM apps\nare developed by third parties and anecdotal evidence indicating inconsistent\nenforcement of policies by LLM platforms, sharing user data with these apps\npresents significant privacy risks. In this paper, we aim to bring transparency\nin data practices of LLM app ecosystems. We examine OpenAI's GPT app ecosystem\nas a case study. We propose an LLM-based framework to analyze the natural\nlanguage specifications of GPT Actions (custom tools) and assess their data\ncollection practices. Our analysis reveals that Actions collect excessive data\nacross 24 categories and 145 data types, with third-party Actions collecting\n6.03% more data on average. We find that several Actions violate OpenAI's\npolicies by collecting sensitive information, such as passwords, which is\nexplicitly prohibited by OpenAI. Lastly, we develop an LLM-based privacy policy\nanalysis framework to automatically check the consistency of data collection by\nActions with disclosures in their privacy policies. Our measurements indicate\nthat the disclosures for most of the collected data types are omitted, with\nonly 5.8% of Actions clearly disclosing their data collection practices."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Evin Jaff"
                    },
                    {
                        "name": "Ke Yang"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Umar Iqbal"
                    }
                ],
                "author_detail": {
                    "name": "Umar Iqbal"
                },
                "author": "Umar Iqbal",
                "arxiv_comment": "Accepted by the ACM Internet Measurement Conference (IMC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15804v1",
                "updated": "2025-05-21T17:57:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    57,
                    38,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:57:38Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    57,
                    38,
                    2,
                    141,
                    0
                ],
                "title": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1."
                },
                "authors": [
                    {
                        "name": "Zongzhao Li"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Mingze Li"
                    },
                    {
                        "name": "Songyou Li"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Tingyang Xu"
                    },
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Wenbing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenbing Huang"
                },
                "author": "Wenbing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14652v2",
                "updated": "2025-05-21T17:55:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    55,
                    36,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T17:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    33,
                    1,
                    140,
                    0
                ],
                "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-Reasoner: Advancing LLM Reasoning Across All Domains"
                },
                "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24370v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24370v3",
                "updated": "2025-05-21T17:51:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    51,
                    27,
                    2,
                    141,
                    0
                ],
                "published": "2025-03-31T17:50:13Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    50,
                    13,
                    0,
                    90,
                    0
                ],
                "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively Controlling Reasoning Models through Thinking Intervention"
                },
                "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We find that the\nThinking Intervention paradigm enhances the capabilities of reasoning models\nacross a wide range of tasks, including instruction following on IFEval and\nOverthinking, instruction hierarchy on SEP, and safety alignment on XSTest and\nSorryBench. Our results demonstrate that Thinking Intervention significantly\noutperforms baseline prompting approaches, achieving up to 6.7% accuracy gains\nin instruction-following scenarios, 15.4% improvements in reasoning about\ninstruction hierarchies, and a 40.0% increase in refusal rates for unsafe\nprompts using open-source DeepSeek R1 models. Overall, our work opens a\npromising new research avenue for controlling reasoning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We find that the\nThinking Intervention paradigm enhances the capabilities of reasoning models\nacross a wide range of tasks, including instruction following on IFEval and\nOverthinking, instruction hierarchy on SEP, and safety alignment on XSTest and\nSorryBench. Our results demonstrate that Thinking Intervention significantly\noutperforms baseline prompting approaches, achieving up to 6.7% accuracy gains\nin instruction-following scenarios, 15.4% improvements in reasoning about\ninstruction hierarchies, and a 40.0% increase in refusal rates for unsafe\nprompts using open-source DeepSeek R1 models. Overall, our work opens a\npromising new research avenue for controlling reasoning LLMs."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Chong Xiang"
                    },
                    {
                        "name": "Jiachen T. Wang"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Prateek Mittal"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Mittal"
                },
                "author": "Prateek Mittal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24370v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24370v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01697v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01697v3",
                "updated": "2025-05-21T17:50:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    50,
                    43,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-03T00:12:40Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    0,
                    12,
                    40,
                    0,
                    34,
                    0
                ],
                "title": "BARE: Leveraging Base Language Models for Few-Shot Synthetic Data\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BARE: Leveraging Base Language Models for Few-Shot Synthetic Data\n  Generation"
                },
                "summary": "As the demand for high-quality data in model training grows, researchers and\ndevelopers are increasingly generating synthetic data to tune and train LLMs.\nHowever, current data generation methods rely on seed sets containing tens of\nthousands of examples to prompt instruction-tuned models. This reliance can be\nespecially problematic when the curation of high-quality examples is expensive\nor difficult. In this paper we explore the novel few-shot synthetic data\ngeneration setting -- generating a high-quality dataset from a few examples. We\nshow that when working with only a few seed examples, instruction-tuned models\nused in current synthetic data methods produce insufficient diversity for\ndownstream tasks. In contrast, we show that base models without post-training,\nlargely untapped for synthetic data generation, offer substantially greater\noutput diversity, albeit with lower instruction following abilities. Leveraging\nthis insight, we propose Base-Refine (BARE), a novel two-stage method that\ncombines the diversity of base models with the quality assurance of\ninstruction-tuned models. BARE excels in few-shot synthetic data generation:\nusing only 3 seed examples it generates diverse, high-quality datasets that\nsignificantly improve downstream task performance. We show that fine-tuning\nLlama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable\nto state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore,\ndata generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2\n1B on GSM8K over data generated by only instruction-models, and an 18.4%\nimprovement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method\nfor RAG data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for high-quality data in model training grows, researchers and\ndevelopers are increasingly generating synthetic data to tune and train LLMs.\nHowever, current data generation methods rely on seed sets containing tens of\nthousands of examples to prompt instruction-tuned models. This reliance can be\nespecially problematic when the curation of high-quality examples is expensive\nor difficult. In this paper we explore the novel few-shot synthetic data\ngeneration setting -- generating a high-quality dataset from a few examples. We\nshow that when working with only a few seed examples, instruction-tuned models\nused in current synthetic data methods produce insufficient diversity for\ndownstream tasks. In contrast, we show that base models without post-training,\nlargely untapped for synthetic data generation, offer substantially greater\noutput diversity, albeit with lower instruction following abilities. Leveraging\nthis insight, we propose Base-Refine (BARE), a novel two-stage method that\ncombines the diversity of base models with the quality assurance of\ninstruction-tuned models. BARE excels in few-shot synthetic data generation:\nusing only 3 seed examples it generates diverse, high-quality datasets that\nsignificantly improve downstream task performance. We show that fine-tuning\nLlama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable\nto state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore,\ndata generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2\n1B on GSM8K over data generated by only instruction-models, and an 18.4%\nimprovement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method\nfor RAG data generation."
                },
                "authors": [
                    {
                        "name": "Alan Zhu"
                    },
                    {
                        "name": "Parth Asawa"
                    },
                    {
                        "name": "Jared Quincy Davis"
                    },
                    {
                        "name": "Lingjiao Chen"
                    },
                    {
                        "name": "Boris Hanin"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01697v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01697v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15795v1",
                "updated": "2025-05-21T17:48:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    48,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:48:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    48,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "Reverse Engineering Human Preferences with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Engineering Human Preferences with Reinforcement Learning"
                },
                "summary": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks."
                },
                "authors": [
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Tan Yi-Chern"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15793v2",
                "updated": "2025-05-22T04:48:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    48,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T17:47:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    47,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving"
                },
                "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Bo Leng"
                    },
                    {
                        "name": "Zhuoren Li"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Guizhe Jin"
                    },
                    {
                        "name": "Ran Yu"
                    },
                    {
                        "name": "Huanxi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Huanxi Wen"
                },
                "author": "Huanxi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15792v1",
                "updated": "2025-05-21T17:46:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    46,
                    38,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:46:38Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    46,
                    38,
                    2,
                    141,
                    0
                ],
                "title": "Long-Form Information Alignment Evaluation Beyond Atomic Facts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Form Information Alignment Evaluation Beyond Atomic Facts"
                },
                "summary": "Information alignment evaluators are vital for various NLG evaluation tasks\nand trustworthy LLM deployment, reducing hallucinations and enhancing user\ntrust. Current fine-grained methods, like FactScore, verify facts individually\nbut neglect inter-fact dependencies, enabling subtle vulnerabilities. In this\nwork, we introduce MontageLie, a challenging benchmark that constructs\ndeceptive narratives by \"montaging\" truthful statements without introducing\nexplicit hallucinations. We demonstrate that both coarse-grained LLM-based\nevaluators and current fine-grained frameworks are susceptible to this attack,\nwith AUC-ROC scores falling below 65%. To enable more robust fine-grained\nevaluation, we propose DoveScore, a novel framework that jointly verifies\nfactual accuracy and event-order consistency. By modeling inter-fact\nrelationships, DoveScore outperforms existing fine-grained methods by over 8%,\nproviding a more robust solution for long-form text alignment evaluation. Our\ncode and datasets are available at https://github.com/dannalily/DoveScore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information alignment evaluators are vital for various NLG evaluation tasks\nand trustworthy LLM deployment, reducing hallucinations and enhancing user\ntrust. Current fine-grained methods, like FactScore, verify facts individually\nbut neglect inter-fact dependencies, enabling subtle vulnerabilities. In this\nwork, we introduce MontageLie, a challenging benchmark that constructs\ndeceptive narratives by \"montaging\" truthful statements without introducing\nexplicit hallucinations. We demonstrate that both coarse-grained LLM-based\nevaluators and current fine-grained frameworks are susceptible to this attack,\nwith AUC-ROC scores falling below 65%. To enable more robust fine-grained\nevaluation, we propose DoveScore, a novel framework that jointly verifies\nfactual accuracy and event-order consistency. By modeling inter-fact\nrelationships, DoveScore outperforms existing fine-grained methods by over 8%,\nproviding a more robust solution for long-form text alignment evaluation. Our\ncode and datasets are available at https://github.com/dannalily/DoveScore."
                },
                "authors": [
                    {
                        "name": "Danna Zheng"
                    },
                    {
                        "name": "Mirella Lapata"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10918v2",
                "updated": "2025-05-21T17:39:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    39,
                    17,
                    2,
                    141,
                    0
                ],
                "published": "2025-03-13T22:13:20Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    22,
                    13,
                    20,
                    3,
                    72,
                    0
                ],
                "title": "Resource Heterogeneity-Aware and Utilization-Enhanced Scheduling for\n  Deep Learning Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Heterogeneity-Aware and Utilization-Enhanced Scheduling for\n  Deep Learning Clusters"
                },
                "summary": "Scheduling deep learning (DL) models to train on powerful clusters with\naccelerators like GPUs and TPUs, presently falls short, either lacking\nfine-grained heterogeneity awareness or leaving resources substantially\nunder-utilized. To fill this gap, we propose a novel design of a task-level\nheterogeneity-aware scheduler, Hadar, based on an optimization framework that\ncan boost resource utilization. Hadar leverages the performance traits of DL\njobs on a heterogeneous DL cluster, characterizes the task-level performance\nheterogeneity in the optimization problem, and makes scheduling decisions\nacross both spatial and temporal dimensions. It involves the primal-dual\nframework employing a dual subroutine, to solve the optimization problem and\nguide the scheduling design. Our trace-driven simulation with representative DL\nmodel training workloads demonstrates that Hadar accelerates the total time\nduration by 1.20x when compared with its state-of-the-art heterogeneity-aware\ncounterpart, Gavel. Further, our Hadar scheduler is enhanced to HadarE by\nforking each job into multiple copies to let a job train concurrently on\nheterogeneous GPUs resided on separate available nodes (i.e., machines or\nservers) for resource utilization enhancement. HadarE is evaluated extensively\non physical DL clusters for comparison with Hadar and Gavel. With substantial\nenhancement in cluster resource utilization (by 1.45x), HadarE exhibits\nconsiderable speed-ups in DL model training, reducing the total time duration\nby 50% (or 80%) on an Amazon's AWS (or our lab) cluster, while producing\ntrained DL models with consistently better inference quality than those trained\nby Hadar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scheduling deep learning (DL) models to train on powerful clusters with\naccelerators like GPUs and TPUs, presently falls short, either lacking\nfine-grained heterogeneity awareness or leaving resources substantially\nunder-utilized. To fill this gap, we propose a novel design of a task-level\nheterogeneity-aware scheduler, Hadar, based on an optimization framework that\ncan boost resource utilization. Hadar leverages the performance traits of DL\njobs on a heterogeneous DL cluster, characterizes the task-level performance\nheterogeneity in the optimization problem, and makes scheduling decisions\nacross both spatial and temporal dimensions. It involves the primal-dual\nframework employing a dual subroutine, to solve the optimization problem and\nguide the scheduling design. Our trace-driven simulation with representative DL\nmodel training workloads demonstrates that Hadar accelerates the total time\nduration by 1.20x when compared with its state-of-the-art heterogeneity-aware\ncounterpart, Gavel. Further, our Hadar scheduler is enhanced to HadarE by\nforking each job into multiple copies to let a job train concurrently on\nheterogeneous GPUs resided on separate available nodes (i.e., machines or\nservers) for resource utilization enhancement. HadarE is evaluated extensively\non physical DL clusters for comparison with Hadar and Gavel. With substantial\nenhancement in cluster resource utilization (by 1.45x), HadarE exhibits\nconsiderable speed-ups in DL model training, reducing the total time duration\nby 50% (or 80%) on an Amazon's AWS (or our lab) cluster, while producing\ntrained DL models with consistently better inference quality than those trained\nby Hadar."
                },
                "authors": [
                    {
                        "name": "Abeda Sultana"
                    },
                    {
                        "name": "Nabin Pakka"
                    },
                    {
                        "name": "Fei Xu"
                    },
                    {
                        "name": "Xu Yuan"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Nian-Feng Tzeng"
                    }
                ],
                "author_detail": {
                    "name": "Nian-Feng Tzeng"
                },
                "author": "Nian-Feng Tzeng",
                "arxiv_comment": "14 pages, 12 figures, IEEE Transactions on Computers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; F.1.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12545v2",
                "updated": "2025-05-21T17:38:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    38,
                    2,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-18T21:02:30Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    21,
                    2,
                    30,
                    6,
                    138,
                    0
                ],
                "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and\n  Safety Interventions Using Customized Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and\n  Safety Interventions Using Customized Large Language Models"
                },
                "summary": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes."
                },
                "authors": [
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Hongru Du"
                    },
                    {
                        "name": "Hao Frank Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Frank Yang"
                },
                "author": "Hao Frank Yang",
                "arxiv_comment": "Last revised 13 Feb 2025. Under review in Nature portfolio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21602v2",
                "updated": "2025-05-21T17:36:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    36,
                    11,
                    2,
                    141,
                    0
                ],
                "published": "2024-10-28T23:12:09Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    23,
                    12,
                    9,
                    0,
                    302,
                    0
                ],
                "title": "A Generative Diffusion Model to Solve Inverse Problems for Robust\n  in-NICU Neonatal MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Diffusion Model to Solve Inverse Problems for Robust\n  in-NICU Neonatal MRI"
                },
                "summary": "We present the first acquisition-agnostic diffusion generative model for\nMagnetic Resonance Imaging (MRI) in the neonatal intensive care unit (NICU) to\nsolve a range of inverse problems for shortening scan time and improving motion\nrobustness. In-NICU MRI scanners leverage permanent magnets at lower\nfield-strengths (i.e., below 1.5 Tesla) for non-invasive assessment of\npotential brain abnormalities during the critical phase of early live\ndevelopment, but suffer from long scan times and motion artifacts. In this\nsetting, training data sizes are small and intrinsically suffer from low\nsignal-to-noise ratio (SNR). This work trains a diffusion probabilistic\ngenerative model using such a real-world training dataset of clinical neonatal\nMRI by applying several novel signal processing and machine learning methods to\nhandle the low SNR and low quantity of data. The model is then used as a\nstatistical image prior to solve various inverse problems at inference time\nwithout requiring any retraining. Experiments demonstrate the generative\nmodel's utility for three real-world applications of neonatal MRI: accelerated\nreconstruction, motion correction, and super-resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first acquisition-agnostic diffusion generative model for\nMagnetic Resonance Imaging (MRI) in the neonatal intensive care unit (NICU) to\nsolve a range of inverse problems for shortening scan time and improving motion\nrobustness. In-NICU MRI scanners leverage permanent magnets at lower\nfield-strengths (i.e., below 1.5 Tesla) for non-invasive assessment of\npotential brain abnormalities during the critical phase of early live\ndevelopment, but suffer from long scan times and motion artifacts. In this\nsetting, training data sizes are small and intrinsically suffer from low\nsignal-to-noise ratio (SNR). This work trains a diffusion probabilistic\ngenerative model using such a real-world training dataset of clinical neonatal\nMRI by applying several novel signal processing and machine learning methods to\nhandle the low SNR and low quantity of data. The model is then used as a\nstatistical image prior to solve various inverse problems at inference time\nwithout requiring any retraining. Experiments demonstrate the generative\nmodel's utility for three real-world applications of neonatal MRI: accelerated\nreconstruction, motion correction, and super-resolution."
                },
                "authors": [
                    {
                        "name": "Yamin Arefeen"
                    },
                    {
                        "name": "Brett Levac"
                    },
                    {
                        "name": "Jonathan I. Tamir"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan I. Tamir"
                },
                "author": "Jonathan I. Tamir",
                "arxiv_comment": "6 pages, 4 figures, submitted to ICIP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15784v1",
                "updated": "2025-05-21T17:35:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    35,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:35:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    35,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "Large Language Models as Computable Approximations to Solomonoff\n  Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Computable Approximations to Solomonoff\n  Induction"
                },
                "summary": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development."
                },
                "authors": [
                    {
                        "name": "Jun Wan"
                    },
                    {
                        "name": "Lingrui Mei"
                    }
                ],
                "author_detail": {
                    "name": "Lingrui Mei"
                },
                "author": "Lingrui Mei",
                "arxiv_comment": "Both authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15781v1",
                "updated": "2025-05-21T17:32:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:32:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "dKV-Cache: The Cache for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dKV-Cache: The Cache for Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs."
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at https://github.com/horseee/dKV-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15778v1",
                "updated": "2025-05-21T17:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    29,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:29:15Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    29,
                    15,
                    2,
                    141,
                    0
                ],
                "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space"
                },
                "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking."
                },
                "authors": [
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xuehai He"
                    },
                    {
                        "name": "Weixiang Yan"
                    },
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Xin Eric Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Eric Wang"
                },
                "author": "Xin Eric Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15777v1",
                "updated": "2025-05-21T17:28:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    28,
                    14,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:28:14Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    28,
                    14,
                    2,
                    141,
                    0
                ],
                "title": "Projection-Based Correction for Enhancing Deep Inverse Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Projection-Based Correction for Enhancing Deep Inverse Networks"
                },
                "summary": "Deep learning-based models have demonstrated remarkable success in solving\nillposed inverse problems; however, many fail to strictly adhere to the\nphysical constraints imposed by the measurement process. In this work, we\nintroduce a projection-based correction method to enhance the inference of deep\ninverse networks by ensuring consistency with the forward model. Specifically,\ngiven an initial estimate from a learned reconstruction network, we apply a\nprojection step that constrains the solution to lie within the valid solution\nspace of the inverse problem. We theoretically demonstrate that if the recovery\nmodel is a well-trained deep inverse network, the solution can be decomposed\ninto range-space and null-space components, where the projection-based\ncorrection reduces to an identity transformation. Extensive simulations and\nexperiments validate the proposed method, demonstrating improved reconstruction\naccuracy across diverse inverse problems and deep network architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based models have demonstrated remarkable success in solving\nillposed inverse problems; however, many fail to strictly adhere to the\nphysical constraints imposed by the measurement process. In this work, we\nintroduce a projection-based correction method to enhance the inference of deep\ninverse networks by ensuring consistency with the forward model. Specifically,\ngiven an initial estimate from a learned reconstruction network, we apply a\nprojection step that constrains the solution to lie within the valid solution\nspace of the inverse problem. We theoretically demonstrate that if the recovery\nmodel is a well-trained deep inverse network, the solution can be decomposed\ninto range-space and null-space components, where the projection-based\ncorrection reduces to an identity transformation. Extensive simulations and\nexperiments validate the proposed method, demonstrating improved reconstruction\naccuracy across diverse inverse problems and deep network architectures."
                },
                "authors": [
                    {
                        "name": "Jorge Bacca"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Bacca"
                },
                "author": "Jorge Bacca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11651v2",
                "updated": "2025-05-21T17:26:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    26,
                    12,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-16T19:22:19Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    19,
                    22,
                    19,
                    4,
                    136,
                    0
                ],
                "title": "MIRACL-VISION: A Large, multilingual, visual document retrieval\n  benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRACL-VISION: A Large, multilingual, visual document retrieval\n  benchmark"
                },
                "summary": "Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval."
                },
                "authors": [
                    {
                        "name": "Radek Osmulski"
                    },
                    {
                        "name": "Gabriel de Souza P. Moreira"
                    },
                    {
                        "name": "Ronay Ak"
                    },
                    {
                        "name": "Mengyao Xu"
                    },
                    {
                        "name": "Benedikt Schifferer"
                    },
                    {
                        "name": "Even Oldridge"
                    }
                ],
                "author_detail": {
                    "name": "Even Oldridge"
                },
                "author": "Even Oldridge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15774v1",
                "updated": "2025-05-21T17:26:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    26,
                    11,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:26:11Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    26,
                    11,
                    2,
                    141,
                    0
                ],
                "title": "Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and\n  Global Information Retention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and\n  Global Information Retention"
                },
                "summary": "Large Language Models (LLMs) encounter significant challenges in\nlong-sequence inference due to computational inefficiency and redundant\nprocessing, driving interest in context compression techniques. Existing\nmethods often rely on token importance to perform hard local compression or\nencode context into latent representations for soft global compression.\nHowever, the uneven distribution of textual content relevance and the diversity\nof demands for user instructions mean these approaches frequently lead to the\nloss of potentially valuable information. To address this, we propose\n$\\textbf{Hy}$brid $\\textbf{Co}$ntext $\\textbf{Co}$mpression (HyCo$_2$) for\nLLMs, which integrates both global and local perspectives to guide context\ncompression while retaining both the essential semantics and critical details\nfor task completion. Specifically, we employ a hybrid adapter to refine global\nsemantics with the global view, based on the observation that different\nadapters excel at different tasks. Then we incorporate a classification layer\nthat assigns a retention probability to each context token based on the local\nview, determining whether it should be retained or discarded. To foster a\nbalanced integration of global and local compression, we introduce auxiliary\nparaphrasing and completion pretraining before instruction tuning. This\npromotes a synergistic integration that emphasizes instruction-relevant\ninformation while preserving essential local details, ultimately balancing\nlocal and global information retention in context compression. Experiments show\nthat our HyCo$_2$ method significantly enhances long-text reasoning while\nreducing token usage. It improves the performance of various LLM series by an\naverage of 13.1\\% across seven knowledge-intensive QA benchmarks. Moreover,\nHyCo$_2$ matches the performance of uncompressed methods while reducing token\nconsumption by 88.8\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encounter significant challenges in\nlong-sequence inference due to computational inefficiency and redundant\nprocessing, driving interest in context compression techniques. Existing\nmethods often rely on token importance to perform hard local compression or\nencode context into latent representations for soft global compression.\nHowever, the uneven distribution of textual content relevance and the diversity\nof demands for user instructions mean these approaches frequently lead to the\nloss of potentially valuable information. To address this, we propose\n$\\textbf{Hy}$brid $\\textbf{Co}$ntext $\\textbf{Co}$mpression (HyCo$_2$) for\nLLMs, which integrates both global and local perspectives to guide context\ncompression while retaining both the essential semantics and critical details\nfor task completion. Specifically, we employ a hybrid adapter to refine global\nsemantics with the global view, based on the observation that different\nadapters excel at different tasks. Then we incorporate a classification layer\nthat assigns a retention probability to each context token based on the local\nview, determining whether it should be retained or discarded. To foster a\nbalanced integration of global and local compression, we introduce auxiliary\nparaphrasing and completion pretraining before instruction tuning. This\npromotes a synergistic integration that emphasizes instruction-relevant\ninformation while preserving essential local details, ultimately balancing\nlocal and global information retention in context compression. Experiments show\nthat our HyCo$_2$ method significantly enhances long-text reasoning while\nreducing token usage. It improves the performance of various LLM series by an\naverage of 13.1\\% across seven knowledge-intensive QA benchmarks. Moreover,\nHyCo$_2$ matches the performance of uncompressed methods while reducing token\nconsumption by 88.8\\%."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15766v1",
                "updated": "2025-05-21T17:13:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Quasar radiation transforms the gas in a merging companion galaxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasar radiation transforms the gas in a merging companion galaxy"
                },
                "summary": "Quasars, powered by gas accretion onto supermassive black holes, rank among\nthe most energetic objects of the Universe. While they are thought to be\nignited by galaxy mergers and affect the surrounding gas, observational\nconstraints on both processes remain scarce. Here we unveil a major merging\nsystem at redshift $z \\approx 2.7$, and demonstrate that radiation from the\nquasar in one galaxy directly alters the gas properties in the other galaxy.\nOur findings reveal that the galaxies, with centroids separated by only a few\nkiloparsecs and approaching each other at speed $\\approx550\\,$km$\\,$s$^{-1}$,\nare massive, form stars, and contain a substantial molecular mass. Yet, dusty\nmolecular gas seen in absorption against the quasar nucleus is highly excited\nand confined within cloudlets with densities $\\sim 10^5$ - $10^6$ cm$^{-3}$ and\nsizes $<$0.02 pc, several orders of magnitude more compact than those observed\nin intervening (non-quasar) environments. This is also approximately 10$^5$\ntimes smaller than currently resolvable through molecular-line emission at high\nredshifts. We infer that, wherever exposed to the quasar radiation, molecular\ngas is disrupted, leaving behind surviving dense clouds too small to give birth\nto new stars. Our results not only underscore the role of major galaxy mergers\nin triggering quasar activity, but also reveal localized negative feedback as a\nprofound alteration of internal gas structure which likely hampers star\nformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasars, powered by gas accretion onto supermassive black holes, rank among\nthe most energetic objects of the Universe. While they are thought to be\nignited by galaxy mergers and affect the surrounding gas, observational\nconstraints on both processes remain scarce. Here we unveil a major merging\nsystem at redshift $z \\approx 2.7$, and demonstrate that radiation from the\nquasar in one galaxy directly alters the gas properties in the other galaxy.\nOur findings reveal that the galaxies, with centroids separated by only a few\nkiloparsecs and approaching each other at speed $\\approx550\\,$km$\\,$s$^{-1}$,\nare massive, form stars, and contain a substantial molecular mass. Yet, dusty\nmolecular gas seen in absorption against the quasar nucleus is highly excited\nand confined within cloudlets with densities $\\sim 10^5$ - $10^6$ cm$^{-3}$ and\nsizes $<$0.02 pc, several orders of magnitude more compact than those observed\nin intervening (non-quasar) environments. This is also approximately 10$^5$\ntimes smaller than currently resolvable through molecular-line emission at high\nredshifts. We infer that, wherever exposed to the quasar radiation, molecular\ngas is disrupted, leaving behind surviving dense clouds too small to give birth\nto new stars. Our results not only underscore the role of major galaxy mergers\nin triggering quasar activity, but also reveal localized negative feedback as a\nprofound alteration of internal gas structure which likely hampers star\nformation."
                },
                "authors": [
                    {
                        "name": "Sergei Balashev"
                    },
                    {
                        "name": "Pasquier Noterdaeme"
                    },
                    {
                        "name": "Neeraj Gupta"
                    },
                    {
                        "name": "Jens-Kristian Krogager"
                    },
                    {
                        "name": "Francoise Combes"
                    },
                    {
                        "name": "Sebastian Lopez"
                    },
                    {
                        "name": "Patrick Petitjean"
                    },
                    {
                        "name": "Alain Omont"
                    },
                    {
                        "name": "Raghunathan Srianand"
                    },
                    {
                        "name": "Rodrigo Cuellar"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Cuellar"
                },
                "author": "Rodrigo Cuellar",
                "arxiv_doi": "10.1038/s41586-025-08966-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41586-025-08966-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.15766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "28 pages, 12 figures, published in Nature on May 21th, 2025,\n  https://www.nature.com/articles/s41586-025-08966-4",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14449v2",
                "updated": "2025-05-21T17:04:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    4,
                    44,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T14:50:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    50,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion\n  Recognition: A Pseudo-Labeling and Unsupervised Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion\n  Recognition: A Pseudo-Labeling and Unsupervised Learning Approach"
                },
                "summary": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 26% improvement in fairness metrics with a\ndrop of less than 4% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential in scenarios where explicit demographic information is\nunavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 26% improvement in fairness metrics with a\ndrop of less than 4% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential in scenarios where explicit demographic information is\nunavailable."
                },
                "authors": [
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Huang-Cheng Chou"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15759v1",
                "updated": "2025-05-21T17:03:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    3,
                    34,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:03:34Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    3,
                    34,
                    2,
                    141,
                    0
                ],
                "title": "Estimating Associations Between Cumulative Exposure and Health via\n  Generalized Distributed Lag Non-Linear Models using Penalized Splines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Associations Between Cumulative Exposure and Health via\n  Generalized Distributed Lag Non-Linear Models using Penalized Splines"
                },
                "summary": "Quantifying associations between short-term exposure to ambient air pollution\nand health outcomes is an important public health priority. Many studies have\ninvestigated the association considering delayed effects within the past few\ndays. Adaptive cumulative exposure distributed lag non-linear models\n(ACE-DLNMs) quantify associations between health outcomes and cumulative\nexposure that is specified in a data-adaptive way. While the ACE-DLNM framework\nis highly interpretable, it is limited to continuous outcomes and does not\nscale well to large datasets. Motivated by a large analysis of daily pollution\nand respiratory hospitalization counts in Canada between 2001 and 2018, we\npropose a generalized ACE-DLNM incorporating penalized splines, improving upon\nexisting ACE-DLNM methods to accommodate general response types. We then\ndevelop a computationally efficient estimation strategy based on profile\nlikelihood and Laplace approximate marginal likelihood with Newton-type\nmethods. We demonstrate the performance and practical advantages of the\nproposed method through simulations. In application to the motivating analysis,\nthe proposed method yields more stable inferences compared to generalized\nadditive models with fixed exposures, while retaining interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying associations between short-term exposure to ambient air pollution\nand health outcomes is an important public health priority. Many studies have\ninvestigated the association considering delayed effects within the past few\ndays. Adaptive cumulative exposure distributed lag non-linear models\n(ACE-DLNMs) quantify associations between health outcomes and cumulative\nexposure that is specified in a data-adaptive way. While the ACE-DLNM framework\nis highly interpretable, it is limited to continuous outcomes and does not\nscale well to large datasets. Motivated by a large analysis of daily pollution\nand respiratory hospitalization counts in Canada between 2001 and 2018, we\npropose a generalized ACE-DLNM incorporating penalized splines, improving upon\nexisting ACE-DLNM methods to accommodate general response types. We then\ndevelop a computationally efficient estimation strategy based on profile\nlikelihood and Laplace approximate marginal likelihood with Newton-type\nmethods. We demonstrate the performance and practical advantages of the\nproposed method through simulations. In application to the motivating analysis,\nthe proposed method yields more stable inferences compared to generalized\nadditive models with fixed exposures, while retaining interpretability."
                },
                "authors": [
                    {
                        "name": "Tianyi Pan"
                    },
                    {
                        "name": "Hwashin Hyun Shin"
                    },
                    {
                        "name": "Glen McGee"
                    },
                    {
                        "name": "Alex Stringer"
                    }
                ],
                "author_detail": {
                    "name": "Alex Stringer"
                },
                "author": "Alex Stringer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14684v2",
                "updated": "2025-05-21T17:02:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    2,
                    34,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T17:59:31Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    31,
                    1,
                    140,
                    0
                ],
                "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits."
                },
                "authors": [
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Shengpei Jiang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Project: https://zju-real.github.io/CoT-Bridge/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13404v2",
                "updated": "2025-05-21T17:00:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    0,
                    54,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-19T17:40:58Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    40,
                    58,
                    0,
                    139,
                    0
                ],
                "title": "Granary: Speech Recognition and Translation Dataset in 25 European\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granary: Speech Recognition and Translation Dataset in 25 European\n  Languages"
                },
                "summary": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary"
                },
                "authors": [
                    {
                        "name": "Nithin Rao Koluguri"
                    },
                    {
                        "name": "Monica Sekoyan"
                    },
                    {
                        "name": "George Zelenfroynd"
                    },
                    {
                        "name": "Sasha Meister"
                    },
                    {
                        "name": "Shuoyang Ding"
                    },
                    {
                        "name": "Sofia Kostandian"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Nikolay Karpov"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Vitaly Lavrukhin"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Alessio Brutti"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted at Interspeech 2025 v2: Added links",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14218v2",
                "updated": "2025-05-21T16:59:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    59,
                    26,
                    2,
                    141,
                    0
                ],
                "published": "2025-04-19T07:53:37Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    7,
                    53,
                    37,
                    5,
                    109,
                    0
                ],
                "title": "Understanding the Repeat Curse in Large Language Models from a Feature\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Repeat Curse in Large Language Models from a Feature\n  Perspective"
                },
                "summary": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse."
                },
                "authors": [
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Jianhua Xu"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Mengdi Li"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "Accepted by ACL 2025, Findings, Long Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15753v1",
                "updated": "2025-05-21T16:58:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    58,
                    14,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:58:14Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    58,
                    14,
                    2,
                    141,
                    0
                ],
                "title": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety\n  Context Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety\n  Context Retrieval"
                },
                "summary": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication."
                },
                "authors": [
                    {
                        "name": "Taiye Chen"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11654v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11654v3",
                "updated": "2025-05-22T04:43:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    43,
                    6,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-16T19:38:06Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    19,
                    38,
                    6,
                    4,
                    136,
                    0
                ],
                "title": "UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal\n  Large Language Models"
                },
                "summary": "Understanding and predicting urban dynamics is crucial for managing\ntransportation systems, optimizing urban planning, and enhancing public\nservices. While neural network-based approaches have achieved success, they\noften rely on task-specific architectures and large volumes of data, limiting\ntheir ability to generalize across diverse urban scenarios. Meanwhile, Large\nLanguage Models (LLMs) offer strong reasoning and generalization capabilities,\nyet their application to spatial-temporal urban dynamics remains underexplored.\nExisting LLM-based methods struggle to effectively integrate multifaceted\nspatial-temporal data and fail to address distributional shifts between\ntraining and testing data, limiting their predictive reliability in real-world\napplications. To bridge this gap, we propose UrbanMind, a novel\nspatial-temporal LLM framework for multifaceted urban dynamics prediction that\nensures both accurate forecasting and robust generalization. At its core,\nUrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with\nspecialized masking strategies that capture intricate spatial-temporal\ndependencies and intercorrelations among multifaceted urban dynamics.\nAdditionally, we design a semantic-aware prompting and fine-tuning strategy\nthat encodes spatial-temporal contextual details into prompts, enhancing LLMs'\nability to reason over spatial-temporal patterns. To further improve\ngeneralization, we introduce a test time adaptation mechanism with a test data\nreconstructor, enabling UrbanMind to dynamically adjust to unseen test data by\nreconstructing LLM-generated embeddings. Extensive experiments on real-world\nurban datasets across multiple cities demonstrate that UrbanMind consistently\noutperforms state-of-the-art baselines, achieving high accuracy and robust\ngeneralization, even in zero-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting urban dynamics is crucial for managing\ntransportation systems, optimizing urban planning, and enhancing public\nservices. While neural network-based approaches have achieved success, they\noften rely on task-specific architectures and large volumes of data, limiting\ntheir ability to generalize across diverse urban scenarios. Meanwhile, Large\nLanguage Models (LLMs) offer strong reasoning and generalization capabilities,\nyet their application to spatial-temporal urban dynamics remains underexplored.\nExisting LLM-based methods struggle to effectively integrate multifaceted\nspatial-temporal data and fail to address distributional shifts between\ntraining and testing data, limiting their predictive reliability in real-world\napplications. To bridge this gap, we propose UrbanMind, a novel\nspatial-temporal LLM framework for multifaceted urban dynamics prediction that\nensures both accurate forecasting and robust generalization. At its core,\nUrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with\nspecialized masking strategies that capture intricate spatial-temporal\ndependencies and intercorrelations among multifaceted urban dynamics.\nAdditionally, we design a semantic-aware prompting and fine-tuning strategy\nthat encodes spatial-temporal contextual details into prompts, enhancing LLMs'\nability to reason over spatial-temporal patterns. To further improve\ngeneralization, we introduce a test time adaptation mechanism with a test data\nreconstructor, enabling UrbanMind to dynamically adjust to unseen test data by\nreconstructing LLM-generated embeddings. Extensive experiments on real-world\nurban datasets across multiple cities demonstrate that UrbanMind consistently\noutperforms state-of-the-art baselines, achieving high accuracy and robust\ngeneralization, even in zero-shot settings."
                },
                "authors": [
                    {
                        "name": "Yuhang Liu"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Ling Tian"
                    },
                    {
                        "name": "Yanhua Li"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "arxiv_comment": "KDD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11654v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24245v2",
                "updated": "2025-05-21T16:55:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    55,
                    47,
                    2,
                    141,
                    0
                ],
                "published": "2025-03-31T15:58:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    58,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches."
                },
                "authors": [
                    {
                        "name": "Dun Yuan"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yan Xin"
                    },
                    {
                        "name": "Jianzhong"
                    },
                    {
                        "name": "Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang"
                },
                "arxiv_affiliation": "Charlie",
                "author": "Zhang",
                "arxiv_comment": "This work has been accepted to ICC 2025 IEEE International Conference\n  on Communications. copyright 2025 IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14573v2",
                "updated": "2025-05-21T16:52:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    52,
                    23,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T16:31:13Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    31,
                    13,
                    1,
                    140,
                    0
                ],
                "title": "Measuring spin precession from massive black hole binaries with\n  gravitational waves: insights from time-domain signal morphology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring spin precession from massive black hole binaries with\n  gravitational waves: insights from time-domain signal morphology"
                },
                "summary": "Robustly measuring binary black hole spins via gravitational waves is key to\nunderstanding these systems' astrophysical origins, but remains challenging --\nespecially for high-mass systems, whose signals are short and dominated by the\nmerger. Nonetheless, events like GW190521 show that strong spin precession can\nindeed be gleaned from high-mass systems. In this work, we track how spin\nprecession imprints on simulated high-mass binary black hole signals\ncycle-by-cycle using time-domain inference. We investigate a suite of signals,\nall with the same spins and (near-unity) mass ratio but different\nsignal-to-noise ratios, total masses, and extrinsic angles, which affect the\nobserved waveform morphology. We truncate each signal at various times and\ninfer source parameters using only the data before or after each cutoff. The\nresultant posterior allows us to identify which time segments of each signal\ninform its spin precession constraints. We find that at a sufficiently high\npost-peak signal-to-noise ratio (SNR, $\\rho\\sim 20$), spin precession can be\nconstrained by the NRSur7dq4 waveform model when just the post-peak data (i.e.,\nringdown) is visible. Similarly, at a large enough pre-cutoff SNR ($\\rho\\sim\n10$), spin precession can be constrained using only pre-peak data (i.e.,\ninspiral); this occurs for signals with detector-frame total mass $\\lesssim 100\nM_{\\odot}$ at GW190521's SNR. Finally, we vary the inclination, polarization,\nand phase angles, finding that their configuration need not be fine-tuned to\nmeasure spin precession, even for very high-mass and short signals with 2-3\nobservable cycles. We do not find that the same morphological features\nconsistently drive precession constraints: in some signals, precession\ninference hinges on the relationship between a loud merger and quiet pre-merger\ncycle, as was the case for GW190521, but this is not generically true.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustly measuring binary black hole spins via gravitational waves is key to\nunderstanding these systems' astrophysical origins, but remains challenging --\nespecially for high-mass systems, whose signals are short and dominated by the\nmerger. Nonetheless, events like GW190521 show that strong spin precession can\nindeed be gleaned from high-mass systems. In this work, we track how spin\nprecession imprints on simulated high-mass binary black hole signals\ncycle-by-cycle using time-domain inference. We investigate a suite of signals,\nall with the same spins and (near-unity) mass ratio but different\nsignal-to-noise ratios, total masses, and extrinsic angles, which affect the\nobserved waveform morphology. We truncate each signal at various times and\ninfer source parameters using only the data before or after each cutoff. The\nresultant posterior allows us to identify which time segments of each signal\ninform its spin precession constraints. We find that at a sufficiently high\npost-peak signal-to-noise ratio (SNR, $\\rho\\sim 20$), spin precession can be\nconstrained by the NRSur7dq4 waveform model when just the post-peak data (i.e.,\nringdown) is visible. Similarly, at a large enough pre-cutoff SNR ($\\rho\\sim\n10$), spin precession can be constrained using only pre-peak data (i.e.,\ninspiral); this occurs for signals with detector-frame total mass $\\lesssim 100\nM_{\\odot}$ at GW190521's SNR. Finally, we vary the inclination, polarization,\nand phase angles, finding that their configuration need not be fine-tuned to\nmeasure spin precession, even for very high-mass and short signals with 2-3\nobservable cycles. We do not find that the same morphological features\nconsistently drive precession constraints: in some signals, precession\ninference hinges on the relationship between a loud merger and quiet pre-merger\ncycle, as was the case for GW190521, but this is not generically true."
                },
                "authors": [
                    {
                        "name": "Simona J. Miller"
                    },
                    {
                        "name": "Maximiliano Isi"
                    },
                    {
                        "name": "Katerina Chatziioannou"
                    },
                    {
                        "name": "Vijay Varma"
                    },
                    {
                        "name": "Sophie Hourihane"
                    }
                ],
                "author_detail": {
                    "name": "Sophie Hourihane"
                },
                "author": "Sophie Hourihane",
                "arxiv_comment": "19 pages main text, 5 pages appendix (excluding references), 13\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15747v2",
                "updated": "2025-05-22T03:58:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    58,
                    27,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T16:51:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    51,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large\n  Language Models and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large\n  Language Models and Knowledge Graphs"
                },
                "summary": "We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research."
                },
                "authors": [
                    {
                        "name": "Kanan Kiguchi"
                    },
                    {
                        "name": "Yunhao Tu"
                    },
                    {
                        "name": "Katsuhiro Ajito"
                    },
                    {
                        "name": "Fady Alnajjar"
                    },
                    {
                        "name": "Kazuyuki Murase"
                    }
                ],
                "author_detail": {
                    "name": "Kazuyuki Murase"
                },
                "author": "Kazuyuki Murase",
                "arxiv_comment": "38 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.1; H.3.1; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15741v1",
                "updated": "2025-05-21T16:48:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    48,
                    28,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:48:28Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    48,
                    28,
                    2,
                    141,
                    0
                ],
                "title": "Evolutionary Computation and Large Language Models: A Survey of Methods,\n  Synergies, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Computation and Large Language Models: A Survey of Methods,\n  Synergies, and Applications"
                },
                "summary": "Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)\nrepresents a promising avenue for advancing artificial intelligence by\ncombining powerful natural language understanding with optimization and search\ncapabilities. This manuscript explores the synergistic potential of LLMs and\nEC, reviewing their intersections, complementary strengths, and emerging\napplications. We identify key opportunities where EC can enhance LLM training,\nfine-tuning, prompt engineering, and architecture search, while LLMs can, in\nturn, aid in automating the design, analysis, and interpretation of ECs. The\nmanuscript explores the synergistic integration of EC and LLMs, highlighting\ntheir bidirectional contributions to advancing artificial intelligence. It\nfirst examines how EC techniques enhance LLMs by optimizing key components such\nas prompt engineering, hyperparameter tuning, and architecture search,\ndemonstrating how evolutionary methods automate and refine these processes.\nSecondly, the survey investigates how LLMs improve EC by automating\nmetaheuristic design, tuning evolutionary algorithms, and generating adaptive\nheuristics, thereby increasing efficiency and scalability. Emerging\nco-evolutionary frameworks are discussed, showcasing applications across\ndiverse fields while acknowledging challenges like computational costs,\ninterpretability, and algorithmic convergence. The survey concludes by\nidentifying open research questions and advocating for hybrid approaches that\ncombine the strengths of EC and LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)\nrepresents a promising avenue for advancing artificial intelligence by\ncombining powerful natural language understanding with optimization and search\ncapabilities. This manuscript explores the synergistic potential of LLMs and\nEC, reviewing their intersections, complementary strengths, and emerging\napplications. We identify key opportunities where EC can enhance LLM training,\nfine-tuning, prompt engineering, and architecture search, while LLMs can, in\nturn, aid in automating the design, analysis, and interpretation of ECs. The\nmanuscript explores the synergistic integration of EC and LLMs, highlighting\ntheir bidirectional contributions to advancing artificial intelligence. It\nfirst examines how EC techniques enhance LLMs by optimizing key components such\nas prompt engineering, hyperparameter tuning, and architecture search,\ndemonstrating how evolutionary methods automate and refine these processes.\nSecondly, the survey investigates how LLMs improve EC by automating\nmetaheuristic design, tuning evolutionary algorithms, and generating adaptive\nheuristics, thereby increasing efficiency and scalability. Emerging\nco-evolutionary frameworks are discussed, showcasing applications across\ndiverse fields while acknowledging challenges like computational costs,\ninterpretability, and algorithmic convergence. The survey concludes by\nidentifying open research questions and advocating for hybrid approaches that\ncombine the strengths of EC and LLMs."
                },
                "authors": [
                    {
                        "name": "Dikshit Chauhan"
                    },
                    {
                        "name": "Bapi Dutta"
                    },
                    {
                        "name": "Indu Bala"
                    },
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Bäck"
                    },
                    {
                        "name": "Anupam Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Anupam Yadav"
                },
                "author": "Anupam Yadav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17034v2",
                "updated": "2025-05-21T16:47:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    47,
                    23,
                    2,
                    141,
                    0
                ],
                "published": "2024-12-22T14:18:39Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    14,
                    18,
                    39,
                    6,
                    357,
                    0
                ],
                "title": "Shaping the Safety Boundaries: Understanding and Defending Against\n  Jailbreaks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping the Safety Boundaries: Understanding and Defending Against\n  Jailbreaks in Large Language Models"
                },
                "summary": "Jailbreaking in Large Language Models (LLMs) is a major security concern as\nit can deceive LLMs to generate harmful text. Yet, there is still insufficient\nunderstanding of how jailbreaking works, which makes it hard to develop\neffective defense strategies. We aim to shed more light into this issue: we\nconduct a detailed large-scale analysis of seven different jailbreak methods\nand find that these disagreements stem from insufficient observation samples.\nIn particular, we introduce \\textit{safety boundary}, and we find that\njailbreaks shift harmful activations outside that safety boundary, where LLMs\nare less sensitive to harmful information. We also find that the low and the\nmiddle layers are critical in such shifts, while deeper layers have less\nimpact. Leveraging on these insights, we propose a novel defense called\n\\textbf{Activation Boundary Defense} (ABD), which adaptively constrains the\nactivations within the safety boundary. We further use Bayesian optimization to\nselectively apply the defense method to the low and the middle layers. Our\nexperiments on several benchmarks show that ABD achieves an average DSR of over\n98\\% against various forms of jailbreak attacks, with less than 2\\% impact on\nthe model's general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking in Large Language Models (LLMs) is a major security concern as\nit can deceive LLMs to generate harmful text. Yet, there is still insufficient\nunderstanding of how jailbreaking works, which makes it hard to develop\neffective defense strategies. We aim to shed more light into this issue: we\nconduct a detailed large-scale analysis of seven different jailbreak methods\nand find that these disagreements stem from insufficient observation samples.\nIn particular, we introduce \\textit{safety boundary}, and we find that\njailbreaks shift harmful activations outside that safety boundary, where LLMs\nare less sensitive to harmful information. We also find that the low and the\nmiddle layers are critical in such shifts, while deeper layers have less\nimpact. Leveraging on these insights, we propose a novel defense called\n\\textbf{Activation Boundary Defense} (ABD), which adaptively constrains the\nactivations within the safety boundary. We further use Bayesian optimization to\nselectively apply the defense method to the low and the middle layers. Our\nexperiments on several benchmarks show that ABD achieves an average DSR of over\n98\\% against various forms of jailbreak attacks, with less than 2\\% impact on\nthe model's general capabilities."
                },
                "authors": [
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Jiahui Geng"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14604v2",
                "updated": "2025-05-21T16:45:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    45,
                    44,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T16:53:40Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    53,
                    40,
                    1,
                    140,
                    0
                ],
                "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let LLMs Break Free from Overthinking via Self-Braking Tuning"
                },
                "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models."
                },
                "authors": [
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page:\n  https://ZJU-REAL.github.io/SBT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15740v1",
                "updated": "2025-05-21T16:45:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    45,
                    43,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:45:43Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    45,
                    43,
                    2,
                    141,
                    0
                ],
                "title": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis\n  and Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis\n  and Refinement"
                },
                "summary": "Formal methods is pivotal for verifying the reliability of critical systems\nthrough rigorous mathematical proofs. However, its adoption is hindered by\nlabor-intensive manual proofs and the expertise required to use theorem\nprovers. Recent advancements in large language models (LLMs) offer new\nopportunities for automated theorem proving. Two promising approaches are\ngenerating tactics step by step and generating a whole proof directly with an\nLLM. However, existing work makes no attempt to combine the two approaches. In\nthis work, we introduce HybridProver, a dual-model proof synthesis framework\nthat combines tactic-based generation and whole-proof synthesis to harness the\nbenefits of both approaches. HybridProver generates whole proof candidates for\nevaluation directly, then extracts proof sketches from those candidates. It\nthen uses a tactic-based generation model that integrates automated tools to\ncomplete the sketches via stepwise refinement. We implement HybridProver for\nthe Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle\ndatasets. Evaluation on the miniF2F dataset illustrates HybridProver's\neffectiveness. We achieve a 59.4% success rate on miniF2F, where the previous\nSOTA is 56.1%. Our ablation studies show that this SOTA result is attributable\nto combining whole-proof and tactic-based generation. Additionally, we show how\nthe dataset quality, training parameters, and sampling diversity affect the\nfinal result during automated theorem proving with LLMs. All of our code,\ndatasets, and LLMs are open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal methods is pivotal for verifying the reliability of critical systems\nthrough rigorous mathematical proofs. However, its adoption is hindered by\nlabor-intensive manual proofs and the expertise required to use theorem\nprovers. Recent advancements in large language models (LLMs) offer new\nopportunities for automated theorem proving. Two promising approaches are\ngenerating tactics step by step and generating a whole proof directly with an\nLLM. However, existing work makes no attempt to combine the two approaches. In\nthis work, we introduce HybridProver, a dual-model proof synthesis framework\nthat combines tactic-based generation and whole-proof synthesis to harness the\nbenefits of both approaches. HybridProver generates whole proof candidates for\nevaluation directly, then extracts proof sketches from those candidates. It\nthen uses a tactic-based generation model that integrates automated tools to\ncomplete the sketches via stepwise refinement. We implement HybridProver for\nthe Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle\ndatasets. Evaluation on the miniF2F dataset illustrates HybridProver's\neffectiveness. We achieve a 59.4% success rate on miniF2F, where the previous\nSOTA is 56.1%. Our ablation studies show that this SOTA result is attributable\nto combining whole-proof and tactic-based generation. Additionally, we show how\nthe dataset quality, training parameters, and sampling diversity affect the\nfinal result during automated theorem proving with LLMs. All of our code,\ndatasets, and LLMs are open source."
                },
                "authors": [
                    {
                        "name": "Jilin Hu"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Yongwang Zhao"
                    },
                    {
                        "name": "Talia Ringer"
                    }
                ],
                "author_detail": {
                    "name": "Talia Ringer"
                },
                "author": "Talia Ringer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15738v1",
                "updated": "2025-05-21T16:43:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    43,
                    17,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:43:17Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    43,
                    17,
                    2,
                    141,
                    0
                ],
                "title": "Alignment Under Pressure: The Case for Informed Adversaries When\n  Evaluating LLM Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment Under Pressure: The Case for Informed Adversaries When\n  Evaluating LLM Defenses"
                },
                "summary": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Yang"
                    },
                    {
                        "name": "Bozhidar Stevanoski"
                    },
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06935v2",
                "updated": "2025-05-21T16:40:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    40,
                    30,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-11T10:50:21Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    10,
                    50,
                    21,
                    6,
                    131,
                    0
                ],
                "title": "Accelerated inference for stochastic compartmental models with\n  over-dispersed partial observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated inference for stochastic compartmental models with\n  over-dispersed partial observations"
                },
                "summary": "An assumed density approximate likelihood is derived for a class of partially\nobserved stochastic compartmental models which permit observational\nover-dispersion. This is achieved by treating time-varying reporting\nprobabilities as latent variables and integrating them out using Laplace\napproximations within Poisson Approximate Likelihoods (LawPAL), resulting in a\nfast deterministic approximation to the marginal likelihood and filtering\ndistributions. We derive an asymptotically exact filtering result in the large\npopulation regime, demonstrating the approximation's ability to recover latent\ndisease states and reporting probabilities. Through simulations we: 1)\ndemonstrate favorable behavior of the maximum approximate likelihood estimator\nin the large population and time horizon regime in terms of ground truth\nrecovery; 2) demonstrate order of magnitude computational speed gains over a\nsequential Monte Carlo likelihood based approach, and explore the statistical\ncompromises our approximation implicitly makes. We conclude by embedding our\nmethodology within the probabilistic programming language Stan for automated\nBayesian inference to develop a model of practical interest using data from the\nCovid-19 outbreak in Switzerland.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An assumed density approximate likelihood is derived for a class of partially\nobserved stochastic compartmental models which permit observational\nover-dispersion. This is achieved by treating time-varying reporting\nprobabilities as latent variables and integrating them out using Laplace\napproximations within Poisson Approximate Likelihoods (LawPAL), resulting in a\nfast deterministic approximation to the marginal likelihood and filtering\ndistributions. We derive an asymptotically exact filtering result in the large\npopulation regime, demonstrating the approximation's ability to recover latent\ndisease states and reporting probabilities. Through simulations we: 1)\ndemonstrate favorable behavior of the maximum approximate likelihood estimator\nin the large population and time horizon regime in terms of ground truth\nrecovery; 2) demonstrate order of magnitude computational speed gains over a\nsequential Monte Carlo likelihood based approach, and explore the statistical\ncompromises our approximation implicitly makes. We conclude by embedding our\nmethodology within the probabilistic programming language Stan for automated\nBayesian inference to develop a model of practical interest using data from the\nCovid-19 outbreak in Switzerland."
                },
                "authors": [
                    {
                        "name": "Michael Whitehouse"
                    }
                ],
                "author_detail": {
                    "name": "Michael Whitehouse"
                },
                "author": "Michael Whitehouse",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15734v1",
                "updated": "2025-05-21T16:40:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    40,
                    12,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:40:12Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    40,
                    12,
                    2,
                    141,
                    0
                ],
                "title": "DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning"
                },
                "summary": "Large language models (LLMs) have improved significantly in their reasoning\nthrough extensive training on massive datasets. However, relying solely on\nadditional data for improvement is becoming increasingly impractical,\nhighlighting the need for models to autonomously enhance their reasoning\nwithout external supervision. In this paper, we propose Debate, Train, Evolve\n(DTE), a novel ground truth-free training framework that uses multi-agent\ndebate traces to evolve a single language model. We also introduce a new\nprompting strategy Reflect-Critique-Refine, to improve debate quality by\nexplicitly instructing agents to critique and refine their reasoning. Extensive\nevaluations on five reasoning benchmarks with six open-weight models show that\nour DTE framework achieve substantial improvements, with an average accuracy\ngain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe\nstrong cross-domain generalization, with an average accuracy gain of 5.8% on\nall other benchmarks, suggesting that our method captures general reasoning\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have improved significantly in their reasoning\nthrough extensive training on massive datasets. However, relying solely on\nadditional data for improvement is becoming increasingly impractical,\nhighlighting the need for models to autonomously enhance their reasoning\nwithout external supervision. In this paper, we propose Debate, Train, Evolve\n(DTE), a novel ground truth-free training framework that uses multi-agent\ndebate traces to evolve a single language model. We also introduce a new\nprompting strategy Reflect-Critique-Refine, to improve debate quality by\nexplicitly instructing agents to critique and refine their reasoning. Extensive\nevaluations on five reasoning benchmarks with six open-weight models show that\nour DTE framework achieve substantial improvements, with an average accuracy\ngain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe\nstrong cross-domain generalization, with an average accuracy gain of 5.8% on\nall other benchmarks, suggesting that our method captures general reasoning\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Gaurav Srivastava"
                    },
                    {
                        "name": "Zhenyu Bi"
                    },
                    {
                        "name": "Meng Lu"
                    },
                    {
                        "name": "Xuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Wang"
                },
                "author": "Xuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15727v1",
                "updated": "2025-05-21T16:34:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    34,
                    7,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:34:07Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    34,
                    7,
                    2,
                    141,
                    0
                ],
                "title": "VocalBench: Benchmarking the Vocal Conversational Abilities for Speech\n  Interaction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VocalBench: Benchmarking the Vocal Conversational Abilities for Speech\n  Interaction Models"
                },
                "summary": "The rapid advancement of large language models (LLMs) has accelerated the\ndevelopment of multi-modal models capable of vocal communication. Unlike\ntext-based interactions, speech conveys rich and diverse information, including\nsemantic content, acoustic variations, paralanguage cues, and environmental\ncontext. However, existing evaluations of speech interaction models\npredominantly focus on the quality of their textual responses, often\noverlooking critical aspects of vocal performance and lacking benchmarks with\nvocal-specific test instances. To address this gap, we propose VocalBench, a\ncomprehensive benchmark designed to evaluate speech interaction models'\ncapabilities in vocal communication. VocalBench comprises 9,400 carefully\ncurated instances across four key dimensions: semantic quality, acoustic\nperformance, conversational abilities, and robustness. It covers 16 fundamental\nskills essential for effective vocal interaction. Experimental results reveal\nsignificant variability in current model capabilities, each exhibiting distinct\nstrengths and weaknesses, and provide valuable insights to guide future\nresearch in speech-based interaction systems. Code and evaluation instances are\navailable at https://github.com/SJTU-OmniAgent/VocalBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has accelerated the\ndevelopment of multi-modal models capable of vocal communication. Unlike\ntext-based interactions, speech conveys rich and diverse information, including\nsemantic content, acoustic variations, paralanguage cues, and environmental\ncontext. However, existing evaluations of speech interaction models\npredominantly focus on the quality of their textual responses, often\noverlooking critical aspects of vocal performance and lacking benchmarks with\nvocal-specific test instances. To address this gap, we propose VocalBench, a\ncomprehensive benchmark designed to evaluate speech interaction models'\ncapabilities in vocal communication. VocalBench comprises 9,400 carefully\ncurated instances across four key dimensions: semantic quality, acoustic\nperformance, conversational abilities, and robustness. It covers 16 fundamental\nskills essential for effective vocal interaction. Experimental results reveal\nsignificant variability in current model capabilities, each exhibiting distinct\nstrengths and weaknesses, and provide valuable insights to guide future\nresearch in speech-based interaction systems. Code and evaluation instances are\navailable at https://github.com/SJTU-OmniAgent/VocalBench."
                },
                "authors": [
                    {
                        "name": "Heyang Liu"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Ziyang Cheng"
                    },
                    {
                        "name": "Ronghua Wu"
                    },
                    {
                        "name": "Qunshan Gu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15722v1",
                "updated": "2025-05-21T16:30:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    30,
                    18,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:30:18Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    30,
                    18,
                    2,
                    141,
                    0
                ],
                "title": "Shared Path: Unraveling Memorization in Multilingual LLMs through\n  Language Similarities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Path: Unraveling Memorization in Multilingual LLMs through\n  Language Similarities"
                },
                "summary": "We present the first comprehensive study of Memorization in Multilingual\nLarge Language Models (MLLMs), analyzing 95 languages using models across\ndiverse model scales, architectures, and memorization definitions. As MLLMs are\nincreasingly deployed, understanding their memorization behavior has become\ncritical. Yet prior work has focused primarily on monolingual models, leaving\nmultilingual memorization underexplored, despite the inherently long-tailed\nnature of training corpora. We find that the prevailing assumption, that\nmemorization is highly correlated with training data availability, fails to\nfully explain memorization patterns in MLLMs. We hypothesize that treating\nlanguages in isolation - ignoring their similarities - obscures the true\npatterns of memorization. To address this, we propose a novel graph-based\ncorrelation metric that incorporates language similarity to analyze\ncross-lingual memorization. Our analysis reveals that among similar languages,\nthose with fewer training tokens tend to exhibit higher memorization, a trend\nthat only emerges when cross-lingual relationships are explicitly modeled.\nThese findings underscore the importance of a language-aware perspective in\nevaluating and mitigating memorization vulnerabilities in MLLMs. This also\nconstitutes empirical evidence that language similarity both explains\nMemorization in MLLMs and underpins Cross-lingual Transferability, with broad\nimplications for multilingual NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of Memorization in Multilingual\nLarge Language Models (MLLMs), analyzing 95 languages using models across\ndiverse model scales, architectures, and memorization definitions. As MLLMs are\nincreasingly deployed, understanding their memorization behavior has become\ncritical. Yet prior work has focused primarily on monolingual models, leaving\nmultilingual memorization underexplored, despite the inherently long-tailed\nnature of training corpora. We find that the prevailing assumption, that\nmemorization is highly correlated with training data availability, fails to\nfully explain memorization patterns in MLLMs. We hypothesize that treating\nlanguages in isolation - ignoring their similarities - obscures the true\npatterns of memorization. To address this, we propose a novel graph-based\ncorrelation metric that incorporates language similarity to analyze\ncross-lingual memorization. Our analysis reveals that among similar languages,\nthose with fewer training tokens tend to exhibit higher memorization, a trend\nthat only emerges when cross-lingual relationships are explicitly modeled.\nThese findings underscore the importance of a language-aware perspective in\nevaluating and mitigating memorization vulnerabilities in MLLMs. This also\nconstitutes empirical evidence that language similarity both explains\nMemorization in MLLMs and underpins Cross-lingual Transferability, with broad\nimplications for multilingual NLP."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Luo"
                    },
                    {
                        "name": "Yiyi Chen"
                    },
                    {
                        "name": "Johannes Bjerva"
                    },
                    {
                        "name": "Qiongxiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Qiongxiu Li"
                },
                "author": "Qiongxiu Li",
                "arxiv_comment": "17 pages, 14 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01565v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01565v6",
                "updated": "2025-05-21T16:29:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    29,
                    45,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-03T13:36:34Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    13,
                    36,
                    34,
                    6,
                    308,
                    0
                ],
                "title": "SQL Injection Jailbreak: A Structural Disaster of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL Injection Jailbreak: A Structural Disaster of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are susceptible to jailbreak attacks that can\ninduce them to generate harmful content. Previous jailbreak methods primarily\nexploited the internal properties or capabilities of LLMs, such as\noptimization-based jailbreak methods and methods that leveraged the model's\ncontext-learning abilities. In this paper, we introduce a novel jailbreak\nmethod, SQL Injection Jailbreak (SIJ), which targets the external properties of\nLLMs, specifically, the way LLMs construct input prompts. By injecting\njailbreak information into user prompts, SIJ successfully induces the model to\noutput harmful content. For open-source models, SIJ achieves near 100% attack\nsuccess rates on five well-known LLMs on the AdvBench and HEx-PHI, while\nincurring lower time costs compared to previous methods. For closed-source\nmodels, SIJ achieves an average attack success rate over 85% across five models\nin the GPT and Doubao series. Additionally, SIJ exposes a new vulnerability in\nLLMs that urgently requires mitigation. To address this, we propose a simple\nadaptive defense method called Self-Reminder-Key to counter SIJ and demonstrate\nits effectiveness through experimental results. Our code is available at\nhttps://github.com/weiyezhimeng/SQL-Injection-Jailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to jailbreak attacks that can\ninduce them to generate harmful content. Previous jailbreak methods primarily\nexploited the internal properties or capabilities of LLMs, such as\noptimization-based jailbreak methods and methods that leveraged the model's\ncontext-learning abilities. In this paper, we introduce a novel jailbreak\nmethod, SQL Injection Jailbreak (SIJ), which targets the external properties of\nLLMs, specifically, the way LLMs construct input prompts. By injecting\njailbreak information into user prompts, SIJ successfully induces the model to\noutput harmful content. For open-source models, SIJ achieves near 100% attack\nsuccess rates on five well-known LLMs on the AdvBench and HEx-PHI, while\nincurring lower time costs compared to previous methods. For closed-source\nmodels, SIJ achieves an average attack success rate over 85% across five models\nin the GPT and Doubao series. Additionally, SIJ exposes a new vulnerability in\nLLMs that urgently requires mitigation. To address this, we propose a simple\nadaptive defense method called Self-Reminder-Key to counter SIJ and demonstrate\nits effectiveness through experimental results. Our code is available at\nhttps://github.com/weiyezhimeng/SQL-Injection-Jailbreak."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "Accepted by findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01565v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01565v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15715v1",
                "updated": "2025-05-21T16:24:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    24,
                    49,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:24:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    24,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with\n  Large Language Models for Mental Health Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with\n  Large Language Models for Mental Health Counseling"
                },
                "summary": "Large language models (LLMs) hold significant potential for mental health\nsupport, capable of generating empathetic responses and simulating therapeutic\nconversations. However, existing LLM-based approaches often lack the clinical\ngrounding necessary for real-world psychological counseling, particularly in\nexplicit diagnostic reasoning aligned with standards like the DSM/ICD and\nincorporating diverse therapeutic modalities beyond basic empathy or single\nstrategies. To address these critical limitations, we propose PsyLLM, the first\nlarge language model designed to systematically integrate both diagnostic and\ntherapeutic reasoning for mental health counseling. To develop the PsyLLM, we\npropose a novel automated data synthesis pipeline. This pipeline processes\nreal-world mental health posts, generates multi-turn dialogue structures, and\nleverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and\nmultiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate\ndetailed clinical reasoning processes. Rigorous multi-dimensional filtering\nensures the generation of high-quality, clinically aligned dialogue data. In\naddition, we introduce a new benchmark and evaluation protocol, assessing\ncounseling quality across four key dimensions: comprehensiveness,\nprofessionalism, authenticity, and safety. Our experiments demonstrate that\nPsyLLM significantly outperforms state-of-the-art baseline models on this\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold significant potential for mental health\nsupport, capable of generating empathetic responses and simulating therapeutic\nconversations. However, existing LLM-based approaches often lack the clinical\ngrounding necessary for real-world psychological counseling, particularly in\nexplicit diagnostic reasoning aligned with standards like the DSM/ICD and\nincorporating diverse therapeutic modalities beyond basic empathy or single\nstrategies. To address these critical limitations, we propose PsyLLM, the first\nlarge language model designed to systematically integrate both diagnostic and\ntherapeutic reasoning for mental health counseling. To develop the PsyLLM, we\npropose a novel automated data synthesis pipeline. This pipeline processes\nreal-world mental health posts, generates multi-turn dialogue structures, and\nleverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and\nmultiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate\ndetailed clinical reasoning processes. Rigorous multi-dimensional filtering\nensures the generation of high-quality, clinically aligned dialogue data. In\naddition, we introduce a new benchmark and evaluation protocol, assessing\ncounseling quality across four key dimensions: comprehensiveness,\nprofessionalism, authenticity, and safety. Our experiments demonstrate that\nPsyLLM significantly outperforms state-of-the-art baseline models on this\nbenchmark."
                },
                "authors": [
                    {
                        "name": "He Hu"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Juzheng Si"
                    },
                    {
                        "name": "Qianning Wang"
                    },
                    {
                        "name": "Hengheng Zhang"
                    },
                    {
                        "name": "Fuji Ren"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Laizhong Cui"
                    }
                ],
                "author_detail": {
                    "name": "Laizhong Cui"
                },
                "author": "Laizhong Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15712v1",
                "updated": "2025-05-21T16:22:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    22,
                    32,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:22:32Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    22,
                    32,
                    2,
                    141,
                    0
                ],
                "title": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games"
                },
                "summary": "This paper introduces TurnaboutLLM, a novel framework and dataset for\nevaluating the deductive reasoning abilities of Large Language Models (LLMs) by\nleveraging the interactive gameplay of detective games Ace Attorney and\nDanganronpa. The framework tasks LLMs with identifying contradictions between\ntestimonies and evidences within long narrative contexts, a challenging task\ndue to the large answer space and diverse reasoning types presented by its\nquestions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at\nlimitations of popular strategies for enhancing deductive reasoning such as\nextensive thinking and Chain-of-Thought prompting. The results also suggest\nvarying effects of context size, the number of reasoning step and answer space\nsize on model performance. Overall, TurnaboutLLM presents a substantial\nchallenge for LLMs' deductive reasoning abilities in complex, narrative-rich\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TurnaboutLLM, a novel framework and dataset for\nevaluating the deductive reasoning abilities of Large Language Models (LLMs) by\nleveraging the interactive gameplay of detective games Ace Attorney and\nDanganronpa. The framework tasks LLMs with identifying contradictions between\ntestimonies and evidences within long narrative contexts, a challenging task\ndue to the large answer space and diverse reasoning types presented by its\nquestions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at\nlimitations of popular strategies for enhancing deductive reasoning such as\nextensive thinking and Chain-of-Thought prompting. The results also suggest\nvarying effects of context size, the number of reasoning step and answer space\nsize on model performance. Overall, TurnaboutLLM presents a substantial\nchallenge for LLMs' deductive reasoning abilities in complex, narrative-rich\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Muyu He"
                    },
                    {
                        "name": "Muhammad Adil Shahid"
                    },
                    {
                        "name": "Jiani Huang"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15710v1",
                "updated": "2025-05-21T16:21:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    21,
                    29,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:21:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    21,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "Advancing LLM Safe Alignment with Safety Representation Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing LLM Safe Alignment with Safety Representation Ranking"
                },
                "summary": "The rapid advancement of large language models (LLMs) has demonstrated\nmilestone success in a variety of tasks, yet their potential for generating\nharmful content has raised significant safety concerns. Existing safety\nevaluation approaches typically operate directly on textual responses,\noverlooking the rich information embedded in the model's internal\nrepresentations. In this paper, we propose Safety Representation Ranking (SRR),\na listwise ranking framework that selects safe responses using hidden states\nfrom the LLM itself. SRR encodes both instructions and candidate completions\nusing intermediate transformer representations and ranks candidates via a\nlightweight similarity-based scorer. Our approach directly leverages internal\nmodel states and supervision at the list level to capture subtle safety\nsignals. Experiments across multiple benchmarks show that SRR significantly\nimproves robustness to adversarial prompts. Our code will be available upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has demonstrated\nmilestone success in a variety of tasks, yet their potential for generating\nharmful content has raised significant safety concerns. Existing safety\nevaluation approaches typically operate directly on textual responses,\noverlooking the rich information embedded in the model's internal\nrepresentations. In this paper, we propose Safety Representation Ranking (SRR),\na listwise ranking framework that selects safe responses using hidden states\nfrom the LLM itself. SRR encodes both instructions and candidate completions\nusing intermediate transformer representations and ranks candidates via a\nlightweight similarity-based scorer. Our approach directly leverages internal\nmodel states and supervision at the list level to capture subtle safety\nsignals. Experiments across multiple benchmarks show that SRR significantly\nimproves robustness to adversarial prompts. Our code will be available upon\npublication."
                },
                "authors": [
                    {
                        "name": "Tianqi Du"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15701v1",
                "updated": "2025-05-21T16:14:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    14,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:14:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    14,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL\n  Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL\n  Graph Databases"
                },
                "summary": "Large Language Models (LLMs) have demonstrated their potential in hardware\ndesign tasks, such as Hardware Description Language (HDL) generation and\ndebugging. Yet, their performance in real-world, repository-level HDL projects\nwith thousands or even tens of thousands of code lines is hindered. To this\nend, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\nAugmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\nrepresentations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\nGraphs (DFGs) to capture both code graph view and hardware graph view.\nHDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\nlimited recall issues inherent in similarity-based semantic retrieval by\nincorporating structural information, but also enhances its extensibility to\nvarious real-world tasks by a task-specific retrieval finetuning. Additionally,\nto address the lack of comprehensive HDL search benchmarks, we introduce\nHDLSearch, a multi-granularity evaluation dataset derived from real-world\nrepository-level projects. Experimental results demonstrate that HDLxGraph\nsignificantly improves average search accuracy, debugging efficiency and\ncompletion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\nRAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\navailable at https://github.com/Nick-Zheng-Q/HDLxGraph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated their potential in hardware\ndesign tasks, such as Hardware Description Language (HDL) generation and\ndebugging. Yet, their performance in real-world, repository-level HDL projects\nwith thousands or even tens of thousands of code lines is hindered. To this\nend, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\nAugmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\nrepresentations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\nGraphs (DFGs) to capture both code graph view and hardware graph view.\nHDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\nlimited recall issues inherent in similarity-based semantic retrieval by\nincorporating structural information, but also enhances its extensibility to\nvarious real-world tasks by a task-specific retrieval finetuning. Additionally,\nto address the lack of comprehensive HDL search benchmarks, we introduce\nHDLSearch, a multi-granularity evaluation dataset derived from real-world\nrepository-level projects. Experimental results demonstrate that HDLxGraph\nsignificantly improves average search accuracy, debugging efficiency and\ncompletion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\nRAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\navailable at https://github.com/Nick-Zheng-Q/HDLxGraph."
                },
                "authors": [
                    {
                        "name": "Pingqing Zheng"
                    },
                    {
                        "name": "Jiayin Qin"
                    },
                    {
                        "name": "Fuqi Zhang"
                    },
                    {
                        "name": "Shang Wu"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Caiwen Ding"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhao"
                },
                "arxiv_affiliation": "Katie",
                "author": "Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08185v2",
                "updated": "2025-05-21T16:10:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    10,
                    6,
                    2,
                    141,
                    0
                ],
                "published": "2024-09-12T16:20:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    20,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Fine-tuning Large Language Models for Entity Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for Entity Matching"
                },
                "summary": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and ability to generalize to unseen entities. Existing research on\nusing LLMs for entity matching has focused on prompt engineering and in-context\nlearning. This paper explores the potential of fine-tuning LLMs for entity\nmatching. We analyze fine-tuning along two dimensions: 1) the representation of\ntraining examples, where we experiment with adding different types of\nLLM-generated explanations to the training set, and 2) the selection and\ngeneration of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodels ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods, only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and ability to generalize to unseen entities. Existing research on\nusing LLMs for entity matching has focused on prompt engineering and in-context\nlearning. This paper explores the potential of fine-tuning LLMs for entity\nmatching. We analyze fine-tuning along two dimensions: 1) the representation of\ntraining examples, where we experiment with adding different types of\nLLM-generated explanations to the training set, and 2) the selection and\ngeneration of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodels ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods, only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Aaron Steiner"
                    },
                    {
                        "name": "Ralph Peeters"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "arxiv_comment": "8 pages, 4 figures. For related code and data, see this\n  https://github.com/wbsg-uni-mannheim/TailorMatch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15695v1",
                "updated": "2025-05-21T16:09:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    9,
                    44,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:09:44Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    9,
                    44,
                    2,
                    141,
                    0
                ],
                "title": "Can Large Language Models be Effective Online Opinion Miners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models be Effective Online Opinion Miners?"
                },
                "summary": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield."
                },
                "authors": [
                    {
                        "name": "Ryang Heo"
                    },
                    {
                        "name": "Yongsik Seo"
                    },
                    {
                        "name": "Junseong Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15692v1",
                "updated": "2025-05-21T16:06:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    6,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:06:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    6,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities"
                },
                "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability."
                },
                "authors": [
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Chonghua Liao"
                    },
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zhengqi Wen"
                    },
                    {
                        "name": "Pengpeng Shao"
                    },
                    {
                        "name": "Huazhe Xu"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15690v1",
                "updated": "2025-05-21T16:05:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    5,
                    29,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:05:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    5,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "Toward Open Earth Science as Fast and Accessible as Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Open Earth Science as Fast and Accessible as Natural Language"
                },
                "summary": "Is natural-language-driven earth observation data analysis now feasible with\nthe assistance of Large Language Models (LLMs)? For open science in service of\npublic interest, feasibility requires reliably high accuracy, interactive\nlatencies, low (sustainable) costs, open LLMs, and openly maintainable software\n-- hence, the challenge. What are the techniques and programming system\nrequirements necessary for satisfying these constraints, and what is the\ncorresponding development and maintenance burden in practice? This study lays\nthe groundwork for exploring these questions, introducing an impactful earth\nscience use-case, and providing a software framework with evaluation data and\nmetrics, along with initial results from employing model scaling,\nprompt-optimization, and inference-time scaling optimization techniques. While\nwe attain high accuracy (near 100%) across 10 of 11 metrics, the analysis\nfurther considers cost (token-spend), latency, and maintainability across this\nspace of techniques. Finally, we enumerate opportunities for further research,\ngeneral programming and evaluation framework development, and ongoing work for\na comprehensive, deployable solution. This is a call for collaboration and\ncontribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is natural-language-driven earth observation data analysis now feasible with\nthe assistance of Large Language Models (LLMs)? For open science in service of\npublic interest, feasibility requires reliably high accuracy, interactive\nlatencies, low (sustainable) costs, open LLMs, and openly maintainable software\n-- hence, the challenge. What are the techniques and programming system\nrequirements necessary for satisfying these constraints, and what is the\ncorresponding development and maintenance burden in practice? This study lays\nthe groundwork for exploring these questions, introducing an impactful earth\nscience use-case, and providing a software framework with evaluation data and\nmetrics, along with initial results from employing model scaling,\nprompt-optimization, and inference-time scaling optimization techniques. While\nwe attain high accuracy (near 100%) across 10 of 11 metrics, the analysis\nfurther considers cost (token-spend), latency, and maintainability across this\nspace of techniques. Finally, we enumerate opportunities for further research,\ngeneral programming and evaluation framework development, and ongoing work for\na comprehensive, deployable solution. This is a call for collaboration and\ncontribution."
                },
                "authors": [
                    {
                        "name": "Marquita Ellis"
                    },
                    {
                        "name": "Iksha Gurung"
                    },
                    {
                        "name": "Muthukumaran Ramasubramanian"
                    },
                    {
                        "name": "Rahul Ramachandran"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Ramachandran"
                },
                "author": "Rahul Ramachandran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2; H.5.2; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17720v3",
                "updated": "2025-05-21T16:04:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    4,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-24T23:23:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    23,
                    23,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Spontaneous Giving and Calculated Greed in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spontaneous Giving and Calculated Greed in Language Models"
                },
                "summary": "Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Hirokazu Shirado"
                    }
                ],
                "author_detail": {
                    "name": "Hirokazu Shirado"
                },
                "author": "Hirokazu Shirado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15687v1",
                "updated": "2025-05-21T16:03:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    3,
                    3,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:03:03Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    3,
                    3,
                    2,
                    141,
                    0
                ],
                "title": "Discovering Pathology Rationale and Token Allocation for Efficient\n  Multimodal Pathology Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Pathology Rationale and Token Allocation for Efficient\n  Multimodal Pathology Reasoning"
                },
                "summary": "Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Cheng Jin"
                    },
                    {
                        "name": "Yihui Wang"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15685v1",
                "updated": "2025-05-21T16:01:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    1,
                    11,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:01:11Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    1,
                    11,
                    2,
                    141,
                    0
                ],
                "title": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems"
                },
                "summary": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions."
                },
                "authors": [
                    {
                        "name": "Xiuchao Sui"
                    },
                    {
                        "name": "Daiying Tian"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Kenneth Kwok"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "17 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15684v1",
                "updated": "2025-05-21T15:58:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy"
                },
                "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption."
                },
                "authors": [
                    {
                        "name": "Gengyang Li"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v1",
                "updated": "2025-05-21T15:58:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability"
                },
                "summary": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Jiaying Zheng"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Jin Dong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17531v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17531v3",
                "updated": "2025-05-22T10:57:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    10,
                    57,
                    51,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-24T13:19:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    19,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "Towards Machine-Generated Code for the Resolution of User Intentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Machine-Generated Code for the Resolution of User Intentions"
                },
                "summary": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code. This development represents a significant progression in\nthe realm of hybrid workflows, where human and artificial intelligence\ncollaborate to address user intentions, with the former responsible for\ndefining these intentions and the latter for implementing the solutions to\naddress them. In this paper, we investigate the feasibility of generating and\nexecuting workflows through code generation that results from prompting an LLM\nwith a concrete user intention, and a simplified application programming\ninterface for a GUI-less operating system. We provide an in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate the general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code. This development represents a significant progression in\nthe realm of hybrid workflows, where human and artificial intelligence\ncollaborate to address user intentions, with the former responsible for\ndefining these intentions and the latter for implementing the solutions to\naddress them. In this paper, we investigate the feasibility of generating and\nexecuting workflows through code generation that results from prompting an LLM\nwith a concrete user intention, and a simplified application programming\ninterface for a GUI-less operating system. We provide an in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate the general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions."
                },
                "authors": [
                    {
                        "name": "Justus Flerlage"
                    },
                    {
                        "name": "Ilja Behnke"
                    },
                    {
                        "name": "Odej Kao"
                    }
                ],
                "author_detail": {
                    "name": "Odej Kao"
                },
                "author": "Odej Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17531v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17531v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04400v2",
                "updated": "2025-05-21T15:55:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    55,
                    17,
                    2,
                    141,
                    0
                ],
                "published": "2025-01-08T10:28:31Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    28,
                    31,
                    2,
                    8,
                    0
                ],
                "title": "Non-intrusive reduced-order modeling for dynamical systems with\n  spatially localized features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-intrusive reduced-order modeling for dynamical systems with\n  spatially localized features"
                },
                "summary": "This work presents a non-intrusive reduced-order modeling framework for\ndynamical systems with spatially localized features characterized by slow\nsingular value decay. The proposed approach builds upon two existing\nmethodologies for reduced and full-order non-intrusive modeling, namely\nOperator Inference (OpInf) and sparse Full-Order Model (sFOM) inference. We\ndecompose the domain into two complementary subdomains that exhibit fast and\nslow singular value decay. The dynamics of the subdomain exhibiting slow\nsingular value decay are learned with sFOM while the dynamics with\nintrinsically low dimensionality on the complementary subdomain are learned\nwith OpInf. The resulting, coupled OpInf-sFOM formulation leverages the\ncomputational efficiency of OpInf and the high resolution of sFOM, and thus\nenables fast non-intrusive predictions for conditions beyond those sampled in\nthe training data set. A novel regularization technique with a closed-form\nsolution based on the Gershgorin disk theorem is introduced to promote stable\nsFOM and OpInf models. We also provide a data-driven indicator for subdomain\nselection and ensure solution smoothness over the interface via a\npost-processing interpolation step. We evaluate the efficiency of the approach\nin terms of offline and online speedup through a quantitative, parametric\ncomputational cost analysis. We demonstrate the coupled OpInf-sFOM formulation\nfor two test cases: a one-dimensional Burgers' model for which accurate\npredictions beyond the span of the training snapshots are presented, and a\ntwo-dimensional parametric model for the Pine Island Glacier ice thickness\ndynamics, for which the OpInf-sFOM model achieves an average prediction error\non the order of $1 \\%$ with an online speedup factor of approximately $8\\times$\ncompared to the numerical simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a non-intrusive reduced-order modeling framework for\ndynamical systems with spatially localized features characterized by slow\nsingular value decay. The proposed approach builds upon two existing\nmethodologies for reduced and full-order non-intrusive modeling, namely\nOperator Inference (OpInf) and sparse Full-Order Model (sFOM) inference. We\ndecompose the domain into two complementary subdomains that exhibit fast and\nslow singular value decay. The dynamics of the subdomain exhibiting slow\nsingular value decay are learned with sFOM while the dynamics with\nintrinsically low dimensionality on the complementary subdomain are learned\nwith OpInf. The resulting, coupled OpInf-sFOM formulation leverages the\ncomputational efficiency of OpInf and the high resolution of sFOM, and thus\nenables fast non-intrusive predictions for conditions beyond those sampled in\nthe training data set. A novel regularization technique with a closed-form\nsolution based on the Gershgorin disk theorem is introduced to promote stable\nsFOM and OpInf models. We also provide a data-driven indicator for subdomain\nselection and ensure solution smoothness over the interface via a\npost-processing interpolation step. We evaluate the efficiency of the approach\nin terms of offline and online speedup through a quantitative, parametric\ncomputational cost analysis. We demonstrate the coupled OpInf-sFOM formulation\nfor two test cases: a one-dimensional Burgers' model for which accurate\npredictions beyond the span of the training snapshots are presented, and a\ntwo-dimensional parametric model for the Pine Island Glacier ice thickness\ndynamics, for which the OpInf-sFOM model achieves an average prediction error\non the order of $1 \\%$ with an online speedup factor of approximately $8\\times$\ncompared to the numerical simulation."
                },
                "authors": [
                    {
                        "name": "Leonidas Gkimisis"
                    },
                    {
                        "name": "Nicole Aretz"
                    },
                    {
                        "name": "Marco Tezzele"
                    },
                    {
                        "name": "Thomas Richter"
                    },
                    {
                        "name": "Peter Benner"
                    },
                    {
                        "name": "Karen E. Willcox"
                    }
                ],
                "author_detail": {
                    "name": "Karen E. Willcox"
                },
                "author": "Karen E. Willcox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.19844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.19844v2",
                "updated": "2025-05-21T15:54:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    54,
                    35,
                    2,
                    141,
                    0
                ],
                "published": "2023-05-31T13:32:27Z",
                "published_parsed": [
                    2023,
                    5,
                    31,
                    13,
                    32,
                    27,
                    2,
                    151,
                    0
                ],
                "title": "Learning Task-preferred Inference Routes for Gradient De-conflict in\n  Multi-output DNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Task-preferred Inference Routes for Gradient De-conflict in\n  Multi-output DNNs"
                },
                "summary": "Multi-output deep neural networks(MONs) contain multiple task branches, and\nthese tasks usually share partial network filters that lead to the entanglement\nof different task inference routes. Due to the inconsistent optimization\nobjectives, the task gradients used for training MONs will interfere with each\nother on the shared routes, which will decrease the overall model performance.\nTo address this issue, we propose a novel gradient de-conflict algorithm named\nDR-MGF(Dynamic Routes and Meta-weighted Gradient Fusion) in this work.\nDifferent from existing de-conflict methods, DR-MGF achieves gradient\nde-conflict in MONs by learning task-preferred inference routes. The proposed\nmethod is motivated by our experimental findings: the shared filters are not\nequally important to different tasks. By designing the learnable task-specific\nimportance variables, DR-MGF evaluates the importance of filters for different\ntasks. Through making the dominances of tasks over filters be proportional to\nthe task-specific importance of filters, DR-MGF can effectively reduce the\ninter-task interference. The task-specific importance variables ultimately\ndetermine task-preferred inference routes at the end of training iterations.\nExtensive experimental results on CIFAR, ImageNet, and NYUv2 illustrate that\nDR-MGF outperforms the existing de-conflict methods both in prediction accuracy\nand convergence speed of MONs. Furthermore, DR-MGF can be extended to general\nMONs without modifying the overall network structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-output deep neural networks(MONs) contain multiple task branches, and\nthese tasks usually share partial network filters that lead to the entanglement\nof different task inference routes. Due to the inconsistent optimization\nobjectives, the task gradients used for training MONs will interfere with each\nother on the shared routes, which will decrease the overall model performance.\nTo address this issue, we propose a novel gradient de-conflict algorithm named\nDR-MGF(Dynamic Routes and Meta-weighted Gradient Fusion) in this work.\nDifferent from existing de-conflict methods, DR-MGF achieves gradient\nde-conflict in MONs by learning task-preferred inference routes. The proposed\nmethod is motivated by our experimental findings: the shared filters are not\nequally important to different tasks. By designing the learnable task-specific\nimportance variables, DR-MGF evaluates the importance of filters for different\ntasks. Through making the dominances of tasks over filters be proportional to\nthe task-specific importance of filters, DR-MGF can effectively reduce the\ninter-task interference. The task-specific importance variables ultimately\ndetermine task-preferred inference routes at the end of training iterations.\nExtensive experimental results on CIFAR, ImageNet, and NYUv2 illustrate that\nDR-MGF outperforms the existing de-conflict methods both in prediction accuracy\nand convergence speed of MONs. Furthermore, DR-MGF can be extended to general\nMONs without modifying the overall network structures."
                },
                "authors": [
                    {
                        "name": "Yi Sun"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Xiaochang Hu"
                    },
                    {
                        "name": "Yifei Shi"
                    },
                    {
                        "name": "Ling-Li Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Ling-Li Zeng"
                },
                "author": "Ling-Li Zeng",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.19844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.19844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15674v1",
                "updated": "2025-05-21T15:53:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    53,
                    28,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:53:28Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    53,
                    28,
                    2,
                    141,
                    0
                ],
                "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language\n  Models"
                },
                "summary": "Large language models require iterative updates to address challenges such as\nknowledge conflicts and outdated information (e.g., incorrect, private, or\nillegal contents). Machine unlearning provides a systematic methodology for\ntargeted knowledge removal from trained models, enabling elimination of\nsensitive information influences. However, mainstream fine-tuning-based\nunlearning methods often fail to balance unlearning efficacy and model ability,\nfrequently resulting in catastrophic model collapse under extensive knowledge\nremoval. Meanwhile, in-context unlearning, which relies solely on contextual\nprompting without modifying the model's intrinsic mechanisms, suffers from\nlimited generalizability and struggles to achieve true unlearning. In this\nwork, we introduce UniErase, a novel unlearning paradigm that employs learnable\nparametric suffix (unlearning token) to steer language models toward targeted\nforgetting behaviors. UniErase operates through two key phases: (I) an\noptimization stage that binds desired unlearning outputs to the model's\nautoregressive probability distribution via token optimization, followed by\n(II) a lightweight model editing phase that activates the learned token to\nprobabilistically induce specified forgetting objective. Serving as a new\nresearch direction for token learning to induce unlearning target, UniErase\nachieves state-of-the-art (SOTA) performance across batch, sequential, and\nprecise unlearning under fictitious and real-world knowledge settings.\nRemarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%\nof the LLM parameters, outperforms previous forgetting SOTA baseline by around\n4.01 times for model ability with even better unlearning efficacy. Similarly,\nUniErase, maintaining more ability, also surpasses previous retaining SOTA by\n35.96% for unlearning efficacy, showing dual top-tier performances in current\nunlearing domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require iterative updates to address challenges such as\nknowledge conflicts and outdated information (e.g., incorrect, private, or\nillegal contents). Machine unlearning provides a systematic methodology for\ntargeted knowledge removal from trained models, enabling elimination of\nsensitive information influences. However, mainstream fine-tuning-based\nunlearning methods often fail to balance unlearning efficacy and model ability,\nfrequently resulting in catastrophic model collapse under extensive knowledge\nremoval. Meanwhile, in-context unlearning, which relies solely on contextual\nprompting without modifying the model's intrinsic mechanisms, suffers from\nlimited generalizability and struggles to achieve true unlearning. In this\nwork, we introduce UniErase, a novel unlearning paradigm that employs learnable\nparametric suffix (unlearning token) to steer language models toward targeted\nforgetting behaviors. UniErase operates through two key phases: (I) an\noptimization stage that binds desired unlearning outputs to the model's\nautoregressive probability distribution via token optimization, followed by\n(II) a lightweight model editing phase that activates the learned token to\nprobabilistically induce specified forgetting objective. Serving as a new\nresearch direction for token learning to induce unlearning target, UniErase\nachieves state-of-the-art (SOTA) performance across batch, sequential, and\nprecise unlearning under fictitious and real-world knowledge settings.\nRemarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%\nof the LLM parameters, outperforms previous forgetting SOTA baseline by around\n4.01 times for model ability with even better unlearning efficacy. Similarly,\nUniErase, maintaining more ability, also surpasses previous retaining SOTA by\n35.96% for unlearning efficacy, showing dual top-tier performances in current\nunlearing domain."
                },
                "authors": [
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17216v2",
                "updated": "2025-05-21T15:51:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    51,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-24T14:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    49,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM\n  Reasoning"
                },
                "summary": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, it remains unclear what the contributing factors to the success\nof Neurosymbolic LLM reasoning are. This paper shows that one important factor\nis the choice of the formal language. By comparing 4 formal languages on 3\ndatasets over 6 LLMs, we show that the choice of formal language affects both\nthe syntactic and the semantic reasoning capability. Thereby, we introduce the\nintermediate language challenge, which is the challenge of picking a suitable\nformal language for neurosymbolic reasoning. Further, we compare the effects of\nusing different in-context-learning examples in an ablation study. We conclude\nthat on average, context-aware encodings help LLMs to reason, while there is no\napparent effect of using comments or markdown syntax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, it remains unclear what the contributing factors to the success\nof Neurosymbolic LLM reasoning are. This paper shows that one important factor\nis the choice of the formal language. By comparing 4 formal languages on 3\ndatasets over 6 LLMs, we show that the choice of formal language affects both\nthe syntactic and the semantic reasoning capability. Thereby, we introduce the\nintermediate language challenge, which is the challenge of picking a suitable\nformal language for neurosymbolic reasoning. Further, we compare the effects of\nusing different in-context-learning examples in an ablation study. We conclude\nthat on average, context-aware encodings help LLMs to reason, while there is no\napparent effect of using comments or markdown syntax."
                },
                "authors": [
                    {
                        "name": "Alexander Beiser"
                    },
                    {
                        "name": "David Penz"
                    },
                    {
                        "name": "Nysret Musliu"
                    }
                ],
                "author_detail": {
                    "name": "Nysret Musliu"
                },
                "author": "Nysret Musliu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12169v2",
                "updated": "2025-05-21T15:49:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    49,
                    11,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-13T07:42:18Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    7,
                    42,
                    18,
                    3,
                    44,
                    0
                ],
                "title": "Antimatter Annihilation Vertex Reconstruction with Deep Learning for\n  ALPHA-g Radial Time Projection Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antimatter Annihilation Vertex Reconstruction with Deep Learning for\n  ALPHA-g Radial Time Projection Chamber"
                },
                "summary": "The ALPHA-g experiment at CERN aims to precisely measure the terrestrial\ngravitational acceleration of antihydrogen atoms. A radial Time Projection\nChamber (rTPC), that surrounds the ALPHA-g magnetic trap, is employed to\ndetermine the annihilation location, called the vertex. The standard approach\nrequires identifying the trajectories of the ionizing particles in the rTPC\nfrom the location of their interaction in the gas (spacepoints), and inferring\nthe vertex positions by finding the point where those trajectories (helices)\npass closest to one another. In this work, we present a novel approach to\nvertex reconstruction using an ensemble of models based on the PointNet deep\nlearning architecture. The newly developed model, PointNet Ensemble for\nAnnihilation Reconstruction (PEAR), directly learns the relation between the\nlocation of the vertices and the rTPC spacepoints, thus eliminating the need to\nidentify and fit the particle tracks. PEAR shows strong performance in\nreconstructing vertical vertex positions from simulated data, that is superior\nto the standard approach for all metrics considered. Furthermore, the deep\nlearning approach can reconstruct the vertical vertex position when the\nstandard approach fails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ALPHA-g experiment at CERN aims to precisely measure the terrestrial\ngravitational acceleration of antihydrogen atoms. A radial Time Projection\nChamber (rTPC), that surrounds the ALPHA-g magnetic trap, is employed to\ndetermine the annihilation location, called the vertex. The standard approach\nrequires identifying the trajectories of the ionizing particles in the rTPC\nfrom the location of their interaction in the gas (spacepoints), and inferring\nthe vertex positions by finding the point where those trajectories (helices)\npass closest to one another. In this work, we present a novel approach to\nvertex reconstruction using an ensemble of models based on the PointNet deep\nlearning architecture. The newly developed model, PointNet Ensemble for\nAnnihilation Reconstruction (PEAR), directly learns the relation between the\nlocation of the vertices and the rTPC spacepoints, thus eliminating the need to\nidentify and fit the particle tracks. PEAR shows strong performance in\nreconstructing vertical vertex positions from simulated data, that is superior\nto the standard approach for all metrics considered. Furthermore, the deep\nlearning approach can reconstruct the vertical vertex position when the\nstandard approach fails."
                },
                "authors": [
                    {
                        "name": "Ashley Ferreira"
                    },
                    {
                        "name": "Mahip Singh"
                    },
                    {
                        "name": "Yukiya Saito"
                    },
                    {
                        "name": "Andrea Capra"
                    },
                    {
                        "name": "Ina Carli"
                    },
                    {
                        "name": "Daniel Duque Quiceno"
                    },
                    {
                        "name": "Wojciech T. Fedorko"
                    },
                    {
                        "name": "Makoto C. Fujiwara"
                    },
                    {
                        "name": "Muyan Li"
                    },
                    {
                        "name": "Lars Martin"
                    },
                    {
                        "name": "Gareth Smith"
                    },
                    {
                        "name": "Anqui Xu"
                    }
                ],
                "author_detail": {
                    "name": "Anqui Xu"
                },
                "author": "Anqui Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15670v1",
                "updated": "2025-05-21T15:48:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    48,
                    30,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:48:30Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    48,
                    30,
                    2,
                    141,
                    0
                ],
                "title": "Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model"
                },
                "summary": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility."
                },
                "authors": [
                    {
                        "name": "Ke Hu"
                    },
                    {
                        "name": "Ehsan Hosseini-Asl"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Edresson Casanova"
                    },
                    {
                        "name": "Subhankar Ghosh"
                    },
                    {
                        "name": "Piotr Żelasko"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Jason Li"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12788v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12788v3",
                "updated": "2025-05-21T15:45:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    45,
                    6,
                    2,
                    141,
                    0
                ],
                "published": "2024-10-16T17:59:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    59,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Meta-Chunking: Learning Text Segmentation and Semantic Completion via\n  Logical Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Chunking: Learning Text Segmentation and Semantic Completion via\n  Logical Perception"
                },
                "summary": "While Retrieval-Augmented Generation (RAG) has emerged as a promising\nparadigm for boosting large language models (LLMs) in knowledge-intensive\ntasks, it often overlooks the crucial aspect of text chunking within its\nworkflow. This paper proposes the Meta-Chunking framework, which specifically\nenhances chunking quality through a dual strategy that identifies optimal\nsegmentation points and preserves global information. Initially, breaking\nlimitations of similarity-based chunking, we design two adaptive chunking\ntechniques based on uncertainty, namely Perplexity Chunking and Margin Sampling\nChunking, by utilizing the logical perception capabilities of LLMs. Given the\ninherent complexity across different texts, we integrate meta-chunk with\ndynamic merging, striking a balance between fine-grained and coarse-grained\ntext chunking. Furthermore, we establish the global information compensation\nmechanism, encompassing a two-stage hierarchical summary generation process and\na three-stage text chunk rewriting procedure focused on missing reflection,\nrefinement, and completion. These components collectively strengthen the\nsemantic integrity and contextual coherence of chunks. Extensive experiments\ndemonstrate that Meta-Chunking effectively addresses challenges of the chunking\ntask within the RAG system, providing LLMs with more logically coherent text\nchunks. Additionally, our methodology validates the feasibility of implementing\nhigh-quality chunking tasks with smaller-scale models, thereby eliminating the\nreliance on robust instruction-following capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval-Augmented Generation (RAG) has emerged as a promising\nparadigm for boosting large language models (LLMs) in knowledge-intensive\ntasks, it often overlooks the crucial aspect of text chunking within its\nworkflow. This paper proposes the Meta-Chunking framework, which specifically\nenhances chunking quality through a dual strategy that identifies optimal\nsegmentation points and preserves global information. Initially, breaking\nlimitations of similarity-based chunking, we design two adaptive chunking\ntechniques based on uncertainty, namely Perplexity Chunking and Margin Sampling\nChunking, by utilizing the logical perception capabilities of LLMs. Given the\ninherent complexity across different texts, we integrate meta-chunk with\ndynamic merging, striking a balance between fine-grained and coarse-grained\ntext chunking. Furthermore, we establish the global information compensation\nmechanism, encompassing a two-stage hierarchical summary generation process and\na three-stage text chunk rewriting procedure focused on missing reflection,\nrefinement, and completion. These components collectively strengthen the\nsemantic integrity and contextual coherence of chunks. Extensive experiments\ndemonstrate that Meta-Chunking effectively addresses challenges of the chunking\ntask within the RAG system, providing LLMs with more logically coherent text\nchunks. Additionally, our methodology validates the feasibility of implementing\nhigh-quality chunking tasks with smaller-scale models, thereby eliminating the\nreliance on robust instruction-following capabilities."
                },
                "authors": [
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Yuchen Feng"
                    },
                    {
                        "name": "Pengnian Qi"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12788v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12788v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13997v2",
                "updated": "2025-05-21T15:37:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    37,
                    25,
                    2,
                    141,
                    0
                ],
                "published": "2025-01-23T11:04:25Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    4,
                    25,
                    3,
                    23,
                    0
                ],
                "title": "Predictive Learning in Energy-based Models with Attractor Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Learning in Energy-based Models with Attractor Structures"
                },
                "summary": "Predictive models are highly advanced in understanding the mechanisms of\nbrain function. Recent advances in machine learning further underscore the\npower of prediction for optimal representation in learning. However, there\nremains a gap in creating a biologically plausible model that explains how the\nneural system achieves prediction. In this paper, we introduce a framework that\nemploys an energy-based model (EBM) to capture the nuanced processes of\npredicting observation after action within the neural system, encompassing\nprediction, learning, and inference. We implement the EBM with a hierarchical\nstructure and integrate a continuous attractor neural network for memory,\nconstructing a biologically plausible model. In experimental evaluations, our\nmodel demonstrates efficacy across diverse scenarios. The range of actions\nincludes eye movement, motion in environments, head turning, and static\nobservation while the environment changes. Our model not only makes accurate\npredictions for environments it was trained on, but also provides reasonable\npredictions for unseen environments, matching the performances of machine\nlearning methods in multiple tasks. We hope that this study contributes to a\ndeep understanding of how the neural system performs prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive models are highly advanced in understanding the mechanisms of\nbrain function. Recent advances in machine learning further underscore the\npower of prediction for optimal representation in learning. However, there\nremains a gap in creating a biologically plausible model that explains how the\nneural system achieves prediction. In this paper, we introduce a framework that\nemploys an energy-based model (EBM) to capture the nuanced processes of\npredicting observation after action within the neural system, encompassing\nprediction, learning, and inference. We implement the EBM with a hierarchical\nstructure and integrate a continuous attractor neural network for memory,\nconstructing a biologically plausible model. In experimental evaluations, our\nmodel demonstrates efficacy across diverse scenarios. The range of actions\nincludes eye movement, motion in environments, head turning, and static\nobservation while the environment changes. Our model not only makes accurate\npredictions for environments it was trained on, but also provides reasonable\npredictions for unseen environments, matching the performances of machine\nlearning methods in multiple tasks. We hope that this study contributes to a\ndeep understanding of how the neural system performs prediction."
                },
                "authors": [
                    {
                        "name": "Xingsi Dong"
                    },
                    {
                        "name": "Xiangyuan Peng"
                    },
                    {
                        "name": "Si Wu"
                    }
                ],
                "author_detail": {
                    "name": "Si Wu"
                },
                "author": "Si Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13389v2",
                "updated": "2025-05-21T15:36:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    36,
                    51,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-19T17:30:13Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    30,
                    13,
                    0,
                    139,
                    0
                ],
                "title": "Faster Video Diffusion with Trainable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Video Diffusion with Trainable Sparse Attention"
                },
                "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models."
                },
                "authors": [
                    {
                        "name": "Peiyuan Zhang"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Yongqi Chen"
                    },
                    {
                        "name": "Will Lin"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15660v1",
                "updated": "2025-05-21T15:35:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    35,
                    57,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:35:57Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    35,
                    57,
                    2,
                    141,
                    0
                ],
                "title": "Exploring the Limits of Vision-Language-Action Manipulations in\n  Cross-task Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of Vision-Language-Action Manipulations in\n  Cross-task Generalization"
                },
                "summary": "The generalization capabilities of vision-language-action (VLA) models to\nunseen tasks are crucial to achieving general-purpose robotic manipulation in\nopen-world settings. However, the cross-task generalization capabilities of\nexisting VLA models remain significantly underexplored. To address this gap, we\nintroduce AGNOSTOS, a novel simulation benchmark designed to rigorously\nevaluate cross-task zero-shot generalization in manipulation. AGNOSTOS\ncomprises 23 unseen manipulation tasks for testing, distinct from common\ntraining task distributions, and incorporates two levels of generalization\ndifficulty to assess robustness. Our systematic evaluation reveals that current\nVLA models, despite being trained on diverse datasets, struggle to generalize\neffectively to these unseen tasks. To overcome this limitation, we propose\nCross-Task In-Context Manipulation (X-ICM), a method that conditions large\nlanguage models (LLMs) on in-context demonstrations from seen tasks to predict\naction sequences for unseen tasks. Additionally, we introduce a dynamics-guided\nsample selection strategy that identifies relevant demonstrations by capturing\ncross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task\nzero-shot generalization performance over leading VLAs. We believe AGNOSTOS and\nX-ICM will serve as valuable tools for advancing general-purpose robotic\nmanipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generalization capabilities of vision-language-action (VLA) models to\nunseen tasks are crucial to achieving general-purpose robotic manipulation in\nopen-world settings. However, the cross-task generalization capabilities of\nexisting VLA models remain significantly underexplored. To address this gap, we\nintroduce AGNOSTOS, a novel simulation benchmark designed to rigorously\nevaluate cross-task zero-shot generalization in manipulation. AGNOSTOS\ncomprises 23 unseen manipulation tasks for testing, distinct from common\ntraining task distributions, and incorporates two levels of generalization\ndifficulty to assess robustness. Our systematic evaluation reveals that current\nVLA models, despite being trained on diverse datasets, struggle to generalize\neffectively to these unseen tasks. To overcome this limitation, we propose\nCross-Task In-Context Manipulation (X-ICM), a method that conditions large\nlanguage models (LLMs) on in-context demonstrations from seen tasks to predict\naction sequences for unseen tasks. Additionally, we introduce a dynamics-guided\nsample selection strategy that identifies relevant demonstrations by capturing\ncross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task\nzero-shot generalization performance over leading VLAs. We believe AGNOSTOS and\nX-ICM will serve as valuable tools for advancing general-purpose robotic\nmanipulation."
                },
                "authors": [
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Jiayi Liu"
                    },
                    {
                        "name": "Teli Ma"
                    },
                    {
                        "name": "Zifang Wang"
                    },
                    {
                        "name": "Ronghe Qiu"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Zhilin Zhao"
                    },
                    {
                        "name": "Junwei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Liang"
                },
                "author": "Junwei Liang",
                "arxiv_comment": "Project Page: https://jiaming-zhou.github.io/AGNOSTOS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10900v2",
                "updated": "2025-05-21T15:33:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    33,
                    20,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-16T06:07:19Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    6,
                    7,
                    19,
                    4,
                    136,
                    0
                ],
                "title": "Explain What You Mean: Intent Augmented Knowledge Graph Recommender\n  Built With An LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explain What You Mean: Intent Augmented Knowledge Graph Recommender\n  Built With An LLM"
                },
                "summary": "Interaction sparsity is a long-standing challenge in recommendation systems.\nSparsity manifests in environments with disproportional cardinality of\ngroupings of entities, such as users and products in an online marketplace. It\nis also found for newly introduced entities, described as the cold-start\nproblem. Recent efforts to mitigate this issue either enrich the connectivity\ndata by incorporating social networks or external knowledge graphs, or\nfine-tune LLMs into interaction augmenters or next-item recommenders. However,\nthese techniques tend to be resource demanding, requiring high computational\npower. They also have several limitations, including data availability, low\nquality, or synthetic noise issues. In this work, we propose LLM-based Intent\nKnowledge Graph Recommender (IKGR), a novel framework that leverages\nretrieval-augmented generation and an encoding approach to construct and\ndensify a knowledge graph. IKGR leverages latent user-item affinities from an\ninteraction knowledge graph and further densifies it through mutual intent\nconnectivity. This addresses sparsity issues and allows the model to make\nintent-grounded recommendations with an interpretable embedding translation\nlayer. Through extensive experiments on real-world datasets, we demonstrate\nthat IKGR overcomes knowledge gaps and achieves substantial gains over\nstate-of-the-art baselines on both publicly available and our internal\nrecommendation datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction sparsity is a long-standing challenge in recommendation systems.\nSparsity manifests in environments with disproportional cardinality of\ngroupings of entities, such as users and products in an online marketplace. It\nis also found for newly introduced entities, described as the cold-start\nproblem. Recent efforts to mitigate this issue either enrich the connectivity\ndata by incorporating social networks or external knowledge graphs, or\nfine-tune LLMs into interaction augmenters or next-item recommenders. However,\nthese techniques tend to be resource demanding, requiring high computational\npower. They also have several limitations, including data availability, low\nquality, or synthetic noise issues. In this work, we propose LLM-based Intent\nKnowledge Graph Recommender (IKGR), a novel framework that leverages\nretrieval-augmented generation and an encoding approach to construct and\ndensify a knowledge graph. IKGR leverages latent user-item affinities from an\ninteraction knowledge graph and further densifies it through mutual intent\nconnectivity. This addresses sparsity issues and allows the model to make\nintent-grounded recommendations with an interpretable embedding translation\nlayer. Through extensive experiments on real-world datasets, we demonstrate\nthat IKGR overcomes knowledge gaps and achieves substantial gains over\nstate-of-the-art baselines on both publicly available and our internal\nrecommendation datasets."
                },
                "authors": [
                    {
                        "name": "Wenqing Zheng"
                    },
                    {
                        "name": "Noah Fatsi"
                    },
                    {
                        "name": "Daniel Barcklow"
                    },
                    {
                        "name": "Dmitri Kalaev"
                    },
                    {
                        "name": "Steven Yao"
                    },
                    {
                        "name": "Owen Reinert"
                    },
                    {
                        "name": "C. Bayan Bruss"
                    },
                    {
                        "name": "Daniele Rosa"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Rosa"
                },
                "author": "Daniele Rosa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15656v1",
                "updated": "2025-05-21T15:32:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    32,
                    14,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:32:14Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    32,
                    14,
                    2,
                    141,
                    0
                ],
                "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!"
                },
                "summary": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction."
                },
                "authors": [
                    {
                        "name": "Zhexin Zhang"
                    },
                    {
                        "name": "Yuhao Sun"
                    },
                    {
                        "name": "Junxiao Yang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15653v1",
                "updated": "2025-05-21T15:31:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    31,
                    0,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:31:00Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    31,
                    0,
                    2,
                    141,
                    0
                ],
                "title": "Quantifying structural uncertainty in chemical reaction network\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying structural uncertainty in chemical reaction network\n  inference"
                },
                "summary": "Dynamical systems in chemistry and biology are complex, and one often does\nnot have comprehensive knowledge about the interactions involved. Chemical\nreaction network (CRN) inference aims to identify, from observing species\nconcentrations over time, the unknown reactions between the species. Existing\napproaches largely focus on identifying a single, most likely CRN, without\naddressing uncertainty about the resulting network structure. However, it is\nimportant to quantify structural uncertainty to have confidence in our\ninference and predictions. In this work, we do so by constructing an\napproximate posterior distribution over CRN structures. This is done by keeping\na large set of suboptimal solutions found in an optimisation framework with\nsparse regularisation, in contrast to existing optimisation approaches which\ndiscard suboptimal solutions. We find that inducing reaction sparsity with\nnonconvex penalty functions results in more parsimonious CRNs compared to the\npopular lasso regularisation. In a real-data example where multiple CRNs have\nbeen previously suggested, we simultaneously recover reactions proposed from\ndifferent literature under our approach to structural uncertainty. We\ndemonstrate how posterior correlations between reactions help identify where\nstructural ambiguities are present. This can be translated into alternative\nreaction pathways suggested by the available data, thus guiding the efforts of\nfuture experimental design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamical systems in chemistry and biology are complex, and one often does\nnot have comprehensive knowledge about the interactions involved. Chemical\nreaction network (CRN) inference aims to identify, from observing species\nconcentrations over time, the unknown reactions between the species. Existing\napproaches largely focus on identifying a single, most likely CRN, without\naddressing uncertainty about the resulting network structure. However, it is\nimportant to quantify structural uncertainty to have confidence in our\ninference and predictions. In this work, we do so by constructing an\napproximate posterior distribution over CRN structures. This is done by keeping\na large set of suboptimal solutions found in an optimisation framework with\nsparse regularisation, in contrast to existing optimisation approaches which\ndiscard suboptimal solutions. We find that inducing reaction sparsity with\nnonconvex penalty functions results in more parsimonious CRNs compared to the\npopular lasso regularisation. In a real-data example where multiple CRNs have\nbeen previously suggested, we simultaneously recover reactions proposed from\ndifferent literature under our approach to structural uncertainty. We\ndemonstrate how posterior correlations between reactions help identify where\nstructural ambiguities are present. This can be translated into alternative\nreaction pathways suggested by the available data, thus guiding the efforts of\nfuture experimental design."
                },
                "authors": [
                    {
                        "name": "Yong See Foo"
                    },
                    {
                        "name": "Adriana Zanca"
                    },
                    {
                        "name": "Jennifer A. Flegg"
                    },
                    {
                        "name": "Ivo Siekmann"
                    }
                ],
                "author_detail": {
                    "name": "Ivo Siekmann"
                },
                "author": "Ivo Siekmann",
                "arxiv_comment": "34 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "34A55, 62F07, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11741v2",
                "updated": "2025-05-21T15:21:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    21,
                    35,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-17T12:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    12,
                    28,
                    11,
                    0,
                    48,
                    0
                ],
                "title": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL"
                },
                "summary": "Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1."
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Ripeng Li"
                    },
                    {
                        "name": "Zhonghong Ou"
                    },
                    {
                        "name": "Jiangfeng Sun"
                    },
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Xiaoran Shang"
                    },
                    {
                        "name": "Meina Song"
                    },
                    {
                        "name": "Yifan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Zhu"
                },
                "author": "Yifan Zhu",
                "arxiv_comment": "28 pages,12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.12005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.12005v2",
                "updated": "2025-05-21T15:21:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    21,
                    29,
                    2,
                    141,
                    0
                ],
                "published": "2022-03-22T19:23:27Z",
                "published_parsed": [
                    2022,
                    3,
                    22,
                    19,
                    23,
                    27,
                    1,
                    81,
                    0
                ],
                "title": "Sequential Bayesian Registration for Functional Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Bayesian Registration for Functional Data"
                },
                "summary": "In many modern applications, discretely-observed data may be naturally\nunderstood as a set of functions. Functional data often exhibit two confounded\nsources of variability: amplitude (y-axis) and phase (x-axis). The extraction\nof amplitude and phase, a process known as registration, is essential in\nexploring the underlying structure of functional data in a variety of areas,\nfrom environmental monitoring to medical imaging. Critically, such data are\noften gathered sequentially with new functional observations arriving over\ntime. Despite this, existing registration procedures do not sequentially update\ninference based on the new data, requiring model refitting. To address these\nchallenges, we introduce a Bayesian framework for sequential registration of\nfunctional data, which updates statistical inference as new sets of functions\nare assimilated. This Bayesian model-based sequential learning approach\nutilizes sequential Monte Carlo sampling to recursively update the alignment of\nobserved functions while accounting for associated uncertainty. Distributed\ncomputing significantly reduces computational cost relative to refitting the\nmodel using an iterative method such as Markov chain Monte Carlo on the full\ndata. Simulation studies and comparisons reveal that the proposed approach\nperforms well even when the target posterior distribution has a challenging\nstructure. We apply the proposed method to three real datasets: (1) functions\nof annual drought intensity near Kaweah River in California, (2) annual sea\nsurface salinity functions near Null Island, and (3) a sequence of repeated\npatterns in electrocardiogram signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many modern applications, discretely-observed data may be naturally\nunderstood as a set of functions. Functional data often exhibit two confounded\nsources of variability: amplitude (y-axis) and phase (x-axis). The extraction\nof amplitude and phase, a process known as registration, is essential in\nexploring the underlying structure of functional data in a variety of areas,\nfrom environmental monitoring to medical imaging. Critically, such data are\noften gathered sequentially with new functional observations arriving over\ntime. Despite this, existing registration procedures do not sequentially update\ninference based on the new data, requiring model refitting. To address these\nchallenges, we introduce a Bayesian framework for sequential registration of\nfunctional data, which updates statistical inference as new sets of functions\nare assimilated. This Bayesian model-based sequential learning approach\nutilizes sequential Monte Carlo sampling to recursively update the alignment of\nobserved functions while accounting for associated uncertainty. Distributed\ncomputing significantly reduces computational cost relative to refitting the\nmodel using an iterative method such as Markov chain Monte Carlo on the full\ndata. Simulation studies and comparisons reveal that the proposed approach\nperforms well even when the target posterior distribution has a challenging\nstructure. We apply the proposed method to three real datasets: (1) functions\nof annual drought intensity near Kaweah River in California, (2) annual sea\nsurface salinity functions near Null Island, and (3) a sequence of repeated\npatterns in electrocardiogram signals."
                },
                "authors": [
                    {
                        "name": "Yoonji Kim"
                    },
                    {
                        "name": "Oksana A. Chkrebtii"
                    },
                    {
                        "name": "Sebastian A. Kurtek"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian A. Kurtek"
                },
                "author": "Sebastian A. Kurtek",
                "arxiv_doi": "10.1007/s11222-025-10640-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11222-025-10640-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2203.12005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.12005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15634v1",
                "updated": "2025-05-21T15:17:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    17,
                    59,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:17:59Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    17,
                    59,
                    2,
                    141,
                    0
                ],
                "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning\n  in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning\n  in Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Yuzhe Yang"
                    },
                    {
                        "name": "Ziyu Yao"
                    },
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15623v1",
                "updated": "2025-05-21T15:12:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    12,
                    20,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:12:20Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    12,
                    20,
                    2,
                    141,
                    0
                ],
                "title": "Can LLMs $\\textit{understand}$ Math? -- Exploring the Pitfalls in\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs $\\textit{understand}$ Math? -- Exploring the Pitfalls in\n  Mathematical Reasoning"
                },
                "summary": "Large language models (LLMs) demonstrate considerable potential in various\nnatural language tasks but face significant challenges in mathematical\nreasoning, particularly in executing precise, multi-step logic. However,\ncurrent evaluation frameworks judge their performance solely based on accuracy,\nwhich only accounts for the final answer. This study explores these pitfalls by\nemploying a novel evaluation framework. We propose an evaluation metric called\nthe MAPLE score, which holistically quantifies reasoning misalignment by\nintegrating error rates, redundancy, and validity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate considerable potential in various\nnatural language tasks but face significant challenges in mathematical\nreasoning, particularly in executing precise, multi-step logic. However,\ncurrent evaluation frameworks judge their performance solely based on accuracy,\nwhich only accounts for the final answer. This study explores these pitfalls by\nemploying a novel evaluation framework. We propose an evaluation metric called\nthe MAPLE score, which holistically quantifies reasoning misalignment by\nintegrating error rates, redundancy, and validity."
                },
                "authors": [
                    {
                        "name": "Tiasa Singha Roy"
                    },
                    {
                        "name": "Aditeya Baral"
                    },
                    {
                        "name": "Ayush Rajesh Jhaveri"
                    },
                    {
                        "name": "Yusuf Baig"
                    }
                ],
                "author_detail": {
                    "name": "Yusuf Baig"
                },
                "author": "Yusuf Baig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15622v1",
                "updated": "2025-05-21T15:12:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    12,
                    14,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:12:14Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    12,
                    14,
                    2,
                    141,
                    0
                ],
                "title": "Benchmarking Energy and Latency in TinyML: A Novel Method for\n  Resource-Constrained AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Energy and Latency in TinyML: A Novel Method for\n  Resource-Constrained AI"
                },
                "summary": "The rise of IoT has increased the need for on-edge machine learning, with\nTinyML emerging as a promising solution for resource-constrained devices such\nas MCU. However, evaluating their performance remains challenging due to\ndiverse architectures and application scenarios. Current solutions have many\nnon-negligible limitations. This work introduces an alternative benchmarking\nmethodology that integrates energy and latency measurements while\ndistinguishing three execution phases pre-inference, inference, and\npost-inference. Additionally, the setup ensures that the device operates\nwithout being powered by an external measurement unit, while automated testing\ncan be leveraged to enhance statistical significance. To evaluate our setup, we\ntested the STM32N6 MCU, which includes a NPU for executing neural networks. Two\nconfigurations were considered: high-performance and Low-power. The variation\nof the EDP was analyzed separately for each phase, providing insights into the\nimpact of hardware configurations on energy efficiency. Each model was tested\n1000 times to ensure statistically relevant results. Our findings demonstrate\nthat reducing the core voltage and clock frequency improve the efficiency of\npre- and post-processing without significantly affecting network execution\nperformance. This approach can also be used for cross-platform comparisons to\ndetermine the most efficient inference platform and to quantify how pre- and\npost-processing overhead varies across different hardware implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of IoT has increased the need for on-edge machine learning, with\nTinyML emerging as a promising solution for resource-constrained devices such\nas MCU. However, evaluating their performance remains challenging due to\ndiverse architectures and application scenarios. Current solutions have many\nnon-negligible limitations. This work introduces an alternative benchmarking\nmethodology that integrates energy and latency measurements while\ndistinguishing three execution phases pre-inference, inference, and\npost-inference. Additionally, the setup ensures that the device operates\nwithout being powered by an external measurement unit, while automated testing\ncan be leveraged to enhance statistical significance. To evaluate our setup, we\ntested the STM32N6 MCU, which includes a NPU for executing neural networks. Two\nconfigurations were considered: high-performance and Low-power. The variation\nof the EDP was analyzed separately for each phase, providing insights into the\nimpact of hardware configurations on energy efficiency. Each model was tested\n1000 times to ensure statistically relevant results. Our findings demonstrate\nthat reducing the core voltage and clock frequency improve the efficiency of\npre- and post-processing without significantly affecting network execution\nperformance. This approach can also be used for cross-platform comparisons to\ndetermine the most efficient inference platform and to quantify how pre- and\npost-processing overhead varies across different hardware implementations."
                },
                "authors": [
                    {
                        "name": "Pietro Bartoli"
                    },
                    {
                        "name": "Christian Veronesi"
                    },
                    {
                        "name": "Andrea Giudici"
                    },
                    {
                        "name": "David Siorpaes"
                    },
                    {
                        "name": "Diana Trojaniello"
                    },
                    {
                        "name": "Franco Zappa"
                    }
                ],
                "author_detail": {
                    "name": "Franco Zappa"
                },
                "author": "Franco Zappa",
                "arxiv_comment": "8 pages, 6 figures The article is already accepted for International\n  Joint Conference on Neural Networks (IJCNN) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15621v1",
                "updated": "2025-05-21T15:11:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    11,
                    26,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:11:26Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    11,
                    26,
                    2,
                    141,
                    0
                ],
                "title": "DS-Bench: A Realistic Benchmark for Data Science Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DS-Bench: A Realistic Benchmark for Data Science Code Generation"
                },
                "summary": "We introduce DS-bench, a new benchmark designed to evaluate large language\nmodels (LLMs) on complicated and realistic data science code generation tasks.\nDS-bench consists of 1,000 carefully constructed problems sourced from\nrealistic problems from GitHub across ten widely used Python data science\nlibraries. Compared to the current state-of-the-art benchmark DS-1000, DS-bench\noffers a more challenging and representative testbed, longer code solutions,\nmore comprehensive data science libraries, clearer and better structured\nproblem descriptions, and stronger test suites. To construct the DS-bench, we\ndevelop a robust pipeline that combines task scope selection, code\nconstruction, test case generation, and problem description synthesis. The\nprocess is paired with rigorous manual editing to ensure alignment and enhance\nevaluation reliability. Experimental result shows that DS-bench exhibits robust\nscaling behavior, where larger models systematically outperform smaller ones,\nvalidating its ability to distinguish model capabilities. The best LLM we test,\nGPT-4o, has a pass@1 of 0.202, indicating that LLMs still have a large room to\nimprove for realistic data science code generation tasks. We believe DS-bench\nwill serve as a rigorous and trustworthy foundation for advancing LLM-based\ndata science programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DS-bench, a new benchmark designed to evaluate large language\nmodels (LLMs) on complicated and realistic data science code generation tasks.\nDS-bench consists of 1,000 carefully constructed problems sourced from\nrealistic problems from GitHub across ten widely used Python data science\nlibraries. Compared to the current state-of-the-art benchmark DS-1000, DS-bench\noffers a more challenging and representative testbed, longer code solutions,\nmore comprehensive data science libraries, clearer and better structured\nproblem descriptions, and stronger test suites. To construct the DS-bench, we\ndevelop a robust pipeline that combines task scope selection, code\nconstruction, test case generation, and problem description synthesis. The\nprocess is paired with rigorous manual editing to ensure alignment and enhance\nevaluation reliability. Experimental result shows that DS-bench exhibits robust\nscaling behavior, where larger models systematically outperform smaller ones,\nvalidating its ability to distinguish model capabilities. The best LLM we test,\nGPT-4o, has a pass@1 of 0.202, indicating that LLMs still have a large room to\nimprove for realistic data science code generation tasks. We believe DS-bench\nwill serve as a rigorous and trustworthy foundation for advancing LLM-based\ndata science programming."
                },
                "authors": [
                    {
                        "name": "Shuyin Ouyang"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Jingwen Guo"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Qihao Zhu"
                    },
                    {
                        "name": "Jie M. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie M. Zhang"
                },
                "author": "Jie M. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15607v1",
                "updated": "2025-05-21T15:00:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    0,
                    7,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:00:07Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    0,
                    7,
                    2,
                    141,
                    0
                ],
                "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with\n  Pedagogy using Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with\n  Pedagogy using Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) can transform education, but their optimization\nfor direct question-answering often undermines effective pedagogy which\nrequires strategically withholding answers. To mitigate this, we propose an\nonline reinforcement learning (RL)-based alignment framework that can quickly\nadapt LLMs into effective tutors using simulated student-tutor interactions by\nemphasizing pedagogical quality and guided problem-solving over simply giving\naway answers. We use our method to train a 7B parameter tutor model without\nhuman annotations which reaches similar performance to larger proprietary\nmodels like LearnLM. We introduce a controllable reward weighting to balance\npedagogical support and student solving accuracy, allowing us to trace the\nPareto frontier between these two objectives. Our models better preserve\nreasoning capabilities than single-turn SFT baselines and can optionally\nenhance interpretability through thinking tags that expose the model's\ninstructional planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can transform education, but their optimization\nfor direct question-answering often undermines effective pedagogy which\nrequires strategically withholding answers. To mitigate this, we propose an\nonline reinforcement learning (RL)-based alignment framework that can quickly\nadapt LLMs into effective tutors using simulated student-tutor interactions by\nemphasizing pedagogical quality and guided problem-solving over simply giving\naway answers. We use our method to train a 7B parameter tutor model without\nhuman annotations which reaches similar performance to larger proprietary\nmodels like LearnLM. We introduce a controllable reward weighting to balance\npedagogical support and student solving accuracy, allowing us to trace the\nPareto frontier between these two objectives. Our models better preserve\nreasoning capabilities than single-turn SFT baselines and can optionally\nenhance interpretability through thinking tags that expose the model's\ninstructional planning."
                },
                "authors": [
                    {
                        "name": "David Dinucu-Jianu"
                    },
                    {
                        "name": "Jakub Macina"
                    },
                    {
                        "name": "Nico Daheim"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "David Dinucu-Jianu and Jakub Macina contributed equally. Code\n  available: https://github.com/eth-lre/PedagogicalRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13057v2",
                "updated": "2025-05-21T14:59:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    59,
                    5,
                    2,
                    141,
                    0
                ],
                "published": "2025-04-17T16:11:42Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    11,
                    42,
                    3,
                    107,
                    0
                ],
                "title": "Covariate balancing estimation and model selection for\n  difference-in-differences approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariate balancing estimation and model selection for\n  difference-in-differences approach"
                },
                "summary": "Remarkable progress has been made in difference-in-differences (DID)\napproaches to causal inference that estimate the average effect of a treatment\non the treated (ATT). Of these, the semiparametric DID (SDID) approach\nincorporates a propensity score analysis into the DID setup. Supposing that the\nATT is a function of covariates, we estimate it by weighting the inverse of the\npropensity score. In this study, as one way to make the estimation robust to\nthe propensity score modeling, we incorporate covariate balancing. Then, by\nattentively constructing the moment conditions used in the covariate balancing,\nwe show that the proposed estimator is doubly robust. In addition to the\nestimation, we also address model selection. In practice, covariate selection\nis an essential task in statistical analysis, but even in the basic setting of\nthe SDID approach, there are no reasonable information criteria. Here, we\nderive a model selection criterion as an asymptotically bias-corrected\nestimator of risk based on the loss function used in the SDID estimation. We\nshow that a penalty term can be derived that is considerably different from\nalmost twice the number of parameters that often appears in AIC-type\ninformation criteria. Numerical experiments show that the proposed method\nestimates the ATT more robustly compared with the method using propensity\nscores given by maximum likelihood estimation, and that the proposed criterion\nclearly reduces the risk targeted in the SDID approach in comparison with the\nintuitive generalization of the existing information criterion. In addition,\nreal data analysis confirms that there is a large difference between the\nresults of the proposed method and those of the existing method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remarkable progress has been made in difference-in-differences (DID)\napproaches to causal inference that estimate the average effect of a treatment\non the treated (ATT). Of these, the semiparametric DID (SDID) approach\nincorporates a propensity score analysis into the DID setup. Supposing that the\nATT is a function of covariates, we estimate it by weighting the inverse of the\npropensity score. In this study, as one way to make the estimation robust to\nthe propensity score modeling, we incorporate covariate balancing. Then, by\nattentively constructing the moment conditions used in the covariate balancing,\nwe show that the proposed estimator is doubly robust. In addition to the\nestimation, we also address model selection. In practice, covariate selection\nis an essential task in statistical analysis, but even in the basic setting of\nthe SDID approach, there are no reasonable information criteria. Here, we\nderive a model selection criterion as an asymptotically bias-corrected\nestimator of risk based on the loss function used in the SDID estimation. We\nshow that a penalty term can be derived that is considerably different from\nalmost twice the number of parameters that often appears in AIC-type\ninformation criteria. Numerical experiments show that the proposed method\nestimates the ATT more robustly compared with the method using propensity\nscores given by maximum likelihood estimation, and that the proposed criterion\nclearly reduces the risk targeted in the SDID approach in comparison with the\nintuitive generalization of the existing information criterion. In addition,\nreal data analysis confirms that there is a large difference between the\nresults of the proposed method and those of the existing method."
                },
                "authors": [
                    {
                        "name": "Takamichi Baba"
                    },
                    {
                        "name": "Yoshiyuki Ninomiya"
                    }
                ],
                "author_detail": {
                    "name": "Yoshiyuki Ninomiya"
                },
                "author": "Yoshiyuki Ninomiya",
                "arxiv_comment": "26 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10259v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10259v3",
                "updated": "2025-05-21T14:54:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    54,
                    38,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-15T13:10:31Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    31,
                    3,
                    135,
                    0
                ],
                "title": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on\n  Resource-Constrained Devices"
                },
                "summary": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload-public .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload-public ."
                },
                "authors": [
                    {
                        "name": "Xiangwen Zhuge"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Fan Dang"
                    },
                    {
                        "name": "Xuan Ding"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Yahui Han"
                    },
                    {
                        "name": "Tianxiang Hao"
                    },
                    {
                        "name": "Zheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yang"
                },
                "author": "Zheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10259v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10259v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15596v1",
                "updated": "2025-05-21T14:50:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    50,
                    30,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:50:30Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    50,
                    30,
                    2,
                    141,
                    0
                ],
                "title": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching\n  Assistants Evaluate and Envision Its Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching\n  Assistants Evaluate and Envision Its Use"
                },
                "summary": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback."
                },
                "authors": [
                    {
                        "name": "Xinyi Lu"
                    },
                    {
                        "name": "Aditya Mahesh"
                    },
                    {
                        "name": "Zejia Shen"
                    },
                    {
                        "name": "Mitchell Dudley"
                    },
                    {
                        "name": "Larissa Sano"
                    },
                    {
                        "name": "Xu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Wang"
                },
                "author": "Xu Wang",
                "arxiv_comment": "To be published in AIED'2025: In Proceedings of the 26th\n  International Conference on Artificial Intelligence in Education. The system\n  prompt and example feedback can be found through\n  http://github.com/UM-Lifelong-Learning-Lab/AIED2025-Exploring-LLM-Generated-Feedback-for-Economics-Essay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15594v1",
                "updated": "2025-05-21T14:49:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    49,
                    24,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:49:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    49,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for\n  Security-Utility Trade off",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for\n  Security-Utility Trade off"
                },
                "summary": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed."
                },
                "authors": [
                    {
                        "name": "Yury Belousov"
                    },
                    {
                        "name": "Brian Pulfer"
                    },
                    {
                        "name": "Vitaliy Kinakh"
                    },
                    {
                        "name": "Slava Voloshynovskiy"
                    }
                ],
                "author_detail": {
                    "name": "Slava Voloshynovskiy"
                },
                "author": "Slava Voloshynovskiy",
                "arxiv_comment": "Paper accepted at the 33rd European Signal Processing Conference\n  (EUSIPCO 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14590v2",
                "updated": "2025-05-21T14:48:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    48,
                    40,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T16:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    41,
                    45,
                    1,
                    140,
                    0
                ],
                "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol"
                },
                "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13615v2",
                "updated": "2025-05-21T14:45:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    45,
                    52,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-19T18:01:25Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    18,
                    1,
                    25,
                    0,
                    139,
                    0
                ],
                "title": "Predicting mosquito flight behavior using Bayesian dynamical systems\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting mosquito flight behavior using Bayesian dynamical systems\n  learning"
                },
                "summary": "Mosquito-borne diseases cause several hundred thousand deaths every year.\nDeciphering mosquito host-seeking behavior is essential to prevent disease\ntransmission through mosquito capture and surveillance. Despite recent\nsubstantial progress, we currently lack a comprehensive quantitative\nunderstanding of how visual and other sensory cues guide mosquitoes to their\ntargets. Here, we combined 3D infrared tracking of Aedes aegypti mosquitoes\nwith Bayesian dynamical systems inference to learn a quantitative biophysical\nmodel of mosquito host-seeking behavior. Trained on more than 20,000,000 data\npoints from mosquito free-flight trajectories recorded in the presence of\nvisual and carbon dioxide cues, the model accurately predicts how mosquitoes\nrespond to human targets. Our results provide a quantitative foundation for\noptimizing mosquito capture and control strategies, a key step towards\nmitigating the impact of mosquito-borne diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mosquito-borne diseases cause several hundred thousand deaths every year.\nDeciphering mosquito host-seeking behavior is essential to prevent disease\ntransmission through mosquito capture and surveillance. Despite recent\nsubstantial progress, we currently lack a comprehensive quantitative\nunderstanding of how visual and other sensory cues guide mosquitoes to their\ntargets. Here, we combined 3D infrared tracking of Aedes aegypti mosquitoes\nwith Bayesian dynamical systems inference to learn a quantitative biophysical\nmodel of mosquito host-seeking behavior. Trained on more than 20,000,000 data\npoints from mosquito free-flight trajectories recorded in the presence of\nvisual and carbon dioxide cues, the model accurately predicts how mosquitoes\nrespond to human targets. Our results provide a quantitative foundation for\noptimizing mosquito capture and control strategies, a key step towards\nmitigating the impact of mosquito-borne diseases."
                },
                "authors": [
                    {
                        "name": "Christopher Zuo"
                    },
                    {
                        "name": "Chenyi Fei"
                    },
                    {
                        "name": "Alexander E. Cohen"
                    },
                    {
                        "name": "Soohwan Kim"
                    },
                    {
                        "name": "Ring T. Carde"
                    },
                    {
                        "name": "Jörn Dunkel"
                    },
                    {
                        "name": "David L. Hu"
                    }
                ],
                "author_detail": {
                    "name": "David L. Hu"
                },
                "author": "David L. Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12719v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12719v3",
                "updated": "2025-05-21T14:45:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    45,
                    38,
                    2,
                    141,
                    0
                ],
                "published": "2024-06-18T15:41:15Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    15,
                    41,
                    15,
                    1,
                    170,
                    0
                ],
                "title": "Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis"
                },
                "summary": "Large Language Models (LLMs), already shown to ace various text comprehension\ntasks, have also remarkably been shown to tackle table comprehension tasks\nwithout specific training. Building on earlier studies of LLMs for tabular\ntasks, we probe how in-context learning (ICL), model scale, instruction tuning,\nand domain bias affect Tabular QA (TQA) robustness by testing LLMs, under\ndiverse augmentations and perturbations, on diverse domains: Wikipedia-based\n$\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$, and scientific $\\textbf{SCITAB}$.\nAlthough instruction tuning and larger, newer LLMs deliver stronger, more\nrobust TQA performance, data contamination and reliability issues, especially\non $\\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and the drops in performance, with sensitivity peaking in the\nmodel's middle layers. We highlight the need for improved interpretable\nmethodologies to develop more reliable LLMs for table comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), already shown to ace various text comprehension\ntasks, have also remarkably been shown to tackle table comprehension tasks\nwithout specific training. Building on earlier studies of LLMs for tabular\ntasks, we probe how in-context learning (ICL), model scale, instruction tuning,\nand domain bias affect Tabular QA (TQA) robustness by testing LLMs, under\ndiverse augmentations and perturbations, on diverse domains: Wikipedia-based\n$\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$, and scientific $\\textbf{SCITAB}$.\nAlthough instruction tuning and larger, newer LLMs deliver stronger, more\nrobust TQA performance, data contamination and reliability issues, especially\non $\\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and the drops in performance, with sensitivity peaking in the\nmodel's middle layers. We highlight the need for improved interpretable\nmethodologies to develop more reliable LLMs for table comprehension."
                },
                "authors": [
                    {
                        "name": "Kushal Raj Bhandari"
                    },
                    {
                        "name": "Sixue Xing"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Jianxi Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Gao"
                },
                "author": "Jianxi Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12719v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12719v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01324v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01324v5",
                "updated": "2025-05-21T14:45:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    45,
                    31,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-02T14:55:34Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    55,
                    34,
                    4,
                    122,
                    0
                ],
                "title": "Design-Based Inference under Random Potential Outcomes via Riesz\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design-Based Inference under Random Potential Outcomes via Riesz\n  Representation"
                },
                "summary": "We introduce a design-based framework for causal inference that accommodates\nrandom potential outcomes, thereby extending the classical Neyman-Rubin model\nin which outcomes are treated as fixed. Each unit's potential outcome is\nmodelled as a structural mapping $\\tilde{y}_i(z, \\omega)$, where $z$ denotes\nthe treatment assignment and \\(\\omega\\) represents latent outcome-level\nrandomness. Inspired by recent connections between design-based inference and\nthe Riesz representation theorem, we embed potential outcomes in a Hilbert\nspace and define treatment effects as linear functionals, yielding estimators\nconstructed via their Riesz representers. This approach preserves the core\nidentification logic of randomised assignment while enabling valid inference\nunder stochastic outcome variation. We establish large-sample properties under\nlocal dependence and develop consistent variance estimators that remain valid\nunder weaker structural assumptions, including partially known dependence. A\nsimulation study illustrates the robustness and finite-sample behaviour of the\nestimators. Overall, the framework unifies design-based reasoning with\nstochastic outcome modelling, broadening the scope of causal inference in\ncomplex experimental settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a design-based framework for causal inference that accommodates\nrandom potential outcomes, thereby extending the classical Neyman-Rubin model\nin which outcomes are treated as fixed. Each unit's potential outcome is\nmodelled as a structural mapping $\\tilde{y}_i(z, \\omega)$, where $z$ denotes\nthe treatment assignment and \\(\\omega\\) represents latent outcome-level\nrandomness. Inspired by recent connections between design-based inference and\nthe Riesz representation theorem, we embed potential outcomes in a Hilbert\nspace and define treatment effects as linear functionals, yielding estimators\nconstructed via their Riesz representers. This approach preserves the core\nidentification logic of randomised assignment while enabling valid inference\nunder stochastic outcome variation. We establish large-sample properties under\nlocal dependence and develop consistent variance estimators that remain valid\nunder weaker structural assumptions, including partially known dependence. A\nsimulation study illustrates the robustness and finite-sample behaviour of the\nestimators. Overall, the framework unifies design-based reasoning with\nstochastic outcome modelling, broadening the scope of causal inference in\ncomplex experimental settings."
                },
                "authors": [
                    {
                        "name": "Yukai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yukai Yang"
                },
                "author": "Yukai Yang",
                "arxiv_comment": "42 pages, 2 figures, 2 Tables, 2 Algorithms. Preprint prepared for\n  journal submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01324v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01324v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G20, 62K99, 62D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14846v2",
                "updated": "2025-05-21T14:42:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    42,
                    25,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-20T18:55:30Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    30,
                    3,
                    51,
                    0
                ],
                "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation"
                },
                "summary": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Ajay Patel"
                    },
                    {
                        "name": "Matt Deitke"
                    },
                    {
                        "name": "Tanmay Gupta"
                    },
                    {
                        "name": "Luca Weihs"
                    },
                    {
                        "name": "Andrew Head"
                    },
                    {
                        "name": "Mark Yatskar"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "name": "Christopher Clark"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Clark"
                },
                "author": "Christopher Clark",
                "arxiv_comment": "Published in ACL 2025, project page:\n  https://yueyang1996.github.io/cosyn/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14336v2",
                "updated": "2025-05-21T14:22:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    22,
                    18,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T13:20:55Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    20,
                    55,
                    1,
                    140,
                    0
                ],
                "title": "Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach"
                },
                "summary": "Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness."
                },
                "authors": [
                    {
                        "name": "Umberto Cappellazzo"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Stavros Petridis"
                    },
                    {
                        "name": "Daniele Falavigna"
                    },
                    {
                        "name": "Alessio Brutti"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Brutti"
                },
                "author": "Alessio Brutti",
                "arxiv_comment": "Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15561v1",
                "updated": "2025-05-21T14:18:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    18,
                    1,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:18:01Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    18,
                    1,
                    2,
                    141,
                    0
                ],
                "title": "Do RAG Systems Suffer From Positional Bias?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do RAG Systems Suffer From Positional Bias?"
                },
                "summary": "Retrieval Augmented Generation enhances LLM accuracy by adding passages\nretrieved from an external corpus to the LLM prompt. This paper investigates\nhow positional bias - the tendency of LLMs to weight information differently\nbased on its position in the prompt - affects not only the LLM's capability to\ncapitalize on relevant passages, but also its susceptibility to distracting\npassages. Through extensive experiments on three benchmarks, we show how\nstate-of-the-art retrieval pipelines, while attempting to retrieve relevant\npassages, systematically bring highly distracting ones to the top ranks, with\nover 60% of queries containing at least one highly distracting passage among\nthe top-10 retrieved passages. As a result, the impact of the LLM positional\nbias, which in controlled settings is often reported as very prominent by\nrelated works, is actually marginal in real scenarios since both relevant and\ndistracting passages are, in turn, penalized. Indeed, our findings reveal that\nsophisticated strategies that attempt to rearrange the passages based on LLM\npositional preferences do not perform better than random shuffling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation enhances LLM accuracy by adding passages\nretrieved from an external corpus to the LLM prompt. This paper investigates\nhow positional bias - the tendency of LLMs to weight information differently\nbased on its position in the prompt - affects not only the LLM's capability to\ncapitalize on relevant passages, but also its susceptibility to distracting\npassages. Through extensive experiments on three benchmarks, we show how\nstate-of-the-art retrieval pipelines, while attempting to retrieve relevant\npassages, systematically bring highly distracting ones to the top ranks, with\nover 60% of queries containing at least one highly distracting passage among\nthe top-10 retrieved passages. As a result, the impact of the LLM positional\nbias, which in controlled settings is often reported as very prominent by\nrelated works, is actually marginal in real scenarios since both relevant and\ndistracting passages are, in turn, penalized. Indeed, our findings reveal that\nsophisticated strategies that attempt to rearrange the passages based on LLM\npositional preferences do not perform better than random shuffling."
                },
                "authors": [
                    {
                        "name": "Florin Cuconasu"
                    },
                    {
                        "name": "Simone Filice"
                    },
                    {
                        "name": "Guy Horowitz"
                    },
                    {
                        "name": "Yoelle Maarek"
                    },
                    {
                        "name": "Fabrizio Silvestri"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Silvestri"
                },
                "author": "Fabrizio Silvestri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15557v1",
                "updated": "2025-05-21T14:16:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    16,
                    56,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:16:56Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    16,
                    56,
                    2,
                    141,
                    0
                ],
                "title": "Modular Jump Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Jump Gaussian Processes"
                },
                "summary": "Gaussian processes (GPs) furnish accurate nonlinear predictions with\nwell-calibrated uncertainty. However, the typical GP setup has a built-in\nstationarity assumption, making it ill-suited for modeling data from processes\nwith sudden changes, or \"jumps\" in the output variable. The \"jump GP\" (JGP) was\ndeveloped for modeling data from such processes, combining local GPs and latent\n\"level\" variables under a joint inferential framework. But joint modeling can\nbe fraught with difficulty. We aim to simplify by suggesting a more modular\nsetup, eschewing joint inference but retaining the main JGP themes: (a)\nlearning optimal neighborhood sizes that locally respect manifolds of\ndiscontinuity; and (b) a new cluster-based (latent) feature to capture regions\nof distinct output levels on both sides of the manifold. We show that each of\n(a) and (b) separately leads to dramatic improvements when modeling processes\nwith jumps. In tandem (but without requiring joint inference) that benefit is\ncompounded, as illustrated on real and synthetic benchmark examples from the\nrecent literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian processes (GPs) furnish accurate nonlinear predictions with\nwell-calibrated uncertainty. However, the typical GP setup has a built-in\nstationarity assumption, making it ill-suited for modeling data from processes\nwith sudden changes, or \"jumps\" in the output variable. The \"jump GP\" (JGP) was\ndeveloped for modeling data from such processes, combining local GPs and latent\n\"level\" variables under a joint inferential framework. But joint modeling can\nbe fraught with difficulty. We aim to simplify by suggesting a more modular\nsetup, eschewing joint inference but retaining the main JGP themes: (a)\nlearning optimal neighborhood sizes that locally respect manifolds of\ndiscontinuity; and (b) a new cluster-based (latent) feature to capture regions\nof distinct output levels on both sides of the manifold. We show that each of\n(a) and (b) separately leads to dramatic improvements when modeling processes\nwith jumps. In tandem (but without requiring joint inference) that benefit is\ncompounded, as illustrated on real and synthetic benchmark examples from the\nrecent literature."
                },
                "authors": [
                    {
                        "name": "Anna R. Flowers"
                    },
                    {
                        "name": "Christopher T. Franck"
                    },
                    {
                        "name": "Mickaël Binois"
                    },
                    {
                        "name": "Chiwoo Park"
                    },
                    {
                        "name": "Robert B. Gramacy"
                    }
                ],
                "author_detail": {
                    "name": "Robert B. Gramacy"
                },
                "author": "Robert B. Gramacy",
                "arxiv_comment": "18 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15554v1",
                "updated": "2025-05-21T14:15:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    15,
                    49,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:15:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    15,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "DayDreamer at CQs-Gen 2025: Generating Critical Questions through\n  Argument Scheme Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DayDreamer at CQs-Gen 2025: Generating Critical Questions through\n  Argument Scheme Completion"
                },
                "summary": "Critical questions are essential resources to provoke critical thinking when\nencountering an argumentative text. We present our system for the Critical\nQuestions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach\nleverages large language models (LLMs) with chain-of-thought prompting to\ngenerate critical questions guided by Walton's argumentation schemes. For each\ninput intervention, we conversationally prompt LLMs to instantiate the\ncorresponding argument scheme template to first obtain structured arguments,\nand then generate relevant critical questions. Following this, we rank all the\navailable critical questions by prompting LLMs to select the top 3 most helpful\nquestions based on the original intervention text. This combination of\nstructured argumentation theory and step-by-step reasoning enables the\ngeneration of contextually relevant and diverse critical questions. Our\npipeline achieves competitive performance in the final test set, showing its\npotential to foster critical thinking given argumentative text and detect\nmissing or uninformed claims. Code available at\n\\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical questions are essential resources to provoke critical thinking when\nencountering an argumentative text. We present our system for the Critical\nQuestions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach\nleverages large language models (LLMs) with chain-of-thought prompting to\ngenerate critical questions guided by Walton's argumentation schemes. For each\ninput intervention, we conversationally prompt LLMs to instantiate the\ncorresponding argument scheme template to first obtain structured arguments,\nand then generate relevant critical questions. Following this, we rank all the\navailable critical questions by prompting LLMs to select the top 3 most helpful\nquestions based on the original intervention text. This combination of\nstructured argumentation theory and step-by-step reasoning enables the\ngeneration of contextually relevant and diverse critical questions. Our\npipeline achieves competitive performance in the final test set, showing its\npotential to foster critical thinking given argumentative text and detect\nmissing or uninformed claims. Code available at\n\\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}."
                },
                "authors": [
                    {
                        "name": "Wendi Zhou"
                    },
                    {
                        "name": "Ameer Saadat-Yazdi"
                    },
                    {
                        "name": "Nadin Kökciyan"
                    }
                ],
                "author_detail": {
                    "name": "Nadin Kökciyan"
                },
                "author": "Nadin Kökciyan",
                "arxiv_comment": "ArgMining 2025 CQs-Gen shared task",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15553v2",
                "updated": "2025-05-22T09:39:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    9,
                    39,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T14:14:47Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    14,
                    47,
                    2,
                    141,
                    0
                ],
                "title": "Social Bias in Popular Question-Answering Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Bias in Popular Question-Answering Benchmarks"
                },
                "summary": "Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs."
                },
                "authors": [
                    {
                        "name": "Angelie Kraft"
                    },
                    {
                        "name": "Judith Simon"
                    },
                    {
                        "name": "Sonja Schimmler"
                    }
                ],
                "author_detail": {
                    "name": "Sonja Schimmler"
                },
                "author": "Sonja Schimmler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15548v1",
                "updated": "2025-05-21T14:12:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    12,
                    30,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:12:30Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    12,
                    30,
                    2,
                    141,
                    0
                ],
                "title": "Short-Range Dependency Effects on Transformer Instability and a\n  Decomposed Attention Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short-Range Dependency Effects on Transformer Instability and a\n  Decomposed Attention Solution"
                },
                "summary": "Transformer language models have driven significant progress across various\nfields, including natural language processing and computer vision. A central\ncomponent of these models is the self-attention (SA) mechanism, which learns\nrich vector representations of tokens by modeling their relationships with\nothers in a sequence. However, despite extensive research, transformers\ncontinue to suffer from training instability -- often manifesting as spikes or\ndivergence in the training loss during a run.\n  In this work, we identify one source of this instability: SA's limited\nability to capture short-range dependencies, especially in tasks like language\nmodeling, where almost every token heavily relies on its nearby neighbors. This\nlimitation causes the pre-softmax logits of SA to grow rapidly, destabilizing\ntraining. To address this, we propose decomposing the SA into local\n(short-range) and global (long-range) attention heads. This decomposed\nattention, referred to as Long Short-attention (LS-attention), mitigates logit\nexplosion and results in more stable training compared to an equivalent\nmulti-head self-attention (MHSA). Empirical comparisons with two alternative\ntraining stabilization methods show that LS-attention reduces the validation\nperplexity to nearly 2/5 of that achieved by one method and reaches a similar\nperplexity as the other method using only 1/20 of the GPU hours. Additionally,\nour experiments demonstrate that LS-attention reduces inference latency by up\nto 36% compared to a state-of-the-art implementation of equivalent MHSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer language models have driven significant progress across various\nfields, including natural language processing and computer vision. A central\ncomponent of these models is the self-attention (SA) mechanism, which learns\nrich vector representations of tokens by modeling their relationships with\nothers in a sequence. However, despite extensive research, transformers\ncontinue to suffer from training instability -- often manifesting as spikes or\ndivergence in the training loss during a run.\n  In this work, we identify one source of this instability: SA's limited\nability to capture short-range dependencies, especially in tasks like language\nmodeling, where almost every token heavily relies on its nearby neighbors. This\nlimitation causes the pre-softmax logits of SA to grow rapidly, destabilizing\ntraining. To address this, we propose decomposing the SA into local\n(short-range) and global (long-range) attention heads. This decomposed\nattention, referred to as Long Short-attention (LS-attention), mitigates logit\nexplosion and results in more stable training compared to an equivalent\nmulti-head self-attention (MHSA). Empirical comparisons with two alternative\ntraining stabilization methods show that LS-attention reduces the validation\nperplexity to nearly 2/5 of that achieved by one method and reaches a similar\nperplexity as the other method using only 1/20 of the GPU hours. Additionally,\nour experiments demonstrate that LS-attention reduces inference latency by up\nto 36% compared to a state-of-the-art implementation of equivalent MHSA."
                },
                "authors": [
                    {
                        "name": "Suvadeep Hajra"
                    }
                ],
                "author_detail": {
                    "name": "Suvadeep Hajra"
                },
                "author": "Suvadeep Hajra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13178v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13178v4",
                "updated": "2025-05-21T14:11:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    11,
                    19,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-18T07:35:35Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    35,
                    35,
                    1,
                    49,
                    0
                ],
                "title": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis"
                },
                "summary": "Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark."
                },
                "authors": [
                    {
                        "name": "Jiaqi Zhao"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "17 pages, 3 fugures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13178v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13178v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10316v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10316v4",
                "updated": "2025-05-21T14:09:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    9,
                    13,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-15T16:14:48Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    14,
                    48,
                    4,
                    320,
                    0
                ],
                "title": "M3TR: A Generalist Model for Real-World HD Map Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3TR: A Generalist Model for Real-World HD Map Completion"
                },
                "summary": "Autonomous vehicles rely on HD maps for their operation, but offline HD maps\neventually become outdated. For this reason, online HD map construction methods\nuse live sensor data to infer map information instead. Research on real map\nchanges shows that oftentimes entire parts of an HD map remain unchanged and\ncan be used as a prior. We therefore introduce M3TR (Multi-Masking Map\nTransformer), a generalist approach for HD map completion both with and without\noffline HD map priors. As a necessary foundation, we address shortcomings in\nground truth labels for Argoverse 2 and nuScenes and propose the first\ncomprehensive benchmark for HD map completion. Unlike existing models that\nspecialize in a single kind of map change, which is unrealistic for deployment,\nour Generalist model handles all kinds of changes, matching the effectiveness\nof Expert models. With our map masking as augmentation regime, we can even\nachieve a +1.4 mAP improvement without a prior. Finally, by fully utilizing\nprior HD map elements and optimizing query designs, M3TR outperforms existing\nmethods by +4.3 mAP while being the first real-world deployable model for\noffline HD map priors. Code is available at https://github.com/immel-f/m3tr",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous vehicles rely on HD maps for their operation, but offline HD maps\neventually become outdated. For this reason, online HD map construction methods\nuse live sensor data to infer map information instead. Research on real map\nchanges shows that oftentimes entire parts of an HD map remain unchanged and\ncan be used as a prior. We therefore introduce M3TR (Multi-Masking Map\nTransformer), a generalist approach for HD map completion both with and without\noffline HD map priors. As a necessary foundation, we address shortcomings in\nground truth labels for Argoverse 2 and nuScenes and propose the first\ncomprehensive benchmark for HD map completion. Unlike existing models that\nspecialize in a single kind of map change, which is unrealistic for deployment,\nour Generalist model handles all kinds of changes, matching the effectiveness\nof Expert models. With our map masking as augmentation regime, we can even\nachieve a +1.4 mAP improvement without a prior. Finally, by fully utilizing\nprior HD map elements and optimizing query designs, M3TR outperforms existing\nmethods by +4.3 mAP while being the first real-world deployable model for\noffline HD map priors. Code is available at https://github.com/immel-f/m3tr"
                },
                "authors": [
                    {
                        "name": "Fabian Immel"
                    },
                    {
                        "name": "Richard Fehler"
                    },
                    {
                        "name": "Frank Bieder"
                    },
                    {
                        "name": "Jan-Hendrik Pauls"
                    },
                    {
                        "name": "Christoph Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Stiller"
                },
                "author": "Christoph Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10316v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10316v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15545v1",
                "updated": "2025-05-21T14:08:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    8,
                    42,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:08:42Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    8,
                    42,
                    2,
                    141,
                    0
                ],
                "title": "seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and\n  Adaptation in 3D Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and\n  Adaptation in 3D Semantic Segmentation"
                },
                "summary": "3D semantic segmentation plays a pivotal role in autonomous driving and road\ninfrastructure analysis, yet state-of-the-art 3D models are prone to severe\ndomain shift when deployed across different datasets. We propose a novel\nmulti-view projection framework that excels in both domain generalization (DG)\nand unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans\ninto coherent 3D scenes and renders them from multiple virtual camera poses to\ncreate a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D\nsegmentation model in-domain. During inference, the model processes hundreds of\nviews per scene; the resulting logits are back-projected to 3D with an\nocclusion-aware voting scheme to generate final point-wise labels. Our\nframework is modular and enables extensive exploration of key design\nparameters, such as view generation optimization (VGO), visualization modality\noptimization (MODO), and 2D model choice. We evaluate on the nuScenes and\nSemanticKITTI datasets under both the DG and UDA settings. We achieve\nstate-of-the-art results in UDA and close to state-of-the-art in DG, with\nparticularly large gains on large, static classes. Our code and dataset\ngeneration tools will be publicly available at\nhttps://github.com/andrewcaunes/ia4markings",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D semantic segmentation plays a pivotal role in autonomous driving and road\ninfrastructure analysis, yet state-of-the-art 3D models are prone to severe\ndomain shift when deployed across different datasets. We propose a novel\nmulti-view projection framework that excels in both domain generalization (DG)\nand unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans\ninto coherent 3D scenes and renders them from multiple virtual camera poses to\ncreate a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D\nsegmentation model in-domain. During inference, the model processes hundreds of\nviews per scene; the resulting logits are back-projected to 3D with an\nocclusion-aware voting scheme to generate final point-wise labels. Our\nframework is modular and enables extensive exploration of key design\nparameters, such as view generation optimization (VGO), visualization modality\noptimization (MODO), and 2D model choice. We evaluate on the nuScenes and\nSemanticKITTI datasets under both the DG and UDA settings. We achieve\nstate-of-the-art results in UDA and close to state-of-the-art in DG, with\nparticularly large gains on large, static classes. Our code and dataset\ngeneration tools will be publicly available at\nhttps://github.com/andrewcaunes/ia4markings"
                },
                "authors": [
                    {
                        "name": "Andrew Caunes"
                    },
                    {
                        "name": "Thierry Chateau"
                    },
                    {
                        "name": "Vincent Fremont"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Fremont"
                },
                "author": "Vincent Fremont",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v4",
                "updated": "2025-05-21T14:05:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    5,
                    56,
                    2,
                    141,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19559v2",
                "updated": "2025-05-21T14:02:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    2,
                    49,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-26T20:54:51Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    20,
                    54,
                    51,
                    2,
                    57,
                    0
                ],
                "title": "Stay Focused: Problem Drift in Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stay Focused: Problem Drift in Multi-Agent Debate"
                },
                "summary": "Multi-agent debate - multiple instances of large language models discussing\nproblems in turn-based interaction - has shown promise for solving knowledge\nand reasoning tasks. However, these methods show limitations when solving\ncomplex problems that require longer reasoning chains. We analyze how\nmulti-agent debate over multiple turns drifts away from the initial problem,\nthus harming task performance. We define this phenomenon as problem drift and\nquantify its presence across ten tasks (i.e., three generative, three\nknowledge, three reasoning, and one instruction-following task). To identify\nthe reasons for this issue, eight human experts analyze 170 multi-agent\ndiscussions suffering from problem drift. We find the most common issues\nrelated to this drift are the lack of progress (35% of cases), low-quality\nfeedback (26% of cases), and a lack of clarity (25% of cases). To address\nproblem drift, we propose DRIFTJudge, an LLM-as-a-judge method, to detect\nproblem drift at test-time. We also propose DRIFTPolicy, a method that\nmitigates problem drift cases to improve task performance. Our study is a step\ntoward understanding a key limitation of multi-agent debate, highlighting why\nlonger debates can harm task performance and how problem drift could be\naddressed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent debate - multiple instances of large language models discussing\nproblems in turn-based interaction - has shown promise for solving knowledge\nand reasoning tasks. However, these methods show limitations when solving\ncomplex problems that require longer reasoning chains. We analyze how\nmulti-agent debate over multiple turns drifts away from the initial problem,\nthus harming task performance. We define this phenomenon as problem drift and\nquantify its presence across ten tasks (i.e., three generative, three\nknowledge, three reasoning, and one instruction-following task). To identify\nthe reasons for this issue, eight human experts analyze 170 multi-agent\ndiscussions suffering from problem drift. We find the most common issues\nrelated to this drift are the lack of progress (35% of cases), low-quality\nfeedback (26% of cases), and a lack of clarity (25% of cases). To address\nproblem drift, we propose DRIFTJudge, an LLM-as-a-judge method, to detect\nproblem drift at test-time. We also propose DRIFTPolicy, a method that\nmitigates problem drift cases to improve task performance. Our study is a step\ntoward understanding a key limitation of multi-agent debate, highlighting why\nlonger debates can harm task performance and how problem drift could be\naddressed."
                },
                "authors": [
                    {
                        "name": "Jonas Becker"
                    },
                    {
                        "name": "Lars Benedikt Kaesberg"
                    },
                    {
                        "name": "Andreas Stephan"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "arxiv_comment": "34 pages, 10 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03122v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03122v4",
                "updated": "2025-05-21T14:00:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    0,
                    20,
                    2,
                    141,
                    0
                ],
                "published": "2025-03-05T02:37:41Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    2,
                    37,
                    41,
                    2,
                    64,
                    0
                ],
                "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models"
                },
                "summary": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling."
                },
                "authors": [
                    {
                        "name": "Zichao Li"
                    },
                    {
                        "name": "Xueru Wen"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Yuqiu Ji"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03122v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03122v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14544v2",
                "updated": "2025-05-21T13:57:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    57,
                    1,
                    2,
                    141,
                    0
                ],
                "published": "2025-01-24T14:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    47,
                    42,
                    4,
                    24,
                    0
                ],
                "title": "Distributed Conformal Prediction via Message Passing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Conformal Prediction via Message Passing"
                },
                "summary": "Post-hoc calibration of pre-trained models is critical for ensuring reliable\ninference, especially in safety-critical domains such as healthcare. Conformal\nPrediction (CP) offers a robust post-hoc calibration framework, providing\ndistribution-free statistical coverage guarantees for prediction sets by\nleveraging held-out datasets. In this work, we address a decentralized setting\nwhere each device has limited calibration data and can communicate only with\nits neighbors over an arbitrary graph topology. We propose two\nmessage-passing-based approaches for achieving reliable inference via CP:\nquantile-based distributed conformal prediction (Q-DCP) and histogram-based\ndistributed conformal prediction (H-DCP). Q-DCP employs distributed quantile\nregression enhanced with tailored smoothing and regularization terms to\naccelerate convergence, while H-DCP uses a consensus-based histogram estimation\napproach. Through extensive experiments, we investigate the trade-offs between\nhyperparameter tuning requirements, communication overhead, coverage\nguarantees, and prediction set sizes across different network topologies. The\ncode of our work is released on:\nhttps://github.com/HaifengWen/Distributed-Conformal-Prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-hoc calibration of pre-trained models is critical for ensuring reliable\ninference, especially in safety-critical domains such as healthcare. Conformal\nPrediction (CP) offers a robust post-hoc calibration framework, providing\ndistribution-free statistical coverage guarantees for prediction sets by\nleveraging held-out datasets. In this work, we address a decentralized setting\nwhere each device has limited calibration data and can communicate only with\nits neighbors over an arbitrary graph topology. We propose two\nmessage-passing-based approaches for achieving reliable inference via CP:\nquantile-based distributed conformal prediction (Q-DCP) and histogram-based\ndistributed conformal prediction (H-DCP). Q-DCP employs distributed quantile\nregression enhanced with tailored smoothing and regularization terms to\naccelerate convergence, while H-DCP uses a consensus-based histogram estimation\napproach. Through extensive experiments, we investigate the trade-offs between\nhyperparameter tuning requirements, communication overhead, coverage\nguarantees, and prediction set sizes across different network topologies. The\ncode of our work is released on:\nhttps://github.com/HaifengWen/Distributed-Conformal-Prediction."
                },
                "authors": [
                    {
                        "name": "Haifeng Wen"
                    },
                    {
                        "name": "Hong Xing"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "19 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15530v1",
                "updated": "2025-05-21T13:52:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    42,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:52:42Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    42,
                    2,
                    141,
                    0
                ],
                "title": "A fast deep-learning approach to probing primordial black hole\n  populations in gravitational wave events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast deep-learning approach to probing primordial black hole\n  populations in gravitational wave events"
                },
                "summary": "Primordial black holes (PBHs), envisioned as a compelling dark matter\ncandidate and a window onto early-Universe physics, may contribute to the part\nof the gravitational-wave (GW) signals detected by the LIGO-Virgo-KAGRA\nnetwork. Traditional hierarchical Bayesian analysis, which relies on precise\nGW-event posterior estimates, for extracting the information of potential PBH\npopulation from GW events become computationally prohibitive for catalogs of\nhundreds of events. Here, we present a fast deep-learning framework, leveraging\nTransformer and normalizing flows, that maps GW-event posterior samples to\njoint posterior distributions over the hyperparameters of the PBH population.\nOur approach yields accurate credible intervals while reducing end-to-end\ninference time to $\\mathcal{O}(1)$ s on a single GPU. These results underscore\nthe potential of deep learning for fast, high-accurately PBH population studies\nin the era of next-generation GW detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Primordial black holes (PBHs), envisioned as a compelling dark matter\ncandidate and a window onto early-Universe physics, may contribute to the part\nof the gravitational-wave (GW) signals detected by the LIGO-Virgo-KAGRA\nnetwork. Traditional hierarchical Bayesian analysis, which relies on precise\nGW-event posterior estimates, for extracting the information of potential PBH\npopulation from GW events become computationally prohibitive for catalogs of\nhundreds of events. Here, we present a fast deep-learning framework, leveraging\nTransformer and normalizing flows, that maps GW-event posterior samples to\njoint posterior distributions over the hyperparameters of the PBH population.\nOur approach yields accurate credible intervals while reducing end-to-end\ninference time to $\\mathcal{O}(1)$ s on a single GPU. These results underscore\nthe potential of deep learning for fast, high-accurately PBH population studies\nin the era of next-generation GW detectors."
                },
                "authors": [
                    {
                        "name": "Jun-Qian Jiang"
                    },
                    {
                        "name": "Hai-Long Huang"
                    },
                    {
                        "name": "Jibin He"
                    },
                    {
                        "name": "Yu-Tong Wang"
                    },
                    {
                        "name": "Yun-Song Piao"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Song Piao"
                },
                "author": "Yun-Song Piao",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15524v1",
                "updated": "2025-05-21T13:50:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    50,
                    23,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:50:23Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    50,
                    23,
                    2,
                    141,
                    0
                ],
                "title": "Evaluate Bias without Manual Test Sets: A Concept Representation\n  Perspective for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluate Bias without Manual Test Sets: A Concept Representation\n  Perspective for LLMs"
                },
                "summary": "Bias in Large Language Models (LLMs) significantly undermines their\nreliability and fairness. We focus on a common form of bias: when two reference\nconcepts in the model's concept space, such as sentiment polarities (e.g.,\n\"positive\" and \"negative\"), are asymmetrically correlated with a third, target\nconcept, such as a reviewing aspect, the model exhibits unintended bias. For\ninstance, the understanding of \"food\" should not skew toward any particular\nsentiment. Existing bias evaluation methods assess behavioral differences of\nLLMs by constructing labeled data for different social groups and measuring\nmodel responses across them, a process that requires substantial human effort\nand captures only a limited set of social concepts. To overcome these\nlimitations, we propose BiasLens, a test-set-free bias analysis framework based\non the structure of the model's vector space. BiasLens combines Concept\nActivation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract\ninterpretable concept representations, and quantifies bias by measuring the\nvariation in representational similarity between the target concept and each of\nthe reference concepts. Even without labeled data, BiasLens shows strong\nagreement with traditional bias evaluation metrics (Spearman correlation r >\n0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect\nusing existing methods. For example, in simulated clinical scenarios, a\npatient's insurance status can cause the LLM to produce biased diagnostic\nassessments. Overall, BiasLens offers a scalable, interpretable, and efficient\nparadigm for bias discovery, paving the way for improving fairness and\ntransparency in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Large Language Models (LLMs) significantly undermines their\nreliability and fairness. We focus on a common form of bias: when two reference\nconcepts in the model's concept space, such as sentiment polarities (e.g.,\n\"positive\" and \"negative\"), are asymmetrically correlated with a third, target\nconcept, such as a reviewing aspect, the model exhibits unintended bias. For\ninstance, the understanding of \"food\" should not skew toward any particular\nsentiment. Existing bias evaluation methods assess behavioral differences of\nLLMs by constructing labeled data for different social groups and measuring\nmodel responses across them, a process that requires substantial human effort\nand captures only a limited set of social concepts. To overcome these\nlimitations, we propose BiasLens, a test-set-free bias analysis framework based\non the structure of the model's vector space. BiasLens combines Concept\nActivation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract\ninterpretable concept representations, and quantifies bias by measuring the\nvariation in representational similarity between the target concept and each of\nthe reference concepts. Even without labeled data, BiasLens shows strong\nagreement with traditional bias evaluation metrics (Spearman correlation r >\n0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect\nusing existing methods. For example, in simulated clinical scenarios, a\npatient's insurance status can cause the LLM to produce biased diagnostic\nassessments. Overall, BiasLens offers a scalable, interpretable, and efficient\nparadigm for bias discovery, paving the way for improving fairness and\ntransparency in LLMs."
                },
                "authors": [
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Kaiyang Wan"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Zixiang Xu"
                    },
                    {
                        "name": "Yanbo Wang"
                    },
                    {
                        "name": "Veselin Stoyanov"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.15817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15817v1",
                "updated": "2025-05-21T17:59:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    59,
                    54,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:59:54Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    59,
                    54,
                    2,
                    141,
                    0
                ],
                "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason via Mixture-of-Thought for Logical Reasoning"
                },
                "summary": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference."
                },
                "authors": [
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Lichang Chen"
                    },
                    {
                        "name": "Simeng Han"
                    },
                    {
                        "name": "R. Thomas McCoy"
                    },
                    {
                        "name": "Heng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Huang"
                },
                "author": "Heng Huang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15805v1",
                "updated": "2025-05-21T17:58:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    58,
                    11,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:58:11Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    58,
                    11,
                    2,
                    141,
                    0
                ],
                "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering"
                },
                "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security."
                },
                "authors": [
                    {
                        "name": "Hwan Chang"
                    },
                    {
                        "name": "Yumin Kim"
                    },
                    {
                        "name": "Yonghyun Jun"
                    },
                    {
                        "name": "Hwanhee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwanhee Lee"
                },
                "author": "Hwanhee Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13247v2",
                "updated": "2025-05-21T17:58:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    58,
                    4,
                    2,
                    141,
                    0
                ],
                "published": "2024-08-23T17:42:06Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    42,
                    6,
                    4,
                    236,
                    0
                ],
                "title": "An In-Depth Investigation of Data Collection in LLM App Ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An In-Depth Investigation of Data Collection in LLM App Ecosystems"
                },
                "summary": "LLM app (tool) ecosystems are rapidly evolving to support sophisticated use\ncases that often require extensive user data collection. Given that LLM apps\nare developed by third parties and anecdotal evidence indicating inconsistent\nenforcement of policies by LLM platforms, sharing user data with these apps\npresents significant privacy risks. In this paper, we aim to bring transparency\nin data practices of LLM app ecosystems. We examine OpenAI's GPT app ecosystem\nas a case study. We propose an LLM-based framework to analyze the natural\nlanguage specifications of GPT Actions (custom tools) and assess their data\ncollection practices. Our analysis reveals that Actions collect excessive data\nacross 24 categories and 145 data types, with third-party Actions collecting\n6.03% more data on average. We find that several Actions violate OpenAI's\npolicies by collecting sensitive information, such as passwords, which is\nexplicitly prohibited by OpenAI. Lastly, we develop an LLM-based privacy policy\nanalysis framework to automatically check the consistency of data collection by\nActions with disclosures in their privacy policies. Our measurements indicate\nthat the disclosures for most of the collected data types are omitted, with\nonly 5.8% of Actions clearly disclosing their data collection practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM app (tool) ecosystems are rapidly evolving to support sophisticated use\ncases that often require extensive user data collection. Given that LLM apps\nare developed by third parties and anecdotal evidence indicating inconsistent\nenforcement of policies by LLM platforms, sharing user data with these apps\npresents significant privacy risks. In this paper, we aim to bring transparency\nin data practices of LLM app ecosystems. We examine OpenAI's GPT app ecosystem\nas a case study. We propose an LLM-based framework to analyze the natural\nlanguage specifications of GPT Actions (custom tools) and assess their data\ncollection practices. Our analysis reveals that Actions collect excessive data\nacross 24 categories and 145 data types, with third-party Actions collecting\n6.03% more data on average. We find that several Actions violate OpenAI's\npolicies by collecting sensitive information, such as passwords, which is\nexplicitly prohibited by OpenAI. Lastly, we develop an LLM-based privacy policy\nanalysis framework to automatically check the consistency of data collection by\nActions with disclosures in their privacy policies. Our measurements indicate\nthat the disclosures for most of the collected data types are omitted, with\nonly 5.8% of Actions clearly disclosing their data collection practices."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Evin Jaff"
                    },
                    {
                        "name": "Ke Yang"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Umar Iqbal"
                    }
                ],
                "author_detail": {
                    "name": "Umar Iqbal"
                },
                "author": "Umar Iqbal",
                "arxiv_comment": "Accepted by the ACM Internet Measurement Conference (IMC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15804v1",
                "updated": "2025-05-21T17:57:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    57,
                    38,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:57:38Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    57,
                    38,
                    2,
                    141,
                    0
                ],
                "title": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1."
                },
                "authors": [
                    {
                        "name": "Zongzhao Li"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Mingze Li"
                    },
                    {
                        "name": "Songyou Li"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Tingyang Xu"
                    },
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Wenbing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenbing Huang"
                },
                "author": "Wenbing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15802v1",
                "updated": "2025-05-21T17:56:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    56,
                    2,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:56:02Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    56,
                    2,
                    2,
                    141,
                    0
                ],
                "title": "A Deep Learning Framework for Two-Dimensional, Multi-Frequency\n  Propagation Factor Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Learning Framework for Two-Dimensional, Multi-Frequency\n  Propagation Factor Estimation"
                },
                "summary": "Accurately estimating the refractive environment over multiple frequencies\nwithin the marine atmospheric boundary layer is crucial for the effective\ndeployment of radar technologies. Traditional parabolic equation simulations,\nwhile effective, can be computationally expensive and time-intensive, limiting\ntheir practical application. This communication explores a novel approach using\ndeep neural networks to estimate the pattern propagation factor, a critical\nparameter for characterizing environmental impacts on signal propagation.\nImage-to-image translation generators designed to ingest modified refractivity\ndata and generate predictions of pattern propagation factors over the same\ndomain were developed. Findings demonstrate that deep neural networks can be\ntrained to analyze multiple frequencies and reasonably predict the pattern\npropagation factor, offering an alternative to traditional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately estimating the refractive environment over multiple frequencies\nwithin the marine atmospheric boundary layer is crucial for the effective\ndeployment of radar technologies. Traditional parabolic equation simulations,\nwhile effective, can be computationally expensive and time-intensive, limiting\ntheir practical application. This communication explores a novel approach using\ndeep neural networks to estimate the pattern propagation factor, a critical\nparameter for characterizing environmental impacts on signal propagation.\nImage-to-image translation generators designed to ingest modified refractivity\ndata and generate predictions of pattern propagation factors over the same\ndomain were developed. Findings demonstrate that deep neural networks can be\ntrained to analyze multiple frequencies and reasonably predict the pattern\npropagation factor, offering an alternative to traditional methods."
                },
                "authors": [
                    {
                        "name": "Sarah E. Wessinger"
                    },
                    {
                        "name": "Leslie N. Smith"
                    },
                    {
                        "name": "Jacob Gull"
                    },
                    {
                        "name": "Jonathan Gehman"
                    },
                    {
                        "name": "Zachary Beever"
                    },
                    {
                        "name": "Andrew J. Kammerer"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Kammerer"
                },
                "author": "Andrew J. Kammerer",
                "arxiv_comment": "Submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14652v2",
                "updated": "2025-05-21T17:55:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    55,
                    36,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T17:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    33,
                    1,
                    140,
                    0
                ],
                "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-Reasoner: Advancing LLM Reasoning Across All Domains"
                },
                "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24370v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24370v3",
                "updated": "2025-05-21T17:51:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    51,
                    27,
                    2,
                    141,
                    0
                ],
                "published": "2025-03-31T17:50:13Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    50,
                    13,
                    0,
                    90,
                    0
                ],
                "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively Controlling Reasoning Models through Thinking Intervention"
                },
                "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We find that the\nThinking Intervention paradigm enhances the capabilities of reasoning models\nacross a wide range of tasks, including instruction following on IFEval and\nOverthinking, instruction hierarchy on SEP, and safety alignment on XSTest and\nSorryBench. Our results demonstrate that Thinking Intervention significantly\noutperforms baseline prompting approaches, achieving up to 6.7% accuracy gains\nin instruction-following scenarios, 15.4% improvements in reasoning about\ninstruction hierarchies, and a 40.0% increase in refusal rates for unsafe\nprompts using open-source DeepSeek R1 models. Overall, our work opens a\npromising new research avenue for controlling reasoning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We find that the\nThinking Intervention paradigm enhances the capabilities of reasoning models\nacross a wide range of tasks, including instruction following on IFEval and\nOverthinking, instruction hierarchy on SEP, and safety alignment on XSTest and\nSorryBench. Our results demonstrate that Thinking Intervention significantly\noutperforms baseline prompting approaches, achieving up to 6.7% accuracy gains\nin instruction-following scenarios, 15.4% improvements in reasoning about\ninstruction hierarchies, and a 40.0% increase in refusal rates for unsafe\nprompts using open-source DeepSeek R1 models. Overall, our work opens a\npromising new research avenue for controlling reasoning LLMs."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Chong Xiang"
                    },
                    {
                        "name": "Jiachen T. Wang"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Prateek Mittal"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Mittal"
                },
                "author": "Prateek Mittal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24370v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24370v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01697v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01697v3",
                "updated": "2025-05-21T17:50:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    50,
                    43,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-03T00:12:40Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    0,
                    12,
                    40,
                    0,
                    34,
                    0
                ],
                "title": "BARE: Leveraging Base Language Models for Few-Shot Synthetic Data\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BARE: Leveraging Base Language Models for Few-Shot Synthetic Data\n  Generation"
                },
                "summary": "As the demand for high-quality data in model training grows, researchers and\ndevelopers are increasingly generating synthetic data to tune and train LLMs.\nHowever, current data generation methods rely on seed sets containing tens of\nthousands of examples to prompt instruction-tuned models. This reliance can be\nespecially problematic when the curation of high-quality examples is expensive\nor difficult. In this paper we explore the novel few-shot synthetic data\ngeneration setting -- generating a high-quality dataset from a few examples. We\nshow that when working with only a few seed examples, instruction-tuned models\nused in current synthetic data methods produce insufficient diversity for\ndownstream tasks. In contrast, we show that base models without post-training,\nlargely untapped for synthetic data generation, offer substantially greater\noutput diversity, albeit with lower instruction following abilities. Leveraging\nthis insight, we propose Base-Refine (BARE), a novel two-stage method that\ncombines the diversity of base models with the quality assurance of\ninstruction-tuned models. BARE excels in few-shot synthetic data generation:\nusing only 3 seed examples it generates diverse, high-quality datasets that\nsignificantly improve downstream task performance. We show that fine-tuning\nLlama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable\nto state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore,\ndata generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2\n1B on GSM8K over data generated by only instruction-models, and an 18.4%\nimprovement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method\nfor RAG data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for high-quality data in model training grows, researchers and\ndevelopers are increasingly generating synthetic data to tune and train LLMs.\nHowever, current data generation methods rely on seed sets containing tens of\nthousands of examples to prompt instruction-tuned models. This reliance can be\nespecially problematic when the curation of high-quality examples is expensive\nor difficult. In this paper we explore the novel few-shot synthetic data\ngeneration setting -- generating a high-quality dataset from a few examples. We\nshow that when working with only a few seed examples, instruction-tuned models\nused in current synthetic data methods produce insufficient diversity for\ndownstream tasks. In contrast, we show that base models without post-training,\nlargely untapped for synthetic data generation, offer substantially greater\noutput diversity, albeit with lower instruction following abilities. Leveraging\nthis insight, we propose Base-Refine (BARE), a novel two-stage method that\ncombines the diversity of base models with the quality assurance of\ninstruction-tuned models. BARE excels in few-shot synthetic data generation:\nusing only 3 seed examples it generates diverse, high-quality datasets that\nsignificantly improve downstream task performance. We show that fine-tuning\nLlama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable\nto state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore,\ndata generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2\n1B on GSM8K over data generated by only instruction-models, and an 18.4%\nimprovement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method\nfor RAG data generation."
                },
                "authors": [
                    {
                        "name": "Alan Zhu"
                    },
                    {
                        "name": "Parth Asawa"
                    },
                    {
                        "name": "Jared Quincy Davis"
                    },
                    {
                        "name": "Lingjiao Chen"
                    },
                    {
                        "name": "Boris Hanin"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01697v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01697v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15795v1",
                "updated": "2025-05-21T17:48:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    48,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:48:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    48,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "Reverse Engineering Human Preferences with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Engineering Human Preferences with Reinforcement Learning"
                },
                "summary": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks."
                },
                "authors": [
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Tan Yi-Chern"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15793v2",
                "updated": "2025-05-22T04:48:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    48,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T17:47:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    47,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving"
                },
                "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Bo Leng"
                    },
                    {
                        "name": "Zhuoren Li"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Guizhe Jin"
                    },
                    {
                        "name": "Ran Yu"
                    },
                    {
                        "name": "Huanxi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Huanxi Wen"
                },
                "author": "Huanxi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15792v1",
                "updated": "2025-05-21T17:46:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    46,
                    38,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:46:38Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    46,
                    38,
                    2,
                    141,
                    0
                ],
                "title": "Long-Form Information Alignment Evaluation Beyond Atomic Facts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Form Information Alignment Evaluation Beyond Atomic Facts"
                },
                "summary": "Information alignment evaluators are vital for various NLG evaluation tasks\nand trustworthy LLM deployment, reducing hallucinations and enhancing user\ntrust. Current fine-grained methods, like FactScore, verify facts individually\nbut neglect inter-fact dependencies, enabling subtle vulnerabilities. In this\nwork, we introduce MontageLie, a challenging benchmark that constructs\ndeceptive narratives by \"montaging\" truthful statements without introducing\nexplicit hallucinations. We demonstrate that both coarse-grained LLM-based\nevaluators and current fine-grained frameworks are susceptible to this attack,\nwith AUC-ROC scores falling below 65%. To enable more robust fine-grained\nevaluation, we propose DoveScore, a novel framework that jointly verifies\nfactual accuracy and event-order consistency. By modeling inter-fact\nrelationships, DoveScore outperforms existing fine-grained methods by over 8%,\nproviding a more robust solution for long-form text alignment evaluation. Our\ncode and datasets are available at https://github.com/dannalily/DoveScore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information alignment evaluators are vital for various NLG evaluation tasks\nand trustworthy LLM deployment, reducing hallucinations and enhancing user\ntrust. Current fine-grained methods, like FactScore, verify facts individually\nbut neglect inter-fact dependencies, enabling subtle vulnerabilities. In this\nwork, we introduce MontageLie, a challenging benchmark that constructs\ndeceptive narratives by \"montaging\" truthful statements without introducing\nexplicit hallucinations. We demonstrate that both coarse-grained LLM-based\nevaluators and current fine-grained frameworks are susceptible to this attack,\nwith AUC-ROC scores falling below 65%. To enable more robust fine-grained\nevaluation, we propose DoveScore, a novel framework that jointly verifies\nfactual accuracy and event-order consistency. By modeling inter-fact\nrelationships, DoveScore outperforms existing fine-grained methods by over 8%,\nproviding a more robust solution for long-form text alignment evaluation. Our\ncode and datasets are available at https://github.com/dannalily/DoveScore."
                },
                "authors": [
                    {
                        "name": "Danna Zheng"
                    },
                    {
                        "name": "Mirella Lapata"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12545v2",
                "updated": "2025-05-21T17:38:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    38,
                    2,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-18T21:02:30Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    21,
                    2,
                    30,
                    6,
                    138,
                    0
                ],
                "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and\n  Safety Interventions Using Customized Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and\n  Safety Interventions Using Customized Large Language Models"
                },
                "summary": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes."
                },
                "authors": [
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Hongru Du"
                    },
                    {
                        "name": "Hao Frank Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Frank Yang"
                },
                "author": "Hao Frank Yang",
                "arxiv_comment": "Last revised 13 Feb 2025. Under review in Nature portfolio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10717v2",
                "updated": "2025-05-21T17:36:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    36,
                    21,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-15T21:40:21Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    21,
                    40,
                    21,
                    3,
                    135,
                    0
                ],
                "title": "A Modular Approach for Clinical SLMs Driven by Synthetic Data with\n  Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modular Approach for Clinical SLMs Driven by Synthetic Data with\n  Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment"
                },
                "summary": "High computation costs and latency of large language models such as GPT-4\nhave limited their deployment in clinical settings. Small language models\n(SLMs) offer a cost-effective alternative, but their limited capacity requires\nbiomedical domain adaptation, which remains challenging. An additional\nbottleneck is the unavailability and high sensitivity of clinical data. To\naddress these challenges, we propose a novel framework for adapting SLMs into\nhigh-performing clinical models. We introduce the MediPhi collection of\n3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning\nof experts on relevant medical and clinical corpora (PMC, Medical Guideline,\nMedWiki, etc.), model merging, and clinical-tasks alignment. To cover most\nclinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our\nexpert models deliver relative improvements on this benchmark over the base\nmodel without any task-specific fine-tuning: 64.3% on medical entities, 49.5%\non radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by\n14%). We unify the expert models into MediPhi via model merging, preserving\ngains across benchmarks. Furthermore, we built the MediFlow collection, a\nsynthetic dataset of 2.5 million high-quality instructions on 14 medical NLP\ntasks, 98 fine-grained document types, and JSON format support. Alignment of\nMediPhi using supervised fine-tuning and direct preference optimization\nachieves further gains of 18.9% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High computation costs and latency of large language models such as GPT-4\nhave limited their deployment in clinical settings. Small language models\n(SLMs) offer a cost-effective alternative, but their limited capacity requires\nbiomedical domain adaptation, which remains challenging. An additional\nbottleneck is the unavailability and high sensitivity of clinical data. To\naddress these challenges, we propose a novel framework for adapting SLMs into\nhigh-performing clinical models. We introduce the MediPhi collection of\n3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning\nof experts on relevant medical and clinical corpora (PMC, Medical Guideline,\nMedWiki, etc.), model merging, and clinical-tasks alignment. To cover most\nclinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our\nexpert models deliver relative improvements on this benchmark over the base\nmodel without any task-specific fine-tuning: 64.3% on medical entities, 49.5%\non radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by\n14%). We unify the expert models into MediPhi via model merging, preserving\ngains across benchmarks. Furthermore, we built the MediFlow collection, a\nsynthetic dataset of 2.5 million high-quality instructions on 14 medical NLP\ntasks, 98 fine-grained document types, and JSON format support. Alignment of\nMediPhi using supervised fine-tuning and direct preference optimization\nachieves further gains of 18.9% on average."
                },
                "authors": [
                    {
                        "name": "Jean-Philippe Corbeil"
                    },
                    {
                        "name": "Amin Dada"
                    },
                    {
                        "name": "Jean-Michel Attendu"
                    },
                    {
                        "name": "Asma Ben Abacha"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Lucas Caccia"
                    },
                    {
                        "name": "François Beaulieu"
                    },
                    {
                        "name": "Thomas Lin"
                    },
                    {
                        "name": "Jens Kleesiek"
                    },
                    {
                        "name": "Paul Vozila"
                    }
                ],
                "author_detail": {
                    "name": "Paul Vozila"
                },
                "author": "Paul Vozila",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15784v1",
                "updated": "2025-05-21T17:35:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    35,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:35:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    35,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "Large Language Models as Computable Approximations to Solomonoff\n  Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Computable Approximations to Solomonoff\n  Induction"
                },
                "summary": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development."
                },
                "authors": [
                    {
                        "name": "Jun Wan"
                    },
                    {
                        "name": "Lingrui Mei"
                    }
                ],
                "author_detail": {
                    "name": "Lingrui Mei"
                },
                "author": "Lingrui Mei",
                "arxiv_comment": "Both authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15778v1",
                "updated": "2025-05-21T17:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    29,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:29:15Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    29,
                    15,
                    2,
                    141,
                    0
                ],
                "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space"
                },
                "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking."
                },
                "authors": [
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xuehai He"
                    },
                    {
                        "name": "Weixiang Yan"
                    },
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Xin Eric Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Eric Wang"
                },
                "author": "Xin Eric Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11651v2",
                "updated": "2025-05-21T17:26:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    26,
                    12,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-16T19:22:19Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    19,
                    22,
                    19,
                    4,
                    136,
                    0
                ],
                "title": "MIRACL-VISION: A Large, multilingual, visual document retrieval\n  benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRACL-VISION: A Large, multilingual, visual document retrieval\n  benchmark"
                },
                "summary": "Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval."
                },
                "authors": [
                    {
                        "name": "Radek Osmulski"
                    },
                    {
                        "name": "Gabriel de Souza P. Moreira"
                    },
                    {
                        "name": "Ronay Ak"
                    },
                    {
                        "name": "Mengyao Xu"
                    },
                    {
                        "name": "Benedikt Schifferer"
                    },
                    {
                        "name": "Even Oldridge"
                    }
                ],
                "author_detail": {
                    "name": "Even Oldridge"
                },
                "author": "Even Oldridge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15774v1",
                "updated": "2025-05-21T17:26:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    26,
                    11,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:26:11Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    26,
                    11,
                    2,
                    141,
                    0
                ],
                "title": "Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and\n  Global Information Retention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and\n  Global Information Retention"
                },
                "summary": "Large Language Models (LLMs) encounter significant challenges in\nlong-sequence inference due to computational inefficiency and redundant\nprocessing, driving interest in context compression techniques. Existing\nmethods often rely on token importance to perform hard local compression or\nencode context into latent representations for soft global compression.\nHowever, the uneven distribution of textual content relevance and the diversity\nof demands for user instructions mean these approaches frequently lead to the\nloss of potentially valuable information. To address this, we propose\n$\\textbf{Hy}$brid $\\textbf{Co}$ntext $\\textbf{Co}$mpression (HyCo$_2$) for\nLLMs, which integrates both global and local perspectives to guide context\ncompression while retaining both the essential semantics and critical details\nfor task completion. Specifically, we employ a hybrid adapter to refine global\nsemantics with the global view, based on the observation that different\nadapters excel at different tasks. Then we incorporate a classification layer\nthat assigns a retention probability to each context token based on the local\nview, determining whether it should be retained or discarded. To foster a\nbalanced integration of global and local compression, we introduce auxiliary\nparaphrasing and completion pretraining before instruction tuning. This\npromotes a synergistic integration that emphasizes instruction-relevant\ninformation while preserving essential local details, ultimately balancing\nlocal and global information retention in context compression. Experiments show\nthat our HyCo$_2$ method significantly enhances long-text reasoning while\nreducing token usage. It improves the performance of various LLM series by an\naverage of 13.1\\% across seven knowledge-intensive QA benchmarks. Moreover,\nHyCo$_2$ matches the performance of uncompressed methods while reducing token\nconsumption by 88.8\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encounter significant challenges in\nlong-sequence inference due to computational inefficiency and redundant\nprocessing, driving interest in context compression techniques. Existing\nmethods often rely on token importance to perform hard local compression or\nencode context into latent representations for soft global compression.\nHowever, the uneven distribution of textual content relevance and the diversity\nof demands for user instructions mean these approaches frequently lead to the\nloss of potentially valuable information. To address this, we propose\n$\\textbf{Hy}$brid $\\textbf{Co}$ntext $\\textbf{Co}$mpression (HyCo$_2$) for\nLLMs, which integrates both global and local perspectives to guide context\ncompression while retaining both the essential semantics and critical details\nfor task completion. Specifically, we employ a hybrid adapter to refine global\nsemantics with the global view, based on the observation that different\nadapters excel at different tasks. Then we incorporate a classification layer\nthat assigns a retention probability to each context token based on the local\nview, determining whether it should be retained or discarded. To foster a\nbalanced integration of global and local compression, we introduce auxiliary\nparaphrasing and completion pretraining before instruction tuning. This\npromotes a synergistic integration that emphasizes instruction-relevant\ninformation while preserving essential local details, ultimately balancing\nlocal and global information retention in context compression. Experiments show\nthat our HyCo$_2$ method significantly enhances long-text reasoning while\nreducing token usage. It improves the performance of various LLM series by an\naverage of 13.1\\% across seven knowledge-intensive QA benchmarks. Moreover,\nHyCo$_2$ matches the performance of uncompressed methods while reducing token\nconsumption by 88.8\\%."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14684v2",
                "updated": "2025-05-21T17:02:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    2,
                    34,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T17:59:31Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    59,
                    31,
                    1,
                    140,
                    0
                ],
                "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits."
                },
                "authors": [
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Shengpei Jiang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Project: https://zju-real.github.io/CoT-Bridge/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14218v2",
                "updated": "2025-05-21T16:59:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    59,
                    26,
                    2,
                    141,
                    0
                ],
                "published": "2025-04-19T07:53:37Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    7,
                    53,
                    37,
                    5,
                    109,
                    0
                ],
                "title": "Understanding the Repeat Curse in Large Language Models from a Feature\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Repeat Curse in Large Language Models from a Feature\n  Perspective"
                },
                "summary": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse."
                },
                "authors": [
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Jianhua Xu"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Mengdi Li"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "Accepted by ACL 2025, Findings, Long Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15753v1",
                "updated": "2025-05-21T16:58:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    58,
                    14,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:58:14Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    58,
                    14,
                    2,
                    141,
                    0
                ],
                "title": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety\n  Context Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety\n  Context Retrieval"
                },
                "summary": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication."
                },
                "authors": [
                    {
                        "name": "Taiye Chen"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11654v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11654v3",
                "updated": "2025-05-22T04:43:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    43,
                    6,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-16T19:38:06Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    19,
                    38,
                    6,
                    4,
                    136,
                    0
                ],
                "title": "UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal\n  Large Language Models"
                },
                "summary": "Understanding and predicting urban dynamics is crucial for managing\ntransportation systems, optimizing urban planning, and enhancing public\nservices. While neural network-based approaches have achieved success, they\noften rely on task-specific architectures and large volumes of data, limiting\ntheir ability to generalize across diverse urban scenarios. Meanwhile, Large\nLanguage Models (LLMs) offer strong reasoning and generalization capabilities,\nyet their application to spatial-temporal urban dynamics remains underexplored.\nExisting LLM-based methods struggle to effectively integrate multifaceted\nspatial-temporal data and fail to address distributional shifts between\ntraining and testing data, limiting their predictive reliability in real-world\napplications. To bridge this gap, we propose UrbanMind, a novel\nspatial-temporal LLM framework for multifaceted urban dynamics prediction that\nensures both accurate forecasting and robust generalization. At its core,\nUrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with\nspecialized masking strategies that capture intricate spatial-temporal\ndependencies and intercorrelations among multifaceted urban dynamics.\nAdditionally, we design a semantic-aware prompting and fine-tuning strategy\nthat encodes spatial-temporal contextual details into prompts, enhancing LLMs'\nability to reason over spatial-temporal patterns. To further improve\ngeneralization, we introduce a test time adaptation mechanism with a test data\nreconstructor, enabling UrbanMind to dynamically adjust to unseen test data by\nreconstructing LLM-generated embeddings. Extensive experiments on real-world\nurban datasets across multiple cities demonstrate that UrbanMind consistently\noutperforms state-of-the-art baselines, achieving high accuracy and robust\ngeneralization, even in zero-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting urban dynamics is crucial for managing\ntransportation systems, optimizing urban planning, and enhancing public\nservices. While neural network-based approaches have achieved success, they\noften rely on task-specific architectures and large volumes of data, limiting\ntheir ability to generalize across diverse urban scenarios. Meanwhile, Large\nLanguage Models (LLMs) offer strong reasoning and generalization capabilities,\nyet their application to spatial-temporal urban dynamics remains underexplored.\nExisting LLM-based methods struggle to effectively integrate multifaceted\nspatial-temporal data and fail to address distributional shifts between\ntraining and testing data, limiting their predictive reliability in real-world\napplications. To bridge this gap, we propose UrbanMind, a novel\nspatial-temporal LLM framework for multifaceted urban dynamics prediction that\nensures both accurate forecasting and robust generalization. At its core,\nUrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with\nspecialized masking strategies that capture intricate spatial-temporal\ndependencies and intercorrelations among multifaceted urban dynamics.\nAdditionally, we design a semantic-aware prompting and fine-tuning strategy\nthat encodes spatial-temporal contextual details into prompts, enhancing LLMs'\nability to reason over spatial-temporal patterns. To further improve\ngeneralization, we introduce a test time adaptation mechanism with a test data\nreconstructor, enabling UrbanMind to dynamically adjust to unseen test data by\nreconstructing LLM-generated embeddings. Extensive experiments on real-world\nurban datasets across multiple cities demonstrate that UrbanMind consistently\noutperforms state-of-the-art baselines, achieving high accuracy and robust\ngeneralization, even in zero-shot settings."
                },
                "authors": [
                    {
                        "name": "Yuhang Liu"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Ling Tian"
                    },
                    {
                        "name": "Yanhua Li"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "arxiv_comment": "KDD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11654v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24245v2",
                "updated": "2025-05-21T16:55:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    55,
                    47,
                    2,
                    141,
                    0
                ],
                "published": "2025-03-31T15:58:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    58,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches."
                },
                "authors": [
                    {
                        "name": "Dun Yuan"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yan Xin"
                    },
                    {
                        "name": "Jianzhong"
                    },
                    {
                        "name": "Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang"
                },
                "arxiv_affiliation": "Charlie",
                "author": "Zhang",
                "arxiv_comment": "This work has been accepted to ICC 2025 IEEE International Conference\n  on Communications. copyright 2025 IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15747v2",
                "updated": "2025-05-22T03:58:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    58,
                    27,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T16:51:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    51,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large\n  Language Models and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large\n  Language Models and Knowledge Graphs"
                },
                "summary": "We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research."
                },
                "authors": [
                    {
                        "name": "Kanan Kiguchi"
                    },
                    {
                        "name": "Yunhao Tu"
                    },
                    {
                        "name": "Katsuhiro Ajito"
                    },
                    {
                        "name": "Fady Alnajjar"
                    },
                    {
                        "name": "Kazuyuki Murase"
                    }
                ],
                "author_detail": {
                    "name": "Kazuyuki Murase"
                },
                "author": "Kazuyuki Murase",
                "arxiv_comment": "38 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.1; H.3.1; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15741v1",
                "updated": "2025-05-21T16:48:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    48,
                    28,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:48:28Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    48,
                    28,
                    2,
                    141,
                    0
                ],
                "title": "Evolutionary Computation and Large Language Models: A Survey of Methods,\n  Synergies, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Computation and Large Language Models: A Survey of Methods,\n  Synergies, and Applications"
                },
                "summary": "Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)\nrepresents a promising avenue for advancing artificial intelligence by\ncombining powerful natural language understanding with optimization and search\ncapabilities. This manuscript explores the synergistic potential of LLMs and\nEC, reviewing their intersections, complementary strengths, and emerging\napplications. We identify key opportunities where EC can enhance LLM training,\nfine-tuning, prompt engineering, and architecture search, while LLMs can, in\nturn, aid in automating the design, analysis, and interpretation of ECs. The\nmanuscript explores the synergistic integration of EC and LLMs, highlighting\ntheir bidirectional contributions to advancing artificial intelligence. It\nfirst examines how EC techniques enhance LLMs by optimizing key components such\nas prompt engineering, hyperparameter tuning, and architecture search,\ndemonstrating how evolutionary methods automate and refine these processes.\nSecondly, the survey investigates how LLMs improve EC by automating\nmetaheuristic design, tuning evolutionary algorithms, and generating adaptive\nheuristics, thereby increasing efficiency and scalability. Emerging\nco-evolutionary frameworks are discussed, showcasing applications across\ndiverse fields while acknowledging challenges like computational costs,\ninterpretability, and algorithmic convergence. The survey concludes by\nidentifying open research questions and advocating for hybrid approaches that\ncombine the strengths of EC and LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)\nrepresents a promising avenue for advancing artificial intelligence by\ncombining powerful natural language understanding with optimization and search\ncapabilities. This manuscript explores the synergistic potential of LLMs and\nEC, reviewing their intersections, complementary strengths, and emerging\napplications. We identify key opportunities where EC can enhance LLM training,\nfine-tuning, prompt engineering, and architecture search, while LLMs can, in\nturn, aid in automating the design, analysis, and interpretation of ECs. The\nmanuscript explores the synergistic integration of EC and LLMs, highlighting\ntheir bidirectional contributions to advancing artificial intelligence. It\nfirst examines how EC techniques enhance LLMs by optimizing key components such\nas prompt engineering, hyperparameter tuning, and architecture search,\ndemonstrating how evolutionary methods automate and refine these processes.\nSecondly, the survey investigates how LLMs improve EC by automating\nmetaheuristic design, tuning evolutionary algorithms, and generating adaptive\nheuristics, thereby increasing efficiency and scalability. Emerging\nco-evolutionary frameworks are discussed, showcasing applications across\ndiverse fields while acknowledging challenges like computational costs,\ninterpretability, and algorithmic convergence. The survey concludes by\nidentifying open research questions and advocating for hybrid approaches that\ncombine the strengths of EC and LLMs."
                },
                "authors": [
                    {
                        "name": "Dikshit Chauhan"
                    },
                    {
                        "name": "Bapi Dutta"
                    },
                    {
                        "name": "Indu Bala"
                    },
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Bäck"
                    },
                    {
                        "name": "Anupam Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Anupam Yadav"
                },
                "author": "Anupam Yadav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17034v2",
                "updated": "2025-05-21T16:47:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    47,
                    23,
                    2,
                    141,
                    0
                ],
                "published": "2024-12-22T14:18:39Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    14,
                    18,
                    39,
                    6,
                    357,
                    0
                ],
                "title": "Shaping the Safety Boundaries: Understanding and Defending Against\n  Jailbreaks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping the Safety Boundaries: Understanding and Defending Against\n  Jailbreaks in Large Language Models"
                },
                "summary": "Jailbreaking in Large Language Models (LLMs) is a major security concern as\nit can deceive LLMs to generate harmful text. Yet, there is still insufficient\nunderstanding of how jailbreaking works, which makes it hard to develop\neffective defense strategies. We aim to shed more light into this issue: we\nconduct a detailed large-scale analysis of seven different jailbreak methods\nand find that these disagreements stem from insufficient observation samples.\nIn particular, we introduce \\textit{safety boundary}, and we find that\njailbreaks shift harmful activations outside that safety boundary, where LLMs\nare less sensitive to harmful information. We also find that the low and the\nmiddle layers are critical in such shifts, while deeper layers have less\nimpact. Leveraging on these insights, we propose a novel defense called\n\\textbf{Activation Boundary Defense} (ABD), which adaptively constrains the\nactivations within the safety boundary. We further use Bayesian optimization to\nselectively apply the defense method to the low and the middle layers. Our\nexperiments on several benchmarks show that ABD achieves an average DSR of over\n98\\% against various forms of jailbreak attacks, with less than 2\\% impact on\nthe model's general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking in Large Language Models (LLMs) is a major security concern as\nit can deceive LLMs to generate harmful text. Yet, there is still insufficient\nunderstanding of how jailbreaking works, which makes it hard to develop\neffective defense strategies. We aim to shed more light into this issue: we\nconduct a detailed large-scale analysis of seven different jailbreak methods\nand find that these disagreements stem from insufficient observation samples.\nIn particular, we introduce \\textit{safety boundary}, and we find that\njailbreaks shift harmful activations outside that safety boundary, where LLMs\nare less sensitive to harmful information. We also find that the low and the\nmiddle layers are critical in such shifts, while deeper layers have less\nimpact. Leveraging on these insights, we propose a novel defense called\n\\textbf{Activation Boundary Defense} (ABD), which adaptively constrains the\nactivations within the safety boundary. We further use Bayesian optimization to\nselectively apply the defense method to the low and the middle layers. Our\nexperiments on several benchmarks show that ABD achieves an average DSR of over\n98\\% against various forms of jailbreak attacks, with less than 2\\% impact on\nthe model's general capabilities."
                },
                "authors": [
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Jiahui Geng"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14604v2",
                "updated": "2025-05-21T16:45:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    45,
                    44,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T16:53:40Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    53,
                    40,
                    1,
                    140,
                    0
                ],
                "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let LLMs Break Free from Overthinking via Self-Braking Tuning"
                },
                "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models."
                },
                "authors": [
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page:\n  https://ZJU-REAL.github.io/SBT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15740v1",
                "updated": "2025-05-21T16:45:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    45,
                    43,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:45:43Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    45,
                    43,
                    2,
                    141,
                    0
                ],
                "title": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis\n  and Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis\n  and Refinement"
                },
                "summary": "Formal methods is pivotal for verifying the reliability of critical systems\nthrough rigorous mathematical proofs. However, its adoption is hindered by\nlabor-intensive manual proofs and the expertise required to use theorem\nprovers. Recent advancements in large language models (LLMs) offer new\nopportunities for automated theorem proving. Two promising approaches are\ngenerating tactics step by step and generating a whole proof directly with an\nLLM. However, existing work makes no attempt to combine the two approaches. In\nthis work, we introduce HybridProver, a dual-model proof synthesis framework\nthat combines tactic-based generation and whole-proof synthesis to harness the\nbenefits of both approaches. HybridProver generates whole proof candidates for\nevaluation directly, then extracts proof sketches from those candidates. It\nthen uses a tactic-based generation model that integrates automated tools to\ncomplete the sketches via stepwise refinement. We implement HybridProver for\nthe Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle\ndatasets. Evaluation on the miniF2F dataset illustrates HybridProver's\neffectiveness. We achieve a 59.4% success rate on miniF2F, where the previous\nSOTA is 56.1%. Our ablation studies show that this SOTA result is attributable\nto combining whole-proof and tactic-based generation. Additionally, we show how\nthe dataset quality, training parameters, and sampling diversity affect the\nfinal result during automated theorem proving with LLMs. All of our code,\ndatasets, and LLMs are open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal methods is pivotal for verifying the reliability of critical systems\nthrough rigorous mathematical proofs. However, its adoption is hindered by\nlabor-intensive manual proofs and the expertise required to use theorem\nprovers. Recent advancements in large language models (LLMs) offer new\nopportunities for automated theorem proving. Two promising approaches are\ngenerating tactics step by step and generating a whole proof directly with an\nLLM. However, existing work makes no attempt to combine the two approaches. In\nthis work, we introduce HybridProver, a dual-model proof synthesis framework\nthat combines tactic-based generation and whole-proof synthesis to harness the\nbenefits of both approaches. HybridProver generates whole proof candidates for\nevaluation directly, then extracts proof sketches from those candidates. It\nthen uses a tactic-based generation model that integrates automated tools to\ncomplete the sketches via stepwise refinement. We implement HybridProver for\nthe Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle\ndatasets. Evaluation on the miniF2F dataset illustrates HybridProver's\neffectiveness. We achieve a 59.4% success rate on miniF2F, where the previous\nSOTA is 56.1%. Our ablation studies show that this SOTA result is attributable\nto combining whole-proof and tactic-based generation. Additionally, we show how\nthe dataset quality, training parameters, and sampling diversity affect the\nfinal result during automated theorem proving with LLMs. All of our code,\ndatasets, and LLMs are open source."
                },
                "authors": [
                    {
                        "name": "Jilin Hu"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Yongwang Zhao"
                    },
                    {
                        "name": "Talia Ringer"
                    }
                ],
                "author_detail": {
                    "name": "Talia Ringer"
                },
                "author": "Talia Ringer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15738v1",
                "updated": "2025-05-21T16:43:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    43,
                    17,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:43:17Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    43,
                    17,
                    2,
                    141,
                    0
                ],
                "title": "Alignment Under Pressure: The Case for Informed Adversaries When\n  Evaluating LLM Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment Under Pressure: The Case for Informed Adversaries When\n  Evaluating LLM Defenses"
                },
                "summary": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Yang"
                    },
                    {
                        "name": "Bozhidar Stevanoski"
                    },
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15734v1",
                "updated": "2025-05-21T16:40:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    40,
                    12,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:40:12Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    40,
                    12,
                    2,
                    141,
                    0
                ],
                "title": "DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning"
                },
                "summary": "Large language models (LLMs) have improved significantly in their reasoning\nthrough extensive training on massive datasets. However, relying solely on\nadditional data for improvement is becoming increasingly impractical,\nhighlighting the need for models to autonomously enhance their reasoning\nwithout external supervision. In this paper, we propose Debate, Train, Evolve\n(DTE), a novel ground truth-free training framework that uses multi-agent\ndebate traces to evolve a single language model. We also introduce a new\nprompting strategy Reflect-Critique-Refine, to improve debate quality by\nexplicitly instructing agents to critique and refine their reasoning. Extensive\nevaluations on five reasoning benchmarks with six open-weight models show that\nour DTE framework achieve substantial improvements, with an average accuracy\ngain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe\nstrong cross-domain generalization, with an average accuracy gain of 5.8% on\nall other benchmarks, suggesting that our method captures general reasoning\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have improved significantly in their reasoning\nthrough extensive training on massive datasets. However, relying solely on\nadditional data for improvement is becoming increasingly impractical,\nhighlighting the need for models to autonomously enhance their reasoning\nwithout external supervision. In this paper, we propose Debate, Train, Evolve\n(DTE), a novel ground truth-free training framework that uses multi-agent\ndebate traces to evolve a single language model. We also introduce a new\nprompting strategy Reflect-Critique-Refine, to improve debate quality by\nexplicitly instructing agents to critique and refine their reasoning. Extensive\nevaluations on five reasoning benchmarks with six open-weight models show that\nour DTE framework achieve substantial improvements, with an average accuracy\ngain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe\nstrong cross-domain generalization, with an average accuracy gain of 5.8% on\nall other benchmarks, suggesting that our method captures general reasoning\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Gaurav Srivastava"
                    },
                    {
                        "name": "Zhenyu Bi"
                    },
                    {
                        "name": "Meng Lu"
                    },
                    {
                        "name": "Xuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Wang"
                },
                "author": "Xuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15727v1",
                "updated": "2025-05-21T16:34:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    34,
                    7,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:34:07Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    34,
                    7,
                    2,
                    141,
                    0
                ],
                "title": "VocalBench: Benchmarking the Vocal Conversational Abilities for Speech\n  Interaction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VocalBench: Benchmarking the Vocal Conversational Abilities for Speech\n  Interaction Models"
                },
                "summary": "The rapid advancement of large language models (LLMs) has accelerated the\ndevelopment of multi-modal models capable of vocal communication. Unlike\ntext-based interactions, speech conveys rich and diverse information, including\nsemantic content, acoustic variations, paralanguage cues, and environmental\ncontext. However, existing evaluations of speech interaction models\npredominantly focus on the quality of their textual responses, often\noverlooking critical aspects of vocal performance and lacking benchmarks with\nvocal-specific test instances. To address this gap, we propose VocalBench, a\ncomprehensive benchmark designed to evaluate speech interaction models'\ncapabilities in vocal communication. VocalBench comprises 9,400 carefully\ncurated instances across four key dimensions: semantic quality, acoustic\nperformance, conversational abilities, and robustness. It covers 16 fundamental\nskills essential for effective vocal interaction. Experimental results reveal\nsignificant variability in current model capabilities, each exhibiting distinct\nstrengths and weaknesses, and provide valuable insights to guide future\nresearch in speech-based interaction systems. Code and evaluation instances are\navailable at https://github.com/SJTU-OmniAgent/VocalBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has accelerated the\ndevelopment of multi-modal models capable of vocal communication. Unlike\ntext-based interactions, speech conveys rich and diverse information, including\nsemantic content, acoustic variations, paralanguage cues, and environmental\ncontext. However, existing evaluations of speech interaction models\npredominantly focus on the quality of their textual responses, often\noverlooking critical aspects of vocal performance and lacking benchmarks with\nvocal-specific test instances. To address this gap, we propose VocalBench, a\ncomprehensive benchmark designed to evaluate speech interaction models'\ncapabilities in vocal communication. VocalBench comprises 9,400 carefully\ncurated instances across four key dimensions: semantic quality, acoustic\nperformance, conversational abilities, and robustness. It covers 16 fundamental\nskills essential for effective vocal interaction. Experimental results reveal\nsignificant variability in current model capabilities, each exhibiting distinct\nstrengths and weaknesses, and provide valuable insights to guide future\nresearch in speech-based interaction systems. Code and evaluation instances are\navailable at https://github.com/SJTU-OmniAgent/VocalBench."
                },
                "authors": [
                    {
                        "name": "Heyang Liu"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Ziyang Cheng"
                    },
                    {
                        "name": "Ronghua Wu"
                    },
                    {
                        "name": "Qunshan Gu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15725v1",
                "updated": "2025-05-21T16:31:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    31,
                    28,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:31:28Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    31,
                    28,
                    2,
                    141,
                    0
                ],
                "title": "UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV\n  Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV\n  Imitation Learning"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) are evolving into language-interactive\nplatforms, enabling more intuitive forms of human-drone interaction. While\nprior works have primarily focused on high-level planning and long-horizon\nnavigation, we shift attention to language-guided fine-grained trajectory\ncontrol, where UAVs execute short-range, reactive flight behaviors in response\nto language instructions. We formalize this problem as the Flying-on-a-Word\n(Flow) task and introduce UAV imitation learning as an effective approach. In\nthis framework, UAVs learn fine-grained control policies by mimicking expert\npilot trajectories paired with atomic language instructions. To support this\nparadigm, we present UAV-Flow, the first real-world benchmark for\nlanguage-conditioned, fine-grained UAV control. It includes a task formulation,\na large-scale dataset collected in diverse environments, a deployable control\nframework, and a simulation suite for systematic evaluation. Our design enables\nUAVs to closely imitate the precise, expert-level flight trajectories of human\npilots and supports direct deployment without sim-to-real gap. We conduct\nextensive experiments on UAV-Flow, benchmarking VLN and VLA paradigms. Results\nshow that VLA models are superior to VLN baselines and highlight the critical\nrole of spatial grounding in the fine-grained Flow setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) are evolving into language-interactive\nplatforms, enabling more intuitive forms of human-drone interaction. While\nprior works have primarily focused on high-level planning and long-horizon\nnavigation, we shift attention to language-guided fine-grained trajectory\ncontrol, where UAVs execute short-range, reactive flight behaviors in response\nto language instructions. We formalize this problem as the Flying-on-a-Word\n(Flow) task and introduce UAV imitation learning as an effective approach. In\nthis framework, UAVs learn fine-grained control policies by mimicking expert\npilot trajectories paired with atomic language instructions. To support this\nparadigm, we present UAV-Flow, the first real-world benchmark for\nlanguage-conditioned, fine-grained UAV control. It includes a task formulation,\na large-scale dataset collected in diverse environments, a deployable control\nframework, and a simulation suite for systematic evaluation. Our design enables\nUAVs to closely imitate the precise, expert-level flight trajectories of human\npilots and supports direct deployment without sim-to-real gap. We conduct\nextensive experiments on UAV-Flow, benchmarking VLN and VLA paradigms. Results\nshow that VLA models are superior to VLN baselines and highlight the critical\nrole of spatial grounding in the fine-grained Flow setting."
                },
                "authors": [
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Donglin Yang"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Wenhao Zheng"
                    },
                    {
                        "name": "wenjun wu"
                    },
                    {
                        "name": "Bin Dai"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15722v1",
                "updated": "2025-05-21T16:30:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    30,
                    18,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:30:18Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    30,
                    18,
                    2,
                    141,
                    0
                ],
                "title": "Shared Path: Unraveling Memorization in Multilingual LLMs through\n  Language Similarities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Path: Unraveling Memorization in Multilingual LLMs through\n  Language Similarities"
                },
                "summary": "We present the first comprehensive study of Memorization in Multilingual\nLarge Language Models (MLLMs), analyzing 95 languages using models across\ndiverse model scales, architectures, and memorization definitions. As MLLMs are\nincreasingly deployed, understanding their memorization behavior has become\ncritical. Yet prior work has focused primarily on monolingual models, leaving\nmultilingual memorization underexplored, despite the inherently long-tailed\nnature of training corpora. We find that the prevailing assumption, that\nmemorization is highly correlated with training data availability, fails to\nfully explain memorization patterns in MLLMs. We hypothesize that treating\nlanguages in isolation - ignoring their similarities - obscures the true\npatterns of memorization. To address this, we propose a novel graph-based\ncorrelation metric that incorporates language similarity to analyze\ncross-lingual memorization. Our analysis reveals that among similar languages,\nthose with fewer training tokens tend to exhibit higher memorization, a trend\nthat only emerges when cross-lingual relationships are explicitly modeled.\nThese findings underscore the importance of a language-aware perspective in\nevaluating and mitigating memorization vulnerabilities in MLLMs. This also\nconstitutes empirical evidence that language similarity both explains\nMemorization in MLLMs and underpins Cross-lingual Transferability, with broad\nimplications for multilingual NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of Memorization in Multilingual\nLarge Language Models (MLLMs), analyzing 95 languages using models across\ndiverse model scales, architectures, and memorization definitions. As MLLMs are\nincreasingly deployed, understanding their memorization behavior has become\ncritical. Yet prior work has focused primarily on monolingual models, leaving\nmultilingual memorization underexplored, despite the inherently long-tailed\nnature of training corpora. We find that the prevailing assumption, that\nmemorization is highly correlated with training data availability, fails to\nfully explain memorization patterns in MLLMs. We hypothesize that treating\nlanguages in isolation - ignoring their similarities - obscures the true\npatterns of memorization. To address this, we propose a novel graph-based\ncorrelation metric that incorporates language similarity to analyze\ncross-lingual memorization. Our analysis reveals that among similar languages,\nthose with fewer training tokens tend to exhibit higher memorization, a trend\nthat only emerges when cross-lingual relationships are explicitly modeled.\nThese findings underscore the importance of a language-aware perspective in\nevaluating and mitigating memorization vulnerabilities in MLLMs. This also\nconstitutes empirical evidence that language similarity both explains\nMemorization in MLLMs and underpins Cross-lingual Transferability, with broad\nimplications for multilingual NLP."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Luo"
                    },
                    {
                        "name": "Yiyi Chen"
                    },
                    {
                        "name": "Johannes Bjerva"
                    },
                    {
                        "name": "Qiongxiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Qiongxiu Li"
                },
                "author": "Qiongxiu Li",
                "arxiv_comment": "17 pages, 14 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01565v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01565v6",
                "updated": "2025-05-21T16:29:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    29,
                    45,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-03T13:36:34Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    13,
                    36,
                    34,
                    6,
                    308,
                    0
                ],
                "title": "SQL Injection Jailbreak: A Structural Disaster of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL Injection Jailbreak: A Structural Disaster of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are susceptible to jailbreak attacks that can\ninduce them to generate harmful content. Previous jailbreak methods primarily\nexploited the internal properties or capabilities of LLMs, such as\noptimization-based jailbreak methods and methods that leveraged the model's\ncontext-learning abilities. In this paper, we introduce a novel jailbreak\nmethod, SQL Injection Jailbreak (SIJ), which targets the external properties of\nLLMs, specifically, the way LLMs construct input prompts. By injecting\njailbreak information into user prompts, SIJ successfully induces the model to\noutput harmful content. For open-source models, SIJ achieves near 100% attack\nsuccess rates on five well-known LLMs on the AdvBench and HEx-PHI, while\nincurring lower time costs compared to previous methods. For closed-source\nmodels, SIJ achieves an average attack success rate over 85% across five models\nin the GPT and Doubao series. Additionally, SIJ exposes a new vulnerability in\nLLMs that urgently requires mitigation. To address this, we propose a simple\nadaptive defense method called Self-Reminder-Key to counter SIJ and demonstrate\nits effectiveness through experimental results. Our code is available at\nhttps://github.com/weiyezhimeng/SQL-Injection-Jailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to jailbreak attacks that can\ninduce them to generate harmful content. Previous jailbreak methods primarily\nexploited the internal properties or capabilities of LLMs, such as\noptimization-based jailbreak methods and methods that leveraged the model's\ncontext-learning abilities. In this paper, we introduce a novel jailbreak\nmethod, SQL Injection Jailbreak (SIJ), which targets the external properties of\nLLMs, specifically, the way LLMs construct input prompts. By injecting\njailbreak information into user prompts, SIJ successfully induces the model to\noutput harmful content. For open-source models, SIJ achieves near 100% attack\nsuccess rates on five well-known LLMs on the AdvBench and HEx-PHI, while\nincurring lower time costs compared to previous methods. For closed-source\nmodels, SIJ achieves an average attack success rate over 85% across five models\nin the GPT and Doubao series. Additionally, SIJ exposes a new vulnerability in\nLLMs that urgently requires mitigation. To address this, we propose a simple\nadaptive defense method called Self-Reminder-Key to counter SIJ and demonstrate\nits effectiveness through experimental results. Our code is available at\nhttps://github.com/weiyezhimeng/SQL-Injection-Jailbreak."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "Accepted by findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01565v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01565v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15715v1",
                "updated": "2025-05-21T16:24:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    24,
                    49,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:24:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    24,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with\n  Large Language Models for Mental Health Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with\n  Large Language Models for Mental Health Counseling"
                },
                "summary": "Large language models (LLMs) hold significant potential for mental health\nsupport, capable of generating empathetic responses and simulating therapeutic\nconversations. However, existing LLM-based approaches often lack the clinical\ngrounding necessary for real-world psychological counseling, particularly in\nexplicit diagnostic reasoning aligned with standards like the DSM/ICD and\nincorporating diverse therapeutic modalities beyond basic empathy or single\nstrategies. To address these critical limitations, we propose PsyLLM, the first\nlarge language model designed to systematically integrate both diagnostic and\ntherapeutic reasoning for mental health counseling. To develop the PsyLLM, we\npropose a novel automated data synthesis pipeline. This pipeline processes\nreal-world mental health posts, generates multi-turn dialogue structures, and\nleverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and\nmultiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate\ndetailed clinical reasoning processes. Rigorous multi-dimensional filtering\nensures the generation of high-quality, clinically aligned dialogue data. In\naddition, we introduce a new benchmark and evaluation protocol, assessing\ncounseling quality across four key dimensions: comprehensiveness,\nprofessionalism, authenticity, and safety. Our experiments demonstrate that\nPsyLLM significantly outperforms state-of-the-art baseline models on this\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold significant potential for mental health\nsupport, capable of generating empathetic responses and simulating therapeutic\nconversations. However, existing LLM-based approaches often lack the clinical\ngrounding necessary for real-world psychological counseling, particularly in\nexplicit diagnostic reasoning aligned with standards like the DSM/ICD and\nincorporating diverse therapeutic modalities beyond basic empathy or single\nstrategies. To address these critical limitations, we propose PsyLLM, the first\nlarge language model designed to systematically integrate both diagnostic and\ntherapeutic reasoning for mental health counseling. To develop the PsyLLM, we\npropose a novel automated data synthesis pipeline. This pipeline processes\nreal-world mental health posts, generates multi-turn dialogue structures, and\nleverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and\nmultiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate\ndetailed clinical reasoning processes. Rigorous multi-dimensional filtering\nensures the generation of high-quality, clinically aligned dialogue data. In\naddition, we introduce a new benchmark and evaluation protocol, assessing\ncounseling quality across four key dimensions: comprehensiveness,\nprofessionalism, authenticity, and safety. Our experiments demonstrate that\nPsyLLM significantly outperforms state-of-the-art baseline models on this\nbenchmark."
                },
                "authors": [
                    {
                        "name": "He Hu"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Juzheng Si"
                    },
                    {
                        "name": "Qianning Wang"
                    },
                    {
                        "name": "Hengheng Zhang"
                    },
                    {
                        "name": "Fuji Ren"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Laizhong Cui"
                    }
                ],
                "author_detail": {
                    "name": "Laizhong Cui"
                },
                "author": "Laizhong Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15712v1",
                "updated": "2025-05-21T16:22:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    22,
                    32,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:22:32Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    22,
                    32,
                    2,
                    141,
                    0
                ],
                "title": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games"
                },
                "summary": "This paper introduces TurnaboutLLM, a novel framework and dataset for\nevaluating the deductive reasoning abilities of Large Language Models (LLMs) by\nleveraging the interactive gameplay of detective games Ace Attorney and\nDanganronpa. The framework tasks LLMs with identifying contradictions between\ntestimonies and evidences within long narrative contexts, a challenging task\ndue to the large answer space and diverse reasoning types presented by its\nquestions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at\nlimitations of popular strategies for enhancing deductive reasoning such as\nextensive thinking and Chain-of-Thought prompting. The results also suggest\nvarying effects of context size, the number of reasoning step and answer space\nsize on model performance. Overall, TurnaboutLLM presents a substantial\nchallenge for LLMs' deductive reasoning abilities in complex, narrative-rich\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TurnaboutLLM, a novel framework and dataset for\nevaluating the deductive reasoning abilities of Large Language Models (LLMs) by\nleveraging the interactive gameplay of detective games Ace Attorney and\nDanganronpa. The framework tasks LLMs with identifying contradictions between\ntestimonies and evidences within long narrative contexts, a challenging task\ndue to the large answer space and diverse reasoning types presented by its\nquestions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at\nlimitations of popular strategies for enhancing deductive reasoning such as\nextensive thinking and Chain-of-Thought prompting. The results also suggest\nvarying effects of context size, the number of reasoning step and answer space\nsize on model performance. Overall, TurnaboutLLM presents a substantial\nchallenge for LLMs' deductive reasoning abilities in complex, narrative-rich\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Muyu He"
                    },
                    {
                        "name": "Muhammad Adil Shahid"
                    },
                    {
                        "name": "Jiani Huang"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15710v1",
                "updated": "2025-05-21T16:21:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    21,
                    29,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:21:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    21,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "Advancing LLM Safe Alignment with Safety Representation Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing LLM Safe Alignment with Safety Representation Ranking"
                },
                "summary": "The rapid advancement of large language models (LLMs) has demonstrated\nmilestone success in a variety of tasks, yet their potential for generating\nharmful content has raised significant safety concerns. Existing safety\nevaluation approaches typically operate directly on textual responses,\noverlooking the rich information embedded in the model's internal\nrepresentations. In this paper, we propose Safety Representation Ranking (SRR),\na listwise ranking framework that selects safe responses using hidden states\nfrom the LLM itself. SRR encodes both instructions and candidate completions\nusing intermediate transformer representations and ranks candidates via a\nlightweight similarity-based scorer. Our approach directly leverages internal\nmodel states and supervision at the list level to capture subtle safety\nsignals. Experiments across multiple benchmarks show that SRR significantly\nimproves robustness to adversarial prompts. Our code will be available upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has demonstrated\nmilestone success in a variety of tasks, yet their potential for generating\nharmful content has raised significant safety concerns. Existing safety\nevaluation approaches typically operate directly on textual responses,\noverlooking the rich information embedded in the model's internal\nrepresentations. In this paper, we propose Safety Representation Ranking (SRR),\na listwise ranking framework that selects safe responses using hidden states\nfrom the LLM itself. SRR encodes both instructions and candidate completions\nusing intermediate transformer representations and ranks candidates via a\nlightweight similarity-based scorer. Our approach directly leverages internal\nmodel states and supervision at the list level to capture subtle safety\nsignals. Experiments across multiple benchmarks show that SRR significantly\nimproves robustness to adversarial prompts. Our code will be available upon\npublication."
                },
                "authors": [
                    {
                        "name": "Tianqi Du"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15701v1",
                "updated": "2025-05-21T16:14:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    14,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:14:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    14,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL\n  Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL\n  Graph Databases"
                },
                "summary": "Large Language Models (LLMs) have demonstrated their potential in hardware\ndesign tasks, such as Hardware Description Language (HDL) generation and\ndebugging. Yet, their performance in real-world, repository-level HDL projects\nwith thousands or even tens of thousands of code lines is hindered. To this\nend, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\nAugmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\nrepresentations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\nGraphs (DFGs) to capture both code graph view and hardware graph view.\nHDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\nlimited recall issues inherent in similarity-based semantic retrieval by\nincorporating structural information, but also enhances its extensibility to\nvarious real-world tasks by a task-specific retrieval finetuning. Additionally,\nto address the lack of comprehensive HDL search benchmarks, we introduce\nHDLSearch, a multi-granularity evaluation dataset derived from real-world\nrepository-level projects. Experimental results demonstrate that HDLxGraph\nsignificantly improves average search accuracy, debugging efficiency and\ncompletion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\nRAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\navailable at https://github.com/Nick-Zheng-Q/HDLxGraph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated their potential in hardware\ndesign tasks, such as Hardware Description Language (HDL) generation and\ndebugging. Yet, their performance in real-world, repository-level HDL projects\nwith thousands or even tens of thousands of code lines is hindered. To this\nend, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\nAugmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\nrepresentations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\nGraphs (DFGs) to capture both code graph view and hardware graph view.\nHDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\nlimited recall issues inherent in similarity-based semantic retrieval by\nincorporating structural information, but also enhances its extensibility to\nvarious real-world tasks by a task-specific retrieval finetuning. Additionally,\nto address the lack of comprehensive HDL search benchmarks, we introduce\nHDLSearch, a multi-granularity evaluation dataset derived from real-world\nrepository-level projects. Experimental results demonstrate that HDLxGraph\nsignificantly improves average search accuracy, debugging efficiency and\ncompletion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\nRAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\navailable at https://github.com/Nick-Zheng-Q/HDLxGraph."
                },
                "authors": [
                    {
                        "name": "Pingqing Zheng"
                    },
                    {
                        "name": "Jiayin Qin"
                    },
                    {
                        "name": "Fuqi Zhang"
                    },
                    {
                        "name": "Shang Wu"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Caiwen Ding"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhao"
                },
                "arxiv_affiliation": "Katie",
                "author": "Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09847v2",
                "updated": "2025-05-21T16:12:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    12,
                    30,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-14T23:12:20Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    23,
                    12,
                    20,
                    2,
                    134,
                    0
                ],
                "title": "Causal Predictive Optimization and Generation for Business AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Predictive Optimization and Generation for Business AI"
                },
                "summary": "The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field."
                },
                "authors": [
                    {
                        "name": "Liyang Zhao"
                    },
                    {
                        "name": "Olurotimi Seton"
                    },
                    {
                        "name": "Himadeep Reddy Reddivari"
                    },
                    {
                        "name": "Suvendu Jena"
                    },
                    {
                        "name": "Shadow Zhao"
                    },
                    {
                        "name": "Rachit Kumar"
                    },
                    {
                        "name": "Changshuai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Changshuai Wei"
                },
                "author": "Changshuai Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08185v2",
                "updated": "2025-05-21T16:10:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    10,
                    6,
                    2,
                    141,
                    0
                ],
                "published": "2024-09-12T16:20:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    20,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Fine-tuning Large Language Models for Entity Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for Entity Matching"
                },
                "summary": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and ability to generalize to unseen entities. Existing research on\nusing LLMs for entity matching has focused on prompt engineering and in-context\nlearning. This paper explores the potential of fine-tuning LLMs for entity\nmatching. We analyze fine-tuning along two dimensions: 1) the representation of\ntraining examples, where we experiment with adding different types of\nLLM-generated explanations to the training set, and 2) the selection and\ngeneration of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodels ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods, only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and ability to generalize to unseen entities. Existing research on\nusing LLMs for entity matching has focused on prompt engineering and in-context\nlearning. This paper explores the potential of fine-tuning LLMs for entity\nmatching. We analyze fine-tuning along two dimensions: 1) the representation of\ntraining examples, where we experiment with adding different types of\nLLM-generated explanations to the training set, and 2) the selection and\ngeneration of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodels ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods, only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Aaron Steiner"
                    },
                    {
                        "name": "Ralph Peeters"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "arxiv_comment": "8 pages, 4 figures. For related code and data, see this\n  https://github.com/wbsg-uni-mannheim/TailorMatch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15695v1",
                "updated": "2025-05-21T16:09:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    9,
                    44,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:09:44Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    9,
                    44,
                    2,
                    141,
                    0
                ],
                "title": "Can Large Language Models be Effective Online Opinion Miners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models be Effective Online Opinion Miners?"
                },
                "summary": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield."
                },
                "authors": [
                    {
                        "name": "Ryang Heo"
                    },
                    {
                        "name": "Yongsik Seo"
                    },
                    {
                        "name": "Junseong Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10814v2",
                "updated": "2025-05-21T16:09:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    9,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2024-12-14T12:39:19Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    12,
                    39,
                    19,
                    5,
                    349,
                    0
                ],
                "title": "Diffusion-based Method for Satellite Pattern-of-Life Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Method for Satellite Pattern-of-Life Identification"
                },
                "summary": "Satellite pattern-of-life (PoL) identification is crucial for space safety\nand satellite monitoring, involving the analysis of typical satellite behaviors\nsuch as station-keeping, drift, etc. However, existing PoL identification\nmethods remain underdeveloped due to the complexity of aerospace systems,\nvariability in satellite behaviors, and fluctuating observation sampling rates.\nIn a first attempt, we developed a domain expertise-informed machine learning\nmethod (Expert-ML) to combine satellite orbital movement knowledge and machine\nlearning models. The Expert-ML method achieved high accuracy results in\nsimulation data and real-world data with normal sampling rate. However, this\napproach lacks of generality as it requires domain expertise and its\nperformance degraded significantly when data sampling rate varied. To achieve\ngenerality, we propose a novel diffusion-based PoL identification method.\nDistinct from prior approaches, the proposed method leverages a diffusion model\nto achieve end-to-end identification without manual refinement or\ndomain-specific knowledge. Specifically, we employ a multivariate time-series\nencoder to capture hidden representations of satellite positional data. The\nencoded features are subsequently incorporated as conditional information in\nthe denoising process to generate PoL labels. Through experimentation across\nreal-world satellite settings, our proposed diffusion-based method demonstrates\nits high identification quality and provides a robust solution even with\nreduced data sampling rates, indicating its great potential in practical\nsatellite behavior pattern identification, tracking and related mission\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellite pattern-of-life (PoL) identification is crucial for space safety\nand satellite monitoring, involving the analysis of typical satellite behaviors\nsuch as station-keeping, drift, etc. However, existing PoL identification\nmethods remain underdeveloped due to the complexity of aerospace systems,\nvariability in satellite behaviors, and fluctuating observation sampling rates.\nIn a first attempt, we developed a domain expertise-informed machine learning\nmethod (Expert-ML) to combine satellite orbital movement knowledge and machine\nlearning models. The Expert-ML method achieved high accuracy results in\nsimulation data and real-world data with normal sampling rate. However, this\napproach lacks of generality as it requires domain expertise and its\nperformance degraded significantly when data sampling rate varied. To achieve\ngenerality, we propose a novel diffusion-based PoL identification method.\nDistinct from prior approaches, the proposed method leverages a diffusion model\nto achieve end-to-end identification without manual refinement or\ndomain-specific knowledge. Specifically, we employ a multivariate time-series\nencoder to capture hidden representations of satellite positional data. The\nencoded features are subsequently incorporated as conditional information in\nthe denoising process to generate PoL labels. Through experimentation across\nreal-world satellite settings, our proposed diffusion-based method demonstrates\nits high identification quality and provides a robust solution even with\nreduced data sampling rates, indicating its great potential in practical\nsatellite behavior pattern identification, tracking and related mission\ndeployment."
                },
                "authors": [
                    {
                        "name": "Yongchao Ye"
                    },
                    {
                        "name": "Xinting Zhu"
                    },
                    {
                        "name": "Xuejin Shen"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Lishuai Li"
                    },
                    {
                        "name": "S. Joe Qin"
                    }
                ],
                "author_detail": {
                    "name": "S. Joe Qin"
                },
                "author": "S. Joe Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15690v1",
                "updated": "2025-05-21T16:05:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    5,
                    29,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:05:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    5,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "Toward Open Earth Science as Fast and Accessible as Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Open Earth Science as Fast and Accessible as Natural Language"
                },
                "summary": "Is natural-language-driven earth observation data analysis now feasible with\nthe assistance of Large Language Models (LLMs)? For open science in service of\npublic interest, feasibility requires reliably high accuracy, interactive\nlatencies, low (sustainable) costs, open LLMs, and openly maintainable software\n-- hence, the challenge. What are the techniques and programming system\nrequirements necessary for satisfying these constraints, and what is the\ncorresponding development and maintenance burden in practice? This study lays\nthe groundwork for exploring these questions, introducing an impactful earth\nscience use-case, and providing a software framework with evaluation data and\nmetrics, along with initial results from employing model scaling,\nprompt-optimization, and inference-time scaling optimization techniques. While\nwe attain high accuracy (near 100%) across 10 of 11 metrics, the analysis\nfurther considers cost (token-spend), latency, and maintainability across this\nspace of techniques. Finally, we enumerate opportunities for further research,\ngeneral programming and evaluation framework development, and ongoing work for\na comprehensive, deployable solution. This is a call for collaboration and\ncontribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is natural-language-driven earth observation data analysis now feasible with\nthe assistance of Large Language Models (LLMs)? For open science in service of\npublic interest, feasibility requires reliably high accuracy, interactive\nlatencies, low (sustainable) costs, open LLMs, and openly maintainable software\n-- hence, the challenge. What are the techniques and programming system\nrequirements necessary for satisfying these constraints, and what is the\ncorresponding development and maintenance burden in practice? This study lays\nthe groundwork for exploring these questions, introducing an impactful earth\nscience use-case, and providing a software framework with evaluation data and\nmetrics, along with initial results from employing model scaling,\nprompt-optimization, and inference-time scaling optimization techniques. While\nwe attain high accuracy (near 100%) across 10 of 11 metrics, the analysis\nfurther considers cost (token-spend), latency, and maintainability across this\nspace of techniques. Finally, we enumerate opportunities for further research,\ngeneral programming and evaluation framework development, and ongoing work for\na comprehensive, deployable solution. This is a call for collaboration and\ncontribution."
                },
                "authors": [
                    {
                        "name": "Marquita Ellis"
                    },
                    {
                        "name": "Iksha Gurung"
                    },
                    {
                        "name": "Muthukumaran Ramasubramanian"
                    },
                    {
                        "name": "Rahul Ramachandran"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Ramachandran"
                },
                "author": "Rahul Ramachandran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2; H.5.2; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17720v3",
                "updated": "2025-05-21T16:04:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    4,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-24T23:23:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    23,
                    23,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Spontaneous Giving and Calculated Greed in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spontaneous Giving and Calculated Greed in Language Models"
                },
                "summary": "Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Hirokazu Shirado"
                    }
                ],
                "author_detail": {
                    "name": "Hirokazu Shirado"
                },
                "author": "Hirokazu Shirado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15687v1",
                "updated": "2025-05-21T16:03:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    3,
                    3,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:03:03Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    3,
                    3,
                    2,
                    141,
                    0
                ],
                "title": "Discovering Pathology Rationale and Token Allocation for Efficient\n  Multimodal Pathology Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Pathology Rationale and Token Allocation for Efficient\n  Multimodal Pathology Reasoning"
                },
                "summary": "Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Cheng Jin"
                    },
                    {
                        "name": "Yihui Wang"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15685v1",
                "updated": "2025-05-21T16:01:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    1,
                    11,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T16:01:11Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    1,
                    11,
                    2,
                    141,
                    0
                ],
                "title": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems"
                },
                "summary": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions."
                },
                "authors": [
                    {
                        "name": "Xiuchao Sui"
                    },
                    {
                        "name": "Daiying Tian"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Kenneth Kwok"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "17 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15684v1",
                "updated": "2025-05-21T15:58:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy"
                },
                "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption."
                },
                "authors": [
                    {
                        "name": "Gengyang Li"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v1",
                "updated": "2025-05-21T15:58:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability"
                },
                "summary": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Jiaying Zheng"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Jin Dong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17531v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17531v3",
                "updated": "2025-05-22T10:57:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    10,
                    57,
                    51,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-24T13:19:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    19,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "Towards Machine-Generated Code for the Resolution of User Intentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Machine-Generated Code for the Resolution of User Intentions"
                },
                "summary": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code. This development represents a significant progression in\nthe realm of hybrid workflows, where human and artificial intelligence\ncollaborate to address user intentions, with the former responsible for\ndefining these intentions and the latter for implementing the solutions to\naddress them. In this paper, we investigate the feasibility of generating and\nexecuting workflows through code generation that results from prompting an LLM\nwith a concrete user intention, and a simplified application programming\ninterface for a GUI-less operating system. We provide an in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate the general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code. This development represents a significant progression in\nthe realm of hybrid workflows, where human and artificial intelligence\ncollaborate to address user intentions, with the former responsible for\ndefining these intentions and the latter for implementing the solutions to\naddress them. In this paper, we investigate the feasibility of generating and\nexecuting workflows through code generation that results from prompting an LLM\nwith a concrete user intention, and a simplified application programming\ninterface for a GUI-less operating system. We provide an in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate the general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions."
                },
                "authors": [
                    {
                        "name": "Justus Flerlage"
                    },
                    {
                        "name": "Ilja Behnke"
                    },
                    {
                        "name": "Odej Kao"
                    }
                ],
                "author_detail": {
                    "name": "Odej Kao"
                },
                "author": "Odej Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17531v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17531v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15674v1",
                "updated": "2025-05-21T15:53:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    53,
                    28,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:53:28Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    53,
                    28,
                    2,
                    141,
                    0
                ],
                "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language\n  Models"
                },
                "summary": "Large language models require iterative updates to address challenges such as\nknowledge conflicts and outdated information (e.g., incorrect, private, or\nillegal contents). Machine unlearning provides a systematic methodology for\ntargeted knowledge removal from trained models, enabling elimination of\nsensitive information influences. However, mainstream fine-tuning-based\nunlearning methods often fail to balance unlearning efficacy and model ability,\nfrequently resulting in catastrophic model collapse under extensive knowledge\nremoval. Meanwhile, in-context unlearning, which relies solely on contextual\nprompting without modifying the model's intrinsic mechanisms, suffers from\nlimited generalizability and struggles to achieve true unlearning. In this\nwork, we introduce UniErase, a novel unlearning paradigm that employs learnable\nparametric suffix (unlearning token) to steer language models toward targeted\nforgetting behaviors. UniErase operates through two key phases: (I) an\noptimization stage that binds desired unlearning outputs to the model's\nautoregressive probability distribution via token optimization, followed by\n(II) a lightweight model editing phase that activates the learned token to\nprobabilistically induce specified forgetting objective. Serving as a new\nresearch direction for token learning to induce unlearning target, UniErase\nachieves state-of-the-art (SOTA) performance across batch, sequential, and\nprecise unlearning under fictitious and real-world knowledge settings.\nRemarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%\nof the LLM parameters, outperforms previous forgetting SOTA baseline by around\n4.01 times for model ability with even better unlearning efficacy. Similarly,\nUniErase, maintaining more ability, also surpasses previous retaining SOTA by\n35.96% for unlearning efficacy, showing dual top-tier performances in current\nunlearing domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require iterative updates to address challenges such as\nknowledge conflicts and outdated information (e.g., incorrect, private, or\nillegal contents). Machine unlearning provides a systematic methodology for\ntargeted knowledge removal from trained models, enabling elimination of\nsensitive information influences. However, mainstream fine-tuning-based\nunlearning methods often fail to balance unlearning efficacy and model ability,\nfrequently resulting in catastrophic model collapse under extensive knowledge\nremoval. Meanwhile, in-context unlearning, which relies solely on contextual\nprompting without modifying the model's intrinsic mechanisms, suffers from\nlimited generalizability and struggles to achieve true unlearning. In this\nwork, we introduce UniErase, a novel unlearning paradigm that employs learnable\nparametric suffix (unlearning token) to steer language models toward targeted\nforgetting behaviors. UniErase operates through two key phases: (I) an\noptimization stage that binds desired unlearning outputs to the model's\nautoregressive probability distribution via token optimization, followed by\n(II) a lightweight model editing phase that activates the learned token to\nprobabilistically induce specified forgetting objective. Serving as a new\nresearch direction for token learning to induce unlearning target, UniErase\nachieves state-of-the-art (SOTA) performance across batch, sequential, and\nprecise unlearning under fictitious and real-world knowledge settings.\nRemarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%\nof the LLM parameters, outperforms previous forgetting SOTA baseline by around\n4.01 times for model ability with even better unlearning efficacy. Similarly,\nUniErase, maintaining more ability, also surpasses previous retaining SOTA by\n35.96% for unlearning efficacy, showing dual top-tier performances in current\nunlearing domain."
                },
                "authors": [
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17216v2",
                "updated": "2025-05-21T15:51:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    51,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-24T14:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    49,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM\n  Reasoning"
                },
                "summary": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, it remains unclear what the contributing factors to the success\nof Neurosymbolic LLM reasoning are. This paper shows that one important factor\nis the choice of the formal language. By comparing 4 formal languages on 3\ndatasets over 6 LLMs, we show that the choice of formal language affects both\nthe syntactic and the semantic reasoning capability. Thereby, we introduce the\nintermediate language challenge, which is the challenge of picking a suitable\nformal language for neurosymbolic reasoning. Further, we compare the effects of\nusing different in-context-learning examples in an ablation study. We conclude\nthat on average, context-aware encodings help LLMs to reason, while there is no\napparent effect of using comments or markdown syntax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, it remains unclear what the contributing factors to the success\nof Neurosymbolic LLM reasoning are. This paper shows that one important factor\nis the choice of the formal language. By comparing 4 formal languages on 3\ndatasets over 6 LLMs, we show that the choice of formal language affects both\nthe syntactic and the semantic reasoning capability. Thereby, we introduce the\nintermediate language challenge, which is the challenge of picking a suitable\nformal language for neurosymbolic reasoning. Further, we compare the effects of\nusing different in-context-learning examples in an ablation study. We conclude\nthat on average, context-aware encodings help LLMs to reason, while there is no\napparent effect of using comments or markdown syntax."
                },
                "authors": [
                    {
                        "name": "Alexander Beiser"
                    },
                    {
                        "name": "David Penz"
                    },
                    {
                        "name": "Nysret Musliu"
                    }
                ],
                "author_detail": {
                    "name": "Nysret Musliu"
                },
                "author": "Nysret Musliu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15670v1",
                "updated": "2025-05-21T15:48:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    48,
                    30,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:48:30Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    48,
                    30,
                    2,
                    141,
                    0
                ],
                "title": "Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model"
                },
                "summary": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility."
                },
                "authors": [
                    {
                        "name": "Ke Hu"
                    },
                    {
                        "name": "Ehsan Hosseini-Asl"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Edresson Casanova"
                    },
                    {
                        "name": "Subhankar Ghosh"
                    },
                    {
                        "name": "Piotr Żelasko"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Jason Li"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12788v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12788v3",
                "updated": "2025-05-21T15:45:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    45,
                    6,
                    2,
                    141,
                    0
                ],
                "published": "2024-10-16T17:59:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    59,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Meta-Chunking: Learning Text Segmentation and Semantic Completion via\n  Logical Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Chunking: Learning Text Segmentation and Semantic Completion via\n  Logical Perception"
                },
                "summary": "While Retrieval-Augmented Generation (RAG) has emerged as a promising\nparadigm for boosting large language models (LLMs) in knowledge-intensive\ntasks, it often overlooks the crucial aspect of text chunking within its\nworkflow. This paper proposes the Meta-Chunking framework, which specifically\nenhances chunking quality through a dual strategy that identifies optimal\nsegmentation points and preserves global information. Initially, breaking\nlimitations of similarity-based chunking, we design two adaptive chunking\ntechniques based on uncertainty, namely Perplexity Chunking and Margin Sampling\nChunking, by utilizing the logical perception capabilities of LLMs. Given the\ninherent complexity across different texts, we integrate meta-chunk with\ndynamic merging, striking a balance between fine-grained and coarse-grained\ntext chunking. Furthermore, we establish the global information compensation\nmechanism, encompassing a two-stage hierarchical summary generation process and\na three-stage text chunk rewriting procedure focused on missing reflection,\nrefinement, and completion. These components collectively strengthen the\nsemantic integrity and contextual coherence of chunks. Extensive experiments\ndemonstrate that Meta-Chunking effectively addresses challenges of the chunking\ntask within the RAG system, providing LLMs with more logically coherent text\nchunks. Additionally, our methodology validates the feasibility of implementing\nhigh-quality chunking tasks with smaller-scale models, thereby eliminating the\nreliance on robust instruction-following capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval-Augmented Generation (RAG) has emerged as a promising\nparadigm for boosting large language models (LLMs) in knowledge-intensive\ntasks, it often overlooks the crucial aspect of text chunking within its\nworkflow. This paper proposes the Meta-Chunking framework, which specifically\nenhances chunking quality through a dual strategy that identifies optimal\nsegmentation points and preserves global information. Initially, breaking\nlimitations of similarity-based chunking, we design two adaptive chunking\ntechniques based on uncertainty, namely Perplexity Chunking and Margin Sampling\nChunking, by utilizing the logical perception capabilities of LLMs. Given the\ninherent complexity across different texts, we integrate meta-chunk with\ndynamic merging, striking a balance between fine-grained and coarse-grained\ntext chunking. Furthermore, we establish the global information compensation\nmechanism, encompassing a two-stage hierarchical summary generation process and\na three-stage text chunk rewriting procedure focused on missing reflection,\nrefinement, and completion. These components collectively strengthen the\nsemantic integrity and contextual coherence of chunks. Extensive experiments\ndemonstrate that Meta-Chunking effectively addresses challenges of the chunking\ntask within the RAG system, providing LLMs with more logically coherent text\nchunks. Additionally, our methodology validates the feasibility of implementing\nhigh-quality chunking tasks with smaller-scale models, thereby eliminating the\nreliance on robust instruction-following capabilities."
                },
                "authors": [
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Yuchen Feng"
                    },
                    {
                        "name": "Pengnian Qi"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12788v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12788v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15660v1",
                "updated": "2025-05-21T15:35:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    35,
                    57,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:35:57Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    35,
                    57,
                    2,
                    141,
                    0
                ],
                "title": "Exploring the Limits of Vision-Language-Action Manipulations in\n  Cross-task Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of Vision-Language-Action Manipulations in\n  Cross-task Generalization"
                },
                "summary": "The generalization capabilities of vision-language-action (VLA) models to\nunseen tasks are crucial to achieving general-purpose robotic manipulation in\nopen-world settings. However, the cross-task generalization capabilities of\nexisting VLA models remain significantly underexplored. To address this gap, we\nintroduce AGNOSTOS, a novel simulation benchmark designed to rigorously\nevaluate cross-task zero-shot generalization in manipulation. AGNOSTOS\ncomprises 23 unseen manipulation tasks for testing, distinct from common\ntraining task distributions, and incorporates two levels of generalization\ndifficulty to assess robustness. Our systematic evaluation reveals that current\nVLA models, despite being trained on diverse datasets, struggle to generalize\neffectively to these unseen tasks. To overcome this limitation, we propose\nCross-Task In-Context Manipulation (X-ICM), a method that conditions large\nlanguage models (LLMs) on in-context demonstrations from seen tasks to predict\naction sequences for unseen tasks. Additionally, we introduce a dynamics-guided\nsample selection strategy that identifies relevant demonstrations by capturing\ncross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task\nzero-shot generalization performance over leading VLAs. We believe AGNOSTOS and\nX-ICM will serve as valuable tools for advancing general-purpose robotic\nmanipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generalization capabilities of vision-language-action (VLA) models to\nunseen tasks are crucial to achieving general-purpose robotic manipulation in\nopen-world settings. However, the cross-task generalization capabilities of\nexisting VLA models remain significantly underexplored. To address this gap, we\nintroduce AGNOSTOS, a novel simulation benchmark designed to rigorously\nevaluate cross-task zero-shot generalization in manipulation. AGNOSTOS\ncomprises 23 unseen manipulation tasks for testing, distinct from common\ntraining task distributions, and incorporates two levels of generalization\ndifficulty to assess robustness. Our systematic evaluation reveals that current\nVLA models, despite being trained on diverse datasets, struggle to generalize\neffectively to these unseen tasks. To overcome this limitation, we propose\nCross-Task In-Context Manipulation (X-ICM), a method that conditions large\nlanguage models (LLMs) on in-context demonstrations from seen tasks to predict\naction sequences for unseen tasks. Additionally, we introduce a dynamics-guided\nsample selection strategy that identifies relevant demonstrations by capturing\ncross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task\nzero-shot generalization performance over leading VLAs. We believe AGNOSTOS and\nX-ICM will serve as valuable tools for advancing general-purpose robotic\nmanipulation."
                },
                "authors": [
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Jiayi Liu"
                    },
                    {
                        "name": "Teli Ma"
                    },
                    {
                        "name": "Zifang Wang"
                    },
                    {
                        "name": "Ronghe Qiu"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Zhilin Zhao"
                    },
                    {
                        "name": "Junwei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Liang"
                },
                "author": "Junwei Liang",
                "arxiv_comment": "Project Page: https://jiaming-zhou.github.io/AGNOSTOS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10900v2",
                "updated": "2025-05-21T15:33:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    33,
                    20,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-16T06:07:19Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    6,
                    7,
                    19,
                    4,
                    136,
                    0
                ],
                "title": "Explain What You Mean: Intent Augmented Knowledge Graph Recommender\n  Built With An LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explain What You Mean: Intent Augmented Knowledge Graph Recommender\n  Built With An LLM"
                },
                "summary": "Interaction sparsity is a long-standing challenge in recommendation systems.\nSparsity manifests in environments with disproportional cardinality of\ngroupings of entities, such as users and products in an online marketplace. It\nis also found for newly introduced entities, described as the cold-start\nproblem. Recent efforts to mitigate this issue either enrich the connectivity\ndata by incorporating social networks or external knowledge graphs, or\nfine-tune LLMs into interaction augmenters or next-item recommenders. However,\nthese techniques tend to be resource demanding, requiring high computational\npower. They also have several limitations, including data availability, low\nquality, or synthetic noise issues. In this work, we propose LLM-based Intent\nKnowledge Graph Recommender (IKGR), a novel framework that leverages\nretrieval-augmented generation and an encoding approach to construct and\ndensify a knowledge graph. IKGR leverages latent user-item affinities from an\ninteraction knowledge graph and further densifies it through mutual intent\nconnectivity. This addresses sparsity issues and allows the model to make\nintent-grounded recommendations with an interpretable embedding translation\nlayer. Through extensive experiments on real-world datasets, we demonstrate\nthat IKGR overcomes knowledge gaps and achieves substantial gains over\nstate-of-the-art baselines on both publicly available and our internal\nrecommendation datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction sparsity is a long-standing challenge in recommendation systems.\nSparsity manifests in environments with disproportional cardinality of\ngroupings of entities, such as users and products in an online marketplace. It\nis also found for newly introduced entities, described as the cold-start\nproblem. Recent efforts to mitigate this issue either enrich the connectivity\ndata by incorporating social networks or external knowledge graphs, or\nfine-tune LLMs into interaction augmenters or next-item recommenders. However,\nthese techniques tend to be resource demanding, requiring high computational\npower. They also have several limitations, including data availability, low\nquality, or synthetic noise issues. In this work, we propose LLM-based Intent\nKnowledge Graph Recommender (IKGR), a novel framework that leverages\nretrieval-augmented generation and an encoding approach to construct and\ndensify a knowledge graph. IKGR leverages latent user-item affinities from an\ninteraction knowledge graph and further densifies it through mutual intent\nconnectivity. This addresses sparsity issues and allows the model to make\nintent-grounded recommendations with an interpretable embedding translation\nlayer. Through extensive experiments on real-world datasets, we demonstrate\nthat IKGR overcomes knowledge gaps and achieves substantial gains over\nstate-of-the-art baselines on both publicly available and our internal\nrecommendation datasets."
                },
                "authors": [
                    {
                        "name": "Wenqing Zheng"
                    },
                    {
                        "name": "Noah Fatsi"
                    },
                    {
                        "name": "Daniel Barcklow"
                    },
                    {
                        "name": "Dmitri Kalaev"
                    },
                    {
                        "name": "Steven Yao"
                    },
                    {
                        "name": "Owen Reinert"
                    },
                    {
                        "name": "C. Bayan Bruss"
                    },
                    {
                        "name": "Daniele Rosa"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Rosa"
                },
                "author": "Daniele Rosa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15656v1",
                "updated": "2025-05-21T15:32:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    32,
                    14,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:32:14Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    32,
                    14,
                    2,
                    141,
                    0
                ],
                "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!"
                },
                "summary": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction."
                },
                "authors": [
                    {
                        "name": "Zhexin Zhang"
                    },
                    {
                        "name": "Yuhao Sun"
                    },
                    {
                        "name": "Junxiao Yang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11741v2",
                "updated": "2025-05-21T15:21:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    21,
                    35,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-17T12:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    12,
                    28,
                    11,
                    0,
                    48,
                    0
                ],
                "title": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL"
                },
                "summary": "Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1."
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Ripeng Li"
                    },
                    {
                        "name": "Zhonghong Ou"
                    },
                    {
                        "name": "Jiangfeng Sun"
                    },
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Xiaoran Shang"
                    },
                    {
                        "name": "Meina Song"
                    },
                    {
                        "name": "Yifan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Zhu"
                },
                "author": "Yifan Zhu",
                "arxiv_comment": "28 pages,12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15634v1",
                "updated": "2025-05-21T15:17:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    17,
                    59,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:17:59Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    17,
                    59,
                    2,
                    141,
                    0
                ],
                "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning\n  in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning\n  in Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Yuzhe Yang"
                    },
                    {
                        "name": "Ziyu Yao"
                    },
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15623v1",
                "updated": "2025-05-21T15:12:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    12,
                    20,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:12:20Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    12,
                    20,
                    2,
                    141,
                    0
                ],
                "title": "Can LLMs $\\textit{understand}$ Math? -- Exploring the Pitfalls in\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs $\\textit{understand}$ Math? -- Exploring the Pitfalls in\n  Mathematical Reasoning"
                },
                "summary": "Large language models (LLMs) demonstrate considerable potential in various\nnatural language tasks but face significant challenges in mathematical\nreasoning, particularly in executing precise, multi-step logic. However,\ncurrent evaluation frameworks judge their performance solely based on accuracy,\nwhich only accounts for the final answer. This study explores these pitfalls by\nemploying a novel evaluation framework. We propose an evaluation metric called\nthe MAPLE score, which holistically quantifies reasoning misalignment by\nintegrating error rates, redundancy, and validity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate considerable potential in various\nnatural language tasks but face significant challenges in mathematical\nreasoning, particularly in executing precise, multi-step logic. However,\ncurrent evaluation frameworks judge their performance solely based on accuracy,\nwhich only accounts for the final answer. This study explores these pitfalls by\nemploying a novel evaluation framework. We propose an evaluation metric called\nthe MAPLE score, which holistically quantifies reasoning misalignment by\nintegrating error rates, redundancy, and validity."
                },
                "authors": [
                    {
                        "name": "Tiasa Singha Roy"
                    },
                    {
                        "name": "Aditeya Baral"
                    },
                    {
                        "name": "Ayush Rajesh Jhaveri"
                    },
                    {
                        "name": "Yusuf Baig"
                    }
                ],
                "author_detail": {
                    "name": "Yusuf Baig"
                },
                "author": "Yusuf Baig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15621v1",
                "updated": "2025-05-21T15:11:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    11,
                    26,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:11:26Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    11,
                    26,
                    2,
                    141,
                    0
                ],
                "title": "DS-Bench: A Realistic Benchmark for Data Science Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DS-Bench: A Realistic Benchmark for Data Science Code Generation"
                },
                "summary": "We introduce DS-bench, a new benchmark designed to evaluate large language\nmodels (LLMs) on complicated and realistic data science code generation tasks.\nDS-bench consists of 1,000 carefully constructed problems sourced from\nrealistic problems from GitHub across ten widely used Python data science\nlibraries. Compared to the current state-of-the-art benchmark DS-1000, DS-bench\noffers a more challenging and representative testbed, longer code solutions,\nmore comprehensive data science libraries, clearer and better structured\nproblem descriptions, and stronger test suites. To construct the DS-bench, we\ndevelop a robust pipeline that combines task scope selection, code\nconstruction, test case generation, and problem description synthesis. The\nprocess is paired with rigorous manual editing to ensure alignment and enhance\nevaluation reliability. Experimental result shows that DS-bench exhibits robust\nscaling behavior, where larger models systematically outperform smaller ones,\nvalidating its ability to distinguish model capabilities. The best LLM we test,\nGPT-4o, has a pass@1 of 0.202, indicating that LLMs still have a large room to\nimprove for realistic data science code generation tasks. We believe DS-bench\nwill serve as a rigorous and trustworthy foundation for advancing LLM-based\ndata science programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DS-bench, a new benchmark designed to evaluate large language\nmodels (LLMs) on complicated and realistic data science code generation tasks.\nDS-bench consists of 1,000 carefully constructed problems sourced from\nrealistic problems from GitHub across ten widely used Python data science\nlibraries. Compared to the current state-of-the-art benchmark DS-1000, DS-bench\noffers a more challenging and representative testbed, longer code solutions,\nmore comprehensive data science libraries, clearer and better structured\nproblem descriptions, and stronger test suites. To construct the DS-bench, we\ndevelop a robust pipeline that combines task scope selection, code\nconstruction, test case generation, and problem description synthesis. The\nprocess is paired with rigorous manual editing to ensure alignment and enhance\nevaluation reliability. Experimental result shows that DS-bench exhibits robust\nscaling behavior, where larger models systematically outperform smaller ones,\nvalidating its ability to distinguish model capabilities. The best LLM we test,\nGPT-4o, has a pass@1 of 0.202, indicating that LLMs still have a large room to\nimprove for realistic data science code generation tasks. We believe DS-bench\nwill serve as a rigorous and trustworthy foundation for advancing LLM-based\ndata science programming."
                },
                "authors": [
                    {
                        "name": "Shuyin Ouyang"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Jingwen Guo"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Qihao Zhu"
                    },
                    {
                        "name": "Jie M. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie M. Zhang"
                },
                "author": "Jie M. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15607v1",
                "updated": "2025-05-21T15:00:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    0,
                    7,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:00:07Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    0,
                    7,
                    2,
                    141,
                    0
                ],
                "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with\n  Pedagogy using Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with\n  Pedagogy using Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) can transform education, but their optimization\nfor direct question-answering often undermines effective pedagogy which\nrequires strategically withholding answers. To mitigate this, we propose an\nonline reinforcement learning (RL)-based alignment framework that can quickly\nadapt LLMs into effective tutors using simulated student-tutor interactions by\nemphasizing pedagogical quality and guided problem-solving over simply giving\naway answers. We use our method to train a 7B parameter tutor model without\nhuman annotations which reaches similar performance to larger proprietary\nmodels like LearnLM. We introduce a controllable reward weighting to balance\npedagogical support and student solving accuracy, allowing us to trace the\nPareto frontier between these two objectives. Our models better preserve\nreasoning capabilities than single-turn SFT baselines and can optionally\nenhance interpretability through thinking tags that expose the model's\ninstructional planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can transform education, but their optimization\nfor direct question-answering often undermines effective pedagogy which\nrequires strategically withholding answers. To mitigate this, we propose an\nonline reinforcement learning (RL)-based alignment framework that can quickly\nadapt LLMs into effective tutors using simulated student-tutor interactions by\nemphasizing pedagogical quality and guided problem-solving over simply giving\naway answers. We use our method to train a 7B parameter tutor model without\nhuman annotations which reaches similar performance to larger proprietary\nmodels like LearnLM. We introduce a controllable reward weighting to balance\npedagogical support and student solving accuracy, allowing us to trace the\nPareto frontier between these two objectives. Our models better preserve\nreasoning capabilities than single-turn SFT baselines and can optionally\nenhance interpretability through thinking tags that expose the model's\ninstructional planning."
                },
                "authors": [
                    {
                        "name": "David Dinucu-Jianu"
                    },
                    {
                        "name": "Jakub Macina"
                    },
                    {
                        "name": "Nico Daheim"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "David Dinucu-Jianu and Jakub Macina contributed equally. Code\n  available: https://github.com/eth-lre/PedagogicalRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10259v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10259v3",
                "updated": "2025-05-21T14:54:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    54,
                    38,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-15T13:10:31Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    31,
                    3,
                    135,
                    0
                ],
                "title": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on\n  Resource-Constrained Devices"
                },
                "summary": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload-public .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload-public ."
                },
                "authors": [
                    {
                        "name": "Xiangwen Zhuge"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Fan Dang"
                    },
                    {
                        "name": "Xuan Ding"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Yahui Han"
                    },
                    {
                        "name": "Tianxiang Hao"
                    },
                    {
                        "name": "Zheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yang"
                },
                "author": "Zheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10259v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10259v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15599v1",
                "updated": "2025-05-21T14:52:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    52,
                    31,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:52:31Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    52,
                    31,
                    2,
                    141,
                    0
                ],
                "title": "Device-Independent Ternary Quantum Key Distribution Protocol Based on\n  the Impossible Colouring Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-Independent Ternary Quantum Key Distribution Protocol Based on\n  the Impossible Colouring Game"
                },
                "summary": "We propose a Ternary Device-Independent Quantum Key Distribution (TDIQKD)\nprotocol based on the two-party Impossible Colouring pseudo-telepathy game,\nusing maximally entangled qutrit states to enable secure key generation between\ndistant parties. The protocol makes use of contextuality from the\nKochen-Specker theorem, providing a quantum advantage in a task that is\nclassically unachievable, and eliminates reliance on assumptions about the\ninternal workings of quantum devices. A specially designed qutrit quantum\ncircuit is employed for state preparation, and security is rigorously analyzed\nthrough a composable framework using smooth minimum entropy, von Neumann\nentropy, and Shannon entropy. The protocol achieves optimal key rate in the\nideal case and maintains security under significant noise, with a finite-key\nanalysis supporting practical deployment. The protocol is secure within an\nadequate security framework and demonstrates a higher key generation rate\ncompared to standard QKD protocols, emphasizing the potential of\nhigh-dimensional quantum systems for secure communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a Ternary Device-Independent Quantum Key Distribution (TDIQKD)\nprotocol based on the two-party Impossible Colouring pseudo-telepathy game,\nusing maximally entangled qutrit states to enable secure key generation between\ndistant parties. The protocol makes use of contextuality from the\nKochen-Specker theorem, providing a quantum advantage in a task that is\nclassically unachievable, and eliminates reliance on assumptions about the\ninternal workings of quantum devices. A specially designed qutrit quantum\ncircuit is employed for state preparation, and security is rigorously analyzed\nthrough a composable framework using smooth minimum entropy, von Neumann\nentropy, and Shannon entropy. The protocol achieves optimal key rate in the\nideal case and maintains security under significant noise, with a finite-key\nanalysis supporting practical deployment. The protocol is secure within an\nadequate security framework and demonstrates a higher key generation rate\ncompared to standard QKD protocols, emphasizing the potential of\nhigh-dimensional quantum systems for secure communication."
                },
                "authors": [
                    {
                        "name": "Aniket Basak"
                    },
                    {
                        "name": "Rajeet Ghosh"
                    },
                    {
                        "name": "Rohit Sarma Sarkar"
                    },
                    {
                        "name": "Chandan Goswami"
                    },
                    {
                        "name": "Avishek Adhikari"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Adhikari"
                },
                "author": "Avishek Adhikari",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15596v1",
                "updated": "2025-05-21T14:50:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    50,
                    30,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:50:30Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    50,
                    30,
                    2,
                    141,
                    0
                ],
                "title": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching\n  Assistants Evaluate and Envision Its Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching\n  Assistants Evaluate and Envision Its Use"
                },
                "summary": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback."
                },
                "authors": [
                    {
                        "name": "Xinyi Lu"
                    },
                    {
                        "name": "Aditya Mahesh"
                    },
                    {
                        "name": "Zejia Shen"
                    },
                    {
                        "name": "Mitchell Dudley"
                    },
                    {
                        "name": "Larissa Sano"
                    },
                    {
                        "name": "Xu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Wang"
                },
                "author": "Xu Wang",
                "arxiv_comment": "To be published in AIED'2025: In Proceedings of the 26th\n  International Conference on Artificial Intelligence in Education. The system\n  prompt and example feedback can be found through\n  http://github.com/UM-Lifelong-Learning-Lab/AIED2025-Exploring-LLM-Generated-Feedback-for-Economics-Essay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14590v2",
                "updated": "2025-05-21T14:48:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    48,
                    40,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T16:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    41,
                    45,
                    1,
                    140,
                    0
                ],
                "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol"
                },
                "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Heli Xu"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15592v1",
                "updated": "2025-05-21T14:46:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    46,
                    57,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:46:57Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    46,
                    57,
                    2,
                    141,
                    0
                ],
                "title": "VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic\n  Segmentation"
                },
                "summary": "Large-scale pretrained vision backbones have transformed computer vision by\nproviding powerful feature extractors that enable various downstream tasks,\nincluding training-free approaches like visual prompting for semantic\nsegmentation. Despite their success in generic scenarios, these models often\nfall short when applied to specialized technical domains where the visual\nfeatures differ significantly from their training distribution. To bridge this\ngap, we introduce VP Lab, a comprehensive iterative framework that enhances\nvisual prompting for robust segmentation model development. At the core of VP\nLab lies E-PEFT, a novel ensemble of parameter-efficient fine-tuning techniques\nspecifically designed to adapt our visual prompting pipeline to specific\ndomains in a manner that is both parameter- and data-efficient. Our approach\nnot only surpasses the state-of-the-art in parameter-efficient fine-tuning for\nthe Segment Anything Model (SAM), but also facilitates an interactive,\nnear-real-time loop, allowing users to observe progressively improving results\nas they experiment within the framework. By integrating E-PEFT with visual\nprompting, we demonstrate a remarkable 50\\% increase in semantic segmentation\nmIoU performance across various technical datasets using only 5 validated\nimages, establishing a new paradigm for fast, efficient, and interactive model\ndeployment in new, challenging domains. This work comes in the form of a\ndemonstration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pretrained vision backbones have transformed computer vision by\nproviding powerful feature extractors that enable various downstream tasks,\nincluding training-free approaches like visual prompting for semantic\nsegmentation. Despite their success in generic scenarios, these models often\nfall short when applied to specialized technical domains where the visual\nfeatures differ significantly from their training distribution. To bridge this\ngap, we introduce VP Lab, a comprehensive iterative framework that enhances\nvisual prompting for robust segmentation model development. At the core of VP\nLab lies E-PEFT, a novel ensemble of parameter-efficient fine-tuning techniques\nspecifically designed to adapt our visual prompting pipeline to specific\ndomains in a manner that is both parameter- and data-efficient. Our approach\nnot only surpasses the state-of-the-art in parameter-efficient fine-tuning for\nthe Segment Anything Model (SAM), but also facilitates an interactive,\nnear-real-time loop, allowing users to observe progressively improving results\nas they experiment within the framework. By integrating E-PEFT with visual\nprompting, we demonstrate a remarkable 50\\% increase in semantic segmentation\nmIoU performance across various technical datasets using only 5 validated\nimages, establishing a new paradigm for fast, efficient, and interactive model\ndeployment in new, challenging domains. This work comes in the form of a\ndemonstration."
                },
                "authors": [
                    {
                        "name": "Niccolo Avogaro"
                    },
                    {
                        "name": "Thomas Frick"
                    },
                    {
                        "name": "Yagmur G. Cinar"
                    },
                    {
                        "name": "Daniel Caraballo"
                    },
                    {
                        "name": "Cezary Skura"
                    },
                    {
                        "name": "Filip M. Janicki"
                    },
                    {
                        "name": "Piotr Kluska"
                    },
                    {
                        "name": "Brown Ebouky"
                    },
                    {
                        "name": "Nicola Farronato"
                    },
                    {
                        "name": "Florian Scheidegger"
                    },
                    {
                        "name": "Cristiano Malossi"
                    },
                    {
                        "name": "Konrad Schindler"
                    },
                    {
                        "name": "Andrea Bartezzaghi"
                    },
                    {
                        "name": "Roy Assaf"
                    },
                    {
                        "name": "Mattia Rigotti"
                    }
                ],
                "author_detail": {
                    "name": "Mattia Rigotti"
                },
                "author": "Mattia Rigotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12719v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12719v3",
                "updated": "2025-05-21T14:45:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    45,
                    38,
                    2,
                    141,
                    0
                ],
                "published": "2024-06-18T15:41:15Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    15,
                    41,
                    15,
                    1,
                    170,
                    0
                ],
                "title": "Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis"
                },
                "summary": "Large Language Models (LLMs), already shown to ace various text comprehension\ntasks, have also remarkably been shown to tackle table comprehension tasks\nwithout specific training. Building on earlier studies of LLMs for tabular\ntasks, we probe how in-context learning (ICL), model scale, instruction tuning,\nand domain bias affect Tabular QA (TQA) robustness by testing LLMs, under\ndiverse augmentations and perturbations, on diverse domains: Wikipedia-based\n$\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$, and scientific $\\textbf{SCITAB}$.\nAlthough instruction tuning and larger, newer LLMs deliver stronger, more\nrobust TQA performance, data contamination and reliability issues, especially\non $\\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and the drops in performance, with sensitivity peaking in the\nmodel's middle layers. We highlight the need for improved interpretable\nmethodologies to develop more reliable LLMs for table comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), already shown to ace various text comprehension\ntasks, have also remarkably been shown to tackle table comprehension tasks\nwithout specific training. Building on earlier studies of LLMs for tabular\ntasks, we probe how in-context learning (ICL), model scale, instruction tuning,\nand domain bias affect Tabular QA (TQA) robustness by testing LLMs, under\ndiverse augmentations and perturbations, on diverse domains: Wikipedia-based\n$\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$, and scientific $\\textbf{SCITAB}$.\nAlthough instruction tuning and larger, newer LLMs deliver stronger, more\nrobust TQA performance, data contamination and reliability issues, especially\non $\\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and the drops in performance, with sensitivity peaking in the\nmodel's middle layers. We highlight the need for improved interpretable\nmethodologies to develop more reliable LLMs for table comprehension."
                },
                "authors": [
                    {
                        "name": "Kushal Raj Bhandari"
                    },
                    {
                        "name": "Sixue Xing"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Jianxi Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Gao"
                },
                "author": "Jianxi Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12719v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12719v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14846v2",
                "updated": "2025-05-21T14:42:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    42,
                    25,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-20T18:55:30Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    55,
                    30,
                    3,
                    51,
                    0
                ],
                "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation"
                },
                "summary": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Ajay Patel"
                    },
                    {
                        "name": "Matt Deitke"
                    },
                    {
                        "name": "Tanmay Gupta"
                    },
                    {
                        "name": "Luca Weihs"
                    },
                    {
                        "name": "Andrew Head"
                    },
                    {
                        "name": "Mark Yatskar"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "name": "Christopher Clark"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Clark"
                },
                "author": "Christopher Clark",
                "arxiv_comment": "Published in ACL 2025, project page:\n  https://yueyang1996.github.io/cosyn/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15571v1",
                "updated": "2025-05-21T14:25:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    25,
                    31,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:25:31Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    25,
                    31,
                    2,
                    141,
                    0
                ],
                "title": "Temporal Spectrum Cartography in Low-Altitude Economy Networks: A\n  Generative AI Framework with Multi-Agent Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Spectrum Cartography in Low-Altitude Economy Networks: A\n  Generative AI Framework with Multi-Agent Learning"
                },
                "summary": "This paper introduces a two-stage generative AI (GenAI) framework tailored\nfor temporal spectrum cartography in low-altitude economy networks (LAENets).\nLAENets, characterized by diverse aerial devices such as UAVs, rely heavily on\nwireless communication technologies while facing challenges, including spectrum\ncongestion and dynamic environmental interference. Traditional spectrum\ncartography methods have limitations in handling the temporal and spatial\ncomplexities inherent to these networks. Addressing these challenges, the\nproposed framework first employs a Reconstructive Masked Autoencoder (RecMAE)\ncapable of accurately reconstructing spectrum maps from sparse and temporally\nvarying sensor data using a novel dual-mask mechanism. This approach\nsignificantly enhances the precision of reconstructed radio frequency (RF)\npower maps. In the second stage, the Multi-agent Diffusion Policy (MADP) method\nintegrates diffusion-based reinforcement learning to optimize the trajectories\nof dynamic UAV sensors. By leveraging temporal-attention encoding, this method\neffectively manages spatial exploration and exploitation to minimize cumulative\nreconstruction errors. Extensive numerical experiments validate that this\nintegrated GenAI framework outperforms traditional interpolation methods and\ndeep learning baselines by achieving 57.35% and 88.68% reconstruction error\nreduction, respectively. The proposed trajectory planner substantially improves\nspectrum map accuracy, reconstruction stability, and sensor deployment\nefficiency in dynamically evolving low-altitude environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a two-stage generative AI (GenAI) framework tailored\nfor temporal spectrum cartography in low-altitude economy networks (LAENets).\nLAENets, characterized by diverse aerial devices such as UAVs, rely heavily on\nwireless communication technologies while facing challenges, including spectrum\ncongestion and dynamic environmental interference. Traditional spectrum\ncartography methods have limitations in handling the temporal and spatial\ncomplexities inherent to these networks. Addressing these challenges, the\nproposed framework first employs a Reconstructive Masked Autoencoder (RecMAE)\ncapable of accurately reconstructing spectrum maps from sparse and temporally\nvarying sensor data using a novel dual-mask mechanism. This approach\nsignificantly enhances the precision of reconstructed radio frequency (RF)\npower maps. In the second stage, the Multi-agent Diffusion Policy (MADP) method\nintegrates diffusion-based reinforcement learning to optimize the trajectories\nof dynamic UAV sensors. By leveraging temporal-attention encoding, this method\neffectively manages spatial exploration and exploitation to minimize cumulative\nreconstruction errors. Extensive numerical experiments validate that this\nintegrated GenAI framework outperforms traditional interpolation methods and\ndeep learning baselines by achieving 57.35% and 88.68% reconstruction error\nreduction, respectively. The proposed trajectory planner substantially improves\nspectrum map accuracy, reconstruction stability, and sensor deployment\nefficiency in dynamically evolving low-altitude environments."
                },
                "authors": [
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Zan Li"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14336v2",
                "updated": "2025-05-21T14:22:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    22,
                    18,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T13:20:55Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    20,
                    55,
                    1,
                    140,
                    0
                ],
                "title": "Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach"
                },
                "summary": "Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness."
                },
                "authors": [
                    {
                        "name": "Umberto Cappellazzo"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Stavros Petridis"
                    },
                    {
                        "name": "Daniele Falavigna"
                    },
                    {
                        "name": "Alessio Brutti"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Brutti"
                },
                "author": "Alessio Brutti",
                "arxiv_comment": "Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15564v1",
                "updated": "2025-05-21T14:19:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    19,
                    24,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:19:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    19,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "TinyDrive: Multiscale Visual Question Answering with Selective Token\n  Routing for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyDrive: Multiscale Visual Question Answering with Selective Token\n  Routing for Autonomous Driving"
                },
                "summary": "Vision Language Models (VLMs) employed for visual question-answering (VQA) in\nautonomous driving often require substantial computational resources that pose\na challenge for their deployment in resource-constrained vehicles. To address\nthis challenge, we introduce TinyDrive, a lightweight yet effective VLM for\nmulti-view VQA in driving scenarios. Our model comprises two key components\nincluding a multiscale vision encoder and a dual-level prioritization mechanism\nfor tokens and sequences. The multiscale encoder facilitates the processing of\nmulti-view images at diverse resolutions through scale injection and\ncross-scale gating to generate enhanced visual representations. At the token\nlevel, we design a token routing mechanism that dynamically selects and process\nthe most informative tokens based on learned importance scores. At the sequence\nlevel, we propose integrating normalized loss, uncertainty estimates, and a\ndiversity metric to formulate sequence scores that rank and preserve samples\nwithin a sequence priority buffer. Samples with higher scores are more\nfrequently selected for training. TinyDrive is first evaluated on our\ncustom-curated VQA dataset, and it is subsequently tested on the public DriveLM\nbenchmark, where it achieves state-of-the-art language understanding\nperformance. Notably, it achieves relative improvements of 11.1% and 35.4% in\nBLEU-4 and METEOR scores, respectively, despite having a significantly smaller\nparameter count.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) employed for visual question-answering (VQA) in\nautonomous driving often require substantial computational resources that pose\na challenge for their deployment in resource-constrained vehicles. To address\nthis challenge, we introduce TinyDrive, a lightweight yet effective VLM for\nmulti-view VQA in driving scenarios. Our model comprises two key components\nincluding a multiscale vision encoder and a dual-level prioritization mechanism\nfor tokens and sequences. The multiscale encoder facilitates the processing of\nmulti-view images at diverse resolutions through scale injection and\ncross-scale gating to generate enhanced visual representations. At the token\nlevel, we design a token routing mechanism that dynamically selects and process\nthe most informative tokens based on learned importance scores. At the sequence\nlevel, we propose integrating normalized loss, uncertainty estimates, and a\ndiversity metric to formulate sequence scores that rank and preserve samples\nwithin a sequence priority buffer. Samples with higher scores are more\nfrequently selected for training. TinyDrive is first evaluated on our\ncustom-curated VQA dataset, and it is subsequently tested on the public DriveLM\nbenchmark, where it achieves state-of-the-art language understanding\nperformance. Notably, it achieves relative improvements of 11.1% and 35.4% in\nBLEU-4 and METEOR scores, respectively, despite having a significantly smaller\nparameter count."
                },
                "authors": [
                    {
                        "name": "Hossein Hassani"
                    },
                    {
                        "name": "Soodeh Nikan"
                    },
                    {
                        "name": "Abdallah Shami"
                    }
                ],
                "author_detail": {
                    "name": "Abdallah Shami"
                },
                "author": "Abdallah Shami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15561v1",
                "updated": "2025-05-21T14:18:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    18,
                    1,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:18:01Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    18,
                    1,
                    2,
                    141,
                    0
                ],
                "title": "Do RAG Systems Suffer From Positional Bias?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do RAG Systems Suffer From Positional Bias?"
                },
                "summary": "Retrieval Augmented Generation enhances LLM accuracy by adding passages\nretrieved from an external corpus to the LLM prompt. This paper investigates\nhow positional bias - the tendency of LLMs to weight information differently\nbased on its position in the prompt - affects not only the LLM's capability to\ncapitalize on relevant passages, but also its susceptibility to distracting\npassages. Through extensive experiments on three benchmarks, we show how\nstate-of-the-art retrieval pipelines, while attempting to retrieve relevant\npassages, systematically bring highly distracting ones to the top ranks, with\nover 60% of queries containing at least one highly distracting passage among\nthe top-10 retrieved passages. As a result, the impact of the LLM positional\nbias, which in controlled settings is often reported as very prominent by\nrelated works, is actually marginal in real scenarios since both relevant and\ndistracting passages are, in turn, penalized. Indeed, our findings reveal that\nsophisticated strategies that attempt to rearrange the passages based on LLM\npositional preferences do not perform better than random shuffling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation enhances LLM accuracy by adding passages\nretrieved from an external corpus to the LLM prompt. This paper investigates\nhow positional bias - the tendency of LLMs to weight information differently\nbased on its position in the prompt - affects not only the LLM's capability to\ncapitalize on relevant passages, but also its susceptibility to distracting\npassages. Through extensive experiments on three benchmarks, we show how\nstate-of-the-art retrieval pipelines, while attempting to retrieve relevant\npassages, systematically bring highly distracting ones to the top ranks, with\nover 60% of queries containing at least one highly distracting passage among\nthe top-10 retrieved passages. As a result, the impact of the LLM positional\nbias, which in controlled settings is often reported as very prominent by\nrelated works, is actually marginal in real scenarios since both relevant and\ndistracting passages are, in turn, penalized. Indeed, our findings reveal that\nsophisticated strategies that attempt to rearrange the passages based on LLM\npositional preferences do not perform better than random shuffling."
                },
                "authors": [
                    {
                        "name": "Florin Cuconasu"
                    },
                    {
                        "name": "Simone Filice"
                    },
                    {
                        "name": "Guy Horowitz"
                    },
                    {
                        "name": "Yoelle Maarek"
                    },
                    {
                        "name": "Fabrizio Silvestri"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Silvestri"
                },
                "author": "Fabrizio Silvestri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15554v1",
                "updated": "2025-05-21T14:15:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    15,
                    49,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:15:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    15,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "DayDreamer at CQs-Gen 2025: Generating Critical Questions through\n  Argument Scheme Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DayDreamer at CQs-Gen 2025: Generating Critical Questions through\n  Argument Scheme Completion"
                },
                "summary": "Critical questions are essential resources to provoke critical thinking when\nencountering an argumentative text. We present our system for the Critical\nQuestions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach\nleverages large language models (LLMs) with chain-of-thought prompting to\ngenerate critical questions guided by Walton's argumentation schemes. For each\ninput intervention, we conversationally prompt LLMs to instantiate the\ncorresponding argument scheme template to first obtain structured arguments,\nand then generate relevant critical questions. Following this, we rank all the\navailable critical questions by prompting LLMs to select the top 3 most helpful\nquestions based on the original intervention text. This combination of\nstructured argumentation theory and step-by-step reasoning enables the\ngeneration of contextually relevant and diverse critical questions. Our\npipeline achieves competitive performance in the final test set, showing its\npotential to foster critical thinking given argumentative text and detect\nmissing or uninformed claims. Code available at\n\\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical questions are essential resources to provoke critical thinking when\nencountering an argumentative text. We present our system for the Critical\nQuestions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach\nleverages large language models (LLMs) with chain-of-thought prompting to\ngenerate critical questions guided by Walton's argumentation schemes. For each\ninput intervention, we conversationally prompt LLMs to instantiate the\ncorresponding argument scheme template to first obtain structured arguments,\nand then generate relevant critical questions. Following this, we rank all the\navailable critical questions by prompting LLMs to select the top 3 most helpful\nquestions based on the original intervention text. This combination of\nstructured argumentation theory and step-by-step reasoning enables the\ngeneration of contextually relevant and diverse critical questions. Our\npipeline achieves competitive performance in the final test set, showing its\npotential to foster critical thinking given argumentative text and detect\nmissing or uninformed claims. Code available at\n\\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}."
                },
                "authors": [
                    {
                        "name": "Wendi Zhou"
                    },
                    {
                        "name": "Ameer Saadat-Yazdi"
                    },
                    {
                        "name": "Nadin Kökciyan"
                    }
                ],
                "author_detail": {
                    "name": "Nadin Kökciyan"
                },
                "author": "Nadin Kökciyan",
                "arxiv_comment": "ArgMining 2025 CQs-Gen shared task",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15553v2",
                "updated": "2025-05-22T09:39:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    9,
                    39,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T14:14:47Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    14,
                    47,
                    2,
                    141,
                    0
                ],
                "title": "Social Bias in Popular Question-Answering Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Bias in Popular Question-Answering Benchmarks"
                },
                "summary": "Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs."
                },
                "authors": [
                    {
                        "name": "Angelie Kraft"
                    },
                    {
                        "name": "Judith Simon"
                    },
                    {
                        "name": "Sonja Schimmler"
                    }
                ],
                "author_detail": {
                    "name": "Sonja Schimmler"
                },
                "author": "Sonja Schimmler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13178v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13178v4",
                "updated": "2025-05-21T14:11:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    11,
                    19,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-18T07:35:35Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    35,
                    35,
                    1,
                    49,
                    0
                ],
                "title": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis"
                },
                "summary": "Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark."
                },
                "authors": [
                    {
                        "name": "Jiaqi Zhao"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "17 pages, 3 fugures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13178v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13178v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10316v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10316v4",
                "updated": "2025-05-21T14:09:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    9,
                    13,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-15T16:14:48Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    14,
                    48,
                    4,
                    320,
                    0
                ],
                "title": "M3TR: A Generalist Model for Real-World HD Map Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3TR: A Generalist Model for Real-World HD Map Completion"
                },
                "summary": "Autonomous vehicles rely on HD maps for their operation, but offline HD maps\neventually become outdated. For this reason, online HD map construction methods\nuse live sensor data to infer map information instead. Research on real map\nchanges shows that oftentimes entire parts of an HD map remain unchanged and\ncan be used as a prior. We therefore introduce M3TR (Multi-Masking Map\nTransformer), a generalist approach for HD map completion both with and without\noffline HD map priors. As a necessary foundation, we address shortcomings in\nground truth labels for Argoverse 2 and nuScenes and propose the first\ncomprehensive benchmark for HD map completion. Unlike existing models that\nspecialize in a single kind of map change, which is unrealistic for deployment,\nour Generalist model handles all kinds of changes, matching the effectiveness\nof Expert models. With our map masking as augmentation regime, we can even\nachieve a +1.4 mAP improvement without a prior. Finally, by fully utilizing\nprior HD map elements and optimizing query designs, M3TR outperforms existing\nmethods by +4.3 mAP while being the first real-world deployable model for\noffline HD map priors. Code is available at https://github.com/immel-f/m3tr",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous vehicles rely on HD maps for their operation, but offline HD maps\neventually become outdated. For this reason, online HD map construction methods\nuse live sensor data to infer map information instead. Research on real map\nchanges shows that oftentimes entire parts of an HD map remain unchanged and\ncan be used as a prior. We therefore introduce M3TR (Multi-Masking Map\nTransformer), a generalist approach for HD map completion both with and without\noffline HD map priors. As a necessary foundation, we address shortcomings in\nground truth labels for Argoverse 2 and nuScenes and propose the first\ncomprehensive benchmark for HD map completion. Unlike existing models that\nspecialize in a single kind of map change, which is unrealistic for deployment,\nour Generalist model handles all kinds of changes, matching the effectiveness\nof Expert models. With our map masking as augmentation regime, we can even\nachieve a +1.4 mAP improvement without a prior. Finally, by fully utilizing\nprior HD map elements and optimizing query designs, M3TR outperforms existing\nmethods by +4.3 mAP while being the first real-world deployable model for\noffline HD map priors. Code is available at https://github.com/immel-f/m3tr"
                },
                "authors": [
                    {
                        "name": "Fabian Immel"
                    },
                    {
                        "name": "Richard Fehler"
                    },
                    {
                        "name": "Frank Bieder"
                    },
                    {
                        "name": "Jan-Hendrik Pauls"
                    },
                    {
                        "name": "Christoph Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Stiller"
                },
                "author": "Christoph Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10316v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10316v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v4",
                "updated": "2025-05-21T14:05:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    5,
                    56,
                    2,
                    141,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19559v2",
                "updated": "2025-05-21T14:02:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    2,
                    49,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-26T20:54:51Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    20,
                    54,
                    51,
                    2,
                    57,
                    0
                ],
                "title": "Stay Focused: Problem Drift in Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stay Focused: Problem Drift in Multi-Agent Debate"
                },
                "summary": "Multi-agent debate - multiple instances of large language models discussing\nproblems in turn-based interaction - has shown promise for solving knowledge\nand reasoning tasks. However, these methods show limitations when solving\ncomplex problems that require longer reasoning chains. We analyze how\nmulti-agent debate over multiple turns drifts away from the initial problem,\nthus harming task performance. We define this phenomenon as problem drift and\nquantify its presence across ten tasks (i.e., three generative, three\nknowledge, three reasoning, and one instruction-following task). To identify\nthe reasons for this issue, eight human experts analyze 170 multi-agent\ndiscussions suffering from problem drift. We find the most common issues\nrelated to this drift are the lack of progress (35% of cases), low-quality\nfeedback (26% of cases), and a lack of clarity (25% of cases). To address\nproblem drift, we propose DRIFTJudge, an LLM-as-a-judge method, to detect\nproblem drift at test-time. We also propose DRIFTPolicy, a method that\nmitigates problem drift cases to improve task performance. Our study is a step\ntoward understanding a key limitation of multi-agent debate, highlighting why\nlonger debates can harm task performance and how problem drift could be\naddressed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent debate - multiple instances of large language models discussing\nproblems in turn-based interaction - has shown promise for solving knowledge\nand reasoning tasks. However, these methods show limitations when solving\ncomplex problems that require longer reasoning chains. We analyze how\nmulti-agent debate over multiple turns drifts away from the initial problem,\nthus harming task performance. We define this phenomenon as problem drift and\nquantify its presence across ten tasks (i.e., three generative, three\nknowledge, three reasoning, and one instruction-following task). To identify\nthe reasons for this issue, eight human experts analyze 170 multi-agent\ndiscussions suffering from problem drift. We find the most common issues\nrelated to this drift are the lack of progress (35% of cases), low-quality\nfeedback (26% of cases), and a lack of clarity (25% of cases). To address\nproblem drift, we propose DRIFTJudge, an LLM-as-a-judge method, to detect\nproblem drift at test-time. We also propose DRIFTPolicy, a method that\nmitigates problem drift cases to improve task performance. Our study is a step\ntoward understanding a key limitation of multi-agent debate, highlighting why\nlonger debates can harm task performance and how problem drift could be\naddressed."
                },
                "authors": [
                    {
                        "name": "Jonas Becker"
                    },
                    {
                        "name": "Lars Benedikt Kaesberg"
                    },
                    {
                        "name": "Andreas Stephan"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "arxiv_comment": "34 pages, 10 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03122v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03122v4",
                "updated": "2025-05-21T14:00:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    0,
                    20,
                    2,
                    141,
                    0
                ],
                "published": "2025-03-05T02:37:41Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    2,
                    37,
                    41,
                    2,
                    64,
                    0
                ],
                "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models"
                },
                "summary": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling."
                },
                "authors": [
                    {
                        "name": "Zichao Li"
                    },
                    {
                        "name": "Xueru Wen"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Yuqiu Ji"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03122v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03122v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15524v1",
                "updated": "2025-05-21T13:50:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    50,
                    23,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:50:23Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    50,
                    23,
                    2,
                    141,
                    0
                ],
                "title": "Evaluate Bias without Manual Test Sets: A Concept Representation\n  Perspective for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluate Bias without Manual Test Sets: A Concept Representation\n  Perspective for LLMs"
                },
                "summary": "Bias in Large Language Models (LLMs) significantly undermines their\nreliability and fairness. We focus on a common form of bias: when two reference\nconcepts in the model's concept space, such as sentiment polarities (e.g.,\n\"positive\" and \"negative\"), are asymmetrically correlated with a third, target\nconcept, such as a reviewing aspect, the model exhibits unintended bias. For\ninstance, the understanding of \"food\" should not skew toward any particular\nsentiment. Existing bias evaluation methods assess behavioral differences of\nLLMs by constructing labeled data for different social groups and measuring\nmodel responses across them, a process that requires substantial human effort\nand captures only a limited set of social concepts. To overcome these\nlimitations, we propose BiasLens, a test-set-free bias analysis framework based\non the structure of the model's vector space. BiasLens combines Concept\nActivation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract\ninterpretable concept representations, and quantifies bias by measuring the\nvariation in representational similarity between the target concept and each of\nthe reference concepts. Even without labeled data, BiasLens shows strong\nagreement with traditional bias evaluation metrics (Spearman correlation r >\n0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect\nusing existing methods. For example, in simulated clinical scenarios, a\npatient's insurance status can cause the LLM to produce biased diagnostic\nassessments. Overall, BiasLens offers a scalable, interpretable, and efficient\nparadigm for bias discovery, paving the way for improving fairness and\ntransparency in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Large Language Models (LLMs) significantly undermines their\nreliability and fairness. We focus on a common form of bias: when two reference\nconcepts in the model's concept space, such as sentiment polarities (e.g.,\n\"positive\" and \"negative\"), are asymmetrically correlated with a third, target\nconcept, such as a reviewing aspect, the model exhibits unintended bias. For\ninstance, the understanding of \"food\" should not skew toward any particular\nsentiment. Existing bias evaluation methods assess behavioral differences of\nLLMs by constructing labeled data for different social groups and measuring\nmodel responses across them, a process that requires substantial human effort\nand captures only a limited set of social concepts. To overcome these\nlimitations, we propose BiasLens, a test-set-free bias analysis framework based\non the structure of the model's vector space. BiasLens combines Concept\nActivation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract\ninterpretable concept representations, and quantifies bias by measuring the\nvariation in representational similarity between the target concept and each of\nthe reference concepts. Even without labeled data, BiasLens shows strong\nagreement with traditional bias evaluation metrics (Spearman correlation r >\n0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect\nusing existing methods. For example, in simulated clinical scenarios, a\npatient's insurance status can cause the LLM to produce biased diagnostic\nassessments. Overall, BiasLens offers a scalable, interpretable, and efficient\nparadigm for bias discovery, paving the way for improving fairness and\ntransparency in LLMs."
                },
                "authors": [
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Kaiyang Wan"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Zixiang Xu"
                    },
                    {
                        "name": "Yanbo Wang"
                    },
                    {
                        "name": "Veselin Stoyanov"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02847v3",
                "updated": "2025-05-21T13:45:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    45,
                    40,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-01T19:06:10Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    6,
                    10,
                    3,
                    121,
                    0
                ],
                "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models"
                },
                "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents."
                },
                "authors": [
                    {
                        "name": "Bang Zhang"
                    },
                    {
                        "name": "Ruotian Ma"
                    },
                    {
                        "name": "Qingxuan Jiang"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Zheng Xie"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Xiaolong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Li"
                },
                "author": "Xiaolong Li",
                "arxiv_comment": "code: https://github.com/Tencent/digitalhuman/tree/main/SAGE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14172v2",
                "updated": "2025-05-21T13:38:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    38,
                    48,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-20T10:25:17Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    10,
                    25,
                    17,
                    1,
                    140,
                    0
                ],
                "title": "The Strawberry Problem: Emergence of Character-level Understanding in\n  Tokenized Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Strawberry Problem: Emergence of Character-level Understanding in\n  Tokenized Language Models"
                },
                "summary": "Despite their remarkable progress across diverse domains, Large Language\nModels (LLMs) consistently fail at simple character-level tasks, such as\ncounting letters in words, due to a fundamental limitation: tokenization. In\nthis work, we frame this limitation as a problem of low mutual information and\nanalyze it in terms of concept emergence. Using a suite of 19 synthetic tasks\nthat isolate character-level reasoning in a controlled setting, we show that\nsuch capabilities emerge slowly, suddenly, and only late in training. We\nfurther show that percolation-based models of concept emergence explain these\npatterns, suggesting that learning character composition is not fundamentally\ndifferent from learning commonsense knowledge. To address this bottleneck, we\npropose a lightweight architectural modification that significantly improves\ncharacter-level reasoning while preserving the inductive advantages of subword\nmodels. Together, our results bridge low-level perceptual gaps in tokenized LMs\nand provide a principled framework for understanding and mitigating their\nstructural blind spots. We make our code publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable progress across diverse domains, Large Language\nModels (LLMs) consistently fail at simple character-level tasks, such as\ncounting letters in words, due to a fundamental limitation: tokenization. In\nthis work, we frame this limitation as a problem of low mutual information and\nanalyze it in terms of concept emergence. Using a suite of 19 synthetic tasks\nthat isolate character-level reasoning in a controlled setting, we show that\nsuch capabilities emerge slowly, suddenly, and only late in training. We\nfurther show that percolation-based models of concept emergence explain these\npatterns, suggesting that learning character composition is not fundamentally\ndifferent from learning commonsense knowledge. To address this bottleneck, we\npropose a lightweight architectural modification that significantly improves\ncharacter-level reasoning while preserving the inductive advantages of subword\nmodels. Together, our results bridge low-level perceptual gaps in tokenized LMs\nand provide a principled framework for understanding and mitigating their\nstructural blind spots. We make our code publicly available."
                },
                "authors": [
                    {
                        "name": "Adrian Cosma"
                    },
                    {
                        "name": "Stefan Ruseti"
                    },
                    {
                        "name": "Emilian Radoi"
                    },
                    {
                        "name": "Mihai Dascalu"
                    }
                ],
                "author_detail": {
                    "name": "Mihai Dascalu"
                },
                "author": "Mihai Dascalu",
                "arxiv_comment": "1 Table, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09662v2",
                "updated": "2025-05-21T13:29:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    29,
                    57,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-14T14:31:33Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    31,
                    33,
                    2,
                    134,
                    0
                ],
                "title": "Large Language Models Are More Persuasive Than Incentivized Human\n  Persuaders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are More Persuasive Than Incentivized Human\n  Persuaders"
                },
                "summary": "We directly compare the persuasion capabilities of a frontier large language\nmodel (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an\ninteractive, real-time conversational quiz setting. In this preregistered,\nlarge-scale incentivized experiment, participants (quiz takers) completed an\nonline quiz where persuaders (either humans or LLMs) attempted to persuade quiz\ntakers toward correct or incorrect answers. We find that LLM persuaders\nachieved significantly higher compliance with their directional persuasion\nattempts than incentivized human persuaders, demonstrating superior persuasive\ncapabilities in both truthful (toward correct answers) and deceptive (toward\nincorrect answers) contexts. We also find that LLM persuaders significantly\nincreased quiz takers' accuracy, leading to higher earnings, when steering quiz\ntakers toward correct answers, and significantly decreased their accuracy,\nleading to lower earnings, when steering them toward incorrect answers.\nOverall, our findings suggest that AI's persuasion capabilities already exceed\nthose of humans that have real-money bonuses tied to performance. Our findings\nof increasingly capable AI persuaders thus underscore the urgency of emerging\nalignment and governance frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We directly compare the persuasion capabilities of a frontier large language\nmodel (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an\ninteractive, real-time conversational quiz setting. In this preregistered,\nlarge-scale incentivized experiment, participants (quiz takers) completed an\nonline quiz where persuaders (either humans or LLMs) attempted to persuade quiz\ntakers toward correct or incorrect answers. We find that LLM persuaders\nachieved significantly higher compliance with their directional persuasion\nattempts than incentivized human persuaders, demonstrating superior persuasive\ncapabilities in both truthful (toward correct answers) and deceptive (toward\nincorrect answers) contexts. We also find that LLM persuaders significantly\nincreased quiz takers' accuracy, leading to higher earnings, when steering quiz\ntakers toward correct answers, and significantly decreased their accuracy,\nleading to lower earnings, when steering them toward incorrect answers.\nOverall, our findings suggest that AI's persuasion capabilities already exceed\nthose of humans that have real-money bonuses tied to performance. Our findings\nof increasingly capable AI persuaders thus underscore the urgency of emerging\nalignment and governance frameworks."
                },
                "authors": [
                    {
                        "name": "Philipp Schoenegger"
                    },
                    {
                        "name": "Francesco Salvi"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaoli Nan"
                    },
                    {
                        "name": "Ramit Debnath"
                    },
                    {
                        "name": "Barbara Fasolo"
                    },
                    {
                        "name": "Evelina Leivada"
                    },
                    {
                        "name": "Gabriel Recchia"
                    },
                    {
                        "name": "Fritz Günther"
                    },
                    {
                        "name": "Ali Zarifhonarvar"
                    },
                    {
                        "name": "Joe Kwon"
                    },
                    {
                        "name": "Zahoor Ul Islam"
                    },
                    {
                        "name": "Marco Dehnert"
                    },
                    {
                        "name": "Daryl Y. H. Lee"
                    },
                    {
                        "name": "Madeline G. Reinecke"
                    },
                    {
                        "name": "David G. Kamper"
                    },
                    {
                        "name": "Mert Kobaş"
                    },
                    {
                        "name": "Adam Sandford"
                    },
                    {
                        "name": "Jonas Kgomo"
                    },
                    {
                        "name": "Luke Hewitt"
                    },
                    {
                        "name": "Shreya Kapoor"
                    },
                    {
                        "name": "Kerem Oktar"
                    },
                    {
                        "name": "Eyup Engin Kucuk"
                    },
                    {
                        "name": "Bo Feng"
                    },
                    {
                        "name": "Cameron R. Jones"
                    },
                    {
                        "name": "Izzy Gainsburg"
                    },
                    {
                        "name": "Sebastian Olschewski"
                    },
                    {
                        "name": "Nora Heinzelmann"
                    },
                    {
                        "name": "Francisco Cruz"
                    },
                    {
                        "name": "Ben M. Tappin"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Peter S. Park"
                    },
                    {
                        "name": "Rayan Onyonka"
                    },
                    {
                        "name": "Arthur Hjorth"
                    },
                    {
                        "name": "Peter Slattery"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Lennart Finke"
                    },
                    {
                        "name": "Igor Grossmann"
                    },
                    {
                        "name": "Alessandro Salatiello"
                    },
                    {
                        "name": "Ezra Karger"
                    }
                ],
                "author_detail": {
                    "name": "Ezra Karger"
                },
                "author": "Ezra Karger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.1.2; K.4.1; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15501v1",
                "updated": "2025-05-21T13:22:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    22,
                    34,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:22:34Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    22,
                    34,
                    2,
                    141,
                    0
                ],
                "title": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks:\n  Memorization and Generalization with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks:\n  Memorization and Generalization with Knowledge Graphs"
                },
                "summary": "We introduce the concept of protoknowledge to formalize and measure how\nsequences of tokens encoding Knowledge Graphs are internalized during\npretraining and utilized at inference time by Large Language Models (LLMs).\nIndeed, LLMs have demonstrated the ability to memorize vast amounts of token\nsequences during pretraining, and a central open question is how they leverage\nthis memorization as reusable knowledge through generalization. We then\ncategorize protoknowledge into lexical, hierarchical, and topological forms,\nvarying on the type of knowledge that needs to be activated. We measure\nprotoknowledge through Knowledge Activation Tasks (KATs), analyzing its general\nproperties such as semantic bias. We then investigate the impact of\nprotoknowledge on Text-to-SPARQL performance by varying prompting strategies\ndepending on input conditions. To this end, we adopt a novel analysis framework\nthat assesses whether model predictions align with the successful activation of\nthe relevant protoknowledge for each query. This methodology provides a\npractical tool to explore Semantic-Level Data Contamination and serves as an\neffective strategy for Closed-Pretraining models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the concept of protoknowledge to formalize and measure how\nsequences of tokens encoding Knowledge Graphs are internalized during\npretraining and utilized at inference time by Large Language Models (LLMs).\nIndeed, LLMs have demonstrated the ability to memorize vast amounts of token\nsequences during pretraining, and a central open question is how they leverage\nthis memorization as reusable knowledge through generalization. We then\ncategorize protoknowledge into lexical, hierarchical, and topological forms,\nvarying on the type of knowledge that needs to be activated. We measure\nprotoknowledge through Knowledge Activation Tasks (KATs), analyzing its general\nproperties such as semantic bias. We then investigate the impact of\nprotoknowledge on Text-to-SPARQL performance by varying prompting strategies\ndepending on input conditions. To this end, we adopt a novel analysis framework\nthat assesses whether model predictions align with the successful activation of\nthe relevant protoknowledge for each query. This methodology provides a\npractical tool to explore Semantic-Level Data Contamination and serves as an\neffective strategy for Closed-Pretraining models."
                },
                "authors": [
                    {
                        "name": "Federico Ranaldi"
                    },
                    {
                        "name": "Andrea Zugarini"
                    },
                    {
                        "name": "Leonardo Ranaldi"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Massimo Zanzotto"
                },
                "author": "Fabio Massimo Zanzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11415v2",
                "updated": "2025-05-21T13:20:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    20,
                    21,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-16T16:28:38Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    16,
                    28,
                    38,
                    4,
                    136,
                    0
                ],
                "title": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse\n  Mixture-of-Experts Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse\n  Mixture-of-Experts Systems"
                },
                "summary": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for\nscaling Large Language Models (LLMs) efficiently, but it depends on\nheterogeneous compute and memory resources. These factors jointly affect system\nCost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing\nbenchmarks often fail to capture these trade-offs accurately, complicating\npractical deployment decisions. To address this, we introduce MoE-CAP, a\nbenchmark specifically designed for MoE systems. Our analysis reveals that\nachieving an optimal balance across CAP is difficult with current hardware; MoE\nsystems typically optimize two of the three dimensions at the expense of the\nthird-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose\nthe CAP Radar Diagram. We further introduce sparsity-aware performance\nmetrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS\nUtilization (S-MFU)-to enable accurate performance benchmarking of MoE systems\nacross diverse hardware platforms and deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for\nscaling Large Language Models (LLMs) efficiently, but it depends on\nheterogeneous compute and memory resources. These factors jointly affect system\nCost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing\nbenchmarks often fail to capture these trade-offs accurately, complicating\npractical deployment decisions. To address this, we introduce MoE-CAP, a\nbenchmark specifically designed for MoE systems. Our analysis reveals that\nachieving an optimal balance across CAP is difficult with current hardware; MoE\nsystems typically optimize two of the three dimensions at the expense of the\nthird-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose\nthe CAP Radar Diagram. We further introduce sparsity-aware performance\nmetrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS\nUtilization (S-MFU)-to enable accurate performance benchmarking of MoE systems\nacross diverse hardware platforms and deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Congjie He"
                    },
                    {
                        "name": "Man-Kit Sit"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Ziming Miao"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Tairan Xu"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Edoardo Ponti"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai",
                "arxiv_comment": "Duplicate submission of arXiv:2412.07067",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13761v2",
                "updated": "2025-05-21T13:17:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    17,
                    7,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-19T22:27:18Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    22,
                    27,
                    18,
                    0,
                    139,
                    0
                ],
                "title": "Simulation Agent: A Framework for Integrating Simulation and Large\n  Language Models for Enhanced Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation Agent: A Framework for Integrating Simulation and Large\n  Language Models for Enhanced Decision-Making"
                },
                "summary": "Simulations, although powerful in accurately replicating real-world systems,\noften remain inaccessible to non-technical users due to their complexity.\nConversely, large language models (LLMs) provide intuitive, language-based\ninteractions but can lack the structured, causal understanding required to\nreliably model complex real-world dynamics. We introduce our simulation agent\nframework, a novel approach that integrates the strengths of both simulation\nmodels and LLMs. This framework helps empower users by leveraging the\nconversational capabilities of LLMs to interact seamlessly with sophisticated\nsimulation systems, while simultaneously utilizing the simulations to ground\nthe LLMs in accurate and structured representations of real-world phenomena.\nThis integrated approach helps provide a robust and generalizable foundation\nfor empirical validation and offers broad applicability across diverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulations, although powerful in accurately replicating real-world systems,\noften remain inaccessible to non-technical users due to their complexity.\nConversely, large language models (LLMs) provide intuitive, language-based\ninteractions but can lack the structured, causal understanding required to\nreliably model complex real-world dynamics. We introduce our simulation agent\nframework, a novel approach that integrates the strengths of both simulation\nmodels and LLMs. This framework helps empower users by leveraging the\nconversational capabilities of LLMs to interact seamlessly with sophisticated\nsimulation systems, while simultaneously utilizing the simulations to ground\nthe LLMs in accurate and structured representations of real-world phenomena.\nThis integrated approach helps provide a robust and generalizable foundation\nfor empirical validation and offers broad applicability across diverse domains."
                },
                "authors": [
                    {
                        "name": "Jacob Kleiman"
                    },
                    {
                        "name": "Kevin Frank"
                    },
                    {
                        "name": "Joseph Voyles"
                    },
                    {
                        "name": "Sindy Campagna"
                    }
                ],
                "author_detail": {
                    "name": "Sindy Campagna"
                },
                "author": "Sindy Campagna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15490v1",
                "updated": "2025-05-21T13:15:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    15,
                    35,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:15:35Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    15,
                    35,
                    2,
                    141,
                    0
                ],
                "title": "Collaborative Problem-Solving in an Optimization Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Problem-Solving in an Optimization Game"
                },
                "summary": "Dialogue agents that support human users in solving complex tasks have\nreceived much attention recently. Many such tasks are NP-hard optimization\nproblems that require careful collaborative exploration of the solution space.\nWe introduce a novel dialogue game in which the agents collaboratively solve a\ntwo-player Traveling Salesman problem, along with an agent that combines LLM\nprompting with symbolic mechanisms for state tracking and grounding. Our best\nagent solves 45% of games optimally in self-play. It also demonstrates an\nability to collaborate successfully with human users and generalize to\nunfamiliar graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue agents that support human users in solving complex tasks have\nreceived much attention recently. Many such tasks are NP-hard optimization\nproblems that require careful collaborative exploration of the solution space.\nWe introduce a novel dialogue game in which the agents collaboratively solve a\ntwo-player Traveling Salesman problem, along with an agent that combines LLM\nprompting with symbolic mechanisms for state tracking and grounding. Our best\nagent solves 45% of games optimally in self-play. It also demonstrates an\nability to collaborate successfully with human users and generalize to\nunfamiliar graphs."
                },
                "authors": [
                    {
                        "name": "Isidora Jeknic"
                    },
                    {
                        "name": "Alex Duchnowski"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15480v1",
                "updated": "2025-05-21T12:55:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    55,
                    28,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:55:28Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    55,
                    28,
                    2,
                    141,
                    0
                ],
                "title": "KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific\n  Question-Answering Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific\n  Question-Answering Performance"
                },
                "summary": "Supervised fine-tuning (SFT) is a common approach to improve the\ndomain-specific question-answering (QA) performance of large language models\n(LLMs). However, recent literature reveals that due to the conflicts between\nLLMs' internal knowledge and the context knowledge of training data, vanilla\nSFT using the full QA training set is usually suboptimal. In this paper, we\nfirst design a query diversification strategy for robust conflict detection and\nthen conduct a series of experiments to analyze the impact of knowledge\nconflict. We find that 1) training samples with varied conflicts contribute\ndifferently, where SFT on the data with large conflicts leads to catastrophic\nperformance drops; 2) compared to directly filtering out the conflict data,\nappropriately applying the conflict data would be more beneficial. Motivated by\nthis, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely\nKaFT) approach to effectively boost LLMs' performance. The core of KaFT is to\nadapt the training weight by assigning different rewards for different training\nsamples according to conflict level. Extensive experiments show that KaFT\nbrings consistent and significant improvements across four LLMs. More analyses\nprove that KaFT effectively improves the model generalization and alleviates\nthe hallucination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is a common approach to improve the\ndomain-specific question-answering (QA) performance of large language models\n(LLMs). However, recent literature reveals that due to the conflicts between\nLLMs' internal knowledge and the context knowledge of training data, vanilla\nSFT using the full QA training set is usually suboptimal. In this paper, we\nfirst design a query diversification strategy for robust conflict detection and\nthen conduct a series of experiments to analyze the impact of knowledge\nconflict. We find that 1) training samples with varied conflicts contribute\ndifferently, where SFT on the data with large conflicts leads to catastrophic\nperformance drops; 2) compared to directly filtering out the conflict data,\nappropriately applying the conflict data would be more beneficial. Motivated by\nthis, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely\nKaFT) approach to effectively boost LLMs' performance. The core of KaFT is to\nadapt the training weight by assigning different rewards for different training\nsamples according to conflict level. Extensive experiments show that KaFT\nbrings consistent and significant improvements across four LLMs. More analyses\nprove that KaFT effectively improves the model generalization and alleviates\nthe hallucination."
                },
                "authors": [
                    {
                        "name": "Qihuang Zhong"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Xiantao Cai"
                    },
                    {
                        "name": "Juhua Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted to ACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16825v2",
                "updated": "2025-05-21T12:53:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    53,
                    32,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-24T04:22:57Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    4,
                    22,
                    57,
                    0,
                    55,
                    0
                ],
                "title": "Finding the Sweet Spot: Preference Data Construction for Scaling\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding the Sweet Spot: Preference Data Construction for Scaling\n  Preference Optimization"
                },
                "summary": "Iterative data generation and model retraining are widely used to align large\nlanguage models (LLMs). It typically involves a policy model to generate\non-policy responses and a reward model to guide training data selection. Direct\nPreference Optimization (DPO) further enhances this process by constructing\npreference pairs of chosen and rejected responses. In this work, we aim to\n\\emph{scale up} the number of on-policy samples via repeated random sampling to\nimprove alignment performance. Conventional practice selects the sample with\nthe highest reward as chosen and the lowest as rejected for DPO. However, our\nexperiments reveal that this strategy leads to a \\emph{decline} in performance\nas the sample size increases. To address this, we investigate preference data\nconstruction through the lens of underlying normal distribution of sample\nrewards. We categorize the reward space into seven representative points and\nsystematically explore all 21 ($C_7^2$) pairwise combinations. Through\nevaluations on four models using AlpacaEval 2, we find that selecting the\nrejected response at reward position $\\mu - 2\\sigma$ rather than the minimum\nreward, is crucial for optimal performance. We finally introduce a scalable\npreference data construction strategy that consistently enhances model\nperformance as the sample scale increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative data generation and model retraining are widely used to align large\nlanguage models (LLMs). It typically involves a policy model to generate\non-policy responses and a reward model to guide training data selection. Direct\nPreference Optimization (DPO) further enhances this process by constructing\npreference pairs of chosen and rejected responses. In this work, we aim to\n\\emph{scale up} the number of on-policy samples via repeated random sampling to\nimprove alignment performance. Conventional practice selects the sample with\nthe highest reward as chosen and the lowest as rejected for DPO. However, our\nexperiments reveal that this strategy leads to a \\emph{decline} in performance\nas the sample size increases. To address this, we investigate preference data\nconstruction through the lens of underlying normal distribution of sample\nrewards. We categorize the reward space into seven representative points and\nsystematically explore all 21 ($C_7^2$) pairwise combinations. Through\nevaluations on four models using AlpacaEval 2, we find that selecting the\nrejected response at reward position $\\mu - 2\\sigma$ rather than the minimum\nreward, is crucial for optimal performance. We finally introduce a scalable\npreference data construction strategy that consistently enhances model\nperformance as the sample scale increases."
                },
                "authors": [
                    {
                        "name": "Yao Xiao"
                    },
                    {
                        "name": "Hai Ye"
                    },
                    {
                        "name": "Linyao Chen"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Xiaoli Li"
                    },
                    {
                        "name": "Roy Ka-wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-wei Lee"
                },
                "author": "Roy Ka-wei Lee",
                "arxiv_comment": "ACL25 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15475v1",
                "updated": "2025-05-21T12:49:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    49,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:49:37Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    49,
                    37,
                    2,
                    141,
                    0
                ],
                "title": "LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in\n  Large Language Models"
                },
                "summary": "Nowadays, Large Language Models (LLMs) have attracted widespread attention\ndue to their powerful performance. However, due to the unavoidable exposure to\nsocially biased data during training, LLMs tend to exhibit social biases,\nparticularly gender bias. To better explore and quantifying the degree of\ngender bias in LLMs, we propose a pair of datasets named GenBiasEval and\nGenHintEval, respectively. The GenBiasEval is responsible for evaluating the\ndegree of gender bias in LLMs, accompanied by an evaluation metric named\nAFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is\nused to assess whether LLMs can provide responses consistent with prompts that\ncontain gender hints, along with the accompanying evaluation metric UB-Score\n(UnBias Score). Besides, in order to mitigate gender bias in LLMs more\neffectively, we present the LFTF (Locating First and Then Fine-Tuning)\nalgorithm.The algorithm first ranks specific LLM blocks by their relevance to\ngender bias in descending order using a metric called BMI (Block Mitigating\nImportance Score). Based on this ranking, the block most strongly associated\nwith gender bias is then fine-tuned using a carefully designed loss function.\nNumerous experiments have shown that our proposed LFTF algorithm can\nsignificantly mitigate gender bias in LLMs while maintaining their general\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, Large Language Models (LLMs) have attracted widespread attention\ndue to their powerful performance. However, due to the unavoidable exposure to\nsocially biased data during training, LLMs tend to exhibit social biases,\nparticularly gender bias. To better explore and quantifying the degree of\ngender bias in LLMs, we propose a pair of datasets named GenBiasEval and\nGenHintEval, respectively. The GenBiasEval is responsible for evaluating the\ndegree of gender bias in LLMs, accompanied by an evaluation metric named\nAFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is\nused to assess whether LLMs can provide responses consistent with prompts that\ncontain gender hints, along with the accompanying evaluation metric UB-Score\n(UnBias Score). Besides, in order to mitigate gender bias in LLMs more\neffectively, we present the LFTF (Locating First and Then Fine-Tuning)\nalgorithm.The algorithm first ranks specific LLM blocks by their relevance to\ngender bias in descending order using a metric called BMI (Block Mitigating\nImportance Score). Based on this ranking, the block most strongly associated\nwith gender bias is then fine-tuned using a carefully designed loss function.\nNumerous experiments have shown that our proposed LFTF algorithm can\nsignificantly mitigate gender bias in LLMs while maintaining their general\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Zhanyue Qin"
                    },
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Deyuan Liu"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Junxian Cai"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zhiying Tu"
                    },
                    {
                        "name": "Dianhui Chu"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Dianbo Sui"
                    }
                ],
                "author_detail": {
                    "name": "Dianbo Sui"
                },
                "author": "Dianbo Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15471v1",
                "updated": "2025-05-21T12:46:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    46,
                    42,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:46:42Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    46,
                    42,
                    2,
                    141,
                    0
                ],
                "title": "CoLA: Collaborative Low-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLA: Collaborative Low-Rank Adaptation"
                },
                "summary": "The scaling law of Large Language Models (LLMs) reveals a power-law\nrelationship, showing diminishing return on performance as model scale\nincreases. While training LLMs from scratch is resource-intensive, fine-tuning\na pre-trained model for specific tasks has become a practical alternative. Full\nfine-tuning (FFT) achieves strong performance; however, it is computationally\nexpensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like\nLoRA, have been proposed to address these challenges by freezing the\npre-trained model and adding lightweight task-specific modules. LoRA, in\nparticular, has proven effective, but its application to multi-task scenarios\nis limited by interference between tasks. Recent approaches, such as\nMixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these\nissues but still struggle with sample scarcity and noise interference due to\ntheir fixed structure. In response, we propose CoLA, a more flexible LoRA\narchitecture with an efficient initialization scheme, and introduces three\ncollaborative strategies to enhance performance by better utilizing the\nquantitative relationships between matrices $A$ and $B$. Our experiments\ndemonstrate the effectiveness and robustness of CoLA, outperforming existing\nPEFT methods, especially in low-sample scenarios. Our data and code are fully\npublicly available at https://github.com/zyy-2001/CoLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scaling law of Large Language Models (LLMs) reveals a power-law\nrelationship, showing diminishing return on performance as model scale\nincreases. While training LLMs from scratch is resource-intensive, fine-tuning\na pre-trained model for specific tasks has become a practical alternative. Full\nfine-tuning (FFT) achieves strong performance; however, it is computationally\nexpensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like\nLoRA, have been proposed to address these challenges by freezing the\npre-trained model and adding lightweight task-specific modules. LoRA, in\nparticular, has proven effective, but its application to multi-task scenarios\nis limited by interference between tasks. Recent approaches, such as\nMixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these\nissues but still struggle with sample scarcity and noise interference due to\ntheir fixed structure. In response, we propose CoLA, a more flexible LoRA\narchitecture with an efficient initialization scheme, and introduces three\ncollaborative strategies to enhance performance by better utilizing the\nquantitative relationships between matrices $A$ and $B$. Our experiments\ndemonstrate the effectiveness and robustness of CoLA, outperforming existing\nPEFT methods, especially in low-sample scenarios. Our data and code are fully\npublicly available at https://github.com/zyy-2001/CoLA."
                },
                "authors": [
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Chang Yao"
                    },
                    {
                        "name": "Jingyuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Chen"
                },
                "author": "Jingyuan Chen",
                "arxiv_comment": "Accepted by ACL 2025, Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15469v1",
                "updated": "2025-05-21T12:45:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    45,
                    49,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:45:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    45,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "A Qualitative Investigation into LLM-Generated Multilingual Code\n  Comments and Automatic Evaluation Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Qualitative Investigation into LLM-Generated Multilingual Code\n  Comments and Automatic Evaluation Metrics"
                },
                "summary": "Large Language Models are essential coding assistants, yet their training is\npredominantly English-centric. In this study, we evaluate the performance of\ncode language models in non-English contexts, identifying challenges in their\nadoption and integration into multilingual workflows. We conduct an open-coding\nstudy to analyze errors in code comments generated by five state-of-the-art\ncode models, CodeGemma, CodeLlama, CodeQwen1.5, GraniteCode, and StarCoder2\nacross five natural languages: Chinese, Dutch, English, Greek, and Polish. Our\nstudy yields a dataset of 12,500 labeled generations, which we publicly\nrelease. We then assess the reliability of standard metrics in capturing\ncomment \\textit{correctness} across languages and evaluate their\ntrustworthiness as judgment criteria. Through our open-coding investigation, we\nidentified a taxonomy of 26 distinct error categories in model-generated code\ncomments. They highlight variations in language cohesion, informativeness, and\nsyntax adherence across different natural languages. Our analysis shows that,\nwhile these models frequently produce partially correct comments, modern neural\nmetrics fail to reliably differentiate meaningful completions from random\nnoise. Notably, the significant score overlap between expert-rated correct and\nincorrect comments calls into question the effectiveness of these metrics in\nassessing generated comments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are essential coding assistants, yet their training is\npredominantly English-centric. In this study, we evaluate the performance of\ncode language models in non-English contexts, identifying challenges in their\nadoption and integration into multilingual workflows. We conduct an open-coding\nstudy to analyze errors in code comments generated by five state-of-the-art\ncode models, CodeGemma, CodeLlama, CodeQwen1.5, GraniteCode, and StarCoder2\nacross five natural languages: Chinese, Dutch, English, Greek, and Polish. Our\nstudy yields a dataset of 12,500 labeled generations, which we publicly\nrelease. We then assess the reliability of standard metrics in capturing\ncomment \\textit{correctness} across languages and evaluate their\ntrustworthiness as judgment criteria. Through our open-coding investigation, we\nidentified a taxonomy of 26 distinct error categories in model-generated code\ncomments. They highlight variations in language cohesion, informativeness, and\nsyntax adherence across different natural languages. Our analysis shows that,\nwhile these models frequently produce partially correct comments, modern neural\nmetrics fail to reliably differentiate meaningful completions from random\nnoise. Notably, the significant score overlap between expert-rated correct and\nincorrect comments calls into question the effectiveness of these metrics in\nassessing generated comments."
                },
                "authors": [
                    {
                        "name": "Jonathan Katzy"
                    },
                    {
                        "name": "Yongcheng Huang"
                    },
                    {
                        "name": "Gopal-Raj Panchu"
                    },
                    {
                        "name": "Maksym Ziemlewski"
                    },
                    {
                        "name": "Paris Loizides"
                    },
                    {
                        "name": "Sander Vermeulen"
                    },
                    {
                        "name": "Arie van Deursen"
                    },
                    {
                        "name": "Maliheh Izadi"
                    }
                ],
                "author_detail": {
                    "name": "Maliheh Izadi"
                },
                "author": "Maliheh Izadi",
                "arxiv_comment": "Accepted PROMISE '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15456v1",
                "updated": "2025-05-21T12:38:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    38,
                    36,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:38:36Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    38,
                    36,
                    2,
                    141,
                    0
                ],
                "title": "Teaching Language Models to Evolve with Users: Dynamic Profile Modeling\n  for Personalized Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Language Models to Evolve with Users: Dynamic Profile Modeling\n  for Personalized Alignment"
                },
                "summary": "Personalized alignment is essential for enabling large language models (LLMs)\nto engage effectively in user-centric dialogue. While recent prompt-based and\noffline optimization methods offer preliminary solutions, they fall short in\ncold-start scenarios and long-term personalization due to their inherently\nstatic and shallow designs. In this work, we introduce the Reinforcement\nLearning for Personalized Alignment (RLPA) framework, in which an LLM interacts\nwith a simulated user model to iteratively infer and refine user profiles\nthrough dialogue. The training process is guided by a dual-level reward\nstructure: the Profile Reward encourages accurate construction of user\nrepresentations, while the Response Reward incentivizes generation of responses\nconsistent with the inferred profile. We instantiate RLPA by fine-tuning\nQwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art\nperformance in personalized dialogue. Empirical evaluations demonstrate that\nQwen-RLPA consistently outperforms prompting and offline fine-tuning baselines,\nand even surpasses advanced commercial models such as Claude-3.5 and GPT-4o.\nFurther analysis highlights Qwen-RLPA's robustness in reconciling conflicting\nuser preferences, sustaining long-term personalization and delivering more\nefficient inference compared to recent reasoning-focused LLMs. These results\nemphasize the potential of dynamic profile inference as a more effective\nparadigm for building personalized dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized alignment is essential for enabling large language models (LLMs)\nto engage effectively in user-centric dialogue. While recent prompt-based and\noffline optimization methods offer preliminary solutions, they fall short in\ncold-start scenarios and long-term personalization due to their inherently\nstatic and shallow designs. In this work, we introduce the Reinforcement\nLearning for Personalized Alignment (RLPA) framework, in which an LLM interacts\nwith a simulated user model to iteratively infer and refine user profiles\nthrough dialogue. The training process is guided by a dual-level reward\nstructure: the Profile Reward encourages accurate construction of user\nrepresentations, while the Response Reward incentivizes generation of responses\nconsistent with the inferred profile. We instantiate RLPA by fine-tuning\nQwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art\nperformance in personalized dialogue. Empirical evaluations demonstrate that\nQwen-RLPA consistently outperforms prompting and offline fine-tuning baselines,\nand even surpasses advanced commercial models such as Claude-3.5 and GPT-4o.\nFurther analysis highlights Qwen-RLPA's robustness in reconciling conflicting\nuser preferences, sustaining long-term personalization and delivering more\nefficient inference compared to recent reasoning-focused LLMs. These results\nemphasize the potential of dynamic profile inference as a more effective\nparadigm for building personalized dialogue systems."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhao"
                    },
                    {
                        "name": "Xingyu Sui"
                    },
                    {
                        "name": "Yulin Hu"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Haixiao Liu"
                    },
                    {
                        "name": "Biye Li"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "30 pages, 18 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15450v1",
                "updated": "2025-05-21T12:31:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    31,
                    45,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:31:45Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    31,
                    45,
                    2,
                    141,
                    0
                ],
                "title": "Comprehensive Evaluation and Analysis for NSFW Concept Erasure in\n  Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Evaluation and Analysis for NSFW Concept Erasure in\n  Text-to-Image Diffusion Models"
                },
                "summary": "Text-to-image diffusion models have gained widespread application across\nvarious domains, demonstrating remarkable creative potential. However, the\nstrong generalization capabilities of diffusion models can inadvertently lead\nto the generation of not-safe-for-work (NSFW) content, posing significant risks\nto their safe deployment. While several concept erasure methods have been\nproposed to mitigate the issue associated with NSFW content, a comprehensive\nevaluation of their effectiveness across various scenarios remains absent. To\nbridge this gap, we introduce a full-pipeline toolkit specifically designed for\nconcept erasure and conduct the first systematic study of NSFW concept erasure\nmethods. By examining the interplay between the underlying mechanisms and\nempirical observations, we provide in-depth insights and practical guidance for\nthe effective application of concept erasure methods in various real-world\nscenarios, with the aim of advancing the understanding of content safety in\ndiffusion models and establishing a solid foundation for future research and\ndevelopment in this critical area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion models have gained widespread application across\nvarious domains, demonstrating remarkable creative potential. However, the\nstrong generalization capabilities of diffusion models can inadvertently lead\nto the generation of not-safe-for-work (NSFW) content, posing significant risks\nto their safe deployment. While several concept erasure methods have been\nproposed to mitigate the issue associated with NSFW content, a comprehensive\nevaluation of their effectiveness across various scenarios remains absent. To\nbridge this gap, we introduce a full-pipeline toolkit specifically designed for\nconcept erasure and conduct the first systematic study of NSFW concept erasure\nmethods. By examining the interplay between the underlying mechanisms and\nempirical observations, we provide in-depth insights and practical guidance for\nthe effective application of concept erasure methods in various real-world\nscenarios, with the aim of advancing the understanding of content safety in\ndiffusion models and establishing a solid foundation for future research and\ndevelopment in this critical area."
                },
                "authors": [
                    {
                        "name": "Die Chen"
                    },
                    {
                        "name": "Zhiwen Li"
                    },
                    {
                        "name": "Cen Chen"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Xiaodan Li"
                    },
                    {
                        "name": "Jinyan Ye"
                    },
                    {
                        "name": "Yingda Chen"
                    },
                    {
                        "name": "Yaliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yaliang Li"
                },
                "author": "Yaliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17403v3",
                "updated": "2025-05-21T12:31:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    31,
                    35,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-24T18:30:36Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    30,
                    36,
                    0,
                    55,
                    0
                ],
                "title": "Large Language Models are Powerful Electronic Health Record Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Powerful Electronic Health Record Encoders"
                },
                "summary": "Electronic Health Records (EHRs) offer considerable potential for clinical\nprediction, but their complexity and heterogeneity present significant\nchallenges for traditional machine learning methods. Recently, domain-specific\nEHR foundation models trained on large volumes of unlabeled EHR data have shown\nimproved predictive accuracy and generalization. However, their development is\nconstrained by limited access to diverse, high-quality datasets, and by\ninconsistencies in coding standards and clinical practices. In this study, we\nexplore the use of general-purpose Large Language Models (LLMs) to encode EHR\ninto high-dimensional representations for downstream clinical prediction tasks.\nWe convert structured EHR data into markdown-formatted plain text documents by\nreplacing medical codes with natural language descriptions. This enables the\nuse of LLMs and their extensive semantic understanding and generalization\ncapabilities as effective encoders of EHRs without requiring access to private\nmedical training data. We show that LLM-based embeddings can often match or\neven surpass the performance of a specialized EHR foundation model,\nCLMBR-T-Base, across 15 diverse clinical tasks from the EHRSHOT benchmark. To\ndemonstrate generalizability, we further evaluate the approach on the UK\nBiobank (UKB) cohort, a population distinct from that used to train\nCLMBR-T-Base. Notably, one of the tested LLM-based models achieves superior\nperformance for disease onset, hospitalization, and mortality prediction,\nhighlighting robustness to shifts in patient populations. Our findings suggest\nthat repurposed general-purpose LLMs for EHR encoding provide a scalable and\ngeneralizable alternative to domain-specific models for clinical prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) offer considerable potential for clinical\nprediction, but their complexity and heterogeneity present significant\nchallenges for traditional machine learning methods. Recently, domain-specific\nEHR foundation models trained on large volumes of unlabeled EHR data have shown\nimproved predictive accuracy and generalization. However, their development is\nconstrained by limited access to diverse, high-quality datasets, and by\ninconsistencies in coding standards and clinical practices. In this study, we\nexplore the use of general-purpose Large Language Models (LLMs) to encode EHR\ninto high-dimensional representations for downstream clinical prediction tasks.\nWe convert structured EHR data into markdown-formatted plain text documents by\nreplacing medical codes with natural language descriptions. This enables the\nuse of LLMs and their extensive semantic understanding and generalization\ncapabilities as effective encoders of EHRs without requiring access to private\nmedical training data. We show that LLM-based embeddings can often match or\neven surpass the performance of a specialized EHR foundation model,\nCLMBR-T-Base, across 15 diverse clinical tasks from the EHRSHOT benchmark. To\ndemonstrate generalizability, we further evaluate the approach on the UK\nBiobank (UKB) cohort, a population distinct from that used to train\nCLMBR-T-Base. Notably, one of the tested LLM-based models achieves superior\nperformance for disease onset, hospitalization, and mortality prediction,\nhighlighting robustness to shifts in patient populations. Our findings suggest\nthat repurposed general-purpose LLMs for EHR encoding provide a scalable and\ngeneralizable alternative to domain-specific models for clinical prediction."
                },
                "authors": [
                    {
                        "name": "Stefan Hegselmann"
                    },
                    {
                        "name": "Georg von Arnim"
                    },
                    {
                        "name": "Tillmann Rheude"
                    },
                    {
                        "name": "Noel Kronenberg"
                    },
                    {
                        "name": "David Sontag"
                    },
                    {
                        "name": "Gerhard Hindricks"
                    },
                    {
                        "name": "Roland Eils"
                    },
                    {
                        "name": "Benjamin Wild"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Wild"
                },
                "author": "Benjamin Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15444v1",
                "updated": "2025-05-21T12:25:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    25,
                    12,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:25:12Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    25,
                    12,
                    2,
                    141,
                    0
                ],
                "title": "Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation\n  Framework Using Role-Specific Token Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation\n  Framework Using Role-Specific Token Optimization"
                },
                "summary": "Existing studies have optimized retrieval-augmented generation (RAG) across\nvarious sub-tasks, such as query understanding and retrieval refinement, but\nintegrating these optimizations into a unified framework remains challenging.\nTo tackle this problem, this work proposes RoleRAG, a unified RAG framework\nthat achieves efficient multi-task processing through role-specific token\noptimization. RoleRAG comprises six modules, each handling a specific sub-task\nwithin the RAG process. Additionally, we introduce a query graph to represent\nthe decomposition of the query, which can be dynamically resolved according to\nthe decomposing state. All modules are driven by the same underlying LLM,\ndistinguished by task-specific role tokens that are individually optimized.\nThis design allows RoleRAG to dynamically activate different modules within a\nsingle LLM instance, thereby streamlining deployment and reducing resource\nconsumption. Experimental results on five open-domain question-answering\ndatasets demonstrate the effectiveness, generalizability, and flexibility of\nour framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing studies have optimized retrieval-augmented generation (RAG) across\nvarious sub-tasks, such as query understanding and retrieval refinement, but\nintegrating these optimizations into a unified framework remains challenging.\nTo tackle this problem, this work proposes RoleRAG, a unified RAG framework\nthat achieves efficient multi-task processing through role-specific token\noptimization. RoleRAG comprises six modules, each handling a specific sub-task\nwithin the RAG process. Additionally, we introduce a query graph to represent\nthe decomposition of the query, which can be dynamically resolved according to\nthe decomposing state. All modules are driven by the same underlying LLM,\ndistinguished by task-specific role tokens that are individually optimized.\nThis design allows RoleRAG to dynamically activate different modules within a\nsingle LLM instance, thereby streamlining deployment and reducing resource\nconsumption. Experimental results on five open-domain question-answering\ndatasets demonstrate the effectiveness, generalizability, and flexibility of\nour framework."
                },
                "authors": [
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Jiajie Jin"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15443v1",
                "updated": "2025-05-21T12:23:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    23,
                    40,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:23:40Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    23,
                    40,
                    2,
                    141,
                    0
                ],
                "title": "AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs"
                },
                "summary": "Uncertainty estimation remains a critical challenge in adapting pre-trained\nlanguage models to classification tasks, particularly under parameter-efficient\nfine-tuning approaches such as adapters. We introduce AdUE1, an efficient\npost-hoc uncertainty estimation (UE) method, to enhance softmax-based\nestimates. Our approach (1) uses a differentiable approximation of the maximum\nfunction and (2) applies additional regularization through L2-SP, anchoring the\nfine-tuned head weights and regularizing the model. Evaluations on five NLP\nclassification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2,\nQwen) demonstrate that our method consistently outperforms established\nbaselines such as Mahalanobis distance and softmax response. Our approach is\nlightweight (no base-model changes) and produces better-calibrated confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty estimation remains a critical challenge in adapting pre-trained\nlanguage models to classification tasks, particularly under parameter-efficient\nfine-tuning approaches such as adapters. We introduce AdUE1, an efficient\npost-hoc uncertainty estimation (UE) method, to enhance softmax-based\nestimates. Our approach (1) uses a differentiable approximation of the maximum\nfunction and (2) applies additional regularization through L2-SP, anchoring the\nfine-tuned head weights and regularizing the model. Evaluations on five NLP\nclassification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2,\nQwen) demonstrate that our method consistently outperforms established\nbaselines such as Mahalanobis distance and softmax response. Our approach is\nlightweight (no base-model changes) and produces better-calibrated confidence."
                },
                "authors": [
                    {
                        "name": "Artem Zabolotnyi"
                    },
                    {
                        "name": "Roman Makarov"
                    },
                    {
                        "name": "Mile Mitrovic"
                    },
                    {
                        "name": "Polina Proskura"
                    },
                    {
                        "name": "Oleg Travkin"
                    },
                    {
                        "name": "Roman Alferov"
                    },
                    {
                        "name": "Alexey Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Zaytsev"
                },
                "author": "Alexey Zaytsev",
                "arxiv_comment": "9 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15438v1",
                "updated": "2025-05-21T12:19:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    19,
                    55,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:19:55Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    19,
                    55,
                    2,
                    141,
                    0
                ],
                "title": "Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign\n  Language Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign\n  Language Translation"
                },
                "summary": "Sign Language Translation (SLT) aims to map sign language videos to spoken\nlanguage text. A common approach relies on gloss annotations as an intermediate\nrepresentation, decomposing SLT into two sub-tasks: video-to-gloss recognition\nand gloss-to-text translation. While effective, this paradigm depends on\nexpert-annotated gloss labels, which are costly and rarely available in\nexisting datasets, limiting its scalability. To address this challenge, we\npropose a gloss-free pseudo gloss generation framework that eliminates the need\nfor human-annotated glosses while preserving the structured intermediate\nrepresentation. Specifically, we prompt a Large Language Model (LLM) with a few\nexample text-gloss pairs using in-context learning to produce draft sign\nglosses from spoken language text. To enhance the correspondence between\nLLM-generated pseudo glosses and the sign sequences in video, we correct the\nordering in the pseudo glosses for better alignment via a weakly supervised\nlearning process. This reordering facilitates the incorporation of auxiliary\nalignment objectives, and allows for the use of efficient supervision via a\nConnectionist Temporal Classification (CTC) loss. We train our SLT mode, which\nconsists of a vision encoder and a translator, through a three-stage pipeline,\nwhich progressively narrows the modality gap between sign language and spoken\nlanguage. Despite its simplicity, our approach outperforms previous\nstate-of-the-art gloss-free frameworks on two SLT benchmarks and achieves\ncompetitive results compared to gloss-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Language Translation (SLT) aims to map sign language videos to spoken\nlanguage text. A common approach relies on gloss annotations as an intermediate\nrepresentation, decomposing SLT into two sub-tasks: video-to-gloss recognition\nand gloss-to-text translation. While effective, this paradigm depends on\nexpert-annotated gloss labels, which are costly and rarely available in\nexisting datasets, limiting its scalability. To address this challenge, we\npropose a gloss-free pseudo gloss generation framework that eliminates the need\nfor human-annotated glosses while preserving the structured intermediate\nrepresentation. Specifically, we prompt a Large Language Model (LLM) with a few\nexample text-gloss pairs using in-context learning to produce draft sign\nglosses from spoken language text. To enhance the correspondence between\nLLM-generated pseudo glosses and the sign sequences in video, we correct the\nordering in the pseudo glosses for better alignment via a weakly supervised\nlearning process. This reordering facilitates the incorporation of auxiliary\nalignment objectives, and allows for the use of efficient supervision via a\nConnectionist Temporal Classification (CTC) loss. We train our SLT mode, which\nconsists of a vision encoder and a translator, through a three-stage pipeline,\nwhich progressively narrows the modality gap between sign language and spoken\nlanguage. Despite its simplicity, our approach outperforms previous\nstate-of-the-art gloss-free frameworks on two SLT benchmarks and achieves\ncompetitive results compared to gloss-based methods."
                },
                "authors": [
                    {
                        "name": "Jianyuan Guo"
                    },
                    {
                        "name": "Peike Li"
                    },
                    {
                        "name": "Trevor Cohn"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Cohn"
                },
                "author": "Trevor Cohn",
                "arxiv_comment": "Technical report, 21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15436v1",
                "updated": "2025-05-21T12:18:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    18,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:18:15Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    18,
                    15,
                    2,
                    141,
                    0
                ],
                "title": "Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal\n  Reasoning via RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal\n  Reasoning via RL"
                },
                "summary": "Vision language models (VLMs) have achieved impressive performance across a\nvariety of computer vision tasks. However, the multimodal reasoning capability\nhas not been fully explored in existing models. In this paper, we propose a\nChain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and\nzooming in on key image regions based on obtained visual cues and the given\nquestions, achieving efficient multimodal reasoning. To enable this CoF\ncapability, we present a two-stage training pipeline, including supervised\nfine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we\nconstruct the MM-CoF dataset, comprising 3K samples derived from a visual agent\ndesigned to adaptively identify key regions to solve visual tasks with\ndifferent image resolutions and questions. We use MM-CoF to fine-tune the\nQwen2.5-VL model for cold start. In the RL stage, we leverage the outcome\naccuracies and formats as rewards to update the Qwen2.5-VL model, enabling\nfurther refining the search and reasoning strategy of models without human\npriors. Our model achieves significant improvements on multiple benchmarks. On\nthe V* benchmark that requires strong visual reasoning capability, our model\noutperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to\n4K, demonstrating the effectiveness of the proposed CoF method and facilitating\nthe more efficient deployment of VLMs in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language models (VLMs) have achieved impressive performance across a\nvariety of computer vision tasks. However, the multimodal reasoning capability\nhas not been fully explored in existing models. In this paper, we propose a\nChain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and\nzooming in on key image regions based on obtained visual cues and the given\nquestions, achieving efficient multimodal reasoning. To enable this CoF\ncapability, we present a two-stage training pipeline, including supervised\nfine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we\nconstruct the MM-CoF dataset, comprising 3K samples derived from a visual agent\ndesigned to adaptively identify key regions to solve visual tasks with\ndifferent image resolutions and questions. We use MM-CoF to fine-tune the\nQwen2.5-VL model for cold start. In the RL stage, we leverage the outcome\naccuracies and formats as rewards to update the Qwen2.5-VL model, enabling\nfurther refining the search and reasoning strategy of models without human\npriors. Our model achieves significant improvements on multiple benchmarks. On\nthe V* benchmark that requires strong visual reasoning capability, our model\noutperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to\n4K, demonstrating the effectiveness of the proposed CoF method and facilitating\nthe more efficient deployment of VLMs in practical applications."
                },
                "authors": [
                    {
                        "name": "Xintong Zhang"
                    },
                    {
                        "name": "Zhi Gao"
                    },
                    {
                        "name": "Bofei Zhang"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xiaowen Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Yunde Jia"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15433v1",
                "updated": "2025-05-21T12:14:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    14,
                    26,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T12:14:26Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    14,
                    26,
                    2,
                    141,
                    0
                ],
                "title": "Set-LLM: A Permutation-Invariant LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set-LLM: A Permutation-Invariant LLM"
                },
                "summary": "While large language models (LLMs) demonstrate impressive capabilities across\nnumerous applications, their robustness remains a critical concern. This paper\nis motivated by a specific vulnerability: the order sensitivity of LLMs. This\nvulnerability manifests itself as the order bias observed when LLMs decide\nbetween possible options (for example, a preference for the first option) and\nthe tendency of LLMs to provide different answers when options are reordered.\nThe use cases for this scenario extend beyond the classical case of\nmultiple-choice question answering to the use of LLMs as automated evaluators\nin AI pipelines, comparing output generated by different models. We introduce\nSet-LLM, a novel architectural adaptation for pretrained LLMs that enables the\nprocessing of mixed set-text inputs with permutation invariance guarantees. The\nadaptations involve a new attention mask and new positional encodings\nspecifically designed for sets. We provide a theoretical proof of invariance\nand demonstrate through experiments that Set-LLM can be trained effectively,\nachieving comparable or improved performance and maintaining the runtime of the\noriginal model, while eliminating order sensitivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) demonstrate impressive capabilities across\nnumerous applications, their robustness remains a critical concern. This paper\nis motivated by a specific vulnerability: the order sensitivity of LLMs. This\nvulnerability manifests itself as the order bias observed when LLMs decide\nbetween possible options (for example, a preference for the first option) and\nthe tendency of LLMs to provide different answers when options are reordered.\nThe use cases for this scenario extend beyond the classical case of\nmultiple-choice question answering to the use of LLMs as automated evaluators\nin AI pipelines, comparing output generated by different models. We introduce\nSet-LLM, a novel architectural adaptation for pretrained LLMs that enables the\nprocessing of mixed set-text inputs with permutation invariance guarantees. The\nadaptations involve a new attention mask and new positional encodings\nspecifically designed for sets. We provide a theoretical proof of invariance\nand demonstrate through experiments that Set-LLM can be trained effectively,\nachieving comparable or improved performance and maintaining the runtime of the\noriginal model, while eliminating order sensitivity."
                },
                "authors": [
                    {
                        "name": "Beni Egressy"
                    },
                    {
                        "name": "Jan Stühmer"
                    }
                ],
                "author_detail": {
                    "name": "Jan Stühmer"
                },
                "author": "Jan Stühmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v2",
                "updated": "2025-05-22T06:44:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    44,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]