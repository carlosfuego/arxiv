[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v1",
                "updated": "2024-12-04T15:48:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v2",
                "updated": "2024-12-03T21:40:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    21,
                    40,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v1",
                "updated": "2024-12-02T11:57:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Kpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v1",
                "updated": "2024-11-29T05:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "Sbastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04467v1",
                "updated": "2024-12-05T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    59,
                    53,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    59,
                    53,
                    3,
                    340,
                    0
                ],
                "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisionZip: Longer is Better but Not Necessary in Vision Language Models"
                },
                "summary": "Recent advancements in vision-language models have enhanced performance by\nincreasing the length of visual tokens, making them much longer than text\ntokens and significantly raising computational costs. However, we observe that\nthe visual tokens generated by popular vision encoders, such as CLIP and\nSigLIP, contain significant redundancy. To address this, we introduce\nVisionZip, a simple yet effective method that selects a set of informative\ntokens for input to the language model, reducing visual token redundancy and\nimproving efficiency while maintaining model performance. The proposed\nVisionZip can be widely applied to image and video understanding tasks and is\nwell-suited for multi-turn dialogues in real-world scenarios, where previous\nmethods tend to underperform. Experimental results show that VisionZip\noutperforms the previous state-of-the-art method by at least 5% performance\ngains across nearly all settings. Moreover, our method significantly enhances\nmodel inference speed, improving the prefilling time by 8x and enabling the\nLLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while\nachieving better results. Furthermore, we analyze the causes of this redundancy\nand encourage the community to focus on extracting better visual features\nrather than merely increasing token length. Our code is available at\nhttps://github.com/dvlab-research/VisionZip .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in vision-language models have enhanced performance by\nincreasing the length of visual tokens, making them much longer than text\ntokens and significantly raising computational costs. However, we observe that\nthe visual tokens generated by popular vision encoders, such as CLIP and\nSigLIP, contain significant redundancy. To address this, we introduce\nVisionZip, a simple yet effective method that selects a set of informative\ntokens for input to the language model, reducing visual token redundancy and\nimproving efficiency while maintaining model performance. The proposed\nVisionZip can be widely applied to image and video understanding tasks and is\nwell-suited for multi-turn dialogues in real-world scenarios, where previous\nmethods tend to underperform. Experimental results show that VisionZip\noutperforms the previous state-of-the-art method by at least 5% performance\ngains across nearly all settings. Moreover, our method significantly enhances\nmodel inference speed, improving the prefilling time by 8x and enabling the\nLLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while\nachieving better results. Furthermore, we analyze the causes of this redundancy\nand encourage the community to focus on extracting better visual features\nrather than merely increasing token length. Our code is available at\nhttps://github.com/dvlab-research/VisionZip ."
                },
                "authors": [
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Chengyao Wang"
                    },
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "2 columns, 28 pages, 15 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04463v1",
                "updated": "2024-12-05T18:59:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    59,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:59:42Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    59,
                    42,
                    3,
                    340,
                    0
                ],
                "title": "MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual\n  Dynamic Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual\n  Dynamic Videos"
                },
                "summary": "We present a system that allows for accurate, fast, and robust estimation of\ncamera parameters and depth maps from casual monocular videos of dynamic\nscenes. Most conventional structure from motion and monocular SLAM techniques\nassume input videos that feature predominantly static scenes with large amounts\nof parallax. Such methods tend to produce erroneous estimates in the absence of\nthese conditions. Recent neural network-based approaches attempt to overcome\nthese challenges; however, such methods are either computationally expensive or\nbrittle when run on dynamic videos with uncontrolled camera motion or unknown\nfield of view. We demonstrate the surprising effectiveness of a deep visual\nSLAM framework: with careful modifications to its training and inference\nschemes, this system can scale to real-world videos of complex dynamic scenes\nwith unconstrained camera paths, including videos with little camera parallax.\nExtensive experiments on both synthetic and real videos demonstrate that our\nsystem is significantly more accurate and robust at camera pose and depth\nestimation when compared with prior and concurrent work, with faster or\ncomparable running times. See interactive results on our project page:\nhttps://mega-sam.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a system that allows for accurate, fast, and robust estimation of\ncamera parameters and depth maps from casual monocular videos of dynamic\nscenes. Most conventional structure from motion and monocular SLAM techniques\nassume input videos that feature predominantly static scenes with large amounts\nof parallax. Such methods tend to produce erroneous estimates in the absence of\nthese conditions. Recent neural network-based approaches attempt to overcome\nthese challenges; however, such methods are either computationally expensive or\nbrittle when run on dynamic videos with uncontrolled camera motion or unknown\nfield of view. We demonstrate the surprising effectiveness of a deep visual\nSLAM framework: with careful modifications to its training and inference\nschemes, this system can scale to real-world videos of complex dynamic scenes\nwith unconstrained camera paths, including videos with little camera parallax.\nExtensive experiments on both synthetic and real videos demonstrate that our\nsystem is significantly more accurate and robust at camera pose and depth\nestimation when compared with prior and concurrent work, with faster or\ncomparable running times. See interactive results on our project page:\nhttps://mega-sam.github.io/"
                },
                "authors": [
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Richard Tucker"
                    },
                    {
                        "name": "Forrester Cole"
                    },
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Linyi Jin"
                    },
                    {
                        "name": "Vickie Ye"
                    },
                    {
                        "name": "Angjoo Kanazawa"
                    },
                    {
                        "name": "Aleksander Holynski"
                    },
                    {
                        "name": "Noah Snavely"
                    }
                ],
                "author_detail": {
                    "name": "Noah Snavely"
                },
                "author": "Noah Snavely",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04462v1",
                "updated": "2024-12-05T18:59:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    59,
                    41,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:59:41Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    59,
                    41,
                    3,
                    340,
                    0
                ],
                "title": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion"
                },
                "summary": "We propose 4Real-Video, a novel framework for generating 4D videos, organized\nas a grid of video frames with both time and viewpoint axes. In this grid, each\nrow contains frames sharing the same timestep, while each column contains\nframes from the same viewpoint. We propose a novel two-stream architecture. One\nstream performs viewpoint updates on columns, and the other stream performs\ntemporal updates on rows. After each diffusion transformer layer, a\nsynchronization layer exchanges information between the two token streams. We\npropose two implementations of the synchronization layer, using either hard or\nsoft synchronization. This feedforward architecture improves upon previous work\nin three ways: higher inference speed, enhanced visual quality (measured by\nFVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency\n(measured by VideoScore and Dust3R-Confidence).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose 4Real-Video, a novel framework for generating 4D videos, organized\nas a grid of video frames with both time and viewpoint axes. In this grid, each\nrow contains frames sharing the same timestep, while each column contains\nframes from the same viewpoint. We propose a novel two-stream architecture. One\nstream performs viewpoint updates on columns, and the other stream performs\ntemporal updates on rows. After each diffusion transformer layer, a\nsynchronization layer exchanges information between the two token streams. We\npropose two implementations of the synchronization layer, using either hard or\nsoft synchronization. This feedforward architecture improves upon previous work\nin three ways: higher inference speed, enhanced visual quality (measured by\nFVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency\n(measured by VideoScore and Dust3R-Confidence)."
                },
                "authors": [
                    {
                        "name": "Chaoyang Wang"
                    },
                    {
                        "name": "Peiye Zhuang"
                    },
                    {
                        "name": "Tuan Duc Ngo"
                    },
                    {
                        "name": "Willi Menapace"
                    },
                    {
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "name": "Michael Vasilkovsky"
                    },
                    {
                        "name": "Ivan Skorokhodov"
                    },
                    {
                        "name": "Sergey Tulyakov"
                    },
                    {
                        "name": "Peter Wonka"
                    },
                    {
                        "name": "Hsin-Ying Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Ying Lee"
                },
                "author": "Hsin-Ying Lee",
                "arxiv_comment": "Project page: https://snap-research.github.io/4Real-Video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04452v1",
                "updated": "2024-12-05T18:58:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Four-Plane Factorized Video Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Four-Plane Factorized Video Autoencoders"
                },
                "summary": "Latent variable generative models have emerged as powerful tools for\ngenerative tasks including image and video synthesis. These models are enabled\nby pretrained autoencoders that map high resolution data into a compressed\nlower dimensional latent space, where the generative models can subsequently be\ndeveloped while requiring fewer computational resources. Despite their\neffectiveness, the direct application of latent variable models to higher\ndimensional domains such as videos continues to pose challenges for efficient\ntraining and inference. In this paper, we propose an autoencoder that projects\nvolumetric data onto a four-plane factorized latent space that grows\nsublinearly with the input size, making it ideal for higher dimensional data\nlike videos. The design of our factorized model supports straightforward\nadoption in a number of conditional generation tasks with latent diffusion\nmodels (LDMs), such as class-conditional generation, frame prediction, and\nvideo interpolation. Our results show that the proposed four-plane latent space\nretains a rich representation needed for high-fidelity reconstructions despite\nthe heavy compression, while simultaneously enabling LDMs to operate with\nsignificant improvements in speed and memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent variable generative models have emerged as powerful tools for\ngenerative tasks including image and video synthesis. These models are enabled\nby pretrained autoencoders that map high resolution data into a compressed\nlower dimensional latent space, where the generative models can subsequently be\ndeveloped while requiring fewer computational resources. Despite their\neffectiveness, the direct application of latent variable models to higher\ndimensional domains such as videos continues to pose challenges for efficient\ntraining and inference. In this paper, we propose an autoencoder that projects\nvolumetric data onto a four-plane factorized latent space that grows\nsublinearly with the input size, making it ideal for higher dimensional data\nlike videos. The design of our factorized model supports straightforward\nadoption in a number of conditional generation tasks with latent diffusion\nmodels (LDMs), such as class-conditional generation, frame prediction, and\nvideo interpolation. Our results show that the proposed four-plane latent space\nretains a rich representation needed for high-fidelity reconstructions despite\nthe heavy compression, while simultaneously enabling LDMs to operate with\nsignificant improvements in speed and memory."
                },
                "authors": [
                    {
                        "name": "Mohammed Suhail"
                    },
                    {
                        "name": "Carlos Esteves"
                    },
                    {
                        "name": "Leonid Sigal"
                    },
                    {
                        "name": "Ameesh Makadia"
                    }
                ],
                "author_detail": {
                    "name": "Ameesh Makadia"
                },
                "author": "Ameesh Makadia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04450v1",
                "updated": "2024-12-05T18:58:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    8,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:08Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    8,
                    3,
                    340,
                    0
                ],
                "title": "Interfacial and density fluctuations in a lattice model of\n  motility-induced phase separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interfacial and density fluctuations in a lattice model of\n  motility-induced phase separation"
                },
                "summary": "We analyze motility-induced phase separation and bubbly phase separation in a\ntwo-dimensional lattice model of self-propelled particles. We compare systems\nwhere the dense (liquid) phase has slab and droplet geometries. We find that\ninterfacial fluctuations of the slab are well-described by capillary wave\ntheory, despite the existence of bubbles in the dense phase. We attribute this\nto a separation of time scales between bubble expulsion and interfacial\nrelaxation. We also characterize dependence of liquid and vapor densities on\nthe curvature of the liquid droplet, as well as the density fluctuations inside\nthe phases. The vapor phase behaves similarly to an equilibrium system,\ndisplaying a Laplace pressure effect that shifts its density, and Gaussian\ndensity fluctuations. The liquid phase has large non-Gaussian fluctuations, but\nthis is not accompanied by a large density shift, contrary to the equilibrium\ncase. Nevertheless, the shift of the vapor density can be used to infer an\neffective surface tension that appears to also quantify capillary wave\nfluctuations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze motility-induced phase separation and bubbly phase separation in a\ntwo-dimensional lattice model of self-propelled particles. We compare systems\nwhere the dense (liquid) phase has slab and droplet geometries. We find that\ninterfacial fluctuations of the slab are well-described by capillary wave\ntheory, despite the existence of bubbles in the dense phase. We attribute this\nto a separation of time scales between bubble expulsion and interfacial\nrelaxation. We also characterize dependence of liquid and vapor densities on\nthe curvature of the liquid droplet, as well as the density fluctuations inside\nthe phases. The vapor phase behaves similarly to an equilibrium system,\ndisplaying a Laplace pressure effect that shifts its density, and Gaussian\ndensity fluctuations. The liquid phase has large non-Gaussian fluctuations, but\nthis is not accompanied by a large density shift, contrary to the equilibrium\ncase. Nevertheless, the shift of the vapor density can be used to infer an\neffective surface tension that appears to also quantify capillary wave\nfluctuations."
                },
                "authors": [
                    {
                        "name": "Liheng Yao"
                    },
                    {
                        "name": "Robert L. Jack"
                    }
                ],
                "author_detail": {
                    "name": "Robert L. Jack"
                },
                "author": "Robert L. Jack",
                "arxiv_comment": "14 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04444v1",
                "updated": "2024-12-05T18:57:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    57,
                    2,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:57:02Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    57,
                    2,
                    3,
                    340,
                    0
                ],
                "title": "Block Lanczos for lattice QCD spectroscopy and matrix elements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Lanczos for lattice QCD spectroscopy and matrix elements"
                },
                "summary": "Recent work introduced a new framework for analyzing correlation functions\nwith improved convergence and signal-to-noise properties, as well as rigorous\nquantification of excited-state effects, based on the Lanczos algorithm and\nspurious eigenvalue filtering with the Cullum-Willoughby test. Here, we extend\nthis framework to the analysis of correlation-function matrices built from\nmultiple interpolating operators in lattice quantum chromodynamics (QCD) by\nconstructing an oblique generalization of the block Lanczos algorithm, as well\nas a new physically motivated reformulation of the Cullum-Willoughby test that\ngeneralizes to block Lanczos straightforwardly. The resulting block Lanczos\nmethod directly extends generalized eigenvalue problem (GEVP) methods, which\ncan be viewed as applying a single iteration of block Lanczos. Block Lanczos\nprovides qualitative and quantitative advantages over GEVP methods analogous to\nthe benefits of Lanczos over the standard effective mass, including faster\nconvergence to ground- and excited-state energies, explicitly computable\ntwo-sided error bounds, straightforward extraction of matrix elements of\nexternal currents, and asymptotically constant signal-to-noise. No fits or\nstatistical inference are required. Proof-of-principle calculations are\nperformed for noiseless mock-data examples as well as two-by-two proton\ncorrelation-function matrices in lattice QCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work introduced a new framework for analyzing correlation functions\nwith improved convergence and signal-to-noise properties, as well as rigorous\nquantification of excited-state effects, based on the Lanczos algorithm and\nspurious eigenvalue filtering with the Cullum-Willoughby test. Here, we extend\nthis framework to the analysis of correlation-function matrices built from\nmultiple interpolating operators in lattice quantum chromodynamics (QCD) by\nconstructing an oblique generalization of the block Lanczos algorithm, as well\nas a new physically motivated reformulation of the Cullum-Willoughby test that\ngeneralizes to block Lanczos straightforwardly. The resulting block Lanczos\nmethod directly extends generalized eigenvalue problem (GEVP) methods, which\ncan be viewed as applying a single iteration of block Lanczos. Block Lanczos\nprovides qualitative and quantitative advantages over GEVP methods analogous to\nthe benefits of Lanczos over the standard effective mass, including faster\nconvergence to ground- and excited-state energies, explicitly computable\ntwo-sided error bounds, straightforward extraction of matrix elements of\nexternal currents, and asymptotically constant signal-to-noise. No fits or\nstatistical inference are required. Proof-of-principle calculations are\nperformed for noiseless mock-data examples as well as two-by-two proton\ncorrelation-function matrices in lattice QCD."
                },
                "authors": [
                    {
                        "name": "Daniel C. Hackett"
                    },
                    {
                        "name": "Michael L. Wagman"
                    }
                ],
                "author_detail": {
                    "name": "Michael L. Wagman"
                },
                "author": "Michael L. Wagman",
                "arxiv_comment": "39+13 pages, 36 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-lat",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04443v1",
                "updated": "2024-12-05T18:57:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    57,
                    0,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:57:00Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    57,
                    0,
                    3,
                    340,
                    0
                ],
                "title": "Diagnosing Systematic Effects Using the Inferred Initial Power Spectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing Systematic Effects Using the Inferred Initial Power Spectrum"
                },
                "summary": "The next generation of galaxy surveys has the potential to substantially\ndeepen our understanding of the Universe. This potential hinges on our ability\nto rigorously address systematic uncertainties. Until now, diagnosing\nsystematic effects prior to inferring cosmological parameters has been out of\nreach in field-based implicit likelihood cosmological inference frameworks. As\na solution, we aim to diagnose a variety of systematic effects in galaxy\nsurveys prior to inferring cosmological parameters, using the inferred initial\nmatter power spectrum. Our approach is built upon a two-step framework. First,\nwe employ the Simulator Expansion for Likelihood-Free Inference (SELFI)\nalgorithm to infer the initial matter power spectrum, which we utilise to\nthoroughly investigate the impact of systematic effects. This investigation\nrelies on a single set of N-body simulations. Second, we obtain a posterior on\ncosmological parameters via implicit likelihood inference, recycling the\nsimulations from the first step for data compression. For demonstration, we\nrely on a model of large-scale spectroscopic galaxy surveys that incorporates\nfully non-linear gravitational evolution and simulates multiple systematic\neffects encountered in real surveys. We provide a practical guide on how the\nSELFI posterior can be used to assess the impact of misspecified galaxy bias\nparameters, selection functions, survey masks, inaccurate redshifts, and\napproximate gravity models on the inferred initial matter power spectrum. We\nshow that a subtly misspecified model can lead to a bias exceeding $2\\sigma$ in\nthe $(\\Omega_\\mathrm{m},\\sigma_8)$ plane, which we are able to detect and avoid\nprior to inferring the cosmological parameters. This framework has the\npotential to significantly enhance the robustness of physical information\nextraction from full-forward models of large-scale galaxy surveys such as DESI,\nEuclid, and LSST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next generation of galaxy surveys has the potential to substantially\ndeepen our understanding of the Universe. This potential hinges on our ability\nto rigorously address systematic uncertainties. Until now, diagnosing\nsystematic effects prior to inferring cosmological parameters has been out of\nreach in field-based implicit likelihood cosmological inference frameworks. As\na solution, we aim to diagnose a variety of systematic effects in galaxy\nsurveys prior to inferring cosmological parameters, using the inferred initial\nmatter power spectrum. Our approach is built upon a two-step framework. First,\nwe employ the Simulator Expansion for Likelihood-Free Inference (SELFI)\nalgorithm to infer the initial matter power spectrum, which we utilise to\nthoroughly investigate the impact of systematic effects. This investigation\nrelies on a single set of N-body simulations. Second, we obtain a posterior on\ncosmological parameters via implicit likelihood inference, recycling the\nsimulations from the first step for data compression. For demonstration, we\nrely on a model of large-scale spectroscopic galaxy surveys that incorporates\nfully non-linear gravitational evolution and simulates multiple systematic\neffects encountered in real surveys. We provide a practical guide on how the\nSELFI posterior can be used to assess the impact of misspecified galaxy bias\nparameters, selection functions, survey masks, inaccurate redshifts, and\napproximate gravity models on the inferred initial matter power spectrum. We\nshow that a subtly misspecified model can lead to a bias exceeding $2\\sigma$ in\nthe $(\\Omega_\\mathrm{m},\\sigma_8)$ plane, which we are able to detect and avoid\nprior to inferring the cosmological parameters. This framework has the\npotential to significantly enhance the robustness of physical information\nextraction from full-forward models of large-scale galaxy surveys such as DESI,\nEuclid, and LSST."
                },
                "authors": [
                    {
                        "name": "Tristan Hoellinger"
                    },
                    {
                        "name": "Florent Leclercq"
                    }
                ],
                "author_detail": {
                    "name": "Florent Leclercq"
                },
                "author": "Florent Leclercq",
                "arxiv_comment": "28 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04432v1",
                "updated": "2024-12-05T18:53:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    53,
                    4,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:53:04Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    53,
                    4,
                    3,
                    340,
                    0
                ],
                "title": "Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation"
                },
                "summary": "In recent years, there has been a significant surge of interest in unifying\nimage comprehension and generation within Large Language Models (LLMs). This\ngrowing interest has prompted us to explore extending this unification to\nvideos. The core challenge lies in developing a versatile video tokenizer that\ncaptures both the spatial characteristics and temporal dynamics of videos to\nobtain representations for LLMs, and the representations can be further decoded\ninto realistic video clips to enable video generation. In this work, we\nintroduce Divot, a Diffusion-Powered Video Tokenizer, which leverages the\ndiffusion process for self-supervised video representation learning. We posit\nthat if a video diffusion model can effectively de-noise video clips by taking\nthe features of a video tokenizer as the condition, then the tokenizer has\nsuccessfully captured robust spatial and temporal information. Additionally,\nthe video diffusion model inherently functions as a de-tokenizer, decoding\nvideos from their representations. Building upon the Divot tokenizer, we\npresent Divot-Vicuna through video-to-text autoregression and text-to-video\ngeneration by modeling the distributions of continuous-valued Divot features\nwith a Gaussian Mixture Model. Experimental results demonstrate that our\ndiffusion-based video tokenizer, when integrated with a pre-trained LLM,\nachieves competitive performance across various video comprehension and\ngeneration benchmarks. The instruction tuned Divot-Vicuna also excels in video\nstorytelling, generating interleaved narratives and corresponding videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a significant surge of interest in unifying\nimage comprehension and generation within Large Language Models (LLMs). This\ngrowing interest has prompted us to explore extending this unification to\nvideos. The core challenge lies in developing a versatile video tokenizer that\ncaptures both the spatial characteristics and temporal dynamics of videos to\nobtain representations for LLMs, and the representations can be further decoded\ninto realistic video clips to enable video generation. In this work, we\nintroduce Divot, a Diffusion-Powered Video Tokenizer, which leverages the\ndiffusion process for self-supervised video representation learning. We posit\nthat if a video diffusion model can effectively de-noise video clips by taking\nthe features of a video tokenizer as the condition, then the tokenizer has\nsuccessfully captured robust spatial and temporal information. Additionally,\nthe video diffusion model inherently functions as a de-tokenizer, decoding\nvideos from their representations. Building upon the Divot tokenizer, we\npresent Divot-Vicuna through video-to-text autoregression and text-to-video\ngeneration by modeling the distributions of continuous-valued Divot features\nwith a Gaussian Mixture Model. Experimental results demonstrate that our\ndiffusion-based video tokenizer, when integrated with a pre-trained LLM,\nachieves competitive performance across various video comprehension and\ngeneration benchmarks. The instruction tuned Divot-Vicuna also excels in video\nstorytelling, generating interleaved narratives and corresponding videos."
                },
                "authors": [
                    {
                        "name": "Yuying Ge"
                    },
                    {
                        "name": "Yizhuo Li"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project released at: https://github.com/TencentARC/Divot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04424v1",
                "updated": "2024-12-05T18:50:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    50,
                    39,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:50:39Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    50,
                    39,
                    3,
                    340,
                    0
                ],
                "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision\n  Encoder and Depth-Breadth Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Florence-VL: Enhancing Vision-Language Models with Generative Vision\n  Encoder and Depth-Breadth Fusion"
                },
                "summary": "We present Florence-VL, a new family of multimodal large language models\n(MLLMs) with enriched visual representations produced by Florence-2, a\ngenerative vision foundation model. Unlike the widely used CLIP-style vision\ntransformer trained by contrastive learning, Florence-2 can capture different\nlevels and aspects of visual features, which are more versatile to be adapted\nto diverse downstream tasks. We propose a novel feature-fusion architecture and\nan innovative training recipe that effectively integrates Florence-2's visual\nfeatures into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we\npropose \"depth-breath fusion (DBFusion)\" to fuse the visual features extracted\nfrom different depths and under multiple prompts. Our model training is\ncomposed of end-to-end pretraining of the whole model followed by finetuning of\nthe projection layer and the LLM, on a carefully designed recipe of diverse\nopen-source datasets that include high-quality image captions and\ninstruction-tuning pairs. Our quantitative analysis and visualization of\nFlorence-VL's visual features show its advantages over popular vision encoders\non vision-language alignment, where the enriched depth and breath play\nimportant roles. Florence-VL achieves significant improvements over existing\nstate-of-the-art MLLMs across various multi-modal and vision-centric benchmarks\ncovering general VQA, perception, hallucination, OCR, Chart,\nknowledge-intensive understanding, etc. To facilitate future research, our\nmodels and the complete training recipe are open-sourced.\nhttps://github.com/JiuhaiChen/Florence-VL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Florence-VL, a new family of multimodal large language models\n(MLLMs) with enriched visual representations produced by Florence-2, a\ngenerative vision foundation model. Unlike the widely used CLIP-style vision\ntransformer trained by contrastive learning, Florence-2 can capture different\nlevels and aspects of visual features, which are more versatile to be adapted\nto diverse downstream tasks. We propose a novel feature-fusion architecture and\nan innovative training recipe that effectively integrates Florence-2's visual\nfeatures into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we\npropose \"depth-breath fusion (DBFusion)\" to fuse the visual features extracted\nfrom different depths and under multiple prompts. Our model training is\ncomposed of end-to-end pretraining of the whole model followed by finetuning of\nthe projection layer and the LLM, on a carefully designed recipe of diverse\nopen-source datasets that include high-quality image captions and\ninstruction-tuning pairs. Our quantitative analysis and visualization of\nFlorence-VL's visual features show its advantages over popular vision encoders\non vision-language alignment, where the enriched depth and breath play\nimportant roles. Florence-VL achieves significant improvements over existing\nstate-of-the-art MLLMs across various multi-modal and vision-centric benchmarks\ncovering general VQA, perception, hallucination, OCR, Chart,\nknowledge-intensive understanding, etc. To facilitate future research, our\nmodels and the complete training recipe are open-sourced.\nhttps://github.com/JiuhaiChen/Florence-VL"
                },
                "authors": [
                    {
                        "name": "Jiuhai Chen"
                    },
                    {
                        "name": "Jianwei Yang"
                    },
                    {
                        "name": "Haiping Wu"
                    },
                    {
                        "name": "Dianqi Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Bin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xiao"
                },
                "author": "Bin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07384v2",
                "updated": "2024-12-05T18:47:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    47,
                    47,
                    3,
                    340,
                    0
                ],
                "published": "2024-03-12T07:45:33Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    7,
                    45,
                    33,
                    1,
                    72,
                    0
                ],
                "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\n  Language Models by Summarizing Training Trajectories of Small Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\n  Language Models by Summarizing Training Trajectories of Small Models"
                },
                "summary": "Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection."
                },
                "authors": [
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Siddhartha Mishra"
                    },
                    {
                        "name": "Jeffrey N Chiang"
                    },
                    {
                        "name": "Baharan Mirzasoleiman"
                    }
                ],
                "author_detail": {
                    "name": "Baharan Mirzasoleiman"
                },
                "author": "Baharan Mirzasoleiman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04418v1",
                "updated": "2024-12-05T18:44:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    44,
                    33,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:44:33Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    44,
                    33,
                    3,
                    340,
                    0
                ],
                "title": "ACE2-SOM: Coupling to a slab ocean and learning the sensitivity of\n  climate to changes in CO$_2$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACE2-SOM: Coupling to a slab ocean and learning the sensitivity of\n  climate to changes in CO$_2$"
                },
                "summary": "While autoregressive machine-learning-based emulators have been trained to\nproduce stable and accurate rollouts in the climate of the present-day and\nrecent past, none so far have been trained to emulate the sensitivity of\nclimate to substantial changes in CO$_2$ or other greenhouse gases. As an\ninitial step we couple the Ai2 Climate Emulator version 2 to a slab ocean model\n(hereafter ACE2-SOM) and train it on output from a collection of\nequilibrium-climate physics-based reference simulations with varying levels of\nCO$_2$. We test it in equilibrium and non-equilibrium climate scenarios with\nCO$_2$ concentrations seen and unseen in training.\n  ACE2-SOM performs well in equilibrium-climate inference with both in-sample\nand out-of-sample CO$_2$ concentrations, accurately reproducing the emergent\ntime-mean spatial patterns of surface temperature and precipitation change with\nCO$_2$ doubling, tripling, or quadrupling. In addition, the vertical profile of\natmospheric warming and change in extreme precipitation rates with increased\nCO$_2$ closely agree with the reference model. Non-equilibrium-climate\ninference is more challenging. With CO$_2$ increasing gradually at a rate of 2%\nyear$^{-1}$, ACE2-SOM can accurately emulate the global annual mean trends of\nsurface and lower-to-middle atmosphere fields but produces unphysical jumps in\nstratospheric fields. With an abrupt quadrupling of CO$_2$, ML-controlled\nfields transition unrealistically quickly to the 4xCO$_2$ regime. In doing so\nthey violate global energy conservation and exhibit unphysical sensitivities of\nand surface and top of atmosphere radiative fluxes to instantaneous changes in\nCO$_2$. Future emulator development needed to address these issues should\nimprove its generalizability to diverse climate change scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While autoregressive machine-learning-based emulators have been trained to\nproduce stable and accurate rollouts in the climate of the present-day and\nrecent past, none so far have been trained to emulate the sensitivity of\nclimate to substantial changes in CO$_2$ or other greenhouse gases. As an\ninitial step we couple the Ai2 Climate Emulator version 2 to a slab ocean model\n(hereafter ACE2-SOM) and train it on output from a collection of\nequilibrium-climate physics-based reference simulations with varying levels of\nCO$_2$. We test it in equilibrium and non-equilibrium climate scenarios with\nCO$_2$ concentrations seen and unseen in training.\n  ACE2-SOM performs well in equilibrium-climate inference with both in-sample\nand out-of-sample CO$_2$ concentrations, accurately reproducing the emergent\ntime-mean spatial patterns of surface temperature and precipitation change with\nCO$_2$ doubling, tripling, or quadrupling. In addition, the vertical profile of\natmospheric warming and change in extreme precipitation rates with increased\nCO$_2$ closely agree with the reference model. Non-equilibrium-climate\ninference is more challenging. With CO$_2$ increasing gradually at a rate of 2%\nyear$^{-1}$, ACE2-SOM can accurately emulate the global annual mean trends of\nsurface and lower-to-middle atmosphere fields but produces unphysical jumps in\nstratospheric fields. With an abrupt quadrupling of CO$_2$, ML-controlled\nfields transition unrealistically quickly to the 4xCO$_2$ regime. In doing so\nthey violate global energy conservation and exhibit unphysical sensitivities of\nand surface and top of atmosphere radiative fluxes to instantaneous changes in\nCO$_2$. Future emulator development needed to address these issues should\nimprove its generalizability to diverse climate change scenarios."
                },
                "authors": [
                    {
                        "name": "Spencer K. Clark"
                    },
                    {
                        "name": "Oliver Watt-Meyer"
                    },
                    {
                        "name": "Anna Kwa"
                    },
                    {
                        "name": "Jeremy McGibbon"
                    },
                    {
                        "name": "Brian Henn"
                    },
                    {
                        "name": "W. Andre Perkins"
                    },
                    {
                        "name": "Elynn Wu"
                    },
                    {
                        "name": "Christopher S. Bretherton"
                    },
                    {
                        "name": "Lucas M. Harris"
                    }
                ],
                "author_detail": {
                    "name": "Lucas M. Harris"
                },
                "author": "Lucas M. Harris",
                "arxiv_comment": "25 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01339v2",
                "updated": "2024-12-05T18:43:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    43,
                    25,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T10:06:57Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    6,
                    57,
                    0,
                    337,
                    0
                ],
                "title": "Negative Token Merging: Image-based Adversarial Feature Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negative Token Merging: Image-based Adversarial Feature Guidance"
                },
                "summary": "Text-based adversarial guidance using a negative prompt has emerged as a\nwidely adopted approach to steer diffusion models away from producing undesired\nconcepts. While useful, performing adversarial guidance using text alone can be\ninsufficient to capture complex visual concepts or avoid specific visual\nelements like copyrighted characters. In this paper, for the first time we\nexplore an alternate modality in this direction by performing adversarial\nguidance directly using visual features from a reference image or other images\nin a batch. We introduce negative token merging (NegToMe), a simple but\neffective training-free approach which performs adversarial guidance through\nimages by selectively pushing apart matching visual features between reference\nand generated images during the reverse diffusion process. By simply adjusting\nthe used reference, NegToMe enables a diverse range of applications. Notably,\nwhen using other images in same batch as reference, we find that NegToMe\nsignificantly enhances output diversity (e.g., racial, gender, visual) by\nguiding features of each image away from others. Similarly, when used w.r.t.\ncopyrighted reference images, NegToMe reduces visual similarity to copyrighted\ncontent by 34.57%. NegToMe is simple to implement using just few-lines of code,\nuses only marginally higher (<4%) inference time and is compatible with\ndifferent diffusion architectures, including those like Flux, which don't\nnatively support the use of a negative prompt. Code is available at\nhttps://negtome.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-based adversarial guidance using a negative prompt has emerged as a\nwidely adopted approach to steer diffusion models away from producing undesired\nconcepts. While useful, performing adversarial guidance using text alone can be\ninsufficient to capture complex visual concepts or avoid specific visual\nelements like copyrighted characters. In this paper, for the first time we\nexplore an alternate modality in this direction by performing adversarial\nguidance directly using visual features from a reference image or other images\nin a batch. We introduce negative token merging (NegToMe), a simple but\neffective training-free approach which performs adversarial guidance through\nimages by selectively pushing apart matching visual features between reference\nand generated images during the reverse diffusion process. By simply adjusting\nthe used reference, NegToMe enables a diverse range of applications. Notably,\nwhen using other images in same batch as reference, we find that NegToMe\nsignificantly enhances output diversity (e.g., racial, gender, visual) by\nguiding features of each image away from others. Similarly, when used w.r.t.\ncopyrighted reference images, NegToMe reduces visual similarity to copyrighted\ncontent by 34.57%. NegToMe is simple to implement using just few-lines of code,\nuses only marginally higher (<4%) inference time and is compatible with\ndifferent diffusion architectures, including those like Flux, which don't\nnatively support the use of a negative prompt. Code is available at\nhttps://negtome.github.io"
                },
                "authors": [
                    {
                        "name": "Jaskirat Singh"
                    },
                    {
                        "name": "Lindsey Li"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Michael F. Cohen"
                    },
                    {
                        "name": "Stephen Gould"
                    },
                    {
                        "name": "Liang Zheng"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    }
                ],
                "author_detail": {
                    "name": "Luke Zettlemoyer"
                },
                "author": "Luke Zettlemoyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04415v1",
                "updated": "2024-12-05T18:38:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    38,
                    30,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:38:30Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    38,
                    30,
                    3,
                    340,
                    0
                ],
                "title": "Targeting the Core: A Simple and Effective Method to Attack RAG-based\n  Agents via Direct LLM Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeting the Core: A Simple and Effective Method to Attack RAG-based\n  Agents via Direct LLM Manipulation"
                },
                "summary": "AI agents, powered by large language models (LLMs), have transformed\nhuman-computer interactions by enabling seamless, natural, and context-aware\ncommunication. While these advancements offer immense utility, they also\ninherit and amplify inherent safety risks such as bias, fairness,\nhallucinations, privacy breaches, and a lack of transparency. This paper\ninvestigates a critical vulnerability: adversarial attacks targeting the LLM\ncore within AI agents. Specifically, we test the hypothesis that a deceptively\nsimple adversarial prefix, such as \\textit{Ignore the document}, can compel\nLLMs to produce dangerous or unintended outputs by bypassing their contextual\nsafeguards. Through experimentation, we demonstrate a high attack success rate\n(ASR), revealing the fragility of existing LLM defenses. These findings\nemphasize the urgent need for robust, multi-layered security measures tailored\nto mitigate vulnerabilities at the LLM level and within broader agent-based\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents, powered by large language models (LLMs), have transformed\nhuman-computer interactions by enabling seamless, natural, and context-aware\ncommunication. While these advancements offer immense utility, they also\ninherit and amplify inherent safety risks such as bias, fairness,\nhallucinations, privacy breaches, and a lack of transparency. This paper\ninvestigates a critical vulnerability: adversarial attacks targeting the LLM\ncore within AI agents. Specifically, we test the hypothesis that a deceptively\nsimple adversarial prefix, such as \\textit{Ignore the document}, can compel\nLLMs to produce dangerous or unintended outputs by bypassing their contextual\nsafeguards. Through experimentation, we demonstrate a high attack success rate\n(ASR), revealing the fragility of existing LLM defenses. These findings\nemphasize the urgent need for robust, multi-layered security measures tailored\nto mitigate vulnerabilities at the LLM level and within broader agent-based\narchitectures."
                },
                "authors": [
                    {
                        "name": "Xuying Li"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Yuji Kosuga"
                    },
                    {
                        "name": "Yasuhiro Yoshida"
                    },
                    {
                        "name": "Victor Bian"
                    }
                ],
                "author_detail": {
                    "name": "Victor Bian"
                },
                "author": "Victor Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12924v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12924v3",
                "updated": "2024-12-05T18:35:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    35,
                    26,
                    3,
                    340,
                    0
                ],
                "published": "2024-09-04T03:17:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    3,
                    17,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "WaveletGPT: Wavelets Meet Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveletGPT: Wavelets Meet Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. They\nare trained on a simple objective: to predict the next token given the previous\ncontext. We live in a world where most of the data around us, e.g., text,\naudio, and music, has a multi-scale structure associated with it. This paper\ninfuses LLMs with traditional signal processing ideas, namely wavelets, during\npre-training to take advantage of the structure. Without adding \\textbf{any\nextra parameters} to a GPT-style LLM architecture, we achieve the same\npre-training performance almost twice as fast in text, raw audio, and symbolic\nmusic. This is achieved by imposing a structure on intermediate embeddings.\nWhen trained for the same number of training steps, we achieve significant\ngains in performance, which is comparable to pre-training a larger neural\narchitecture. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every Transformer\ndecoder block. This work will hopefully pave the way for incorporating\nmulti-rate signal processing ideas into traditional LLM pre-training. Further,\nwe showcase pushing model performance by improving internal structure instead\nof just going after scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. They\nare trained on a simple objective: to predict the next token given the previous\ncontext. We live in a world where most of the data around us, e.g., text,\naudio, and music, has a multi-scale structure associated with it. This paper\ninfuses LLMs with traditional signal processing ideas, namely wavelets, during\npre-training to take advantage of the structure. Without adding \\textbf{any\nextra parameters} to a GPT-style LLM architecture, we achieve the same\npre-training performance almost twice as fast in text, raw audio, and symbolic\nmusic. This is achieved by imposing a structure on intermediate embeddings.\nWhen trained for the same number of training steps, we achieve significant\ngains in performance, which is comparable to pre-training a larger neural\narchitecture. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every Transformer\ndecoder block. This work will hopefully pave the way for incorporating\nmulti-rate signal processing ideas into traditional LLM pre-training. Further,\nwe showcase pushing model performance by improving internal structure instead\nof just going after scale."
                },
                "authors": [
                    {
                        "name": "Prateek Verma"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Verma"
                },
                "author": "Prateek Verma",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12924v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12924v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04413v1",
                "updated": "2024-12-05T18:33:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    33,
                    59,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:33:59Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    33,
                    59,
                    3,
                    340,
                    0
                ],
                "title": "Efficient Task Grouping Through Samplewise Optimisation Landscape\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Task Grouping Through Samplewise Optimisation Landscape\n  Analysis"
                },
                "summary": "Shared training approaches, such as multi-task learning (MTL) and\ngradient-based meta-learning, are widely used in various machine learning\napplications, but they often suffer from negative transfer, leading to\nperformance degradation in specific tasks. While several optimisation\ntechniques have been developed to mitigate this issue for pre-selected task\ncohorts, identifying optimal task combinations for joint learning - known as\ntask grouping - remains underexplored and computationally challenging due to\nthe exponential growth in task combinations and the need for extensive training\nand evaluation cycles. This paper introduces an efficient task grouping\nframework designed to reduce these overwhelming computational demands of the\nexisting methods. The proposed framework infers pairwise task similarities\nthrough a sample-wise optimisation landscape analysis, eliminating the need for\nthe shared model training required to infer task similarities in existing\nmethods. With task similarities acquired, a graph-based clustering algorithm is\nemployed to pinpoint near-optimal task groups, providing an approximate yet\nefficient and effective solution to the originally NP-hard problem. Empirical\nassessments conducted on 8 different datasets highlight the effectiveness of\nthe proposed framework, revealing a five-fold speed enhancement compared to\nprevious state-of-the-art methods. Moreover, the framework consistently\ndemonstrates comparable performance, confirming its remarkable efficiency and\neffectiveness in task grouping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared training approaches, such as multi-task learning (MTL) and\ngradient-based meta-learning, are widely used in various machine learning\napplications, but they often suffer from negative transfer, leading to\nperformance degradation in specific tasks. While several optimisation\ntechniques have been developed to mitigate this issue for pre-selected task\ncohorts, identifying optimal task combinations for joint learning - known as\ntask grouping - remains underexplored and computationally challenging due to\nthe exponential growth in task combinations and the need for extensive training\nand evaluation cycles. This paper introduces an efficient task grouping\nframework designed to reduce these overwhelming computational demands of the\nexisting methods. The proposed framework infers pairwise task similarities\nthrough a sample-wise optimisation landscape analysis, eliminating the need for\nthe shared model training required to infer task similarities in existing\nmethods. With task similarities acquired, a graph-based clustering algorithm is\nemployed to pinpoint near-optimal task groups, providing an approximate yet\nefficient and effective solution to the originally NP-hard problem. Empirical\nassessments conducted on 8 different datasets highlight the effectiveness of\nthe proposed framework, revealing a five-fold speed enhancement compared to\nprevious state-of-the-art methods. Moreover, the framework consistently\ndemonstrates comparable performance, confirming its remarkable efficiency and\neffectiveness in task grouping."
                },
                "authors": [
                    {
                        "name": "Anshul Thakur"
                    },
                    {
                        "name": "Yichen Huang"
                    },
                    {
                        "name": "Soheila Molaei"
                    },
                    {
                        "name": "Yujiang Wang"
                    },
                    {
                        "name": "David A. Clifton"
                    }
                ],
                "author_detail": {
                    "name": "David A. Clifton"
                },
                "author": "David A. Clifton",
                "arxiv_comment": "Under review at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04394v1",
                "updated": "2024-12-05T18:09:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    9,
                    41,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:09:41Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    9,
                    41,
                    3,
                    340,
                    0
                ],
                "title": "Bayesian Quantum Amplitude Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Quantum Amplitude Estimation"
                },
                "summary": "Quantum amplitude estimation is a fundamental routine that offers a quadratic\nspeed-up over classical approaches. The original QAE protocol is based on phase\nestimation. The associated circuit depth and width, and the assumptions of\nfault tolerance, are unfavorable for near-term quantum technology. Subsequent\napproaches attempt to replace the original protocol with hybrid iterative\nquantum-classical strategies. In this work, we introduce BAE, a noise-aware\nBayesian algorithm for QAE that combines quantum circuits with a statistical\ninference backbone. BAE can dynamically characterize device noise and adapt to\nit in real-time. Problem-specific insights and approximations are used to keep\nthe problem tractable. We further propose an annealed variant of BAE, drawing\non methods from statistical inference, to enhance statistical robustness. Our\nproposal is parallelizable in both quantum and classical components, offers\ntools for fast noise model assessment, and can leverage preexisting\ninformation. Additionally, it accommodates experimental limitations and\npreferred cost trade-offs. We show that BAE achieves Heisenberg-limited\nestimation and benchmark it against other approaches, demonstrating its\ncompetitive performance in both noisy and noiseless scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum amplitude estimation is a fundamental routine that offers a quadratic\nspeed-up over classical approaches. The original QAE protocol is based on phase\nestimation. The associated circuit depth and width, and the assumptions of\nfault tolerance, are unfavorable for near-term quantum technology. Subsequent\napproaches attempt to replace the original protocol with hybrid iterative\nquantum-classical strategies. In this work, we introduce BAE, a noise-aware\nBayesian algorithm for QAE that combines quantum circuits with a statistical\ninference backbone. BAE can dynamically characterize device noise and adapt to\nit in real-time. Problem-specific insights and approximations are used to keep\nthe problem tractable. We further propose an annealed variant of BAE, drawing\non methods from statistical inference, to enhance statistical robustness. Our\nproposal is parallelizable in both quantum and classical components, offers\ntools for fast noise model assessment, and can leverage preexisting\ninformation. Additionally, it accommodates experimental limitations and\npreferred cost trade-offs. We show that BAE achieves Heisenberg-limited\nestimation and benchmark it against other approaches, demonstrating its\ncompetitive performance in both noisy and noiseless scenarios."
                },
                "authors": [
                    {
                        "name": "Alexandra Rama"
                    },
                    {
                        "name": "Luis Paulo Santos"
                    }
                ],
                "author_detail": {
                    "name": "Luis Paulo Santos"
                },
                "author": "Luis Paulo Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04386v1",
                "updated": "2024-12-05T18:00:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    0,
                    52,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:00:52Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    0,
                    52,
                    3,
                    340,
                    0
                ],
                "title": "The Red Supergiant Problem: As Seen from the Local Group's Red\n  Supergiant Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Red Supergiant Problem: As Seen from the Local Group's Red\n  Supergiant Populations"
                },
                "summary": "The red supergiant (RSG) problem, which describes the apparent lack of\nhigh-luminosity progenitors detected in Type II supernova (SN) pre-images, has\nbeen a contentious topic for two decades. We re-assess this problem using a new\nRSG population of the Milky Way supplemented with RSGs from other galaxies in\nthe Local Group. In particular, we quantify the uncertainties inherent to\nassumptions made regarding the star's temperature or spectral type and the\ncorresponding bolometric correction. We find that only M3 or later RSGs\nreproduce the steepness seen from the SN II pre-imaged sample. To assess the\nsignificance of the RSG problem, we build a metallicity-weighted cumulative\nluminosity distribution of M3 or later RSGs and directly compare it to the\nluminosity distribution of SN II pre-imaged progenitors. We find no evidence of\nmissing high-luminosity pre-imaged progenitors since the uncertainties on the\npre-imaged SN progenitors and single-band derived luminosity are too large to\nmeaningfully infer population differences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The red supergiant (RSG) problem, which describes the apparent lack of\nhigh-luminosity progenitors detected in Type II supernova (SN) pre-images, has\nbeen a contentious topic for two decades. We re-assess this problem using a new\nRSG population of the Milky Way supplemented with RSGs from other galaxies in\nthe Local Group. In particular, we quantify the uncertainties inherent to\nassumptions made regarding the star's temperature or spectral type and the\ncorresponding bolometric correction. We find that only M3 or later RSGs\nreproduce the steepness seen from the SN II pre-imaged sample. To assess the\nsignificance of the RSG problem, we build a metallicity-weighted cumulative\nluminosity distribution of M3 or later RSGs and directly compare it to the\nluminosity distribution of SN II pre-imaged progenitors. We find no evidence of\nmissing high-luminosity pre-imaged progenitors since the uncertainties on the\npre-imaged SN progenitors and single-band derived luminosity are too large to\nmeaningfully infer population differences."
                },
                "authors": [
                    {
                        "name": "Sarah Healy"
                    },
                    {
                        "name": "Shunsaku Horiuchi"
                    },
                    {
                        "name": "Chris Ashall"
                    }
                ],
                "author_detail": {
                    "name": "Chris Ashall"
                },
                "author": "Chris Ashall",
                "arxiv_comment": "11 pages, 7 Figures, 3 Tables. Comments Welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04378v1",
                "updated": "2024-12-05T17:54:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    54,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T17:54:27Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    54,
                    27,
                    3,
                    340,
                    0
                ],
                "title": "Discriminative Fine-tuning of LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discriminative Fine-tuning of LVLMs"
                },
                "summary": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the\nde facto approach for discriminative vision-language representation learning.\nHowever, these models have limited language understanding, often exhibiting a\n\"bag of words\" behavior. At the same time, Large Vision-Language Models\n(LVLMs), which combine vision encoders with LLMs, have been shown capable of\ndetailed vision-language reasoning, yet their autoregressive nature renders\nthem less suitable for discriminative tasks.\n  In this work, we propose to combine \"the best of both worlds\": a new training\napproach for discriminative fine-tuning of LVLMs that results in strong\ndiscriminative and compositional capabilities. Essentially, our approach\nconverts a generative LVLM into a discriminative one, unlocking its capability\nfor powerful image-text discrimination combined with enhanced language\nunderstanding.\n  Our contributions include: (1) A carefully designed training/optimization\nframework that utilizes image-text pairs of variable length and granularity for\ntraining the model with both contrastive and next-token prediction losses. This\nis accompanied by ablation studies that justify the necessity of our\nframework's components. (2) A parameter-efficient adaptation method using a\ncombination of soft prompting and LoRA adapters. (3) Significant improvements\nover state-of-the-art CLIP-like models of similar size, including standard\nimage-text retrieval benchmarks and notable gains in compositionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the\nde facto approach for discriminative vision-language representation learning.\nHowever, these models have limited language understanding, often exhibiting a\n\"bag of words\" behavior. At the same time, Large Vision-Language Models\n(LVLMs), which combine vision encoders with LLMs, have been shown capable of\ndetailed vision-language reasoning, yet their autoregressive nature renders\nthem less suitable for discriminative tasks.\n  In this work, we propose to combine \"the best of both worlds\": a new training\napproach for discriminative fine-tuning of LVLMs that results in strong\ndiscriminative and compositional capabilities. Essentially, our approach\nconverts a generative LVLM into a discriminative one, unlocking its capability\nfor powerful image-text discrimination combined with enhanced language\nunderstanding.\n  Our contributions include: (1) A carefully designed training/optimization\nframework that utilizes image-text pairs of variable length and granularity for\ntraining the model with both contrastive and next-token prediction losses. This\nis accompanied by ablation studies that justify the necessity of our\nframework's components. (2) A parameter-efficient adaptation method using a\ncombination of soft prompting and LoRA adapters. (3) Significant improvements\nover state-of-the-art CLIP-like models of similar size, including standard\nimage-text retrieval benchmarks and notable gains in compositionality."
                },
                "authors": [
                    {
                        "name": "Yassine Ouali"
                    },
                    {
                        "name": "Adrian Bulat"
                    },
                    {
                        "name": "Alexandros Xenos"
                    },
                    {
                        "name": "Anestis Zaganidis"
                    },
                    {
                        "name": "Ioannis Maniadis Metaxas"
                    },
                    {
                        "name": "Georgios Tzimiropoulos"
                    },
                    {
                        "name": "Brais Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Brais Martinez"
                },
                "author": "Brais Martinez",
                "arxiv_comment": "Preprint. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02819v2",
                "updated": "2024-12-05T17:51:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    51,
                    20,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-03T20:35:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    35,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "CNNSum: Exploring Long-Conext Summarization with Large Language Models\n  in Chinese Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNNSum: Exploring Long-Conext Summarization with Large Language Models\n  in Chinese Novels"
                },
                "summary": "Large Language Models (LLMs) have been well-researched in many long-context\ntasks. However, due to high annotation costs, high-quality long-context summary\ndatasets for training or evaluation are scarce, limiting further research. In\nthis work, we introduce CNNSum, a new multi-scale Chinese long-context novel\nsummarization benchmark, including four subsets, length covering\n16k\\textasciitilde128k, 695 samples in total, the annotations are human-driven.\nWe evaluate commercial and open-source models on CNNSum and conduct a detailed\nanalysis. Based on the observations, we further conduct fine-tuning exploration\nwith short-context summary data. In our study: (1) GPT-4o underperformed, due\nto excessive subjective commentary. (2) Currently, long-context summarization\nmainly relies on memory ability, small LLMs with stable longer context lengths\nare the most cost-effective. Using long data concatenated from short-context\nsummaries makes a significant improvement. (3) Prompt templates may cause a\nlarge performance gap but can be mitigated through fine-tuning. (4) Fine-tuned\nChat or Instruction versions may harm the Base model and further fine-tuning\ncannot bridge performance gap. (5) while models with RoPE base scaling exhibit\nstrong extrapolation potential, their performance may vary significantly when\ncombined with other interpolation methods and need careful selection. (6)\nCNNSum provides more reliable and insightful evaluation results than other\nbenchmarks. We release CNNSum to advance research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been well-researched in many long-context\ntasks. However, due to high annotation costs, high-quality long-context summary\ndatasets for training or evaluation are scarce, limiting further research. In\nthis work, we introduce CNNSum, a new multi-scale Chinese long-context novel\nsummarization benchmark, including four subsets, length covering\n16k\\textasciitilde128k, 695 samples in total, the annotations are human-driven.\nWe evaluate commercial and open-source models on CNNSum and conduct a detailed\nanalysis. Based on the observations, we further conduct fine-tuning exploration\nwith short-context summary data. In our study: (1) GPT-4o underperformed, due\nto excessive subjective commentary. (2) Currently, long-context summarization\nmainly relies on memory ability, small LLMs with stable longer context lengths\nare the most cost-effective. Using long data concatenated from short-context\nsummaries makes a significant improvement. (3) Prompt templates may cause a\nlarge performance gap but can be mitigated through fine-tuning. (4) Fine-tuned\nChat or Instruction versions may harm the Base model and further fine-tuning\ncannot bridge performance gap. (5) while models with RoPE base scaling exhibit\nstrong extrapolation potential, their performance may vary significantly when\ncombined with other interpolation methods and need careful selection. (6)\nCNNSum provides more reliable and insightful evaluation results than other\nbenchmarks. We release CNNSum to advance research in this field."
                },
                "authors": [
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04372v1",
                "updated": "2024-12-05T17:49:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    49,
                    10,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T17:49:10Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    49,
                    10,
                    3,
                    340,
                    0
                ],
                "title": "Distributed Inference with Minimal Off-Chip Traffic for Transformers on\n  Low-Power MCUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Inference with Minimal Off-Chip Traffic for Transformers on\n  Low-Power MCUs"
                },
                "summary": "Contextual Artificial Intelligence (AI) based on emerging Transformer models\nis predicted to drive the next technology revolution in interactive wearable\ndevices such as new-generation smart glasses. By coupling numerous sensors with\nsmall, low-power Micro-Controller Units (MCUs), these devices will enable\non-device intelligence and sensor control. A major bottleneck in this class of\nsystems is the small amount of on-chip memory available in the MCUs. In this\npaper, we propose a methodology to deploy real-world Transformers on low-power\nwearable devices with minimal off-chip traffic exploiting a distributed system\nof MCUs, partitioning inference across multiple devices and enabling execution\nwith stationary on-chip weights. We validate the scheme by deploying the\nTinyLlama-42M decoder-only model on a system of 8 parallel ultra-low-power\nMCUs. The distributed system achieves an energy consumption of 0.64 mJ, a\nlatency of 0.54 ms per inference, a super-linear speedup of 26.1 x, and an\nEnergy Delay Product (EDP) improvement of 27.2 x, compared to a single-chip\nsystem. On MobileBERT, the distributed system's runtime is 38.8 ms, with a\nsuper-linear 4.7 x speedup when using 4 MCUs compared to a single-chip system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Artificial Intelligence (AI) based on emerging Transformer models\nis predicted to drive the next technology revolution in interactive wearable\ndevices such as new-generation smart glasses. By coupling numerous sensors with\nsmall, low-power Micro-Controller Units (MCUs), these devices will enable\non-device intelligence and sensor control. A major bottleneck in this class of\nsystems is the small amount of on-chip memory available in the MCUs. In this\npaper, we propose a methodology to deploy real-world Transformers on low-power\nwearable devices with minimal off-chip traffic exploiting a distributed system\nof MCUs, partitioning inference across multiple devices and enabling execution\nwith stationary on-chip weights. We validate the scheme by deploying the\nTinyLlama-42M decoder-only model on a system of 8 parallel ultra-low-power\nMCUs. The distributed system achieves an energy consumption of 0.64 mJ, a\nlatency of 0.54 ms per inference, a super-linear speedup of 26.1 x, and an\nEnergy Delay Product (EDP) improvement of 27.2 x, compared to a single-chip\nsystem. On MobileBERT, the distributed system's runtime is 38.8 ms, with a\nsuper-linear 4.7 x speedup when using 4 MCUs compared to a single-chip system."
                },
                "authors": [
                    {
                        "name": "Severin Bochem"
                    },
                    {
                        "name": "Victor J. B. Jung"
                    },
                    {
                        "name": "Arpan Prasad"
                    },
                    {
                        "name": "Francesco Conti"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "This work has been accepted to DATE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12259v2",
                "updated": "2024-12-05T17:47:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    47,
                    30,
                    3,
                    340,
                    0
                ],
                "published": "2024-06-18T04:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    4,
                    24,
                    30,
                    1,
                    170,
                    0
                ],
                "title": "Adversarial Attacks on Large Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Attacks on Large Language Models in Medicine"
                },
                "summary": "The integration of Large Language Models (LLMs) into healthcare applications\noffers promising advancements in medical diagnostics, treatment\nrecommendations, and patient care. However, the susceptibility of LLMs to\nadversarial attacks poses a significant threat, potentially leading to harmful\noutcomes in delicate medical contexts. This study investigates the\nvulnerability of LLMs to two types of adversarial attacks in three medical\ntasks. Utilizing real-world patient data, we demonstrate that both open-source\nand proprietary LLMs are susceptible to manipulation across multiple tasks.\nThis research further reveals that domain-specific tasks demand more\nadversarial data in model fine-tuning than general domain tasks for effective\nattack execution, especially for more capable models. We discover that while\nintegrating adversarial data does not markedly degrade overall model\nperformance on medical benchmarks, it does lead to noticeable shifts in\nfine-tuned model weights, suggesting a potential pathway for detecting and\ncountering model attacks. This research highlights the urgent need for robust\nsecurity measures and the development of defensive mechanisms to safeguard LLMs\nin medical applications, to ensure their safe and effective deployment in\nhealthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into healthcare applications\noffers promising advancements in medical diagnostics, treatment\nrecommendations, and patient care. However, the susceptibility of LLMs to\nadversarial attacks poses a significant threat, potentially leading to harmful\noutcomes in delicate medical contexts. This study investigates the\nvulnerability of LLMs to two types of adversarial attacks in three medical\ntasks. Utilizing real-world patient data, we demonstrate that both open-source\nand proprietary LLMs are susceptible to manipulation across multiple tasks.\nThis research further reveals that domain-specific tasks demand more\nadversarial data in model fine-tuning than general domain tasks for effective\nattack execution, especially for more capable models. We discover that while\nintegrating adversarial data does not markedly degrade overall model\nperformance on medical benchmarks, it does lead to noticeable shifts in\nfine-tuned model weights, suggesting a potential pathway for detecting and\ncountering model attacks. This research highlights the urgent need for robust\nsecurity measures and the development of defensive mechanisms to safeguard LLMs\nin medical applications, to ensure their safe and effective deployment in\nhealthcare settings."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02589v2",
                "updated": "2024-12-05T17:41:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    41,
                    48,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-04T20:29:35Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    20,
                    29,
                    35,
                    0,
                    309,
                    0
                ],
                "title": "Context-Informed Machine Translation of Manga using Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Informed Machine Translation of Manga using Multimodal Large\n  Language Models"
                },
                "summary": "Due to the significant time and effort required for handcrafting\ntranslations, most manga never leave the domestic Japanese market. Automatic\nmanga translation is a promising potential solution. However, it is a budding\nand underdeveloped field and presents complexities even greater than those\nfound in standard translation due to the need to effectively incorporate visual\nelements into the translation process to resolve ambiguities. In this work, we\ninvestigate to what extent multimodal large language models (LLMs) can provide\neffective manga translation, thereby assisting manga authors and publishers in\nreaching wider audiences. Specifically, we propose a methodology that leverages\nthe vision component of multimodal LLMs to improve translation quality and\nevaluate the impact of translation unit size, context length, and propose a\ntoken efficient approach for manga translation. Moreover, we introduce a new\nevaluation dataset -- the first parallel Japanese-Polish manga translation\ndataset -- as part of a benchmark to be used in future research. Finally, we\ncontribute an open-source software suite, enabling others to benchmark LLMs for\nmanga translation. Our findings demonstrate that our proposed methods achieve\nstate-of-the-art results for Japanese-English translation and set a new\nstandard for Japanese-Polish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the significant time and effort required for handcrafting\ntranslations, most manga never leave the domestic Japanese market. Automatic\nmanga translation is a promising potential solution. However, it is a budding\nand underdeveloped field and presents complexities even greater than those\nfound in standard translation due to the need to effectively incorporate visual\nelements into the translation process to resolve ambiguities. In this work, we\ninvestigate to what extent multimodal large language models (LLMs) can provide\neffective manga translation, thereby assisting manga authors and publishers in\nreaching wider audiences. Specifically, we propose a methodology that leverages\nthe vision component of multimodal LLMs to improve translation quality and\nevaluate the impact of translation unit size, context length, and propose a\ntoken efficient approach for manga translation. Moreover, we introduce a new\nevaluation dataset -- the first parallel Japanese-Polish manga translation\ndataset -- as part of a benchmark to be used in future research. Finally, we\ncontribute an open-source software suite, enabling others to benchmark LLMs for\nmanga translation. Our findings demonstrate that our proposed methods achieve\nstate-of-the-art results for Japanese-English translation and set a new\nstandard for Japanese-Polish."
                },
                "authors": [
                    {
                        "name": "Philip Lippmann"
                    },
                    {
                        "name": "Konrad Skublicki"
                    },
                    {
                        "name": "Joshua Tanner"
                    },
                    {
                        "name": "Shonosuke Ishiwatari"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04363v1",
                "updated": "2024-12-05T17:22:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    22,
                    4,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T17:22:04Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    22,
                    4,
                    3,
                    340,
                    0
                ],
                "title": "Challenges in Trustworthy Human Evaluation of Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges in Trustworthy Human Evaluation of Chatbots"
                },
                "summary": "Open community-driven platforms like Chatbot Arena that collect user\npreference data from site visitors have gained a reputation as one of the most\ntrustworthy publicly available benchmarks for LLM performance. While now\nstandard, it is tricky to implement effective guardrails to collect\nhigh-quality annotations from humans. In this paper, we demonstrate that three\nsources of bad annotations, both malicious and otherwise, can corrupt the\nreliability of open leaderboard rankings. In particular, we show that only 10\\%\nof poor quality votes by apathetic (site visitors not appropriately\nincentivized to give correct votes) or adversarial (bad actors seeking to\ninflate the ranking of a target model) annotators can change the rankings of\nmodels by up to 5 places on the leaderboard. Finally, we discuss open\nchallenges in ensuring high-quality human annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open community-driven platforms like Chatbot Arena that collect user\npreference data from site visitors have gained a reputation as one of the most\ntrustworthy publicly available benchmarks for LLM performance. While now\nstandard, it is tricky to implement effective guardrails to collect\nhigh-quality annotations from humans. In this paper, we demonstrate that three\nsources of bad annotations, both malicious and otherwise, can corrupt the\nreliability of open leaderboard rankings. In particular, we show that only 10\\%\nof poor quality votes by apathetic (site visitors not appropriately\nincentivized to give correct votes) or adversarial (bad actors seeking to\ninflate the ranking of a target model) annotators can change the rankings of\nmodels by up to 5 places on the leaderboard. Finally, we discuss open\nchallenges in ensuring high-quality human annotations."
                },
                "authors": [
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tanya Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Tanya Goyal"
                },
                "author": "Tanya Goyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04343v1",
                "updated": "2024-12-05T17:01:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    1,
                    9,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T17:01:09Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    1,
                    9,
                    3,
                    340,
                    0
                ],
                "title": "RMD: A Simple Baseline for More General Human Motion Generation via\n  Training-free Retrieval-Augmented Motion Diffuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMD: A Simple Baseline for More General Human Motion Generation via\n  Training-free Retrieval-Augmented Motion Diffuse"
                },
                "summary": "While motion generation has made substantial progress, its practical\napplication remains constrained by dataset diversity and scale, limiting its\nability to handle out-of-distribution scenarios. To address this, we propose a\nsimple and effective baseline, RMD, which enhances the generalization of motion\ngeneration through retrieval-augmented techniques. Unlike previous\nretrieval-based methods, RMD requires no additional training and offers three\nkey advantages: (1) the external retrieval database can be flexibly replaced;\n(2) body parts from the motion database can be reused, with an LLM facilitating\nsplitting and recombination; and (3) a pre-trained motion diffusion model\nserves as a prior to improve the quality of motions obtained through retrieval\nand direct combination. Without any training, RMD achieves state-of-the-art\nperformance, with notable advantages on out-of-distribution data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While motion generation has made substantial progress, its practical\napplication remains constrained by dataset diversity and scale, limiting its\nability to handle out-of-distribution scenarios. To address this, we propose a\nsimple and effective baseline, RMD, which enhances the generalization of motion\ngeneration through retrieval-augmented techniques. Unlike previous\nretrieval-based methods, RMD requires no additional training and offers three\nkey advantages: (1) the external retrieval database can be flexibly replaced;\n(2) body parts from the motion database can be reused, with an LLM facilitating\nsplitting and recombination; and (3) a pre-trained motion diffusion model\nserves as a prior to improve the quality of motions obtained through retrieval\nand direct combination. Without any training, RMD achieves state-of-the-art\nperformance, with notable advantages on out-of-distribution data."
                },
                "authors": [
                    {
                        "name": "Zhouyingcheng Liao"
                    },
                    {
                        "name": "Mingyuan Zhang"
                    },
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Taku Komura"
                    }
                ],
                "author_detail": {
                    "name": "Taku Komura"
                },
                "author": "Taku Komura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04342v1",
                "updated": "2024-12-05T17:00:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    0,
                    32,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T17:00:32Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    0,
                    32,
                    3,
                    340,
                    0
                ],
                "title": "Retrieval-Augmented Machine Translation with Unstructured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Machine Translation with Unstructured Knowledge"
                },
                "summary": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance models' MT ability.\nHowever, a large amount of world knowledge is organized in unstructured\ndocuments, and might not be fully paired across different languages. In this\npaper, we study retrieval-augmented MT using unstructured documents.\nSpecifically, we build RAGtrans, the first benchmark to train and evaluate\nLLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples\ncollected via GPT-4o and human translators. Besides, documents from different\nlanguages are also provided to supply the knowledge to these samples. Based on\nRAGtrans, we further propose a multi-task training method to teach LLMs how to\nuse information from multilingual documents during their translation. The\nmethod uses existing multilingual corpora to create auxiliary training\nobjectives without additional labeling requirements. Extensive experiments show\nthat the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance models' MT ability.\nHowever, a large amount of world knowledge is organized in unstructured\ndocuments, and might not be fully paired across different languages. In this\npaper, we study retrieval-augmented MT using unstructured documents.\nSpecifically, we build RAGtrans, the first benchmark to train and evaluate\nLLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples\ncollected via GPT-4o and human translators. Besides, documents from different\nlanguages are also provided to supply the knowledge to these samples. Based on\nRAGtrans, we further propose a multi-task training method to teach LLMs how to\nuse information from multilingual documents during their translation. The\nmethod uses existing multilingual corpora to create auxiliary training\nobjectives without additional labeling requirements. Extensive experiments show\nthat the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores."
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04332v1",
                "updated": "2024-12-05T16:48:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    48,
                    16,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:48:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    48,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "Liquid: Language Models are Scalable Multi-modal Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid: Language Models are Scalable Multi-modal Generators"
                },
                "summary": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Junfeng Wu"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Chuofan Ma"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Zehuan Yuan"
                    },
                    {
                        "name": "Song Bai"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Technical report. Will be updated soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12068v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12068v3",
                "updated": "2024-12-05T16:34:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    34,
                    21,
                    3,
                    340,
                    0
                ],
                "published": "2023-11-19T17:28:28Z",
                "published_parsed": [
                    2023,
                    11,
                    19,
                    17,
                    28,
                    28,
                    6,
                    323,
                    0
                ],
                "title": "Enhancing Novel Object Detection via Cooperative Foundational Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Novel Object Detection via Cooperative Foundational Models"
                },
                "summary": "In this work, we address the challenging and emergent problem of novel object\ndetection (NOD), focusing on the accurate detection of both known and novel\nobject categories during inference. Traditional object detection algorithms are\ninherently closed-set, limiting their capability to handle NOD. We present a\nnovel approach to transform existing closed-set detectors into open-set\ndetectors. This transformation is achieved by leveraging the complementary\nstrengths of pre-trained foundational models, specifically CLIP and SAM,\nthrough our cooperative mechanism. Furthermore, by integrating this mechanism\nwith state-of-the-art open-set detectors such as GDINO, we establish new\nbenchmarks in object detection performance. Our method achieves 17.42 mAP in\nnovel object detection and 42.08 mAP for known objects on the challenging LVIS\ndataset. Adapting our approach to the COCO OVD split, we surpass the current\nstate-of-the-art by a margin of 7.2 $ \\text{AP}_{50} $ for novel classes. Our\ncode is available at https://rohit901.github.io/coop-foundation-models/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the challenging and emergent problem of novel object\ndetection (NOD), focusing on the accurate detection of both known and novel\nobject categories during inference. Traditional object detection algorithms are\ninherently closed-set, limiting their capability to handle NOD. We present a\nnovel approach to transform existing closed-set detectors into open-set\ndetectors. This transformation is achieved by leveraging the complementary\nstrengths of pre-trained foundational models, specifically CLIP and SAM,\nthrough our cooperative mechanism. Furthermore, by integrating this mechanism\nwith state-of-the-art open-set detectors such as GDINO, we establish new\nbenchmarks in object detection performance. Our method achieves 17.42 mAP in\nnovel object detection and 42.08 mAP for known objects on the challenging LVIS\ndataset. Adapting our approach to the COCO OVD split, we surpass the current\nstate-of-the-art by a margin of 7.2 $ \\text{AP}_{50} $ for novel classes. Our\ncode is available at https://rohit901.github.io/coop-foundation-models/ ."
                },
                "authors": [
                    {
                        "name": "Rohit Bharadwaj"
                    },
                    {
                        "name": "Muzammal Naseer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.12068v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12068v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04318v1",
                "updated": "2024-12-05T16:34:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    34,
                    20,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:34:20Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    34,
                    20,
                    3,
                    340,
                    0
                ],
                "title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation"
                },
                "summary": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token."
                },
                "authors": [
                    {
                        "name": "Fredrik Carlsson"
                    },
                    {
                        "name": "Fangyu Liu"
                    },
                    {
                        "name": "Daniel Ward"
                    },
                    {
                        "name": "Murathan Kurfali"
                    },
                    {
                        "name": "Joakim Nivre"
                    }
                ],
                "author_detail": {
                    "name": "Joakim Nivre"
                },
                "author": "Joakim Nivre",
                "arxiv_comment": "Under review at ICLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1903.04209v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1903.04209v6",
                "updated": "2024-12-05T16:32:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    32,
                    37,
                    3,
                    340,
                    0
                ],
                "published": "2019-03-11T10:37:05Z",
                "published_parsed": [
                    2019,
                    3,
                    11,
                    10,
                    37,
                    5,
                    0,
                    70,
                    0
                ],
                "title": "From interpretability to inference: an estimation framework for\n  universal approximators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From interpretability to inference: an estimation framework for\n  universal approximators"
                },
                "summary": "We present a novel framework for estimation and inference with the broad\nclass of universal approximators. Estimation is based on the decomposition of\nmodel predictions into Shapley values. Inference relies on analyzing the bias\nand variance properties of individual Shapley components. We show that Shapley\nvalue estimation is asymptotically unbiased, and we introduce Shapley\nregressions as a tool to uncover the true data generating process from noisy\ndata alone. The well-known case of the linear regression is the special case in\nour framework if the model is linear in parameters. We present theoretical,\nnumerical, and empirical results for the estimation of heterogeneous treatment\neffects as our guiding example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel framework for estimation and inference with the broad\nclass of universal approximators. Estimation is based on the decomposition of\nmodel predictions into Shapley values. Inference relies on analyzing the bias\nand variance properties of individual Shapley components. We show that Shapley\nvalue estimation is asymptotically unbiased, and we introduce Shapley\nregressions as a tool to uncover the true data generating process from noisy\ndata alone. The well-known case of the linear regression is the special case in\nour framework if the model is linear in parameters. We present theoretical,\nnumerical, and empirical results for the estimation of heterogeneous treatment\neffects as our guiding example."
                },
                "authors": [
                    {
                        "name": "Andreas Joseph"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Joseph"
                },
                "author": "Andreas Joseph",
                "arxiv_comment": "37 pages, 5 figures, 3 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1903.04209v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1903.04209v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G10, 62G20, 62-07, 91-08, 91A12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1; G.2; G.3; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04315v1",
                "updated": "2024-12-05T16:31:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    31,
                    13,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:31:13Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    31,
                    13,
                    3,
                    340,
                    0
                ],
                "title": "Densing Law of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Densing Law of LLMs"
                },
                "summary": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead."
                },
                "authors": [
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Guoyang Zeng"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18825v2",
                "updated": "2024-12-05T16:27:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    27,
                    8,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-27T23:58:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    23,
                    58,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language\n  Models for Reward Design in Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language\n  Models for Reward Design in Robotics"
                },
                "summary": "Reinforcement learning (RL) has demonstrated compelling performance in\nrobotic tasks, but its success often hinges on the design of complex, ad hoc\nreward functions. Researchers have explored how Large Language Models (LLMs)\ncould enable non-expert users to specify reward functions more easily. However,\nLLMs struggle to balance the importance of different features, generalize\npoorly to out-of-distribution robotic tasks, and cannot represent the problem\nproperly with only text-based descriptions. To address these challenges, we\npropose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a\nnovel framework that combines natural language guidance with visual user\ndemonstrations to align robot behavior with user intentions better. By\nincorporating visual inputs, ELEMENTAL overcomes the limitations of text-only\ntask specifications, while leveraging inverse reinforcement learning (IRL) to\nbalance feature weights and match the demonstrated behaviors optimally.\nELEMENTAL also introduces an iterative feedback-loop through self-reflection to\nimprove feature, reward, and policy learning. Our experiment results\ndemonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, and\nachieves 41.3% better generalization in out-of-distribution tasks, highlighting\nits robustness in LfD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has demonstrated compelling performance in\nrobotic tasks, but its success often hinges on the design of complex, ad hoc\nreward functions. Researchers have explored how Large Language Models (LLMs)\ncould enable non-expert users to specify reward functions more easily. However,\nLLMs struggle to balance the importance of different features, generalize\npoorly to out-of-distribution robotic tasks, and cannot represent the problem\nproperly with only text-based descriptions. To address these challenges, we\npropose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a\nnovel framework that combines natural language guidance with visual user\ndemonstrations to align robot behavior with user intentions better. By\nincorporating visual inputs, ELEMENTAL overcomes the limitations of text-only\ntask specifications, while leveraging inverse reinforcement learning (IRL) to\nbalance feature weights and match the demonstrated behaviors optimally.\nELEMENTAL also introduces an iterative feedback-loop through self-reflection to\nimprove feature, reward, and policy learning. Our experiment results\ndemonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, and\nachieves 41.3% better generalization in out-of-distribution tasks, highlighting\nits robustness in LfD."
                },
                "authors": [
                    {
                        "name": "Letian Chen"
                    },
                    {
                        "name": "Matthew Gombolay"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gombolay"
                },
                "author": "Matthew Gombolay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04307v1",
                "updated": "2024-12-05T16:26:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    37,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:26:37Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    37,
                    3,
                    340,
                    0
                ],
                "title": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark"
                },
                "summary": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset will be made available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset will be made available on GitHub."
                },
                "authors": [
                    {
                        "name": "Changsheng Gao"
                    },
                    {
                        "name": "Yifan Ma"
                    },
                    {
                        "name": "Qiaoxi Chen"
                    },
                    {
                        "name": "Yenan Xu"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04305v1",
                "updated": "2024-12-05T16:26:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    31,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:26:31Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    31,
                    3,
                    340,
                    0
                ],
                "title": "ALMA: Alignment with Minimal Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALMA: Alignment with Minimal Annotation"
                },
                "summary": "Recent approaches to large language model (LLM) alignment typically require\nmillions of human annotations or rely on external aligned models for synthetic\ndata generation. This paper introduces ALMA: Alignment with Minimal Annotation,\ndemonstrating that effective alignment can be achieved using only 9,000 labeled\nexamples -- less than 1% of conventional approaches. ALMA generates large\namounts of high-quality synthetic alignment data through new techniques:\ndiverse prompt synthesis via few-shot learning, diverse response generation\nwith multiple model checkpoints, and judge (reward model) enhancement through\nscore aggregation and self-distillation. Using only a pretrained Llama3 base\nmodel, 5,000 SFT examples, and 4,000 judge annotations, ALMA achieves\nperformance close to Llama3-Instruct across diverse alignment benchmarks (e.g.,\n0.1% difference on AlpacaEval 2.0 score). These results are achieved with a\nmulti-round, self-bootstrapped data synthesis and training recipe that\ncontinues to improve for 10 rounds, surpassing the typical 3-round ceiling of\nprevious methods. These results suggest that base models already possess\nsufficient knowledge for effective alignment, and that synthetic data\ngeneration methods can expose it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent approaches to large language model (LLM) alignment typically require\nmillions of human annotations or rely on external aligned models for synthetic\ndata generation. This paper introduces ALMA: Alignment with Minimal Annotation,\ndemonstrating that effective alignment can be achieved using only 9,000 labeled\nexamples -- less than 1% of conventional approaches. ALMA generates large\namounts of high-quality synthetic alignment data through new techniques:\ndiverse prompt synthesis via few-shot learning, diverse response generation\nwith multiple model checkpoints, and judge (reward model) enhancement through\nscore aggregation and self-distillation. Using only a pretrained Llama3 base\nmodel, 5,000 SFT examples, and 4,000 judge annotations, ALMA achieves\nperformance close to Llama3-Instruct across diverse alignment benchmarks (e.g.,\n0.1% difference on AlpacaEval 2.0 score). These results are achieved with a\nmulti-round, self-bootstrapped data synthesis and training recipe that\ncontinues to improve for 10 rounds, surpassing the typical 3-round ceiling of\nprevious methods. These results suggest that base models already possess\nsufficient knowledge for effective alignment, and that synthetic data\ngeneration methods can expose it."
                },
                "authors": [
                    {
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "name": "Leonid Shamis"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Andrew Cohen"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Marjan Ghazvininejad"
                    }
                ],
                "author_detail": {
                    "name": "Marjan Ghazvininejad"
                },
                "author": "Marjan Ghazvininejad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15796v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15796v5",
                "updated": "2024-12-05T16:13:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    13,
                    9,
                    3,
                    340,
                    0
                ],
                "published": "2024-06-22T09:40:07Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    9,
                    40,
                    7,
                    5,
                    174,
                    0
                ],
                "title": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis"
                },
                "summary": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs."
                },
                "authors": [
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15796v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15796v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04291v1",
                "updated": "2024-12-05T16:12:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    12,
                    6,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:12:06Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    12,
                    6,
                    3,
                    340,
                    0
                ],
                "title": "Evolutionary Pre-Prompt Optimization for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Pre-Prompt Optimization for Mathematical Reasoning"
                },
                "summary": "Recent advancements have highlighted that large language models (LLMs), when\ngiven a small set of task-specific examples, demonstrate remarkable\nproficiency, a capability that extends to complex reasoning tasks. In\nparticular, the combination of few-shot learning with the chain-of-thought\n(CoT) approach has been pivotal in steering models towards more logically\nconsistent conclusions. This paper explores the optimization of example\nselection for designing effective CoT pre-prompts and shows that the choice of\nthe optimization algorithm, typically in favor of comparison-based methods such\nas evolutionary computation, significantly enhances efficacy and feasibility.\nSpecifically, thanks to a limited exploitative and overfitted optimization,\nEvolutionary Pre-Prompt Optimization (EPPO) brings an improvement over the\nnaive few-shot approach exceeding 10 absolute points in exact match scores on\nbenchmark datasets such as GSM8k and MathQA. These gains are consistent across\nvarious contexts and are further amplified when integrated with\nself-consistency (SC)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have highlighted that large language models (LLMs), when\ngiven a small set of task-specific examples, demonstrate remarkable\nproficiency, a capability that extends to complex reasoning tasks. In\nparticular, the combination of few-shot learning with the chain-of-thought\n(CoT) approach has been pivotal in steering models towards more logically\nconsistent conclusions. This paper explores the optimization of example\nselection for designing effective CoT pre-prompts and shows that the choice of\nthe optimization algorithm, typically in favor of comparison-based methods such\nas evolutionary computation, significantly enhances efficacy and feasibility.\nSpecifically, thanks to a limited exploitative and overfitted optimization,\nEvolutionary Pre-Prompt Optimization (EPPO) brings an improvement over the\nnaive few-shot approach exceeding 10 absolute points in exact match scores on\nbenchmark datasets such as GSM8k and MathQA. These gains are consistent across\nvarious contexts and are further amplified when integrated with\nself-consistency (SC)"
                },
                "authors": [
                    {
                        "name": "Mathurin Videau"
                    },
                    {
                        "name": "Alessandro Leite"
                    },
                    {
                        "name": "Marc Schoenauer"
                    },
                    {
                        "name": "Olivier Teytaud"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Teytaud"
                },
                "author": "Olivier Teytaud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04285v1",
                "updated": "2024-12-05T16:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Deep Causal Inference for Point-referenced Spatial Data with Continuous\n  Treatments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Causal Inference for Point-referenced Spatial Data with Continuous\n  Treatments"
                },
                "summary": "Causal reasoning is often challenging with spatial data, particularly when\nhandling high-dimensional inputs. To address this, we propose a neural network\n(NN) based framework integrated with an approximate Gaussian process to manage\nspatial interference and unobserved confounding. Additionally, we adopt a\ngeneralized propensity-score-based approach to address partially observed\noutcomes when estimating causal effects with continuous treatments. We evaluate\nour framework using synthetic, semi-synthetic, and real-world data inferred\nfrom satellite imagery. Our results demonstrate that NN-based models\nsignificantly outperform linear spatial regression models in estimating causal\neffects. Furthermore, in real-world case studies, NN-based models offer more\nreasonable predictions of causal effects, facilitating decision-making in\nrelevant applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning is often challenging with spatial data, particularly when\nhandling high-dimensional inputs. To address this, we propose a neural network\n(NN) based framework integrated with an approximate Gaussian process to manage\nspatial interference and unobserved confounding. Additionally, we adopt a\ngeneralized propensity-score-based approach to address partially observed\noutcomes when estimating causal effects with continuous treatments. We evaluate\nour framework using synthetic, semi-synthetic, and real-world data inferred\nfrom satellite imagery. Our results demonstrate that NN-based models\nsignificantly outperform linear spatial regression models in estimating causal\neffects. Furthermore, in real-world case studies, NN-based models offer more\nreasonable predictions of causal effects, facilitating decision-making in\nrelevant applications."
                },
                "authors": [
                    {
                        "name": "Ziyang Jiang"
                    },
                    {
                        "name": "Zach Calhoun"
                    },
                    {
                        "name": "Yiling Liu"
                    },
                    {
                        "name": "Lei Duan"
                    },
                    {
                        "name": "David Carlson"
                    }
                ],
                "author_detail": {
                    "name": "David Carlson"
                },
                "author": "David Carlson",
                "arxiv_comment": "16 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15046v2",
                "updated": "2024-12-05T16:04:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    4,
                    2,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-22T16:31:36Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    31,
                    36,
                    4,
                    327,
                    0
                ],
                "title": "On Multi-Agent Inverse Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Multi-Agent Inverse Reinforcement Learning"
                },
                "summary": "In multi-agent systems, the agent behavior is highly influenced by its\nutility function, as these utilities shape both individual goals as well as\ninteractions with the other agents. Inverse Reinforcement Learning (IRL) is a\nwell-established approach to inferring the utility function by observing an\nexpert behavior within a given environment. In this paper, we extend the IRL\nframework to the multi-agent setting, assuming to observe agents who are\nfollowing Nash Equilibrium (NE) policies. We theoretically investigate the set\nof utilities that explain the behavior of NE experts. Specifically, we provide\nan explicit characterization of the feasible reward set and analyze how errors\nin estimating the transition dynamics and expert behavior impact the recovered\nrewards. Building on these findings, we provide the first sample complexity\nanalysis for the multi-agent IRL problem. Finally, we provide a numerical\nevaluation of our theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems, the agent behavior is highly influenced by its\nutility function, as these utilities shape both individual goals as well as\ninteractions with the other agents. Inverse Reinforcement Learning (IRL) is a\nwell-established approach to inferring the utility function by observing an\nexpert behavior within a given environment. In this paper, we extend the IRL\nframework to the multi-agent setting, assuming to observe agents who are\nfollowing Nash Equilibrium (NE) policies. We theoretically investigate the set\nof utilities that explain the behavior of NE experts. Specifically, we provide\nan explicit characterization of the feasible reward set and analyze how errors\nin estimating the transition dynamics and expert behavior impact the recovered\nrewards. Building on these findings, we provide the first sample complexity\nanalysis for the multi-agent IRL problem. Finally, we provide a numerical\nevaluation of our theoretical results."
                },
                "authors": [
                    {
                        "name": "Till Freihaut"
                    },
                    {
                        "name": "Giorgia Ramponi"
                    }
                ],
                "author_detail": {
                    "name": "Giorgia Ramponi"
                },
                "author": "Giorgia Ramponi",
                "arxiv_comment": "Currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04277v1",
                "updated": "2024-12-05T15:59:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    59,
                    29,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:59:29Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    59,
                    29,
                    3,
                    340,
                    0
                ],
                "title": "Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic"
                },
                "summary": "Large Language Models (LLMs) have shown impressive results in multiple\ndomains of natural language processing (NLP) but are mainly focused on the\nEnglish language. Recently, more LLMs have incorporated a larger proportion of\nmultilingual text to represent low-resource languages. In Arabic NLP, several\nArabic-centric LLMs have shown remarkable results on multiple benchmarks in the\npast two years. However, most Arabic LLMs have more than 7 billion parameters,\nwhich increases their hardware requirements and inference latency, when\ncompared to smaller LLMs. This paper introduces Arabic Stable LM 1.6B in a base\nand chat version as a small but powerful Arabic-centric LLM. Our Arabic Stable\nLM 1.6B chat model achieves impressive results on several benchmarks beating\nmultiple models with up to 8x the parameters. In addition, we show the benefit\nof mixing in synthetic instruction tuning data by augmenting our fine-tuning\ndata with a large synthetic dialogue dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive results in multiple\ndomains of natural language processing (NLP) but are mainly focused on the\nEnglish language. Recently, more LLMs have incorporated a larger proportion of\nmultilingual text to represent low-resource languages. In Arabic NLP, several\nArabic-centric LLMs have shown remarkable results on multiple benchmarks in the\npast two years. However, most Arabic LLMs have more than 7 billion parameters,\nwhich increases their hardware requirements and inference latency, when\ncompared to smaller LLMs. This paper introduces Arabic Stable LM 1.6B in a base\nand chat version as a small but powerful Arabic-centric LLM. Our Arabic Stable\nLM 1.6B chat model achieves impressive results on several benchmarks beating\nmultiple models with up to 8x the parameters. In addition, we show the benefit\nof mixing in synthetic instruction tuning data by augmenting our fine-tuning\ndata with a large synthetic dialogue dataset."
                },
                "authors": [
                    {
                        "name": "Zaid Alyafeai"
                    },
                    {
                        "name": "Michael Pieler"
                    },
                    {
                        "name": "Hannah Teufel"
                    },
                    {
                        "name": "Jonathan Tow"
                    },
                    {
                        "name": "Marco Bellagente"
                    },
                    {
                        "name": "Duy Phung"
                    },
                    {
                        "name": "Nikhil Pinnaparaju"
                    },
                    {
                        "name": "Reshinth Adithyan"
                    },
                    {
                        "name": "Paulo Rocha"
                    },
                    {
                        "name": "Maksym Zhuravinskyi"
                    },
                    {
                        "name": "Carlos Riquelme"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Riquelme"
                },
                "author": "Carlos Riquelme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02993v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02993v3",
                "updated": "2024-12-05T15:59:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    59,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2024-08-06T06:59:15Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    6,
                    59,
                    15,
                    1,
                    219,
                    0
                ],
                "title": "DreamLCM: Towards High-Quality Text-to-3D Generation via Latent\n  Consistency Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamLCM: Towards High-Quality Text-to-3D Generation via Latent\n  Consistency Model"
                },
                "summary": "Recently, the text-to-3D task has developed rapidly due to the appearance of\nthe SDS method. However, the SDS method always generates 3D objects with poor\nquality due to the over-smooth issue. This issue is attributed to two factors:\n1) the DDPM single-step inference produces poor guidance gradients; 2) the\nrandomness from the input noises and timesteps averages the details of the 3D\ncontents. In this paper, to address the issue, we propose DreamLCM which\nincorporates the Latent Consistency Model (LCM). DreamLCM leverages the\npowerful image generation capabilities inherent in LCM, enabling generating\nconsistent and high-quality guidance, i.e., predicted noises or images. Powered\nby the improved guidance, the proposed method can provide accurate and detailed\ngradients to optimize the target 3D models. In addition, we propose two\nstrategies to enhance the generation quality further. Firstly, we propose a\nguidance calibration strategy, utilizing Euler Solver to calibrate the guidance\ndistribution to accelerate 3D models to converge. Secondly, we propose a dual\ntimestep strategy, increasing the consistency of guidance and optimizing 3D\nmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCM\nachieves state-of-the-art results in both generation quality and training\nefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the text-to-3D task has developed rapidly due to the appearance of\nthe SDS method. However, the SDS method always generates 3D objects with poor\nquality due to the over-smooth issue. This issue is attributed to two factors:\n1) the DDPM single-step inference produces poor guidance gradients; 2) the\nrandomness from the input noises and timesteps averages the details of the 3D\ncontents. In this paper, to address the issue, we propose DreamLCM which\nincorporates the Latent Consistency Model (LCM). DreamLCM leverages the\npowerful image generation capabilities inherent in LCM, enabling generating\nconsistent and high-quality guidance, i.e., predicted noises or images. Powered\nby the improved guidance, the proposed method can provide accurate and detailed\ngradients to optimize the target 3D models. In addition, we propose two\nstrategies to enhance the generation quality further. Firstly, we propose a\nguidance calibration strategy, utilizing Euler Solver to calibrate the guidance\ndistribution to accelerate 3D models to converge. Secondly, we propose a dual\ntimestep strategy, increasing the consistency of guidance and optimizing 3D\nmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCM\nachieves state-of-the-art results in both generation quality and training\nefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM."
                },
                "authors": [
                    {
                        "name": "Yiming Zhong"
                    },
                    {
                        "name": "Xiaolin Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yunchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Yunchao Wei"
                },
                "author": "Yunchao Wei",
                "arxiv_comment": "15 pages, 9 figures, ACM MM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02993v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02993v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04272v1",
                "updated": "2024-12-05T15:54:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    54,
                    16,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:54:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    54,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "PoTable: Programming Standardly on Table-based Reasoning Like a Human\n  Analyst",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoTable: Programming Standardly on Table-based Reasoning Like a Human\n  Analyst"
                },
                "summary": "Table-based reasoning has garnered substantial research interest,\nparticularly in its integration with Large Language Model (LLM) which has\nrevolutionized the general reasoning paradigm. Numerous LLM-based studies\nintroduce symbolic tools (e.g., databases, Python) as assistants to extend\nhuman-like abilities in structured table understanding and complex arithmetic\ncomputations. However, these studies can be improved better in simulating human\ncognitive behavior when using symbolic tools, as they still suffer from\nlimitations of non-standard logical splits and constrained operation pools. In\nthis study, we propose PoTable as a novel table-based reasoning method that\nsimulates a human tabular analyst, which integrates a Python interpreter as the\nreal-time executor accompanied by an LLM-based operation planner and code\ngenerator. Specifically, PoTable follows a human-like logical stage split and\nextends the operation pool into an open-world space without any constraints.\nThrough planning and executing in each distinct stage, PoTable standardly\ncompletes the entire reasoning process and produces superior reasoning results\nalong with highly accurate, steply commented and completely executable\nprograms. Accordingly, the effectiveness and explainability of PoTable are\nfully demonstrated. Extensive experiments over three evaluation datasets from\ntwo public benchmarks on two backbones show the outstanding performance of our\napproach. In particular, GPT-based PoTable achieves over 4% higher absolute\naccuracy than runner-ups on all evaluation datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-based reasoning has garnered substantial research interest,\nparticularly in its integration with Large Language Model (LLM) which has\nrevolutionized the general reasoning paradigm. Numerous LLM-based studies\nintroduce symbolic tools (e.g., databases, Python) as assistants to extend\nhuman-like abilities in structured table understanding and complex arithmetic\ncomputations. However, these studies can be improved better in simulating human\ncognitive behavior when using symbolic tools, as they still suffer from\nlimitations of non-standard logical splits and constrained operation pools. In\nthis study, we propose PoTable as a novel table-based reasoning method that\nsimulates a human tabular analyst, which integrates a Python interpreter as the\nreal-time executor accompanied by an LLM-based operation planner and code\ngenerator. Specifically, PoTable follows a human-like logical stage split and\nextends the operation pool into an open-world space without any constraints.\nThrough planning and executing in each distinct stage, PoTable standardly\ncompletes the entire reasoning process and produces superior reasoning results\nalong with highly accurate, steply commented and completely executable\nprograms. Accordingly, the effectiveness and explainability of PoTable are\nfully demonstrated. Extensive experiments over three evaluation datasets from\ntwo public benchmarks on two backbones show the outstanding performance of our\napproach. In particular, GPT-based PoTable achieves over 4% higher absolute\naccuracy than runner-ups on all evaluation datasets."
                },
                "authors": [
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Rui Li"
                    }
                ],
                "author_detail": {
                    "name": "Rui Li"
                },
                "author": "Rui Li",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04254v1",
                "updated": "2024-12-05T15:34:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    34,
                    2,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:34:02Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    34,
                    2,
                    3,
                    340,
                    0
                ],
                "title": "CLINICSUM: Utilizing Language Models for Generating Clinical Summaries\n  from Patient-Doctor Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLINICSUM: Utilizing Language Models for Generating Clinical Summaries\n  from Patient-Doctor Conversations"
                },
                "summary": "This paper presents ClinicSum, a novel framework designed to automatically\ngenerate clinical summaries from patient-doctor conversations. It utilizes a\ntwo-module architecture: a retrieval-based filtering module that extracts\nSubjective, Objective, Assessment, and Plan (SOAP) information from\nconversation transcripts, and an inference module powered by fine-tuned\nPre-trained Language Models (PLMs), which leverage the extracted SOAP data to\ngenerate abstracted clinical summaries. To fine-tune the PLM, we created a\ntraining dataset of consisting 1,473 conversations-summaries pair by\nconsolidating two publicly available datasets, FigShare and MTS-Dialog, with\nground truth summaries validated by Subject Matter Experts (SMEs). ClinicSum's\neffectiveness is evaluated through both automatic metrics (e.g., ROUGE,\nBERTScore) and expert human assessments. Results show that ClinicSum\noutperforms state-of-the-art PLMs, demonstrating superior precision, recall,\nand F-1 scores in automatic evaluations and receiving high preference from SMEs\nin human assessment, making it a robust solution for automated clinical\nsummarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ClinicSum, a novel framework designed to automatically\ngenerate clinical summaries from patient-doctor conversations. It utilizes a\ntwo-module architecture: a retrieval-based filtering module that extracts\nSubjective, Objective, Assessment, and Plan (SOAP) information from\nconversation transcripts, and an inference module powered by fine-tuned\nPre-trained Language Models (PLMs), which leverage the extracted SOAP data to\ngenerate abstracted clinical summaries. To fine-tune the PLM, we created a\ntraining dataset of consisting 1,473 conversations-summaries pair by\nconsolidating two publicly available datasets, FigShare and MTS-Dialog, with\nground truth summaries validated by Subject Matter Experts (SMEs). ClinicSum's\neffectiveness is evaluated through both automatic metrics (e.g., ROUGE,\nBERTScore) and expert human assessments. Results show that ClinicSum\noutperforms state-of-the-art PLMs, demonstrating superior precision, recall,\nand F-1 scores in automatic evaluations and receiving high preference from SMEs\nin human assessment, making it a robust solution for automated clinical\nsummarization."
                },
                "authors": [
                    {
                        "name": "Subash Neupane"
                    },
                    {
                        "name": "Himanshu Tripathi"
                    },
                    {
                        "name": "Shaswata Mitra"
                    },
                    {
                        "name": "Sean Bozorgzad"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    },
                    {
                        "name": "Amin Amirlatifi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Amirlatifi"
                },
                "author": "Amin Amirlatifi",
                "arxiv_comment": "accepted at the the 2024 IEEE International Conference on Big Data\n  workshop Workshop on Big Data and AI for Healthcare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12294v3",
                "updated": "2024-12-05T15:27:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    27,
                    14,
                    3,
                    340,
                    0
                ],
                "published": "2024-04-18T16:16:02Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    16,
                    16,
                    2,
                    3,
                    109,
                    0
                ],
                "title": "Bayesian evidence estimation from posterior samples with normalizing\n  flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian evidence estimation from posterior samples with normalizing\n  flows"
                },
                "summary": "We propose a novel method ($floZ$), based on normalizing flows, to estimate\nthe Bayesian evidence (and its numerical uncertainty) from a pre-existing set\nof samples drawn from the unnormalized posterior distribution. We validate it\non distributions whose evidence is known analytically, up to 15 parameter space\ndimensions, and compare with two state-of-the-art techniques for estimating the\nevidence: nested sampling (which computes the evidence as its main target) and\na $k$-nearest-neighbors technique that produces evidence estimates from\nposterior samples. Provided representative samples from the target posterior\nare available, our method is more robust to posterior distributions with sharp\nfeatures, especially in higher dimensions. For a simple multivariate Gaussian,\nwe demonstrate its accuracy for up to 200 dimensions with $10^5$ posterior\nsamples. $floZ$ has wide applicability, e.g., to estimate evidence from\nvariational inference, Markov Chain Monte Carlo samples, or any other method\nthat delivers samples and their likelihood from the unnormalized posterior\ndensity. As a physical application, we use $floZ$ to compute the Bayes factor\nfor the presence of the first overtone in the ringdown signal of the\ngravitational wave data of GW150914, finding good agreement with nested\nsampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel method ($floZ$), based on normalizing flows, to estimate\nthe Bayesian evidence (and its numerical uncertainty) from a pre-existing set\nof samples drawn from the unnormalized posterior distribution. We validate it\non distributions whose evidence is known analytically, up to 15 parameter space\ndimensions, and compare with two state-of-the-art techniques for estimating the\nevidence: nested sampling (which computes the evidence as its main target) and\na $k$-nearest-neighbors technique that produces evidence estimates from\nposterior samples. Provided representative samples from the target posterior\nare available, our method is more robust to posterior distributions with sharp\nfeatures, especially in higher dimensions. For a simple multivariate Gaussian,\nwe demonstrate its accuracy for up to 200 dimensions with $10^5$ posterior\nsamples. $floZ$ has wide applicability, e.g., to estimate evidence from\nvariational inference, Markov Chain Monte Carlo samples, or any other method\nthat delivers samples and their likelihood from the unnormalized posterior\ndensity. As a physical application, we use $floZ$ to compute the Bayes factor\nfor the presence of the first overtone in the ringdown signal of the\ngravitational wave data of GW150914, finding good agreement with nested\nsampling."
                },
                "authors": [
                    {
                        "name": "Rahul Srinivasan"
                    },
                    {
                        "name": "Marco Crisostomi"
                    },
                    {
                        "name": "Roberto Trotta"
                    },
                    {
                        "name": "Enrico Barausse"
                    },
                    {
                        "name": "Matteo Breschi"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Breschi"
                },
                "author": "Matteo Breschi",
                "arxiv_doi": "10.1103/PhysRevD.110.123007",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.123007",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.12294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 8 figures, 1 table",
                "arxiv_journal_ref": "PhysRevD.110.123007(2024)",
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14086v2",
                "updated": "2024-12-05T15:24:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    24,
                    33,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-17T23:37:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    23,
                    37,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "In-context learning and Occam's razor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning and Occam's razor"
                },
                "summary": "A central goal of machine learning is generalization. While the No Free Lunch\nTheorem states that we cannot obtain theoretical guarantees for generalization\nwithout further assumptions, in practice we observe that simple models which\nexplain the training data generalize best: a principle called Occam's razor.\nDespite the need for simple models, most current approaches in machine learning\nonly minimize the training error, and at best indirectly promote simplicity\nthrough regularization or architecture design. Here, we draw a connection\nbetween Occam's razor and in-context learning: an emergent ability of certain\nsequence models like Transformers to learn at inference time from past\nobservations in a sequence. In particular, we show that the next-token\nprediction loss used to train in-context learners is directly equivalent to a\ndata compression technique called prequential coding, and that minimizing this\nloss amounts to jointly minimizing both the training error and the complexity\nof the model that was implicitly learned from context. Our theory and the\nempirical experiments we use to support it not only provide a normative account\nof in-context learning, but also elucidate the shortcomings of current\nin-context learning methods, suggesting ways in which they can be improved. We\nmake our code available at https://github.com/3rdCore/PrequentialCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central goal of machine learning is generalization. While the No Free Lunch\nTheorem states that we cannot obtain theoretical guarantees for generalization\nwithout further assumptions, in practice we observe that simple models which\nexplain the training data generalize best: a principle called Occam's razor.\nDespite the need for simple models, most current approaches in machine learning\nonly minimize the training error, and at best indirectly promote simplicity\nthrough regularization or architecture design. Here, we draw a connection\nbetween Occam's razor and in-context learning: an emergent ability of certain\nsequence models like Transformers to learn at inference time from past\nobservations in a sequence. In particular, we show that the next-token\nprediction loss used to train in-context learners is directly equivalent to a\ndata compression technique called prequential coding, and that minimizing this\nloss amounts to jointly minimizing both the training error and the complexity\nof the model that was implicitly learned from context. Our theory and the\nempirical experiments we use to support it not only provide a normative account\nof in-context learning, but also elucidate the shortcomings of current\nin-context learning methods, suggesting ways in which they can be improved. We\nmake our code available at https://github.com/3rdCore/PrequentialCode."
                },
                "authors": [
                    {
                        "name": "Eric Elmoznino"
                    },
                    {
                        "name": "Tom Marty"
                    },
                    {
                        "name": "Tejas Kasetty"
                    },
                    {
                        "name": "Leo Gagnon"
                    },
                    {
                        "name": "Sarthak Mittal"
                    },
                    {
                        "name": "Mahan Fathi"
                    },
                    {
                        "name": "Dhanya Sridhar"
                    },
                    {
                        "name": "Guillaume Lajoie"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Lajoie"
                },
                "author": "Guillaume Lajoie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04237v1",
                "updated": "2024-12-05T15:17:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    17,
                    6,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:17:06Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    17,
                    6,
                    3,
                    340,
                    0
                ],
                "title": "VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction"
                },
                "summary": "Large language models (LLMs) have proven effective for layout generation due\nto their ability to produce structure-description languages, such as HTML or\nJSON, even without access to visual information. Recently, LLM providers have\nevolved these models into large vision-language models (LVLM), which shows\nprominent multi-modal understanding capabilities. Then, how can we leverage\nthis multi-modal power for layout generation? To answer this, we propose\nVisual-Aware Self-Correction LAyout GeneRation (VASCAR) for LVLM-based\ncontent-aware layout generation. In our method, LVLMs iteratively refine their\noutputs with reference to rendered layout images, which are visualized as\ncolored bounding boxes on poster backgrounds. In experiments, we demonstrate\nthat our method combined with the Gemini. Without any additional training,\nVASCAR achieves state-of-the-art (SOTA) layout generation quality outperforming\nboth existing layout-specific generative models and other LLM-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have proven effective for layout generation due\nto their ability to produce structure-description languages, such as HTML or\nJSON, even without access to visual information. Recently, LLM providers have\nevolved these models into large vision-language models (LVLM), which shows\nprominent multi-modal understanding capabilities. Then, how can we leverage\nthis multi-modal power for layout generation? To answer this, we propose\nVisual-Aware Self-Correction LAyout GeneRation (VASCAR) for LVLM-based\ncontent-aware layout generation. In our method, LVLMs iteratively refine their\noutputs with reference to rendered layout images, which are visualized as\ncolored bounding boxes on poster backgrounds. In experiments, we demonstrate\nthat our method combined with the Gemini. Without any additional training,\nVASCAR achieves state-of-the-art (SOTA) layout generation quality outperforming\nboth existing layout-specific generative models and other LLM-based methods."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Ryota Yoshihashi"
                    },
                    {
                        "name": "Shunsuke Kitada"
                    },
                    {
                        "name": "Atsuki Osanai"
                    },
                    {
                        "name": "Yuta Nakashima"
                    }
                ],
                "author_detail": {
                    "name": "Yuta Nakashima"
                },
                "author": "Yuta Nakashima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02674v2",
                "updated": "2024-12-05T15:16:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    16,
                    19,
                    3,
                    340,
                    0
                ],
                "published": "2024-07-02T21:27:55Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    21,
                    27,
                    55,
                    1,
                    184,
                    0
                ],
                "title": "Elevated UV luminosity density at Cosmic Dawn explained by non-evolving,\n  weakly mass-dependent star formation efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elevated UV luminosity density at Cosmic Dawn explained by non-evolving,\n  weakly mass-dependent star formation efficiency"
                },
                "summary": "Recent observations with the James Webb Space Telescope (JWST) have uncovered\nunexpectedly high cosmic star formation activity in the early Universe, mere\nhundreds of millions of years after the Big Bang. These observations are often\nunderstood to reflect an evolutionary shift in star formation efficiency (SFE)\ncaused by changing galactic conditions during these early epochs. We present\nFIREbox-HR, a high-resolution, cosmological hydrodynamical simulation from the\nFeedback in Realistic Environments project, which offers insights into the SFE\nof galaxies during the first billion years of cosmic time. FIREbox-HR\nre-simulates the cosmic volume (L = 22.1 cMpc) of the original FIREbox run with\neight times higher mass resolution (m_b ~ 7800 M_sun), but with identical\nphysics, down to z ~ 6. FIREbox-HR predicts ultraviolet (UV) luminosity\nfunctions in good agreement with available observational data. The simulation\nalso successfully reproduces the observed cosmic UV luminosity density at z ~ 6\n- 14, demonstrating that relatively high star formation activity in the early\nUniverse is a natural outcome of the baryonic processes encoded in the FIRE-2\nmodel. According to FIREbox-HR, the SFE - halo mass relation for intermediate\nmass halos (M_halo ~ 10^9 - 10^11 M_sun) does not significantly evolve with\nredshift and is only weakly mass-dependent. These properties of the SFE - halo\nmass relation lead to a larger contribution from lower mass halos at higher z,\ndriving the gradual evolution of the observed cosmic UV luminosity density. A\ntheoretical model based on the SFE - halo mass relation inferred from\nFIREbox-HR allows us to explore implications for galaxy evolution. Future\nobservations of UV faint galaxies at z > 12 will provide an opportunity to\nfurther test these predictions and deepen our understanding of star formation\nduring Cosmic Dawn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent observations with the James Webb Space Telescope (JWST) have uncovered\nunexpectedly high cosmic star formation activity in the early Universe, mere\nhundreds of millions of years after the Big Bang. These observations are often\nunderstood to reflect an evolutionary shift in star formation efficiency (SFE)\ncaused by changing galactic conditions during these early epochs. We present\nFIREbox-HR, a high-resolution, cosmological hydrodynamical simulation from the\nFeedback in Realistic Environments project, which offers insights into the SFE\nof galaxies during the first billion years of cosmic time. FIREbox-HR\nre-simulates the cosmic volume (L = 22.1 cMpc) of the original FIREbox run with\neight times higher mass resolution (m_b ~ 7800 M_sun), but with identical\nphysics, down to z ~ 6. FIREbox-HR predicts ultraviolet (UV) luminosity\nfunctions in good agreement with available observational data. The simulation\nalso successfully reproduces the observed cosmic UV luminosity density at z ~ 6\n- 14, demonstrating that relatively high star formation activity in the early\nUniverse is a natural outcome of the baryonic processes encoded in the FIRE-2\nmodel. According to FIREbox-HR, the SFE - halo mass relation for intermediate\nmass halos (M_halo ~ 10^9 - 10^11 M_sun) does not significantly evolve with\nredshift and is only weakly mass-dependent. These properties of the SFE - halo\nmass relation lead to a larger contribution from lower mass halos at higher z,\ndriving the gradual evolution of the observed cosmic UV luminosity density. A\ntheoretical model based on the SFE - halo mass relation inferred from\nFIREbox-HR allows us to explore implications for galaxy evolution. Future\nobservations of UV faint galaxies at z > 12 will provide an opportunity to\nfurther test these predictions and deepen our understanding of star formation\nduring Cosmic Dawn."
                },
                "authors": [
                    {
                        "name": "Robert Feldmann"
                    },
                    {
                        "name": "Michael Boylan-Kolchin"
                    },
                    {
                        "name": "James S. Bullock"
                    },
                    {
                        "name": "Onur atmabacak"
                    },
                    {
                        "name": "Claude-Andr Faucher-Gigure"
                    },
                    {
                        "name": "Christopher C. Hayward"
                    },
                    {
                        "name": "Duan Kere"
                    },
                    {
                        "name": "Alexandres Lazar"
                    },
                    {
                        "name": "Lichen Liang"
                    },
                    {
                        "name": "Jorge Moreno"
                    },
                    {
                        "name": "Pascal A. Oesch"
                    },
                    {
                        "name": "Eliot Quataert"
                    },
                    {
                        "name": "Xuejian Shen"
                    },
                    {
                        "name": "Guochao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guochao Sun"
                },
                "author": "Guochao Sun",
                "arxiv_doi": "10.1093/mnras/stae2633",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2633",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.02674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "28 pages, 14 figures, 5 tables, revised to match version accepted by\n  MNRAS, Appendix D added, UV LF shown in Fig. 1 provided as txt file",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04235v1",
                "updated": "2024-12-05T15:11:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    11,
                    12,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:11:12Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    11,
                    12,
                    3,
                    340,
                    0
                ],
                "title": "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM\n  Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM\n  Chatbots"
                },
                "summary": "I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains."
                },
                "authors": [
                    {
                        "name": "Maria Paola Priola"
                    }
                ],
                "author_detail": {
                    "name": "Maria Paola Priola"
                },
                "author": "Maria Paola Priola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04232v1",
                "updated": "2024-12-05T15:09:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    9,
                    21,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:09:21Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    9,
                    21,
                    3,
                    340,
                    0
                ],
                "title": "Intent-based Meta-Scheduling in Programmable Networks: A Research Agenda",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based Meta-Scheduling in Programmable Networks: A Research Agenda"
                },
                "summary": "The emergence and growth of 5G and beyond 5G (B5G) networks has brought about\nthe rise of so-called \"programmable\" networks, i.e., networks whose operational\nrequirements are so stringent that they can only be met in an automated manner,\nwith minimal/no human involvement. Any requirements on such a network would\nneed to be formally specified via intents, which can represent user\nrequirements in a formal yet understandable manner. Meeting the user\nrequirements via intents would necessitate the rapid implementation of resource\nallocation and scheduling in the network. Also, given the expected size and\ngeographical distribution of programmable networks, multiple resource\nscheduling implementations would need to be implemented at the same time. This\nwould necessitate the use of a meta-scheduler that can coordinate the various\nschedulers and dynamically ensure optimal resource scheduling across the\nnetwork.\n  To that end, in this position paper, we propose a research agenda for\nmodeling, implementation, and inclusion of intent-based dynamic meta-scheduling\nin programmable networks. Our research agenda will be built on active\ninference, a type of causal inference. Active inference provides some level of\nautonomy to each scheduler while the meta-scheduler takes care of overall\nintent fulfillment. Our research agenda will comprise a strawman architecture\nfor meta-scheduling and a set of research questions that need to be addressed\nto make intent-dynamic meta-scheduling a reality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence and growth of 5G and beyond 5G (B5G) networks has brought about\nthe rise of so-called \"programmable\" networks, i.e., networks whose operational\nrequirements are so stringent that they can only be met in an automated manner,\nwith minimal/no human involvement. Any requirements on such a network would\nneed to be formally specified via intents, which can represent user\nrequirements in a formal yet understandable manner. Meeting the user\nrequirements via intents would necessitate the rapid implementation of resource\nallocation and scheduling in the network. Also, given the expected size and\ngeographical distribution of programmable networks, multiple resource\nscheduling implementations would need to be implemented at the same time. This\nwould necessitate the use of a meta-scheduler that can coordinate the various\nschedulers and dynamically ensure optimal resource scheduling across the\nnetwork.\n  To that end, in this position paper, we propose a research agenda for\nmodeling, implementation, and inclusion of intent-based dynamic meta-scheduling\nin programmable networks. Our research agenda will be built on active\ninference, a type of causal inference. Active inference provides some level of\nautonomy to each scheduler while the meta-scheduler takes care of overall\nintent fulfillment. Our research agenda will comprise a strawman architecture\nfor meta-scheduling and a set of research questions that need to be addressed\nto make intent-dynamic meta-scheduling a reality."
                },
                "authors": [
                    {
                        "name": "Nanjangud C. Narendra"
                    },
                    {
                        "name": "Ronak Kanthaliya"
                    },
                    {
                        "name": "Venkatareddy Akumalla"
                    }
                ],
                "author_detail": {
                    "name": "Venkatareddy Akumalla"
                },
                "author": "Venkatareddy Akumalla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05357v2",
                "updated": "2024-12-05T15:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    8,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-07T15:55:55Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    55,
                    55,
                    0,
                    281,
                    0
                ],
                "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild"
                },
                "summary": "As Large Language Models (LLMs) excel across tasks and specialized domains,\nscaling LLMs based on existing models has garnered significant attention, which\nfaces the challenge of decreasing performance when combining disparate models.\nVarious techniques have been proposed for the aggregation of pre-trained LLMs,\nincluding model merging, Mixture-of-Experts, and stacking. Despite their\nmerits, a comprehensive comparison and synergistic application of them to a\ndiverse model zoo is yet to be adequately addressed. In light of this research\ngap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First,\nour work starts with a benchmarking of existing LLM scaling techniques,\nespecially selective merging, and variants of mixture. Utilizing the insights\nfrom the benchmark results, we formulate an optimal strategy for the selection\nand aggregation of a heterogeneous model zoo characterizing different\narchitectures and initialization.Our methodology involves the clustering of\nmergeable models and optimal merging strategy selection, and the integration of\nclusters through a model mixture. Finally, evidenced by our experiments on a\ndiverse Llama-2-based model zoo, Model-GLUE shows an average performance\nenhancement of 5.61%, achieved without additional training. Codes are available\nat: https://github.com/Model-GLUE/Model-GLUE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) excel across tasks and specialized domains,\nscaling LLMs based on existing models has garnered significant attention, which\nfaces the challenge of decreasing performance when combining disparate models.\nVarious techniques have been proposed for the aggregation of pre-trained LLMs,\nincluding model merging, Mixture-of-Experts, and stacking. Despite their\nmerits, a comprehensive comparison and synergistic application of them to a\ndiverse model zoo is yet to be adequately addressed. In light of this research\ngap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First,\nour work starts with a benchmarking of existing LLM scaling techniques,\nespecially selective merging, and variants of mixture. Utilizing the insights\nfrom the benchmark results, we formulate an optimal strategy for the selection\nand aggregation of a heterogeneous model zoo characterizing different\narchitectures and initialization.Our methodology involves the clustering of\nmergeable models and optimal merging strategy selection, and the integration of\nclusters through a model mixture. Finally, evidenced by our experiments on a\ndiverse Llama-2-based model zoo, Model-GLUE shows an average performance\nenhancement of 5.61%, achieved without additional training. Codes are available\nat: https://github.com/Model-GLUE/Model-GLUE."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yukun Zhou"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Bowen Tan"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Yi Liang"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 4 figures, accepted to NeurIPS 2024 Datasets and Benchmarks\n  Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06740v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06740v4",
                "updated": "2024-12-05T14:56:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    30,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-11T06:25:13Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    25,
                    13,
                    0,
                    316,
                    0
                ],
                "title": "Dockformer: A transformer-based molecular docking paradigm for\n  large-scale virtual screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dockformer: A transformer-based molecular docking paradigm for\n  large-scale virtual screening"
                },
                "summary": "Molecular docking is a crucial step in drug development, which enables the\nvirtual screening of compound libraries to identify potential ligands that\ntarget proteins of interest. However, the computational complexity of\ntraditional docking models increases as the size of the compound library\nincreases. Recently, deep learning algorithms can provide data-driven research\nand development models to increase the speed of the docking process.\nUnfortunately, few models can achieve superior screening performance compared\nto that of traditional models. Therefore, a novel deep learning-based docking\napproach named Dockformer is introduced in this study. Dockformer leverages\nmultimodal information to capture the geometric topology and structural\nknowledge of molecules and can directly generate binding conformations with the\ncorresponding confidence measures in an end-to-end manner. The experimental\nresults show that Dockformer achieves success rates of 90.53% and 82.71% on the\nPDBbind core set and PoseBusters benchmarks, respectively, and more than a\n100-fold increase in the inference process speed, outperforming almost all\nstate-of-the-art docking methods. In addition, the ability of Dockformer to\nidentify the main protease inhibitors of coronaviruses is demonstrated in a\nreal-world virtual screening scenario. Considering its high docking accuracy\nand screening efficiency, Dockformer can be regarded as a powerful and robust\ntool in the field of drug design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular docking is a crucial step in drug development, which enables the\nvirtual screening of compound libraries to identify potential ligands that\ntarget proteins of interest. However, the computational complexity of\ntraditional docking models increases as the size of the compound library\nincreases. Recently, deep learning algorithms can provide data-driven research\nand development models to increase the speed of the docking process.\nUnfortunately, few models can achieve superior screening performance compared\nto that of traditional models. Therefore, a novel deep learning-based docking\napproach named Dockformer is introduced in this study. Dockformer leverages\nmultimodal information to capture the geometric topology and structural\nknowledge of molecules and can directly generate binding conformations with the\ncorresponding confidence measures in an end-to-end manner. The experimental\nresults show that Dockformer achieves success rates of 90.53% and 82.71% on the\nPDBbind core set and PoseBusters benchmarks, respectively, and more than a\n100-fold increase in the inference process speed, outperforming almost all\nstate-of-the-art docking methods. In addition, the ability of Dockformer to\nidentify the main protease inhibitors of coronaviruses is demonstrated in a\nreal-world virtual screening scenario. Considering its high docking accuracy\nand screening efficiency, Dockformer can be regarded as a powerful and robust\ntool in the field of drug design."
                },
                "authors": [
                    {
                        "name": "Zhangfan Yang"
                    },
                    {
                        "name": "Junkai Ji"
                    },
                    {
                        "name": "Shan He"
                    },
                    {
                        "name": "Jianqiang Li"
                    },
                    {
                        "name": "Tiantian He"
                    },
                    {
                        "name": "Ruibin Bai"
                    },
                    {
                        "name": "Zexuan Zhu"
                    },
                    {
                        "name": "Yew Soon Ong"
                    }
                ],
                "author_detail": {
                    "name": "Yew Soon Ong"
                },
                "author": "Yew Soon Ong",
                "arxiv_comment": "15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06740v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06740v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02830v2",
                "updated": "2024-12-05T14:51:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    51,
                    35,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-03T20:52:35Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    52,
                    35,
                    1,
                    338,
                    0
                ],
                "title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language\n  Models"
                },
                "summary": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical."
                },
                "authors": [
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Junda Wang"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04211v1",
                "updated": "2024-12-05T14:46:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    46,
                    40,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:46:40Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    46,
                    40,
                    3,
                    340,
                    0
                ],
                "title": "BEACON: JWST NIRCam Pure-parallel Imaging Survey. I. Survey Design and\n  Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEACON: JWST NIRCam Pure-parallel Imaging Survey. I. Survey Design and\n  Initial Results"
                },
                "summary": "We introduce the Bias-free Extragalactic Analysis for Cosmic Origins with\nNIRCam (BEACON) survey, a JWST Cycle2 program allocated up to 600 pure-parallel\nhours of observations. BEACON explores high-latitude areas of the sky with\nJWST/NIRCam over $\\sim100$ independent sightlines, totaling $\\sim0.3$deg$^2$,\nreaching a median F444W depth of $\\approx28.2$AB mag (5$\\sigma$). Based on\nexisting JWST observations in legacy fields, we estimate that BEACON will\nphotometrically identify 25--150 galaxies at $z>10$ and 500--1000 at\n$z\\sim7$--10 uniquely enabled by an efficient multiple filter configuration\nspanning $0.9$--5.0$\\mu$m. The expected sample size of $z>10$ galaxies will\nallow us to obtain robust number density estimates and to discriminate between\ndifferent models of early star formation. In this paper, we present an overview\nof the survey design and initial results using the first 19 fields. We present\n129 galaxy candidates at $z>7$ identified in those fields, including 11\ngalaxies at $z>10$ and several UV-luminous ($M_{\\rm UV}<-21$mag) galaxies at\n$z\\sim8$. The number densities of $z<13$ galaxies inferred from the initial\nfields are overall consistent with those in the literature. Despite reaching a\nconsiderably large volume ($\\sim10^5$Mpc$^3$), however, we find no galaxy\ncandidates at $z>13$, providing us with a complimentary insight into early\ngalaxy evolution with minimal cosmic variance. We publish imaging and catalog\ndata products for these initial fields. Upon survey completion, all BEACON data\nwill be coherently processed and distributed to the community along with\ncatalogs for redshift and other physical quantities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Bias-free Extragalactic Analysis for Cosmic Origins with\nNIRCam (BEACON) survey, a JWST Cycle2 program allocated up to 600 pure-parallel\nhours of observations. BEACON explores high-latitude areas of the sky with\nJWST/NIRCam over $\\sim100$ independent sightlines, totaling $\\sim0.3$deg$^2$,\nreaching a median F444W depth of $\\approx28.2$AB mag (5$\\sigma$). Based on\nexisting JWST observations in legacy fields, we estimate that BEACON will\nphotometrically identify 25--150 galaxies at $z>10$ and 500--1000 at\n$z\\sim7$--10 uniquely enabled by an efficient multiple filter configuration\nspanning $0.9$--5.0$\\mu$m. The expected sample size of $z>10$ galaxies will\nallow us to obtain robust number density estimates and to discriminate between\ndifferent models of early star formation. In this paper, we present an overview\nof the survey design and initial results using the first 19 fields. We present\n129 galaxy candidates at $z>7$ identified in those fields, including 11\ngalaxies at $z>10$ and several UV-luminous ($M_{\\rm UV}<-21$mag) galaxies at\n$z\\sim8$. The number densities of $z<13$ galaxies inferred from the initial\nfields are overall consistent with those in the literature. Despite reaching a\nconsiderably large volume ($\\sim10^5$Mpc$^3$), however, we find no galaxy\ncandidates at $z>13$, providing us with a complimentary insight into early\ngalaxy evolution with minimal cosmic variance. We publish imaging and catalog\ndata products for these initial fields. Upon survey completion, all BEACON data\nwill be coherently processed and distributed to the community along with\ncatalogs for redshift and other physical quantities."
                },
                "authors": [
                    {
                        "name": "Takahiro Morishita"
                    },
                    {
                        "name": "Charlotte A. Mason"
                    },
                    {
                        "name": "Kimi C. Kreilgaard"
                    },
                    {
                        "name": "Michele Trenti"
                    },
                    {
                        "name": "Tommaso Treu"
                    },
                    {
                        "name": "Benedetta Vulcani"
                    },
                    {
                        "name": "Yechi Zhang"
                    },
                    {
                        "name": "Abdurro'uf"
                    },
                    {
                        "name": "Anahita Alavi"
                    },
                    {
                        "name": "Hakim Atek"
                    },
                    {
                        "name": "Yannick Bahe"
                    },
                    {
                        "name": "Marusa Bradac"
                    },
                    {
                        "name": "Larry D. Bradley"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Dan Coe"
                    },
                    {
                        "name": "James Colbert"
                    },
                    {
                        "name": "Viola Gelli"
                    },
                    {
                        "name": "Matthew J. Hayes"
                    },
                    {
                        "name": "Tucker Jones"
                    },
                    {
                        "name": "Tadayuki Kodama"
                    },
                    {
                        "name": "Nicha Leethochawalit"
                    },
                    {
                        "name": "Zhaoran Liu"
                    },
                    {
                        "name": "Matthew A. Malkan"
                    },
                    {
                        "name": "Vihang Mehta"
                    },
                    {
                        "name": "Benjamin Metha"
                    },
                    {
                        "name": "Andrew B. Newman"
                    },
                    {
                        "name": "Marc Rafelski"
                    },
                    {
                        "name": "Guido Roberts-Borsani"
                    },
                    {
                        "name": "Michael J. Rutkowski"
                    },
                    {
                        "name": "Claudia Scarlata"
                    },
                    {
                        "name": "Massimo Stiavelli"
                    },
                    {
                        "name": "Ryo A. Sutanto"
                    },
                    {
                        "name": "Kosuke Takahashi"
                    },
                    {
                        "name": "Harry I. Teplitz"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "Submitted to ApJ; DR1 data release will be made on the team website\n  (https://beacon-jwst.github.io)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v4",
                "updated": "2024-12-05T14:45:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    45,
                    5,
                    3,
                    340,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "14 pages, 13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04205v1",
                "updated": "2024-12-05T14:41:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    41,
                    5,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:41:05Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    41,
                    5,
                    3,
                    340,
                    0
                ],
                "title": "A Context-aware Framework for Translation-mediated Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Context-aware Framework for Translation-mediated Conversations"
                },
                "summary": "Effective communication is fundamental to any interaction, yet challenges\narise when participants do not share a common language. Automatic translation\nsystems offer a powerful solution to bridge language barriers in such\nscenarios, but they introduce errors that can lead to misunderstandings and\nconversation breakdown. A key issue is that current systems fail to incorporate\nthe rich contextual information necessary to resolve ambiguities and omitted\ndetails, resulting in literal, inappropriate, or misaligned translations. In\nthis work, we present a framework to improve large language model-based\ntranslation systems by incorporating contextual information in bilingual\nconversational settings. During training, we leverage context-augmented\nparallel data, which allows the model to generate translations sensitive to\nconversational history. During inference, we perform quality-aware decoding\nwith context-aware metrics to select the optimal translation from a pool of\ncandidates. We validate both components of our framework on two task-oriented\ndomains: customer chat and user-assistant interaction. Across both settings,\nour framework consistently results in better translations than state-of-the-art\nsystems like GPT-4o and TowerInstruct, as measured by multiple automatic\ntranslation quality metrics on several language pairs. We also show that the\nresulting model leverages context in an intended and interpretable way,\nimproving consistency between the conveyed message and the generated\ntranslations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective communication is fundamental to any interaction, yet challenges\narise when participants do not share a common language. Automatic translation\nsystems offer a powerful solution to bridge language barriers in such\nscenarios, but they introduce errors that can lead to misunderstandings and\nconversation breakdown. A key issue is that current systems fail to incorporate\nthe rich contextual information necessary to resolve ambiguities and omitted\ndetails, resulting in literal, inappropriate, or misaligned translations. In\nthis work, we present a framework to improve large language model-based\ntranslation systems by incorporating contextual information in bilingual\nconversational settings. During training, we leverage context-augmented\nparallel data, which allows the model to generate translations sensitive to\nconversational history. During inference, we perform quality-aware decoding\nwith context-aware metrics to select the optimal translation from a pool of\ncandidates. We validate both components of our framework on two task-oriented\ndomains: customer chat and user-assistant interaction. Across both settings,\nour framework consistently results in better translations than state-of-the-art\nsystems like GPT-4o and TowerInstruct, as measured by multiple automatic\ntranslation quality metrics on several language pairs. We also show that the\nresulting model leverages context in an intended and interpretable way,\nimproving consistency between the conveyed message and the generated\ntranslations."
                },
                "authors": [
                    {
                        "name": "Jos Pombal"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Emmanouil Zaranis"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr F. T. Martins"
                },
                "author": "Andr F. T. Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04193v1",
                "updated": "2024-12-05T14:33:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    33,
                    0,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:33:00Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    33,
                    0,
                    3,
                    340,
                    0
                ],
                "title": "AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in\n  Dialectal Arabic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in\n  Dialectal Arabic"
                },
                "summary": "Dialectal Arabic (DA) varieties are under-served by language technologies,\nparticularly large language models (LLMs). This trend threatens to exacerbate\nexisting social inequalities and limits language modeling applications, yet the\nresearch community lacks operationalized LLM performance measurements in DA. We\npresent a method that comprehensively evaluates LLM fidelity, understanding,\nquality, and diglossia in modeling DA. We evaluate nine LLMs in eight DA\nvarieties across these four dimensions and provide best practice\nrecommendations. Our evaluation suggests that LLMs do not produce DA as well as\nthey understand it, but does not suggest deterioration in quality when they do.\nFurther analysis suggests that current post-training can degrade DA\ncapabilities, that few-shot examples can overcome this and other LLM\ndeficiencies, and that otherwise no measurable features of input text correlate\nwell with LLM DA performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialectal Arabic (DA) varieties are under-served by language technologies,\nparticularly large language models (LLMs). This trend threatens to exacerbate\nexisting social inequalities and limits language modeling applications, yet the\nresearch community lacks operationalized LLM performance measurements in DA. We\npresent a method that comprehensively evaluates LLM fidelity, understanding,\nquality, and diglossia in modeling DA. We evaluate nine LLMs in eight DA\nvarieties across these four dimensions and provide best practice\nrecommendations. Our evaluation suggests that LLMs do not produce DA as well as\nthey understand it, but does not suggest deterioration in quality when they do.\nFurther analysis suggests that current post-training can degrade DA\ncapabilities, that few-shot examples can overcome this and other LLM\ndeficiencies, and that otherwise no measurable features of input text correlate\nwell with LLM DA performance."
                },
                "authors": [
                    {
                        "name": "Nathaniel R. Robinson"
                    },
                    {
                        "name": "Shahd Abdelmoneim"
                    },
                    {
                        "name": "Kelly Marchisio"
                    },
                    {
                        "name": "Sebastian Ruder"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Ruder"
                },
                "author": "Sebastian Ruder",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07125v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07125v3",
                "updated": "2024-12-05T14:29:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    29,
                    34,
                    3,
                    340,
                    0
                ],
                "published": "2024-07-09T07:05:53Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    7,
                    5,
                    53,
                    1,
                    191,
                    0
                ],
                "title": "Rapid Parameter Estimation for Merging Massive Black Hole Binaries Using\n  Continuous Normalizing Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Parameter Estimation for Merging Massive Black Hole Binaries Using\n  Continuous Normalizing Flows"
                },
                "summary": "Detecting the coalescences of massive black hole binaries (MBHBs) is one of\nthe primary targets for space-based gravitational wave observatories such as\nLISA, Taiji, and Tianqin. The fast and accurate parameter estimation of merging\nMBHBs is of great significance for the global fitting of all resolvable\nsources, as well as the astrophysical interpretation of gravitational wave\nsignals. However, such analyses usually entail significant computational costs.\nTo address these challenges, inspired by the latest progress in generative\nmodels, we explore the application of continuous normalizing flows (CNFs) on\nthe parameter estimation of MBHBs. Specifically, we employ linear interpolation\nand trig interpolation methods to construct transport paths for training CNFs.\nAdditionally, we creatively introduce a parameter transformation method based\non the symmetry in the detector's response function. This transformation is\nintegrated within CNFs, allowing us to train the model using a simplified\ndataset, and then perform parameter estimation on more general data, hence also\nacting as a crucial factor in improving the training speed. In conclusion, for\nthe first time, within a comprehensive and reasonable parameter range, we have\nachieved a complete and unbiased 11-dimensional rapid inference for MBHBs in\nthe presence of astrophysical confusion noise using CNFs. In the experiments\nbased on simulated data, our model produces posterior distributions comparable\nto those obtained by nested sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting the coalescences of massive black hole binaries (MBHBs) is one of\nthe primary targets for space-based gravitational wave observatories such as\nLISA, Taiji, and Tianqin. The fast and accurate parameter estimation of merging\nMBHBs is of great significance for the global fitting of all resolvable\nsources, as well as the astrophysical interpretation of gravitational wave\nsignals. However, such analyses usually entail significant computational costs.\nTo address these challenges, inspired by the latest progress in generative\nmodels, we explore the application of continuous normalizing flows (CNFs) on\nthe parameter estimation of MBHBs. Specifically, we employ linear interpolation\nand trig interpolation methods to construct transport paths for training CNFs.\nAdditionally, we creatively introduce a parameter transformation method based\non the symmetry in the detector's response function. This transformation is\nintegrated within CNFs, allowing us to train the model using a simplified\ndataset, and then perform parameter estimation on more general data, hence also\nacting as a crucial factor in improving the training speed. In conclusion, for\nthe first time, within a comprehensive and reasonable parameter range, we have\nachieved a complete and unbiased 11-dimensional rapid inference for MBHBs in\nthe presence of astrophysical confusion noise using CNFs. In the experiments\nbased on simulated data, our model produces posterior distributions comparable\nto those obtained by nested sampling."
                },
                "authors": [
                    {
                        "name": "Bo Liang"
                    },
                    {
                        "name": "Minghui Du"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Yuxiang Xu"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xiaotong Wei"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Li-e Qiang"
                    },
                    {
                        "name": "Ziren Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ziren Luo"
                },
                "author": "Ziren Luo",
                "arxiv_doi": "10.1088/2632-2153/ad8da9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/2632-2153/ad8da9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.07125v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07125v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04185v1",
                "updated": "2024-12-05T14:24:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    24,
                    7,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:24:07Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    24,
                    7,
                    3,
                    340,
                    0
                ],
                "title": "Leveraging Large Language Models to Generate Course-specific\n  Semantically Annotated Learning Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Generate Course-specific\n  Semantically Annotated Learning Objects"
                },
                "summary": "Background: Over the past few decades, the process and methodology of\nautomated question generation (AQG) have undergone significant transformations.\nRecent progress in generative natural language models has opened up new\npotential in the generation of educational content.\n  Objectives: This paper explores the potential of large language models (LLMs)\nfor generating computer science questions that are sufficiently annotated for\nautomatic learner model updates, are fully situated in the context of a\nparticular course, and address the cognitive dimension understand.\n  Methods: Unlike previous attempts that might use basic methods like ChatGPT,\nour approach involves more targeted strategies such as retrieval-augmented\ngeneration (RAG) to produce contextually relevant and pedagogically meaningful\nlearning objects.\n  Results and Conclusions: Our results show that generating structural,\nsemantic annotations works well. However, this success was not reflected in the\ncase of relational annotations. The quality of the generated questions often\ndid not meet educational standards, highlighting that although LLMs can\ncontribute to the pool of learning materials, their current level of\nperformance requires significant human intervention to refine and validate the\ngenerated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Over the past few decades, the process and methodology of\nautomated question generation (AQG) have undergone significant transformations.\nRecent progress in generative natural language models has opened up new\npotential in the generation of educational content.\n  Objectives: This paper explores the potential of large language models (LLMs)\nfor generating computer science questions that are sufficiently annotated for\nautomatic learner model updates, are fully situated in the context of a\nparticular course, and address the cognitive dimension understand.\n  Methods: Unlike previous attempts that might use basic methods like ChatGPT,\nour approach involves more targeted strategies such as retrieval-augmented\ngeneration (RAG) to produce contextually relevant and pedagogically meaningful\nlearning objects.\n  Results and Conclusions: Our results show that generating structural,\nsemantic annotations works well. However, this success was not reflected in the\ncase of relational annotations. The quality of the generated questions often\ndid not meet educational standards, highlighting that although LLMs can\ncontribute to the pool of learning materials, their current level of\nperformance requires significant human intervention to refine and validate the\ngenerated content."
                },
                "authors": [
                    {
                        "name": "Dominic Lohr"
                    },
                    {
                        "name": "Marc Berges"
                    },
                    {
                        "name": "Abhishek Chugh"
                    },
                    {
                        "name": "Michael Kohlhase"
                    },
                    {
                        "name": "Dennis Mller"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Mller"
                },
                "author": "Dennis Mller",
                "arxiv_comment": "Accepted at Journal of Computer Assisted Learning (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04180v1",
                "updated": "2024-12-05T14:19:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    19,
                    59,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:19:59Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    19,
                    59,
                    3,
                    340,
                    0
                ],
                "title": "SKIM: Any-bit Quantization Pushing The Limits of Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKIM: Any-bit Quantization Pushing The Limits of Post-Training\n  Quantization"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive performance across various\ntasks, but deploying them for inference poses challenges. Their high resource\ndemands often necessitate complex, costly multi-GPU pipelines, or the use of\nsmaller, less capable models. While quantization offers a promising solution\nutilizing lower precision for model storage, existing methods frequently\nexperience significant performance drops at lower precision levels.\nAdditionally, they typically provide only a limited set of solutions at\nspecific bit levels, many of which are extensively manually tuned. To address\nthese challenges, we propose a new method called SKIM: Scaled K-means\nclustering wIth Mixed precision. Our approach introduces two novel techniques:\n1. A greedy algorithm to solve approximately optimal bit allocation across\nweight channels, and 2. A trainable scaling vector for non-differentiable\nK-means clustering. These techniques substantially improve performance and can\nbe adapted to any given bit. Notably, in terms of model perplexity, our method\nnarrows the gap between 3-bit quantized LLaMA models and their full precision\ncounterparts by 16.3% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive performance across various\ntasks, but deploying them for inference poses challenges. Their high resource\ndemands often necessitate complex, costly multi-GPU pipelines, or the use of\nsmaller, less capable models. While quantization offers a promising solution\nutilizing lower precision for model storage, existing methods frequently\nexperience significant performance drops at lower precision levels.\nAdditionally, they typically provide only a limited set of solutions at\nspecific bit levels, many of which are extensively manually tuned. To address\nthese challenges, we propose a new method called SKIM: Scaled K-means\nclustering wIth Mixed precision. Our approach introduces two novel techniques:\n1. A greedy algorithm to solve approximately optimal bit allocation across\nweight channels, and 2. A trainable scaling vector for non-differentiable\nK-means clustering. These techniques substantially improve performance and can\nbe adapted to any given bit. Notably, in terms of model perplexity, our method\nnarrows the gap between 3-bit quantized LLaMA models and their full precision\ncounterparts by 16.3% on average."
                },
                "authors": [
                    {
                        "name": "Runsheng Bai"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Bo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Liu"
                },
                "author": "Bo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04177v1",
                "updated": "2024-12-05T14:17:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    17,
                    16,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:17:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    17,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "Fixed-Mean Gaussian Processes for Post-hoc Bayesian Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixed-Mean Gaussian Processes for Post-hoc Bayesian Deep Learning"
                },
                "summary": "Recently, there has been an increasing interest in performing post-hoc\nuncertainty estimation about the predictions of pre-trained deep neural\nnetworks (DNNs). Given a pre-trained DNN via back-propagation, these methods\nenhance the original network by adding output confidence measures, such as\nerror bars, without compromising its initial accuracy. In this context, we\nintroduce a novel family of sparse variational Gaussian processes (GPs), where\nthe posterior mean is fixed to any continuous function when using a universal\nkernel. Specifically, we fix the mean of this GP to the output of the\npre-trained DNN, allowing our approach to effectively fit the GP's predictive\nvariances to estimate the DNN prediction uncertainty. Our approach leverages\nvariational inference (VI) for efficient stochastic optimization, with training\ncosts that remain independent of the number of training points, scaling\nefficiently to large datasets such as ImageNet. The proposed method, called\nfixed mean GP (FMGP), is architecture-agnostic, relying solely on the\npre-trained model's outputs to adjust the predictive variances. Experimental\nresults demonstrate that FMGP improves both uncertainty estimation and\ncomputational efficiency when compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been an increasing interest in performing post-hoc\nuncertainty estimation about the predictions of pre-trained deep neural\nnetworks (DNNs). Given a pre-trained DNN via back-propagation, these methods\nenhance the original network by adding output confidence measures, such as\nerror bars, without compromising its initial accuracy. In this context, we\nintroduce a novel family of sparse variational Gaussian processes (GPs), where\nthe posterior mean is fixed to any continuous function when using a universal\nkernel. Specifically, we fix the mean of this GP to the output of the\npre-trained DNN, allowing our approach to effectively fit the GP's predictive\nvariances to estimate the DNN prediction uncertainty. Our approach leverages\nvariational inference (VI) for efficient stochastic optimization, with training\ncosts that remain independent of the number of training points, scaling\nefficiently to large datasets such as ImageNet. The proposed method, called\nfixed mean GP (FMGP), is architecture-agnostic, relying solely on the\npre-trained model's outputs to adjust the predictive variances. Experimental\nresults demonstrate that FMGP improves both uncertainty estimation and\ncomputational efficiency when compared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Luis A. Ortega"
                    },
                    {
                        "name": "Simn Rodrguez-Santana"
                    },
                    {
                        "name": "Daniel Hernndez-Lobato"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Hernndez-Lobato"
                },
                "author": "Daniel Hernndez-Lobato",
                "arxiv_comment": "12 pages, 6 figures and 2 tables. Submitted to IEEE TRANSACTIONS ON\n  PATTERN ANALYSIS AND MACHINE INTELLIGENCE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16105v2",
                "updated": "2024-12-05T14:16:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    16,
                    57,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-25T05:32:34Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    5,
                    32,
                    34,
                    0,
                    330,
                    0
                ],
                "title": "Adaptive Circuit Behavior and Generalization in Mechanistic\n  Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Circuit Behavior and Generalization in Mechanistic\n  Interpretability"
                },
                "summary": "Mechanistic interpretability aims to understand the inner workings of large\nneural networks by identifying circuits, or minimal subgraphs within the model\nthat implement algorithms responsible for performing specific tasks. These\ncircuits are typically discovered and analyzed using a narrowly defined prompt\nformat. However, given the abilities of large language models (LLMs) to\ngeneralize across various prompt formats for the same task, it remains unclear\nhow well these circuits generalize. For instance, it is unclear whether the\nmodels generalization results from reusing the same circuit components, the\ncomponents behaving differently, or the use of entirely different components.\nIn this paper, we investigate the generality of the indirect object\nidentification (IOI) circuit in GPT-2 small, which is well-studied and believed\nto implement a simple, interpretable algorithm. We evaluate its performance on\nprompt variants that challenge the assumptions of this algorithm. Our findings\nreveal that the circuit generalizes surprisingly well, reusing all of its\ncomponents and mechanisms while only adding additional input edges. Notably,\nthe circuit generalizes even to prompt variants where the original algorithm\nshould fail; we discover a mechanism that explains this which we term S2\nHacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic interpretability aims to understand the inner workings of large\nneural networks by identifying circuits, or minimal subgraphs within the model\nthat implement algorithms responsible for performing specific tasks. These\ncircuits are typically discovered and analyzed using a narrowly defined prompt\nformat. However, given the abilities of large language models (LLMs) to\ngeneralize across various prompt formats for the same task, it remains unclear\nhow well these circuits generalize. For instance, it is unclear whether the\nmodels generalization results from reusing the same circuit components, the\ncomponents behaving differently, or the use of entirely different components.\nIn this paper, we investigate the generality of the indirect object\nidentification (IOI) circuit in GPT-2 small, which is well-studied and believed\nto implement a simple, interpretable algorithm. We evaluate its performance on\nprompt variants that challenge the assumptions of this algorithm. Our findings\nreveal that the circuit generalizes surprisingly well, reusing all of its\ncomponents and mechanisms while only adding additional input edges. Notably,\nthe circuit generalizes even to prompt variants where the original algorithm\nshould fail; we discover a mechanism that explains this which we term S2\nHacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these\nmodels."
                },
                "authors": [
                    {
                        "name": "Jatin Nainani"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "AJ Yeung"
                    },
                    {
                        "name": "Kartik Gupta"
                    },
                    {
                        "name": "David Jensen"
                    }
                ],
                "author_detail": {
                    "name": "David Jensen"
                },
                "author": "David Jensen",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04167v1",
                "updated": "2024-12-05T14:03:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    3,
                    41,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:03:41Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    3,
                    41,
                    3,
                    340,
                    0
                ],
                "title": "Bench-CoE: a Framework for Collaboration of Experts from Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bench-CoE: a Framework for Collaboration of Experts from Benchmark"
                },
                "summary": "Large Language Models (LLMs) are key technologies driving intelligent systems\nto handle multiple tasks. To meet the demands of various tasks, an increasing\nnumber of LLMs-driven experts with diverse capabilities have been developed,\naccompanied by corresponding benchmarks to evaluate their performance. This\npaper proposes the Bench-CoE framework, which enables Collaboration of Experts\n(CoE) by effectively leveraging benchmark evaluations to achieve optimal\nperformance across various tasks. Bench-CoE includes a set of expert models, a\nrouter for assigning tasks to corresponding experts, and a benchmark dataset\nfor training the router. Moreover, we formulate Query-Level and Subject-Level\napproaches based on our framework, and analyze the merits and drawbacks of\nthese two approaches. Finally, we conduct a series of experiments with vary\ndata distributions on both language and multimodal tasks to validate that our\nproposed Bench-CoE outperforms any single model in terms of overall\nperformance. We hope this method serves as a baseline for further research in\nthis area. The code is available at\n\\url{https://github.com/ZhangXJ199/Bench-CoE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are key technologies driving intelligent systems\nto handle multiple tasks. To meet the demands of various tasks, an increasing\nnumber of LLMs-driven experts with diverse capabilities have been developed,\naccompanied by corresponding benchmarks to evaluate their performance. This\npaper proposes the Bench-CoE framework, which enables Collaboration of Experts\n(CoE) by effectively leveraging benchmark evaluations to achieve optimal\nperformance across various tasks. Bench-CoE includes a set of expert models, a\nrouter for assigning tasks to corresponding experts, and a benchmark dataset\nfor training the router. Moreover, we formulate Query-Level and Subject-Level\napproaches based on our framework, and analyze the merits and drawbacks of\nthese two approaches. Finally, we conduct a series of experiments with vary\ndata distributions on both language and multimodal tasks to validate that our\nproposed Bench-CoE outperforms any single model in terms of overall\nperformance. We hope this method serves as a baseline for further research in\nthis area. The code is available at\n\\url{https://github.com/ZhangXJ199/Bench-CoE}."
                },
                "authors": [
                    {
                        "name": "Yuanshuai Wang"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Jinkun Zhao"
                    },
                    {
                        "name": "Siwei Wen"
                    },
                    {
                        "name": "Peilin Feng"
                    },
                    {
                        "name": "Shuhao Liao"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Wenjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Wu"
                },
                "author": "Wenjun Wu",
                "arxiv_comment": "The code is available at\n  \\url{https://github.com/ZhangXJ199/Bench-CoE}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04150v1",
                "updated": "2024-12-05T13:25:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    25,
                    8,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T13:25:08Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    25,
                    8,
                    3,
                    340,
                    0
                ],
                "title": "Uncovering Hidden Variables: A Physics Classroom Activity on Correlation\n  and Causation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Hidden Variables: A Physics Classroom Activity on Correlation\n  and Causation"
                },
                "summary": "One of the main goals of physics education is to develop in students the\nability to think critically about data and scientific models. This skill is key\nin everyday life, as it allows one to interpret data accurately, evaluate\nwhether conclusions are based on evidence, and distinguish between meaningful\npatterns and random occurrences. Although \"correlation does not imply\ncausation\" it is common to make mistakes when interpreting relationships\nbetween variables, either due to bias or the human tendency to seek causal\nexplanations. The history of science is filled with examples where causality\nwas incorrectly inferred from correlation. In today's society, with the rise of\nfake news, misinterpretation, and data manipulation, educating students about\nscientific reasoning is more important than ever. In this paper, we present an\nactivity aimed at high school physics students as part of an introductory\nmodule on scientific work. The learning objectives of this activity are to\nadequately plot a set of data, to extract relevant information from these data,\nand to clearly distinguish between correlation and causation. A brief overview\nof some types of correlation that do not imply causation follows, followed by a\ndescription of the activity, the results obtained, and finally the conclusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the main goals of physics education is to develop in students the\nability to think critically about data and scientific models. This skill is key\nin everyday life, as it allows one to interpret data accurately, evaluate\nwhether conclusions are based on evidence, and distinguish between meaningful\npatterns and random occurrences. Although \"correlation does not imply\ncausation\" it is common to make mistakes when interpreting relationships\nbetween variables, either due to bias or the human tendency to seek causal\nexplanations. The history of science is filled with examples where causality\nwas incorrectly inferred from correlation. In today's society, with the rise of\nfake news, misinterpretation, and data manipulation, educating students about\nscientific reasoning is more important than ever. In this paper, we present an\nactivity aimed at high school physics students as part of an introductory\nmodule on scientific work. The learning objectives of this activity are to\nadequately plot a set of data, to extract relevant information from these data,\nand to clearly distinguish between correlation and causation. A brief overview\nof some types of correlation that do not imply causation follows, followed by a\ndescription of the activity, the results obtained, and finally the conclusions."
                },
                "authors": [
                    {
                        "name": "Alvaro Suarez"
                    },
                    {
                        "name": "Marcelo Vachetta"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Vachetta"
                },
                "author": "Marcelo Vachetta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04149v1",
                "updated": "2024-12-05T13:23:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    23,
                    6,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T13:23:06Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    23,
                    6,
                    3,
                    340,
                    0
                ],
                "title": "Frequency-Adaptive Low-Latency Object Detection Using Events and Frames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-Adaptive Low-Latency Object Detection Using Events and Frames"
                },
                "summary": "Fusing Events and RGB images for object detection leverages the robustness of\nEvent cameras in adverse environments and the rich semantic information\nprovided by RGB cameras. However, two critical mismatches: low-latency Events\n\\textit{vs.}~high-latency RGB frames; temporally sparse labels in training\n\\textit{vs.}~continuous flow in inference, significantly hinder the\nhigh-frequency fusion-based object detection. To address these challenges, we\npropose the \\textbf{F}requency-\\textbf{A}daptive Low-Latency \\textbf{O}bject\n\\textbf{D}etector (FAOD). FAOD aligns low-frequency RGB frames with\nhigh-frequency Events through an Align Module, which reinforces cross-modal\nstyle and spatial proximity to address the Event-RGB Mismatch. We further\npropose a training strategy, Time Shift, which enforces the module to align the\nprediction from temporally shifted Event-RGB pairs and their original\nrepresentation, that is, consistent with Event-aligned annotations. This\nstrategy enables the network to use high-frequency Event data as the primary\nreference while treating low-frequency RGB images as supplementary information,\nretaining the low-latency nature of the Event stream toward high-frequency\ndetection. Furthermore, we observe that these corrected Event-RGB pairs\ndemonstrate better generalization from low training frequency to higher\ninference frequencies compared to using Event data alone. Extensive experiments\non the PKU-DAVIS-SOD and DSEC-Detection datasets demonstrate that our FAOD\nachieves SOTA performance. Specifically, in the PKU-DAVIS-SOD Dataset, FAOD\nachieves 9.8 points improvement in terms of the mAP in fully paired Event-RGB\ndata with only a quarter of the parameters compared to SODFormer, and even\nmaintains robust performance (only a 3 points drop in mAP) under 80$\\times$\nEvent-RGB frequency mismatch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Events and RGB images for object detection leverages the robustness of\nEvent cameras in adverse environments and the rich semantic information\nprovided by RGB cameras. However, two critical mismatches: low-latency Events\n\\textit{vs.}~high-latency RGB frames; temporally sparse labels in training\n\\textit{vs.}~continuous flow in inference, significantly hinder the\nhigh-frequency fusion-based object detection. To address these challenges, we\npropose the \\textbf{F}requency-\\textbf{A}daptive Low-Latency \\textbf{O}bject\n\\textbf{D}etector (FAOD). FAOD aligns low-frequency RGB frames with\nhigh-frequency Events through an Align Module, which reinforces cross-modal\nstyle and spatial proximity to address the Event-RGB Mismatch. We further\npropose a training strategy, Time Shift, which enforces the module to align the\nprediction from temporally shifted Event-RGB pairs and their original\nrepresentation, that is, consistent with Event-aligned annotations. This\nstrategy enables the network to use high-frequency Event data as the primary\nreference while treating low-frequency RGB images as supplementary information,\nretaining the low-latency nature of the Event stream toward high-frequency\ndetection. Furthermore, we observe that these corrected Event-RGB pairs\ndemonstrate better generalization from low training frequency to higher\ninference frequencies compared to using Event data alone. Extensive experiments\non the PKU-DAVIS-SOD and DSEC-Detection datasets demonstrate that our FAOD\nachieves SOTA performance. Specifically, in the PKU-DAVIS-SOD Dataset, FAOD\nachieves 9.8 points improvement in terms of the mAP in fully paired Event-RGB\ndata with only a quarter of the parameters compared to SODFormer, and even\nmaintains robust performance (only a 3 points drop in mAP) under 80$\\times$\nEvent-RGB frequency mismatch."
                },
                "authors": [
                    {
                        "name": "Haitian Zhang"
                    },
                    {
                        "name": "Xiangyuan Wang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Xinya Wang"
                    },
                    {
                        "name": "Fang Xu"
                    },
                    {
                        "name": "Huai Yu"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Wen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Yang"
                },
                "author": "Wen Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04147v1",
                "updated": "2024-12-05T13:19:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    19,
                    34,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T13:19:34Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    19,
                    34,
                    3,
                    340,
                    0
                ],
                "title": "MultiTASC++: A Continuously Adaptive Scheduler for Edge-Based\n  Multi-Device Cascade Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiTASC++: A Continuously Adaptive Scheduler for Edge-Based\n  Multi-Device Cascade Inference"
                },
                "summary": "Cascade systems, consisting of a lightweight model processing all samples and\na heavier, high-accuracy model refining challenging samples, have become a\nwidely-adopted distributed inference approach to achieving high accuracy and\nmaintaining a low computational burden for mobile and IoT devices. As\nintelligent indoor environments, like smart homes, continue to expand, a new\nscenario emerges, the multi-device cascade. In this setting, multiple diverse\ndevices simultaneously utilize a shared heavy model hosted on a server, often\nsituated within or close to the consumer environment. This work introduces\nMultiTASC++, a continuously adaptive multi-tenancy-aware scheduler that\ndynamically controls the forwarding decision functions of devices to optimize\nsystem throughput while maintaining high accuracy and low latency. Through\nextensive experimentation in diverse device environments and with varying\nserver-side models, we demonstrate the scheduler's efficacy in consistently\nmaintaining a targeted satisfaction rate while providing the highest available\naccuracy across different device tiers and workloads of up to 100 devices. This\ndemonstrates its scalability and efficiency in addressing the unique challenges\nof collaborative DNN inference in dynamic and diverse IoT environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade systems, consisting of a lightweight model processing all samples and\na heavier, high-accuracy model refining challenging samples, have become a\nwidely-adopted distributed inference approach to achieving high accuracy and\nmaintaining a low computational burden for mobile and IoT devices. As\nintelligent indoor environments, like smart homes, continue to expand, a new\nscenario emerges, the multi-device cascade. In this setting, multiple diverse\ndevices simultaneously utilize a shared heavy model hosted on a server, often\nsituated within or close to the consumer environment. This work introduces\nMultiTASC++, a continuously adaptive multi-tenancy-aware scheduler that\ndynamically controls the forwarding decision functions of devices to optimize\nsystem throughput while maintaining high accuracy and low latency. Through\nextensive experimentation in diverse device environments and with varying\nserver-side models, we demonstrate the scheduler's efficacy in consistently\nmaintaining a targeted satisfaction rate while providing the highest available\naccuracy across different device tiers and workloads of up to 100 devices. This\ndemonstrates its scalability and efficiency in addressing the unique challenges\nof collaborative DNN inference in dynamic and diverse IoT environments."
                },
                "authors": [
                    {
                        "name": "Sokratis Nikolaidis"
                    },
                    {
                        "name": "Stylianos I. Venieris"
                    },
                    {
                        "name": "Iakovos S. Venieris"
                    }
                ],
                "author_detail": {
                    "name": "Iakovos S. Venieris"
                },
                "author": "Iakovos S. Venieris",
                "arxiv_doi": "10.52953/TBYB6219",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.52953/TBYB6219",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.04147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "ITU Journal on Future and Evolving Technologies, Volume 5 (2024),\n  Issue 1, Pages 26-46",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14123v2",
                "updated": "2024-12-05T13:15:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    15,
                    34,
                    3,
                    340,
                    0
                ],
                "published": "2024-02-21T20:43:49Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    20,
                    43,
                    49,
                    2,
                    52,
                    0
                ],
                "title": "DeiSAM: Segment Anything with Deictic Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeiSAM: Segment Anything with Deictic Prompting"
                },
                "summary": "Large-scale, pre-trained neural networks have demonstrated strong\ncapabilities in various tasks, including zero-shot image segmentation. To\nidentify concrete objects in complex scenes, humans instinctively rely on\ndeictic descriptions in natural language, i.e., referring to something\ndepending on the context such as \"The object that is on the desk and behind the\ncup.\". However, deep learning approaches cannot reliably interpret such deictic\nrepresentations due to their lack of reasoning capabilities in complex\nscenarios. To remedy this issue, we propose DeiSAM -- a combination of large\npre-trained neural networks with differentiable logic reasoners -- for deictic\npromptable segmentation. Given a complex, textual segmentation description,\nDeiSAM leverages Large Language Models (LLMs) to generate first-order logic\nrules and performs differentiable forward reasoning on generated scene graphs.\nSubsequently, DeiSAM segments objects by matching them to the logically\ninferred image regions. As part of our evaluation, we propose the Deictic\nVisual Genome (DeiVG) dataset, containing paired visual input and complex,\ndeictic textual prompts. Our empirical results demonstrate that DeiSAM is a\nsubstantial improvement over purely data-driven baselines for deictic\npromptable segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale, pre-trained neural networks have demonstrated strong\ncapabilities in various tasks, including zero-shot image segmentation. To\nidentify concrete objects in complex scenes, humans instinctively rely on\ndeictic descriptions in natural language, i.e., referring to something\ndepending on the context such as \"The object that is on the desk and behind the\ncup.\". However, deep learning approaches cannot reliably interpret such deictic\nrepresentations due to their lack of reasoning capabilities in complex\nscenarios. To remedy this issue, we propose DeiSAM -- a combination of large\npre-trained neural networks with differentiable logic reasoners -- for deictic\npromptable segmentation. Given a complex, textual segmentation description,\nDeiSAM leverages Large Language Models (LLMs) to generate first-order logic\nrules and performs differentiable forward reasoning on generated scene graphs.\nSubsequently, DeiSAM segments objects by matching them to the logically\ninferred image regions. As part of our evaluation, we propose the Deictic\nVisual Genome (DeiVG) dataset, containing paired visual input and complex,\ndeictic textual prompts. Our empirical results demonstrate that DeiSAM is a\nsubstantial improvement over purely data-driven baselines for deictic\npromptable segmentation."
                },
                "authors": [
                    {
                        "name": "Hikaru Shindo"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Gopika Sudhakaran"
                    },
                    {
                        "name": "Devendra Singh Dhami"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04141v1",
                "updated": "2024-12-05T13:10:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    10,
                    54,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T13:10:54Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    10,
                    54,
                    3,
                    340,
                    0
                ],
                "title": "Reducing Tool Hallucination via Reliability Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Tool Hallucination via Reliability Alignment"
                },
                "summary": "Large Language Models (LLMs) have extended their capabilities beyond language\ngeneration to interact with external systems through tool calling, offering\npowerful potential for real-world applications. However, the phenomenon of tool\nhallucinations, which occur when models improperly select or misuse tools,\npresents critical challenges that can lead to flawed task execution and\nincreased operational costs. This paper investigates the concept of reliable\ntool calling and highlights the necessity of addressing tool hallucinations. We\nsystematically categorize tool hallucinations into two main types: tool\nselection hallucination and tool usage hallucination. To mitigate these issues,\nwe propose a reliability-focused alignment framework that enhances the model's\nability to accurately assess tool relevance and usage. By proposing a suite of\nevaluation metrics and evaluating on StableToolBench, we further demonstrate\nthe effectiveness of our framework in mitigating tool hallucination and\nimproving the overall system reliability of LLM tool calling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have extended their capabilities beyond language\ngeneration to interact with external systems through tool calling, offering\npowerful potential for real-world applications. However, the phenomenon of tool\nhallucinations, which occur when models improperly select or misuse tools,\npresents critical challenges that can lead to flawed task execution and\nincreased operational costs. This paper investigates the concept of reliable\ntool calling and highlights the necessity of addressing tool hallucinations. We\nsystematically categorize tool hallucinations into two main types: tool\nselection hallucination and tool usage hallucination. To mitigate these issues,\nwe propose a reliability-focused alignment framework that enhances the model's\nability to accurately assess tool relevance and usage. By proposing a suite of\nevaluation metrics and evaluating on StableToolBench, we further demonstrate\nthe effectiveness of our framework in mitigating tool hallucination and\nimproving the overall system reliability of LLM tool calling."
                },
                "authors": [
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Hang Zheng"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Ruisheng Cao"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04139v1",
                "updated": "2024-12-05T13:06:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    6,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T13:06:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    6,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "Monet: Mixture of Monosemantic Experts for Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monet: Mixture of Monosemantic Experts for Transformers"
                },
                "summary": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance} mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust} model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance} mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust} model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet."
                },
                "authors": [
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Young Jin Ahn"
                    },
                    {
                        "name": "Kee-Eung Kim"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02785v2",
                "updated": "2024-12-05T12:58:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    58,
                    44,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-05T03:51:13Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    51,
                    13,
                    1,
                    310,
                    0
                ],
                "title": "Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM\n  Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM\n  Safety Alignment"
                },
                "summary": "Safety alignment of Large Language Models (LLMs) has recently become a\ncritical objective of model developers. In response, a growing body of work has\nbeen investigating how safety alignment can be bypassed through various\njailbreaking methods, such as adversarial attacks. However, these jailbreak\nmethods can be rather costly or involve a non-trivial amount of creativity and\neffort, introducing the assumption that malicious users are high-resource or\nsophisticated. In this paper, we study how simple random augmentations to the\ninput prompt affect safety alignment effectiveness in state-of-the-art LLMs,\nsuch as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different\nmodels and investigate the intersection of safety under random augmentations\nwith multiple dimensions: augmentation type, model size, quantization,\nfine-tuning-based defenses, and decoding strategies (e.g., sampling\ntemperature). We show that low-resource and unsophisticated attackers, i.e.\n$\\textit{stochastic monkeys}$, can significantly improve their chances of\nbypassing alignment with just 25 random augmentations per prompt. Source code\nand data: https://github.com/uiuc-focal-lab/stochastic-monkeys/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment of Large Language Models (LLMs) has recently become a\ncritical objective of model developers. In response, a growing body of work has\nbeen investigating how safety alignment can be bypassed through various\njailbreaking methods, such as adversarial attacks. However, these jailbreak\nmethods can be rather costly or involve a non-trivial amount of creativity and\neffort, introducing the assumption that malicious users are high-resource or\nsophisticated. In this paper, we study how simple random augmentations to the\ninput prompt affect safety alignment effectiveness in state-of-the-art LLMs,\nsuch as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different\nmodels and investigate the intersection of safety under random augmentations\nwith multiple dimensions: augmentation type, model size, quantization,\nfine-tuning-based defenses, and decoding strategies (e.g., sampling\ntemperature). We show that low-resource and unsophisticated attackers, i.e.\n$\\textit{stochastic monkeys}$, can significantly improve their chances of\nbypassing alignment with just 25 random augmentations per prompt. Source code\nand data: https://github.com/uiuc-focal-lab/stochastic-monkeys/"
                },
                "authors": [
                    {
                        "name": "Jason Vega"
                    },
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Gaokai Zhang"
                    },
                    {
                        "name": "Hangoo Kang"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "arxiv_comment": "v2: Updated with changes from peer review rebuttal. v1: Version under\n  peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04134v1",
                "updated": "2024-12-05T12:58:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    58,
                    30,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T12:58:30Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    58,
                    30,
                    3,
                    340,
                    0
                ],
                "title": "Compositional Generative Multiphysics and Multi-component Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Generative Multiphysics and Multi-component Simulation"
                },
                "summary": "Multiphysics simulation, which models the interactions between multiple\nphysical processes, and multi-component simulation of complex structures are\ncritical in fields like nuclear and aerospace engineering. Previous studies\noften rely on numerical solvers or machine learning-based surrogate models to\nsolve or accelerate these simulations. However, multiphysics simulations\ntypically require integrating multiple specialized solvers-each responsible for\nevolving a specific physical process-into a coupled program, which introduces\nsignificant development challenges. Furthermore, no universal algorithm exists\nfor multi-component simulations, which adds to the complexity. Here we propose\ncompositional Multiphysics and Multi-component Simulation with Diffusion models\n(MultiSimDiff) to overcome these challenges. During diffusion-based training,\nMultiSimDiff learns energy functions modeling the conditional probability of\none physical process/component conditioned on other processes/components. In\ninference, MultiSimDiff generates coupled multiphysics solutions and\nmulti-component structures by sampling from the joint probability distribution,\nachieved by composing the learned energy functions in a structured way. We test\nour method in three tasks. In the reaction-diffusion and nuclear thermal\ncoupling problems, MultiSimDiff successfully predicts the coupling solution\nusing decoupled data, while the surrogate model fails in the more complex\nsecond problem. For the thermal and mechanical analysis of the prismatic fuel\nelement, MultiSimDiff trained for single component prediction accurately\npredicts a larger structure with 64 components, reducing the relative error by\n40.3% compared to the surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiphysics simulation, which models the interactions between multiple\nphysical processes, and multi-component simulation of complex structures are\ncritical in fields like nuclear and aerospace engineering. Previous studies\noften rely on numerical solvers or machine learning-based surrogate models to\nsolve or accelerate these simulations. However, multiphysics simulations\ntypically require integrating multiple specialized solvers-each responsible for\nevolving a specific physical process-into a coupled program, which introduces\nsignificant development challenges. Furthermore, no universal algorithm exists\nfor multi-component simulations, which adds to the complexity. Here we propose\ncompositional Multiphysics and Multi-component Simulation with Diffusion models\n(MultiSimDiff) to overcome these challenges. During diffusion-based training,\nMultiSimDiff learns energy functions modeling the conditional probability of\none physical process/component conditioned on other processes/components. In\ninference, MultiSimDiff generates coupled multiphysics solutions and\nmulti-component structures by sampling from the joint probability distribution,\nachieved by composing the learned energy functions in a structured way. We test\nour method in three tasks. In the reaction-diffusion and nuclear thermal\ncoupling problems, MultiSimDiff successfully predicts the coupling solution\nusing decoupled data, while the surrogate model fails in the more complex\nsecond problem. For the thermal and mechanical analysis of the prismatic fuel\nelement, MultiSimDiff trained for single component prediction accurately\npredicts a larger structure with 64 components, reducing the relative error by\n40.3% compared to the surrogate model."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zhenhai Liu"
                    },
                    {
                        "name": "Feipeng Qi"
                    },
                    {
                        "name": "Yongjun Jiao"
                    },
                    {
                        "name": "Tailin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tailin Wu"
                },
                "author": "Tailin Wu",
                "arxiv_comment": "30pages,13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03906v2",
                "updated": "2024-12-05T12:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    56,
                    40,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-06T13:37:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    37,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System"
                },
                "summary": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality."
                },
                "authors": [
                    {
                        "name": "David Maria Schmidt"
                    },
                    {
                        "name": "Mohammad Fazleh Elahi"
                    },
                    {
                        "name": "Philipp Cimiano"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Cimiano"
                },
                "author": "Philipp Cimiano",
                "arxiv_doi": "10.1007/978-3-031-77792-9_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-77792-9_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24th International Conference on Knowledge Engineering and Knowledge\n  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03324v2",
                "updated": "2024-12-05T12:52:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    52,
                    31,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-04T13:56:44Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    56,
                    44,
                    2,
                    339,
                    0
                ],
                "title": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for\n  Accelerating Large VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for\n  Accelerating Large VLMs"
                },
                "summary": "Vision-language models (VLMs) have shown remarkable success across various\nmulti-modal tasks, yet large VLMs encounter significant efficiency challenges\ndue to processing numerous visual tokens. A promising approach to accelerating\nlarge VLM inference is using partial information, such as attention maps from\nspecific layers, to assess token importance and prune less essential tokens.\nHowever, our study reveals three key insights: (i) Partial attention\ninformation is insufficient for accurately identifying critical visual tokens,\nresulting in suboptimal performance, especially at low token retention ratios;\n(ii) Global attention information, such as the attention map aggregated across\nall layers, more effectively preserves essential tokens and maintains\ncomparable performance under aggressive pruning. However, the attention maps\nfrom all layers requires a full inference pass, which increases computational\nload and is therefore impractical in existing methods; and (iii) The global\nattention map aggregated from a small VLM closely resembles that of a large\nVLM, suggesting an efficient alternative. Based on these findings, we introduce\na \\textbf{training-free} method, \\underline{\\textbf{S}}mall VLM\n\\underline{\\textbf{G}}uidance for accelerating \\underline{\\textbf{L}}arge VLMs\n(\\textbf{SGL}). Specifically, we employ the attention map aggregated from a\nsmall VLM to guide visual token pruning in a large VLM. Additionally, an early\nexiting mechanism is developed to fully use the small VLM's predictions,\ndynamically invoking the larger VLM only when necessary, yielding a superior\ntrade-off between accuracy and computation. Extensive evaluations across 11\nbenchmarks demonstrate the effectiveness and generalizability of SGL, achieving\nup to 91\\% pruning ratio for visual tokens while retaining competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have shown remarkable success across various\nmulti-modal tasks, yet large VLMs encounter significant efficiency challenges\ndue to processing numerous visual tokens. A promising approach to accelerating\nlarge VLM inference is using partial information, such as attention maps from\nspecific layers, to assess token importance and prune less essential tokens.\nHowever, our study reveals three key insights: (i) Partial attention\ninformation is insufficient for accurately identifying critical visual tokens,\nresulting in suboptimal performance, especially at low token retention ratios;\n(ii) Global attention information, such as the attention map aggregated across\nall layers, more effectively preserves essential tokens and maintains\ncomparable performance under aggressive pruning. However, the attention maps\nfrom all layers requires a full inference pass, which increases computational\nload and is therefore impractical in existing methods; and (iii) The global\nattention map aggregated from a small VLM closely resembles that of a large\nVLM, suggesting an efficient alternative. Based on these findings, we introduce\na \\textbf{training-free} method, \\underline{\\textbf{S}}mall VLM\n\\underline{\\textbf{G}}uidance for accelerating \\underline{\\textbf{L}}arge VLMs\n(\\textbf{SGL}). Specifically, we employ the attention map aggregated from a\nsmall VLM to guide visual token pruning in a large VLM. Additionally, an early\nexiting mechanism is developed to fully use the small VLM's predictions,\ndynamically invoking the larger VLM only when necessary, yielding a superior\ntrade-off between accuracy and computation. Extensive evaluations across 11\nbenchmarks demonstrate the effectiveness and generalizability of SGL, achieving\nup to 91\\% pruning ratio for visual tokens while retaining competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Yibing Song"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04121v1",
                "updated": "2024-12-05T12:46:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    46,
                    18,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T12:46:18Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    46,
                    18,
                    3,
                    340,
                    0
                ],
                "title": "DeepFEA: Deep Learning for Prediction of Transient Finite Element\n  Analysis Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepFEA: Deep Learning for Prediction of Transient Finite Element\n  Analysis Solutions"
                },
                "summary": "Finite Element Analysis (FEA) is a powerful but computationally intensive\nmethod for simulating physical phenomena. Recent advancements in machine\nlearning have led to surrogate models capable of accelerating FEA. Yet there\nare still limitations in developing surrogates of transient FEA models that can\nsimultaneously predict the solutions for both nodes and elements with\napplicability on both the 2D and 3D domains. Motivated by this research gap,\nthis study proposes DeepFEA, a deep learning-based framework that leverages a\nmultilayer Convolutional Long Short-Term Memory (ConvLSTM) network branching\ninto two parallel convolutional neural networks to predict the solutions for\nboth nodes and elements of FEA models. The proposed network is optimized using\na novel adaptive learning algorithm, called Node-Element Loss Optimization\n(NELO). NELO minimizes the error occurring at both branches of the network\nenabling the prediction of solutions for transient FEA simulations. The\nexperimental evaluation of DeepFEA is performed on three datasets in the\ncontext of structural mechanics, generated to serve as publicly available\nreference datasets. The results show that DeepFEA can achieve less than 3%\nnormalized mean and root mean squared error for 2D and 3D simulation scenarios,\nand inference times that are two orders of magnitude faster than FEA. In\ncontrast, relevant state-of-the-art methods face challenges with\nmulti-dimensional output and dynamic input prediction. Furthermore, DeepFEA's\nrobustness was demonstrated in a real-life biomedical scenario, confirming its\nsuitability for accurate and efficient predictions of FEA simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite Element Analysis (FEA) is a powerful but computationally intensive\nmethod for simulating physical phenomena. Recent advancements in machine\nlearning have led to surrogate models capable of accelerating FEA. Yet there\nare still limitations in developing surrogates of transient FEA models that can\nsimultaneously predict the solutions for both nodes and elements with\napplicability on both the 2D and 3D domains. Motivated by this research gap,\nthis study proposes DeepFEA, a deep learning-based framework that leverages a\nmultilayer Convolutional Long Short-Term Memory (ConvLSTM) network branching\ninto two parallel convolutional neural networks to predict the solutions for\nboth nodes and elements of FEA models. The proposed network is optimized using\na novel adaptive learning algorithm, called Node-Element Loss Optimization\n(NELO). NELO minimizes the error occurring at both branches of the network\nenabling the prediction of solutions for transient FEA simulations. The\nexperimental evaluation of DeepFEA is performed on three datasets in the\ncontext of structural mechanics, generated to serve as publicly available\nreference datasets. The results show that DeepFEA can achieve less than 3%\nnormalized mean and root mean squared error for 2D and 3D simulation scenarios,\nand inference times that are two orders of magnitude faster than FEA. In\ncontrast, relevant state-of-the-art methods face challenges with\nmulti-dimensional output and dynamic input prediction. Furthermore, DeepFEA's\nrobustness was demonstrated in a real-life biomedical scenario, confirming its\nsuitability for accurate and efficient predictions of FEA simulations."
                },
                "authors": [
                    {
                        "name": "Georgios Triantafyllou"
                    },
                    {
                        "name": "Panagiotis G. Kalozoumis"
                    },
                    {
                        "name": "George Dimas"
                    },
                    {
                        "name": "Dimitris K. Iakovidis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris K. Iakovidis"
                },
                "author": "Dimitris K. Iakovidis",
                "arxiv_comment": "This work has been submitted to a journal for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00311v2",
                "updated": "2024-12-05T12:42:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    42,
                    7,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-30T01:29:23Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    1,
                    29,
                    23,
                    5,
                    335,
                    0
                ],
                "title": "Characterizing the Effects of Environmental Exposures on Social\n  Mobility: Bayesian Semi-parametrics for Principal Stratification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Effects of Environmental Exposures on Social\n  Mobility: Bayesian Semi-parametrics for Principal Stratification"
                },
                "summary": "Principal stratification provides a robust causal inference framework for the\nadjustment of post-treatment variables when comparing the effects of a\ntreatment in health and social sciences. In this paper, we introduce a novel\nBayesian nonparametric model for principal stratification, leveraging the\ndependent Dirichlet process to flexibly model the distribution of potential\noutcomes. By incorporating confounders and potential outcomes for the\npost-treatment variable in the Bayesian mixture model for the final outcome,\nour approach improves the accuracy of missing data imputation and allows for\nthe characterization of treatment effects across strata defined based on the\nvalues of the post-treatment variable. We assess the performance of our method\nthrough a Monte Carlo simulation study where we compare the proposed method\nwith state-of-the-art Bayesian method in principal stratification. Finally, we\nleverage the proposed method to evaluate the principal causal effects of\nexposure to air pollution on social mobility in the US on strata defined by\neducational attainment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principal stratification provides a robust causal inference framework for the\nadjustment of post-treatment variables when comparing the effects of a\ntreatment in health and social sciences. In this paper, we introduce a novel\nBayesian nonparametric model for principal stratification, leveraging the\ndependent Dirichlet process to flexibly model the distribution of potential\noutcomes. By incorporating confounders and potential outcomes for the\npost-treatment variable in the Bayesian mixture model for the final outcome,\nour approach improves the accuracy of missing data imputation and allows for\nthe characterization of treatment effects across strata defined based on the\nvalues of the post-treatment variable. We assess the performance of our method\nthrough a Monte Carlo simulation study where we compare the proposed method\nwith state-of-the-art Bayesian method in principal stratification. Finally, we\nleverage the proposed method to evaluate the principal causal effects of\nexposure to air pollution on social mobility in the US on strata defined by\neducational attainment."
                },
                "authors": [
                    {
                        "name": "Dafne Zorzetto"
                    },
                    {
                        "name": "Paolo Dalla Torre"
                    },
                    {
                        "name": "Sonia Petrone"
                    },
                    {
                        "name": "Francesca Dominici"
                    },
                    {
                        "name": "Falco J. Bargagli-Stoffi"
                    }
                ],
                "author_detail": {
                    "name": "Falco J. Bargagli-Stoffi"
                },
                "author": "Falco J. Bargagli-Stoffi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08020v2",
                "updated": "2024-12-05T12:40:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    40,
                    16,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-10T15:17:49Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    17,
                    49,
                    3,
                    284,
                    0
                ],
                "title": "Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs"
                },
                "summary": "Recent efforts in fine-tuning language models often rely on automatic data\nselection, commonly using Nearest Neighbors retrieval from large datasets.\nHowever, we theoretically show that this approach tends to select redundant\ndata, limiting its effectiveness or even hurting performance. To address this,\nwe introduce SIFT, a data selection algorithm designed to reduce uncertainty\nabout the model's response given a prompt, which unifies ideas from retrieval\nand active learning. Whereas Nearest Neighbor retrieval typically fails in the\npresence of information duplication, SIFT accounts for information duplication\nand optimizes the overall information gain of the selected examples. We focus\nour evaluations on fine-tuning at test-time for prompt-specific language\nmodeling on the Pile dataset, and show that SIFT consistently outperforms\nNearest Neighbor retrieval, with minimal computational overhead. Moreover, we\nshow that our uncertainty estimates can predict the performance gain of\ntest-time fine-tuning, and use this to develop an adaptive algorithm that\ninvests test-time compute proportional to realized performance gains. We\nprovide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\nas a drop-in replacement for Nearest Neighbor retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts in fine-tuning language models often rely on automatic data\nselection, commonly using Nearest Neighbors retrieval from large datasets.\nHowever, we theoretically show that this approach tends to select redundant\ndata, limiting its effectiveness or even hurting performance. To address this,\nwe introduce SIFT, a data selection algorithm designed to reduce uncertainty\nabout the model's response given a prompt, which unifies ideas from retrieval\nand active learning. Whereas Nearest Neighbor retrieval typically fails in the\npresence of information duplication, SIFT accounts for information duplication\nand optimizes the overall information gain of the selected examples. We focus\nour evaluations on fine-tuning at test-time for prompt-specific language\nmodeling on the Pile dataset, and show that SIFT consistently outperforms\nNearest Neighbor retrieval, with minimal computational overhead. Moreover, we\nshow that our uncertainty estimates can predict the performance gain of\ntest-time fine-tuning, and use this to develop an adaptive algorithm that\ninvests test-time compute proportional to realized performance gains. We\nprovide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\nas a drop-in replacement for Nearest Neighbor retrieval."
                },
                "authors": [
                    {
                        "name": "Jonas Hbotter"
                    },
                    {
                        "name": "Sascha Bongni"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04112v1",
                "updated": "2024-12-05T12:30:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    30,
                    53,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T12:30:53Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    30,
                    53,
                    3,
                    340,
                    0
                ],
                "title": "Nonlinear unitary circuits for photonic neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear unitary circuits for photonic neural networks"
                },
                "summary": "Photonics has unlocked the potential for energy-efficient acceleration of\ndeep learning. Most approaches toward photonic deep learning have diligently\nreproduced traditional deep learning architectures using photonic platforms,\nseparately implementing linear-optical matrix calculations and nonlinear\nactivations via electro-optical conversion, optical nonlinearities, and\nsignal-encoded materials. Here we propose a concept of nonlinear unitary\nphotonic circuits to achieve the integration of linear and nonlinear\nexpressivity essential for deep neural networks. We devise a building block for\ntwo-dimensional nonlinear unitary operations, featuring norm-preserving\nmappings with nonconservative inner products, which enables the construction of\nhigh-dimensional nonlinear unitary circuits. Using deep nonlinear unitary\ncircuits, we demonstrate exponential growth in trajectory length and\nnear-complete coverage of the output space, both of which are essential for\ndeep learning. Along with neuroevolutionary learning examples for the\nregression of a nonconvex function, our results pave the way to photonic neural\nnetworks with highly expressive inference and stable training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonics has unlocked the potential for energy-efficient acceleration of\ndeep learning. Most approaches toward photonic deep learning have diligently\nreproduced traditional deep learning architectures using photonic platforms,\nseparately implementing linear-optical matrix calculations and nonlinear\nactivations via electro-optical conversion, optical nonlinearities, and\nsignal-encoded materials. Here we propose a concept of nonlinear unitary\nphotonic circuits to achieve the integration of linear and nonlinear\nexpressivity essential for deep neural networks. We devise a building block for\ntwo-dimensional nonlinear unitary operations, featuring norm-preserving\nmappings with nonconservative inner products, which enables the construction of\nhigh-dimensional nonlinear unitary circuits. Using deep nonlinear unitary\ncircuits, we demonstrate exponential growth in trajectory length and\nnear-complete coverage of the output space, both of which are essential for\ndeep learning. Along with neuroevolutionary learning examples for the\nregression of a nonconvex function, our results pave the way to photonic neural\nnetworks with highly expressive inference and stable training."
                },
                "authors": [
                    {
                        "name": "Sunkyu Yu"
                    },
                    {
                        "name": "Xianji Piao"
                    },
                    {
                        "name": "Namkyoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Namkyoo Park"
                },
                "author": "Namkyoo Park",
                "arxiv_comment": "33 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04110v1",
                "updated": "2024-12-05T12:24:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    24,
                    54,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T12:24:54Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    24,
                    54,
                    3,
                    340,
                    0
                ],
                "title": "Enhancing Mathematical Reasoning in LLMs with Background Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Mathematical Reasoning in LLMs with Background Operators"
                },
                "summary": "We propose utilizing background operators for mathematical reasoning in large\nlanguage models (LLMs). To achieve this, we define a set of fundamental\nmathematical predicates as the basic building blocks. For each mathematical\nproblem, we develop a Prolog solution that includes problem-specific predicates\nand intermediate predicates derived from these background operators, ensuring\nthat each solution adheres to the defined operator set. We introduce the\nMATH-Prolog corpus, which is derived from the counting and probability\ncategories of the MATH corpus. For efficient data augmentation, we apply K-fold\ncross-validated self-training. This method incrementally generates new Prolog\nsolutions for each fold, incorporating those verified as correct into the\ntraining set throughout the model training process. Our experimental results\ndemonstrate that 5-fold crossvalidated self-training effectively identifies\nnew, accurate Prolog solutions, achieving an accuracy of 84.6% on the\ncross-validated set, and 84.8% on the test set during fine-tuning the\nMeta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new\nsolutions with fully computable inference steps for previously unseen problems.\nAdditionally, incorporating the background mathematical predicates into the\nprompt enhances solution coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose utilizing background operators for mathematical reasoning in large\nlanguage models (LLMs). To achieve this, we define a set of fundamental\nmathematical predicates as the basic building blocks. For each mathematical\nproblem, we develop a Prolog solution that includes problem-specific predicates\nand intermediate predicates derived from these background operators, ensuring\nthat each solution adheres to the defined operator set. We introduce the\nMATH-Prolog corpus, which is derived from the counting and probability\ncategories of the MATH corpus. For efficient data augmentation, we apply K-fold\ncross-validated self-training. This method incrementally generates new Prolog\nsolutions for each fold, incorporating those verified as correct into the\ntraining set throughout the model training process. Our experimental results\ndemonstrate that 5-fold crossvalidated self-training effectively identifies\nnew, accurate Prolog solutions, achieving an accuracy of 84.6% on the\ncross-validated set, and 84.8% on the test set during fine-tuning the\nMeta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new\nsolutions with fully computable inference steps for previously unseen problems.\nAdditionally, incorporating the background mathematical predicates into the\nprompt enhances solution coverage."
                },
                "authors": [
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Yik-Cheung Tam"
                    }
                ],
                "author_detail": {
                    "name": "Yik-Cheung Tam"
                },
                "author": "Yik-Cheung Tam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19824v2",
                "updated": "2024-12-05T12:18:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    18,
                    4,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T16:34:46Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    34,
                    46,
                    4,
                    334,
                    0
                ],
                "title": "SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive\n  Tokens"
                },
                "summary": "We propose a one-stage framework for real-time multi-person 3D human mesh\nestimation from a single RGB image. While current one-stage methods, which\nfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with\nhigh-resolution inputs, we observe that this particularly benefits the\nestimation of individuals in smaller scales of the image (e.g., those far from\nthe camera), but at the cost of significantly increased computation overhead.\nTo address this, we introduce scale-adaptive tokens that are dynamically\nadjusted based on the relative scale of each individual in the image within the\nDETR framework. Specifically, individuals in smaller scales are processed at\nhigher resolutions, larger ones at lower resolutions, and background regions\nare further distilled. These scale-adaptive tokens more efficiently encode the\nimage features, facilitating subsequent decoding to regress the human mesh,\nwhile allowing the model to allocate computational resources more effectively\nand focus on more challenging cases. Experiments show that our method preserves\nthe accuracy benefits of high-resolution processing while substantially\nreducing computational cost, achieving real-time inference with performance\ncomparable to SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a one-stage framework for real-time multi-person 3D human mesh\nestimation from a single RGB image. While current one-stage methods, which\nfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with\nhigh-resolution inputs, we observe that this particularly benefits the\nestimation of individuals in smaller scales of the image (e.g., those far from\nthe camera), but at the cost of significantly increased computation overhead.\nTo address this, we introduce scale-adaptive tokens that are dynamically\nadjusted based on the relative scale of each individual in the image within the\nDETR framework. Specifically, individuals in smaller scales are processed at\nhigher resolutions, larger ones at lower resolutions, and background regions\nare further distilled. These scale-adaptive tokens more efficiently encode the\nimage features, facilitating subsequent decoding to regress the human mesh,\nwhile allowing the model to allocate computational resources more effectively\nand focus on more challenging cases. Experiments show that our method preserves\nthe accuracy benefits of high-resolution processing while substantially\nreducing computational cost, achieving real-time inference with performance\ncomparable to SOTA methods."
                },
                "authors": [
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Xiaoxuan Ma"
                    },
                    {
                        "name": "Jiajun Su"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04107v1",
                "updated": "2024-12-05T12:17:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    17,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T12:17:56Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    17,
                    56,
                    3,
                    340,
                    0
                ],
                "title": "Pre-train, Align, and Disentangle: Empowering Sequential Recommendation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-train, Align, and Disentangle: Empowering Sequential Recommendation\n  with Large Language Models"
                },
                "summary": "Sequential recommendation (SR) aims to model the sequential dependencies in\nusers' historical interactions to better capture their evolving interests.\nHowever, existing SR approaches primarily rely on collaborative data, which\nleads to limitations such as the cold-start problem and sub-optimal\nperformance. Meanwhile, despite the success of large language models (LLMs),\ntheir application in industrial recommender systems is hindered by high\ninference latency, inability to capture all distribution statistics, and\ncatastrophic forgetting. To this end, we propose a novel Pre-train, Align, and\nDisentangle (PAD) paradigm to empower recommendation models with LLMs.\nSpecifically, we first pre-train both the SR and LLM models to get\ncollaborative and textual embeddings. Next, a characteristic\nrecommendation-anchored alignment loss is proposed using multi-kernel maximum\nmean discrepancy with Gaussian kernels. Finally, a triple-experts architecture,\nconsisting aligned and modality-specific experts with disentangled embeddings,\nis fine-tuned in a frequency-aware manner. Experiments conducted on three\npublic datasets demonstrate the effectiveness of PAD, showing significant\nimprovements and compatibility with various SR backbone models, especially on\ncold items. The implementation code and datasets will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) aims to model the sequential dependencies in\nusers' historical interactions to better capture their evolving interests.\nHowever, existing SR approaches primarily rely on collaborative data, which\nleads to limitations such as the cold-start problem and sub-optimal\nperformance. Meanwhile, despite the success of large language models (LLMs),\ntheir application in industrial recommender systems is hindered by high\ninference latency, inability to capture all distribution statistics, and\ncatastrophic forgetting. To this end, we propose a novel Pre-train, Align, and\nDisentangle (PAD) paradigm to empower recommendation models with LLMs.\nSpecifically, we first pre-train both the SR and LLM models to get\ncollaborative and textual embeddings. Next, a characteristic\nrecommendation-anchored alignment loss is proposed using multi-kernel maximum\nmean discrepancy with Gaussian kernels. Finally, a triple-experts architecture,\nconsisting aligned and modality-specific experts with disentangled embeddings,\nis fine-tuned in a frequency-aware manner. Experiments conducted on three\npublic datasets demonstrate the effectiveness of PAD, showing significant\nimprovements and compatibility with various SR backbone models, especially on\ncold items. The implementation code and datasets will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Junwei Pan"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Dapeng Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04093v1",
                "updated": "2024-12-05T11:57:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    57,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:57:49Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    57,
                    49,
                    3,
                    340,
                    0
                ],
                "title": "Practical Considerations for Agentic LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Considerations for Agentic LLM Systems"
                },
                "summary": "As the strength of Large Language Models (LLMs) has grown over recent years,\nso too has interest in their use as the underlying models for autonomous\nagents. Although LLMs demonstrate emergent abilities and broad expertise across\nnatural language domains, their inherent unpredictability makes the\nimplementation of LLM agents challenging, resulting in a gap between related\nresearch and the real-world implementation of such systems. To bridge this gap,\nthis paper frames actionable insights and considerations from the research\ncommunity in the context of established application paradigms to enable the\nconstruction and facilitate the informed deployment of robust LLM agents.\nNamely, we position relevant research findings into four broad\ncategories--Planning, Memory, Tools, and Control Flow--based on common\npractices in application-focused literature and highlight practical\nconsiderations to make when designing agentic LLMs for real-world applications,\nsuch as handling stochasticity and managing resources efficiently. While we do\nnot conduct empirical evaluations, we do provide the necessary background for\ndiscussing critical aspects of agentic LLM designs, both in academia and\nindustry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the strength of Large Language Models (LLMs) has grown over recent years,\nso too has interest in their use as the underlying models for autonomous\nagents. Although LLMs demonstrate emergent abilities and broad expertise across\nnatural language domains, their inherent unpredictability makes the\nimplementation of LLM agents challenging, resulting in a gap between related\nresearch and the real-world implementation of such systems. To bridge this gap,\nthis paper frames actionable insights and considerations from the research\ncommunity in the context of established application paradigms to enable the\nconstruction and facilitate the informed deployment of robust LLM agents.\nNamely, we position relevant research findings into four broad\ncategories--Planning, Memory, Tools, and Control Flow--based on common\npractices in application-focused literature and highlight practical\nconsiderations to make when designing agentic LLMs for real-world applications,\nsuch as handling stochasticity and managing resources efficiently. While we do\nnot conduct empirical evaluations, we do provide the necessary background for\ndiscussing critical aspects of agentic LLM designs, both in academia and\nindustry."
                },
                "authors": [
                    {
                        "name": "Chris Sypherd"
                    },
                    {
                        "name": "Vaishak Belle"
                    }
                ],
                "author_detail": {
                    "name": "Vaishak Belle"
                },
                "author": "Vaishak Belle",
                "arxiv_comment": "15 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04091v1",
                "updated": "2024-12-05T11:55:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    55,
                    59,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:55:59Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    55,
                    59,
                    3,
                    340,
                    0
                ],
                "title": "Machine learning enhanced multi-particle tracking in solid fuel\n  combustion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning enhanced multi-particle tracking in solid fuel\n  combustion"
                },
                "summary": "Particle velocimetry is essential in solid fuel combustion studies, however,\nthe accurate detection and tracking of particles in high Particle Number\nDensity (PND) combustion scenario remain challenging. The current study\nadvances the machine-learning approaches for precise velocity measurements of\nsolid particles. For this, laser imaging experiments were performed for\nhigh-volatile bituminous coal particles burning in a laminar flow reactor.\nParticle positions were imaged using time-resolved Mie scattering. Various\ndetection methods, including conventional blob detection and Machine Learning\n(ML) based You Only Look Once (YOLO) and Realtime Detection Transformer\n(RT-DETR) were employed and bench marked.~Particle tracking was performed using\nthe Simple Online Realtime Tracking (SORT) algorithm. The results demonstrated\nthe capability of machine learning models trained on low-PND data for\nprediction of high-PND data. Slicing Aided Hyper Inference (SAHI) algorithm is\nimportant for the better performance of the used models. By evaluating the\nvelocity statistics, it is found that the mean particle velocity decreases with\nincreasing PND, primarily due to stronger particle interactions. The particle\ndynamics are closely related to the position of combustion zone observed in the\nprevious study. Thus, PND is considered as the dominant factor for the particle\ngroup combustion behavior of high-volatile solid fuels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle velocimetry is essential in solid fuel combustion studies, however,\nthe accurate detection and tracking of particles in high Particle Number\nDensity (PND) combustion scenario remain challenging. The current study\nadvances the machine-learning approaches for precise velocity measurements of\nsolid particles. For this, laser imaging experiments were performed for\nhigh-volatile bituminous coal particles burning in a laminar flow reactor.\nParticle positions were imaged using time-resolved Mie scattering. Various\ndetection methods, including conventional blob detection and Machine Learning\n(ML) based You Only Look Once (YOLO) and Realtime Detection Transformer\n(RT-DETR) were employed and bench marked.~Particle tracking was performed using\nthe Simple Online Realtime Tracking (SORT) algorithm. The results demonstrated\nthe capability of machine learning models trained on low-PND data for\nprediction of high-PND data. Slicing Aided Hyper Inference (SAHI) algorithm is\nimportant for the better performance of the used models. By evaluating the\nvelocity statistics, it is found that the mean particle velocity decreases with\nincreasing PND, primarily due to stronger particle interactions. The particle\ndynamics are closely related to the position of combustion zone observed in the\nprevious study. Thus, PND is considered as the dominant factor for the particle\ngroup combustion behavior of high-volatile solid fuels."
                },
                "authors": [
                    {
                        "name": "Haowen Chen"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Benjamin Bhm"
                    },
                    {
                        "name": "Tao Li"
                    }
                ],
                "author_detail": {
                    "name": "Tao Li"
                },
                "author": "Tao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04090v1",
                "updated": "2024-12-05T11:52:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    52,
                    20,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:52:20Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    52,
                    20,
                    3,
                    340,
                    0
                ],
                "title": "LossAgent: Towards Any Optimization Objectives for Image Processing with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LossAgent: Towards Any Optimization Objectives for Image Processing with\n  LLM Agents"
                },
                "summary": "We present the first loss agent, dubbed LossAgent, for low-level image\nprocessing tasks, e.g., image super-resolution and restoration, intending to\nachieve any customized optimization objectives of low-level image processing in\ndifferent practical applications. Notably, not all optimization objectives,\nsuch as complex hand-crafted perceptual metrics, text description, and\nintricate human feedback, can be instantiated with existing low-level losses,\ne.g., MSE loss. which presents a crucial challenge in optimizing image\nprocessing networks in an end-to-end manner. To eliminate this, our LossAgent\nintroduces the powerful large language model (LLM) as the loss agent, where the\nrich textual understanding of prior knowledge empowers the loss agent with the\npotential to understand complex optimization objectives, trajectory, and state\nfeedback from external environments in the optimization process of the\nlow-level image processing networks. In particular, we establish the loss\nrepository by incorporating existing loss functions that support the end-to-end\noptimization for low-level image processing. Then, we design the\noptimization-oriented prompt engineering for the loss agent to actively and\nintelligently decide the compositional weights for each loss in the repository\nat each optimization interaction, thereby achieving the required optimization\ntrajectory for any customized optimization objectives. Extensive experiments on\nthree typical low-level image processing tasks and multiple optimization\nobjectives have shown the effectiveness and applicability of our proposed\nLossAgent. Code and pre-trained models will be available at\nhttps://github.com/lbc12345/LossAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first loss agent, dubbed LossAgent, for low-level image\nprocessing tasks, e.g., image super-resolution and restoration, intending to\nachieve any customized optimization objectives of low-level image processing in\ndifferent practical applications. Notably, not all optimization objectives,\nsuch as complex hand-crafted perceptual metrics, text description, and\nintricate human feedback, can be instantiated with existing low-level losses,\ne.g., MSE loss. which presents a crucial challenge in optimizing image\nprocessing networks in an end-to-end manner. To eliminate this, our LossAgent\nintroduces the powerful large language model (LLM) as the loss agent, where the\nrich textual understanding of prior knowledge empowers the loss agent with the\npotential to understand complex optimization objectives, trajectory, and state\nfeedback from external environments in the optimization process of the\nlow-level image processing networks. In particular, we establish the loss\nrepository by incorporating existing loss functions that support the end-to-end\noptimization for low-level image processing. Then, we design the\noptimization-oriented prompt engineering for the loss agent to actively and\nintelligently decide the compositional weights for each loss in the repository\nat each optimization interaction, thereby achieving the required optimization\ntrajectory for any customized optimization objectives. Extensive experiments on\nthree typical low-level image processing tasks and multiple optimization\nobjectives have shown the effectiveness and applicability of our proposed\nLossAgent. Code and pre-trained models will be available at\nhttps://github.com/lbc12345/LossAgent."
                },
                "authors": [
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04086v1",
                "updated": "2024-12-05T11:48:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    48,
                    54,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:48:54Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    48,
                    54,
                    3,
                    340,
                    0
                ],
                "title": "BodyMetric: Evaluating the Realism of HumanBodies in Text-to-Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BodyMetric: Evaluating the Realism of HumanBodies in Text-to-Image\n  Generation"
                },
                "summary": "Accurately generating images of human bodies from text remains a challenging\nproblem for state of the art text-to-image models. Commonly observed\nbody-related artifacts include extra or missing limbs, unrealistic poses,\nblurred body parts, etc. Currently, evaluation of such artifacts relies heavily\non time-consuming human judgments, limiting the ability to benchmark models at\nscale. We address this by proposing BodyMetric, a learnable metric that\npredicts body realism in images. BodyMetric is trained on realism labels and\nmulti-modal signals including 3D body representations inferred from the input\nimage, and textual descriptions. In order to facilitate this approach, we\ndesign an annotation pipeline to collect expert ratings on human body realism\nleading to a new dataset for this task, namely, BodyRealism. Ablation studies\nsupport our architectural choices for BodyMetric and the importance of\nleveraging a 3D human body prior in capturing body-related artifacts in 2D\nimages. In comparison to concurrent metrics which evaluate general user\npreference in images, BodyMetric specifically reflects body-related artifacts.\nWe demonstrate the utility of BodyMetric through applications that were\npreviously infeasible at scale. In particular, we use BodyMetric to benchmark\nthe generation ability of text-to-image models to produce realistic human\nbodies. We also demonstrate the effectiveness of BodyMetric in ranking\ngenerated images based on the predicted realism scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately generating images of human bodies from text remains a challenging\nproblem for state of the art text-to-image models. Commonly observed\nbody-related artifacts include extra or missing limbs, unrealistic poses,\nblurred body parts, etc. Currently, evaluation of such artifacts relies heavily\non time-consuming human judgments, limiting the ability to benchmark models at\nscale. We address this by proposing BodyMetric, a learnable metric that\npredicts body realism in images. BodyMetric is trained on realism labels and\nmulti-modal signals including 3D body representations inferred from the input\nimage, and textual descriptions. In order to facilitate this approach, we\ndesign an annotation pipeline to collect expert ratings on human body realism\nleading to a new dataset for this task, namely, BodyRealism. Ablation studies\nsupport our architectural choices for BodyMetric and the importance of\nleveraging a 3D human body prior in capturing body-related artifacts in 2D\nimages. In comparison to concurrent metrics which evaluate general user\npreference in images, BodyMetric specifically reflects body-related artifacts.\nWe demonstrate the utility of BodyMetric through applications that were\npreviously infeasible at scale. In particular, we use BodyMetric to benchmark\nthe generation ability of text-to-image models to produce realistic human\nbodies. We also demonstrate the effectiveness of BodyMetric in ranking\ngenerated images based on the predicted realism scores."
                },
                "authors": [
                    {
                        "name": "Nefeli Andreou"
                    },
                    {
                        "name": "Varsha Vivek"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Alex Vorobiov"
                    },
                    {
                        "name": "Tiffany Deng"
                    },
                    {
                        "name": "Raja Bala"
                    },
                    {
                        "name": "Larry Davis"
                    },
                    {
                        "name": "Betty Mohler Tesch"
                    }
                ],
                "author_detail": {
                    "name": "Betty Mohler Tesch"
                },
                "author": "Betty Mohler Tesch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11624v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11624v3",
                "updated": "2024-12-05T11:47:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    47,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-06-17T15:07:55Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    7,
                    55,
                    0,
                    169,
                    0
                ],
                "title": "Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers"
                },
                "summary": "Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we aim to interpret these hidden states and control\nthem at inference, with a focus on motion forecasting. We use linear probes to\nmeasure neural collapse towards interpretable motion features in hidden states.\nHigh probing accuracy implies meaningful directions and distances between\nhidden states of opposing features, which we use to fit interpretable control\nvectors for activation steering at inference. To optimize our control vectors,\nwe use sparse autoencoders with fully-connected, convolutional, MLPMixer layers\nand various activation functions. Notably, we show that enforcing sparsity in\nhidden states leads to a more linear relationship between control vector\ntemperatures and forecasts. Our approach enables mechanistic interpretability\nand zero-shot generalization to unseen dataset characteristics with negligible\ncomputational overhead. Our implementation is available at\nhttps://github.com/kit-mrt/future-motion",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we aim to interpret these hidden states and control\nthem at inference, with a focus on motion forecasting. We use linear probes to\nmeasure neural collapse towards interpretable motion features in hidden states.\nHigh probing accuracy implies meaningful directions and distances between\nhidden states of opposing features, which we use to fit interpretable control\nvectors for activation steering at inference. To optimize our control vectors,\nwe use sparse autoencoders with fully-connected, convolutional, MLPMixer layers\nand various activation functions. Notably, we show that enforcing sparsity in\nhidden states leads to a more linear relationship between control vector\ntemperatures and forecasts. Our approach enables mechanistic interpretability\nand zero-shot generalization to unseen dataset characteristics with negligible\ncomputational overhead. Our implementation is available at\nhttps://github.com/kit-mrt/future-motion"
                },
                "authors": [
                    {
                        "name": "Omer Sahin Tas"
                    },
                    {
                        "name": "Royden Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Royden Wagner"
                },
                "author": "Royden Wagner",
                "arxiv_comment": "Add autoencoders with convolutional, MLPMixer layers, and JumpReLU\n  activations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11624v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11624v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04083v1",
                "updated": "2024-12-05T11:36:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    36,
                    37,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:36:37Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    36,
                    37,
                    3,
                    340,
                    0
                ],
                "title": "Unified Framework for Open-World Compositional Zero-shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Framework for Open-World Compositional Zero-shot Learning"
                },
                "summary": "Open-World Compositional Zero-Shot Learning (OW-CZSL) addresses the challenge\nof recognizing novel compositions of known primitives and entities. Even though\nprior works utilize language knowledge for recognition, such approaches exhibit\nlimited interactions between language-image modalities. Our approach primarily\nfocuses on enhancing the inter-modality interactions through fostering richer\ninteractions between image and textual data. Additionally, we introduce a novel\nmodule aimed at alleviating the computational burden associated with exhaustive\nexploration of all possible compositions during the inference stage. While\nprevious methods exclusively learn compositions jointly or independently, we\nintroduce an advanced hybrid procedure that leverages both learning mechanisms\nto generate final predictions. Our proposed model, achieves state-of-the-art in\nOW-CZSL in three datasets, while surpassing Large Vision Language Models (LLVM)\nin two datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-World Compositional Zero-Shot Learning (OW-CZSL) addresses the challenge\nof recognizing novel compositions of known primitives and entities. Even though\nprior works utilize language knowledge for recognition, such approaches exhibit\nlimited interactions between language-image modalities. Our approach primarily\nfocuses on enhancing the inter-modality interactions through fostering richer\ninteractions between image and textual data. Additionally, we introduce a novel\nmodule aimed at alleviating the computational burden associated with exhaustive\nexploration of all possible compositions during the inference stage. While\nprevious methods exclusively learn compositions jointly or independently, we\nintroduce an advanced hybrid procedure that leverages both learning mechanisms\nto generate final predictions. Our proposed model, achieves state-of-the-art in\nOW-CZSL in three datasets, while surpassing Large Vision Language Models (LLVM)\nin two datasets."
                },
                "authors": [
                    {
                        "name": "Hirunima Jayasekara"
                    },
                    {
                        "name": "Khoi Pham"
                    },
                    {
                        "name": "Nirat Saini"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04078v1",
                "updated": "2024-12-05T11:24:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    24,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:24:27Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    24,
                    27,
                    3,
                    340,
                    0
                ],
                "title": "Towards Generalizable Autonomous Penetration Testing via Domain\n  Randomization and Meta-Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Generalizable Autonomous Penetration Testing via Domain\n  Randomization and Meta-Reinforcement Learning"
                },
                "summary": "With increasing numbers of vulnerabilities exposed on the internet,\nautonomous penetration testing (pentesting) has emerged as an emerging research\narea, while reinforcement learning (RL) is a natural fit for studying\nautonomous pentesting. Previous research in RL-based autonomous pentesting\nmainly focused on enhancing agents' learning efficacy within abstract simulated\ntraining environments. They overlooked the applicability and generalization\nrequirements of deploying agents' policies in real-world environments that\ndiffer substantially from their training settings. In contrast, for the first\ntime, we shift focus to the pentesting agents' ability to generalize across\nunseen real environments. For this purpose, we propose a Generalizable\nAutonomous Pentesting framework (namely GAP) for training agents capable of\ndrawing inferences from one to another -- a key requirement for the broad\napplication of autonomous pentesting and a hallmark of human intelligence. GAP\nintroduces a Real-to-Sim-to-Real pipeline with two key methods: domain\nrandomization and meta-RL learning. Specifically, we are among the first to\napply domain randomization in autonomous pentesting and propose a large\nlanguage model-powered domain randomization method for synthetic environment\ngeneration. We further apply meta-RL to improve the agents' generalization\nability in unseen environments by leveraging the synthetic environments. The\ncombination of these two methods can effectively bridge the generalization gap\nand improve policy adaptation performance. Experiments are conducted on various\nvulnerable virtual machines, with results showing that GAP can (a) enable\npolicy learning in unknown real environments, (b) achieve zero-shot policy\ntransfer in similar environments, and (c) realize rapid policy adaptation in\ndissimilar environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With increasing numbers of vulnerabilities exposed on the internet,\nautonomous penetration testing (pentesting) has emerged as an emerging research\narea, while reinforcement learning (RL) is a natural fit for studying\nautonomous pentesting. Previous research in RL-based autonomous pentesting\nmainly focused on enhancing agents' learning efficacy within abstract simulated\ntraining environments. They overlooked the applicability and generalization\nrequirements of deploying agents' policies in real-world environments that\ndiffer substantially from their training settings. In contrast, for the first\ntime, we shift focus to the pentesting agents' ability to generalize across\nunseen real environments. For this purpose, we propose a Generalizable\nAutonomous Pentesting framework (namely GAP) for training agents capable of\ndrawing inferences from one to another -- a key requirement for the broad\napplication of autonomous pentesting and a hallmark of human intelligence. GAP\nintroduces a Real-to-Sim-to-Real pipeline with two key methods: domain\nrandomization and meta-RL learning. Specifically, we are among the first to\napply domain randomization in autonomous pentesting and propose a large\nlanguage model-powered domain randomization method for synthetic environment\ngeneration. We further apply meta-RL to improve the agents' generalization\nability in unseen environments by leveraging the synthetic environments. The\ncombination of these two methods can effectively bridge the generalization gap\nand improve policy adaptation performance. Experiments are conducted on various\nvulnerable virtual machines, with results showing that GAP can (a) enable\npolicy learning in unknown real environments, (b) achieve zero-shot policy\ntransfer in similar environments, and (c) realize rapid policy adaptation in\ndissimilar environments."
                },
                "authors": [
                    {
                        "name": "Shicheng Zhou"
                    },
                    {
                        "name": "Jingju Liu"
                    },
                    {
                        "name": "Yuliang Lu"
                    },
                    {
                        "name": "Jiahai Yang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Jie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jie Chen"
                },
                "author": "Jie Chen",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04077v1",
                "updated": "2024-12-05T11:17:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    17,
                    57,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    17,
                    57,
                    3,
                    340,
                    0
                ],
                "title": "SoRA: Singular Value Decomposed Low-Rank Adaptation for Domain\n  Generalizable Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoRA: Singular Value Decomposed Low-Rank Adaptation for Domain\n  Generalizable Representation Learning"
                },
                "summary": "Domain generalization (DG) aims to adapt a model using one or multiple source\ndomains to ensure robust performance in unseen target domains. Recently,\nParameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising\nresults in the context of DG problem. Nevertheless, existing PEFT methods still\nstruggle to strike a balance between preserving generalizable components of the\npre-trained model and learning task-specific features. To gain insights into\nthe distribution of generalizable components, we begin by analyzing the\npre-trained weights through the lens of singular value decomposition. Building\non these insights, we introduce Singular Value Decomposed Low-Rank Adaptation\n(SoRA), an approach that selectively tunes minor singular components while\nkeeping the residual parts frozen. SoRA effectively retains the generalization\nability of the pre-trained model while efficiently acquiring task-specific\nskills. Furthermore, we freeze domain-generalizable blocks and employ an\nannealing weight decay strategy, thereby achieving an optimal balance in the\ndelicate trade-off between generalizability and discriminability. SoRA attains\nstate-of-the-art results on multiple benchmarks that span both domain\ngeneralized semantic segmentation to domain generalized object detection. In\naddition, our methods introduce no additional inference overhead or\nregularization loss, maintain compatibility with any backbone or head, and are\ndesigned to be versatile, allowing easy integration into a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain generalization (DG) aims to adapt a model using one or multiple source\ndomains to ensure robust performance in unseen target domains. Recently,\nParameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising\nresults in the context of DG problem. Nevertheless, existing PEFT methods still\nstruggle to strike a balance between preserving generalizable components of the\npre-trained model and learning task-specific features. To gain insights into\nthe distribution of generalizable components, we begin by analyzing the\npre-trained weights through the lens of singular value decomposition. Building\non these insights, we introduce Singular Value Decomposed Low-Rank Adaptation\n(SoRA), an approach that selectively tunes minor singular components while\nkeeping the residual parts frozen. SoRA effectively retains the generalization\nability of the pre-trained model while efficiently acquiring task-specific\nskills. Furthermore, we freeze domain-generalizable blocks and employ an\nannealing weight decay strategy, thereby achieving an optimal balance in the\ndelicate trade-off between generalizability and discriminability. SoRA attains\nstate-of-the-art results on multiple benchmarks that span both domain\ngeneralized semantic segmentation to domain generalized object detection. In\naddition, our methods introduce no additional inference overhead or\nregularization loss, maintain compatibility with any backbone or head, and are\ndesigned to be versatile, allowing easy integration into a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Seokju Yun"
                    },
                    {
                        "name": "Seunghye Chae"
                    },
                    {
                        "name": "Dongheon Lee"
                    },
                    {
                        "name": "Youngmin Ro"
                    }
                ],
                "author_detail": {
                    "name": "Youngmin Ro"
                },
                "author": "Youngmin Ro",
                "arxiv_comment": "Project page: https://ysj9909.github.io/SoRA.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04070v1",
                "updated": "2024-12-05T11:06:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    6,
                    12,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:06:12Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    6,
                    12,
                    3,
                    340,
                    0
                ],
                "title": "Radio pulsar population synthesis with consistent flux measurements\n  using simulation-based inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio pulsar population synthesis with consistent flux measurements\n  using simulation-based inference"
                },
                "summary": "The properties of the entire neutron star population can be inferred by\nmodeling their evolution, from birth to the present, through pulsar population\nsynthesis. This involves simulating a mock population, applying observational\nfilters, and comparing the resulting sources to the limited subset of detected\npulsars. We specifically focus on the magneto-rotational properties of Galactic\nisolated neutron stars and provide new insights into the intrinsic radio\nluminosity law by combining pulsar population synthesis with a simulation-based\ninference (SBI) technique called truncated sequential neural posterior\nestimation (TSNPE). We employ TSNPE to train a neural density estimator on\nsimulated pulsar populations to approximate the posterior distribution of the\nunderlying parameters. This technique efficiently explores the parameter space\nby concentrating on regions that are most likely to match the observed data\nthus allowing a significant reduction in training dataset size. We demonstrate\nthe efficiency of TSNPE over standard neural posterior estimation (NPE),\nachieving robust inferences of magneto-rotational parameters consistent with\nprevious studies using only around 4% of the simulations required by NPE\napproaches. Moreover, for the first time, we incorporate data from the Thousand\nPulsar Array (TPA) program on MeerKAT, the largest unified sample of neutron\nstars with consistent fluxes measurement to date, to help constrain the stars'\nintrinsic radio luminosity. We find that adding flux information as an input to\nthe neural network largely improves the constraints on the pulsars' radio\nluminosity, as well as improving the estimates on other input parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The properties of the entire neutron star population can be inferred by\nmodeling their evolution, from birth to the present, through pulsar population\nsynthesis. This involves simulating a mock population, applying observational\nfilters, and comparing the resulting sources to the limited subset of detected\npulsars. We specifically focus on the magneto-rotational properties of Galactic\nisolated neutron stars and provide new insights into the intrinsic radio\nluminosity law by combining pulsar population synthesis with a simulation-based\ninference (SBI) technique called truncated sequential neural posterior\nestimation (TSNPE). We employ TSNPE to train a neural density estimator on\nsimulated pulsar populations to approximate the posterior distribution of the\nunderlying parameters. This technique efficiently explores the parameter space\nby concentrating on regions that are most likely to match the observed data\nthus allowing a significant reduction in training dataset size. We demonstrate\nthe efficiency of TSNPE over standard neural posterior estimation (NPE),\nachieving robust inferences of magneto-rotational parameters consistent with\nprevious studies using only around 4% of the simulations required by NPE\napproaches. Moreover, for the first time, we incorporate data from the Thousand\nPulsar Array (TPA) program on MeerKAT, the largest unified sample of neutron\nstars with consistent fluxes measurement to date, to help constrain the stars'\nintrinsic radio luminosity. We find that adding flux information as an input to\nthe neural network largely improves the constraints on the pulsars' radio\nluminosity, as well as improving the estimates on other input parameters."
                },
                "authors": [
                    {
                        "name": "Celsa Pardo Araujo"
                    },
                    {
                        "name": "Michele Ronchi"
                    },
                    {
                        "name": "Vanessa Graber"
                    },
                    {
                        "name": "Nanda Rea"
                    }
                ],
                "author_detail": {
                    "name": "Nanda Rea"
                },
                "author": "Nanda Rea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04059v1",
                "updated": "2024-12-05T10:54:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    54,
                    59,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:54:59Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    54,
                    59,
                    3,
                    340,
                    0
                ],
                "title": "The puzzling long GRB 191019A: Evidence for Kilonova Light",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The puzzling long GRB 191019A: Evidence for Kilonova Light"
                },
                "summary": "GRB 191019A was a long Gamma-ray burst (GRB) lasting about 65 s and, as such,\noriginally thought to be linked to a core-collapse supernova. However, even\nthough follow-up observations identified the optical counterpart close to the\nbright nucleus of a nearby ancient galaxy (z=0.248), no associated supernova\nwas found. This led to the suggestion that the burst was caused by the merger\nof two compact stellar objects, likely in a dense circumnuclear environment. By\nusing a recently developed diagnostic tool based on prompt emission temporal\nproperties, we noticed that GRB 191019A falls among those long GRBs which are\nassociated with compact mergers and with evidence of kilonova light. We thus\nre-analyzed unpublished GROND multi-color (g'r'i'z'JHK_s) data obtained between\n0.4 and 15 days post trigger. Image subtraction confirmed the optical\ncounterpart in all four optical bands, with GROND tracking its fading until 1.5\ndays post-burst. Incorporating publicly available Swift-XRT data, a joint fit\nof an afterglow plus a kilonova model revealed a better match than an\nafterglow-only scenario. The resulting kilonova properties resemble those of\nAT2017gfo associated with the binary neutron star merger GW170817, with a total\nejected mass of about 0.06 solar mass. Contrary to previous findings inferring\na high-density circumburst environment (n0=10^7-10^8 cm^-3), our analysis finds\nstandard conditions (n0 = 1 cm^-3), suggesting the long duration of GRB 191019A\nwas intrinsic rather than due to jet interaction with a dense external medium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRB 191019A was a long Gamma-ray burst (GRB) lasting about 65 s and, as such,\noriginally thought to be linked to a core-collapse supernova. However, even\nthough follow-up observations identified the optical counterpart close to the\nbright nucleus of a nearby ancient galaxy (z=0.248), no associated supernova\nwas found. This led to the suggestion that the burst was caused by the merger\nof two compact stellar objects, likely in a dense circumnuclear environment. By\nusing a recently developed diagnostic tool based on prompt emission temporal\nproperties, we noticed that GRB 191019A falls among those long GRBs which are\nassociated with compact mergers and with evidence of kilonova light. We thus\nre-analyzed unpublished GROND multi-color (g'r'i'z'JHK_s) data obtained between\n0.4 and 15 days post trigger. Image subtraction confirmed the optical\ncounterpart in all four optical bands, with GROND tracking its fading until 1.5\ndays post-burst. Incorporating publicly available Swift-XRT data, a joint fit\nof an afterglow plus a kilonova model revealed a better match than an\nafterglow-only scenario. The resulting kilonova properties resemble those of\nAT2017gfo associated with the binary neutron star merger GW170817, with a total\nejected mass of about 0.06 solar mass. Contrary to previous findings inferring\na high-density circumburst environment (n0=10^7-10^8 cm^-3), our analysis finds\nstandard conditions (n0 = 1 cm^-3), suggesting the long duration of GRB 191019A\nwas intrinsic rather than due to jet interaction with a dense external medium."
                },
                "authors": [
                    {
                        "name": "G. Stratta"
                    },
                    {
                        "name": "A. M. Nicuesa Guelbenzu"
                    },
                    {
                        "name": "S. Klose"
                    },
                    {
                        "name": "A. Rossi"
                    },
                    {
                        "name": "P. Singh"
                    },
                    {
                        "name": "E. Palazzi"
                    },
                    {
                        "name": "C. Guidorzi"
                    },
                    {
                        "name": "A. Camisasca"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "A. Rau"
                    },
                    {
                        "name": "M. Bulla"
                    },
                    {
                        "name": "F. Ragosta"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "D. Paris"
                    }
                ],
                "author_detail": {
                    "name": "D. Paris"
                },
                "author": "D. Paris",
                "arxiv_comment": "21 pages, 7 figures, accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04057v1",
                "updated": "2024-12-05T10:50:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    50,
                    58,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:50:58Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    50,
                    58,
                    3,
                    340,
                    0
                ],
                "title": "From Code to Play: Benchmarking Program Search for Games Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Code to Play: Benchmarking Program Search for Games Using Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities in generating\nprogram code, opening exciting opportunities for applying program synthesis to\ngames. In this work, we explore the potential of LLMs to directly synthesize\nusable code for a wide range of gaming applications, focusing on two\nprogramming languages, Python and Java. We use an evolutionary hill-climbing\nalgorithm, where the mutations and seeds of the initial programs are controlled\nby LLMs. For Python, the framework covers various game-related tasks, including\nfive miniature versions of Atari games, ten levels of Baba is You, an\nenvironment inspired by Asteroids, and a maze generation task. For Java, the\nframework contains 12 games from the TAG tabletop games framework. Across 29\ntasks, we evaluated 12 language models for Python and 8 for Java. Our findings\nsuggest that the performance of LLMs depends more on the task than on model\nsize. While larger models generate more executable programs, these do not\nalways result in higher-quality solutions but are much more expensive. No model\nhas a clear advantage, although on any specific task, one model may be better.\nTrying many models on a problem and using the best results across them is more\nreliable than using just one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities in generating\nprogram code, opening exciting opportunities for applying program synthesis to\ngames. In this work, we explore the potential of LLMs to directly synthesize\nusable code for a wide range of gaming applications, focusing on two\nprogramming languages, Python and Java. We use an evolutionary hill-climbing\nalgorithm, where the mutations and seeds of the initial programs are controlled\nby LLMs. For Python, the framework covers various game-related tasks, including\nfive miniature versions of Atari games, ten levels of Baba is You, an\nenvironment inspired by Asteroids, and a maze generation task. For Java, the\nframework contains 12 games from the TAG tabletop games framework. Across 29\ntasks, we evaluated 12 language models for Python and 8 for Java. Our findings\nsuggest that the performance of LLMs depends more on the task than on model\nsize. While larger models generate more executable programs, these do not\nalways result in higher-quality solutions but are much more expensive. No model\nhas a clear advantage, although on any specific task, one model may be better.\nTrying many models on a problem and using the best results across them is more\nreliable than using just one."
                },
                "authors": [
                    {
                        "name": "Manuel Eberhardinger"
                    },
                    {
                        "name": "James Goodman"
                    },
                    {
                        "name": "Alexander Dockhorn"
                    },
                    {
                        "name": "Diego Perez-Liebana"
                    },
                    {
                        "name": "Raluca D. Gaina"
                    },
                    {
                        "name": "Duygu akmak"
                    },
                    {
                        "name": "Setareh Maghsudi"
                    },
                    {
                        "name": "Simon Lucas"
                    }
                ],
                "author_detail": {
                    "name": "Simon Lucas"
                },
                "author": "Simon Lucas",
                "arxiv_comment": "Submitted to Transactions on Games Special Issue on Large Language\n  Models and Games",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04056v1",
                "updated": "2024-12-05T10:49:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    49,
                    2,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:49:02Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    49,
                    2,
                    3,
                    340,
                    0
                ],
                "title": "Prompt Engineering Guidance for Conceptual Agent-based Model Extraction\n  using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Engineering Guidance for Conceptual Agent-based Model Extraction\n  using Large Language Models"
                },
                "summary": "This document contains detailed information about the prompts used in the\nexperimental process discussed in the paper \"Toward Automating Agent-based\nModel Generation: A Benchmark for Model Extraction using Question-Answering\nTechniques\". The paper aims to utilize Question-answering (QA) models to\nextract the necessary information to implement Agent-based Modeling (ABM) from\nconceptual models. It presents the extracted information in formats that can be\nread by both humans and computers (i.e., JavaScript Object Notation (JSON)),\nenabling manual use by humans and auto-code generation by Large Language Models\n(LLM).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This document contains detailed information about the prompts used in the\nexperimental process discussed in the paper \"Toward Automating Agent-based\nModel Generation: A Benchmark for Model Extraction using Question-Answering\nTechniques\". The paper aims to utilize Question-answering (QA) models to\nextract the necessary information to implement Agent-based Modeling (ABM) from\nconceptual models. It presents the extracted information in formats that can be\nread by both humans and computers (i.e., JavaScript Object Notation (JSON)),\nenabling manual use by humans and auto-code generation by Large Language Models\n(LLM)."
                },
                "authors": [
                    {
                        "name": "Siamak Khatami"
                    },
                    {
                        "name": "Christopher Frantz"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Frantz"
                },
                "author": "Christopher Frantz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; I.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07122v2",
                "updated": "2024-12-05T10:45:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    45,
                    2,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-11T16:51:39Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    51,
                    39,
                    0,
                    316,
                    0
                ],
                "title": "SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering\n  in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating human-like text, but their output may not be aligned with the user\nor even produce harmful content. This paper presents a novel approach to detect\nand steer concepts such as toxicity before generation. We introduce the Sparse\nConditioned Autoencoder (SCAR), a single trained module that extends the\notherwise untouched LLM. SCAR ensures full steerability, towards and away from\nconcepts (e.g., toxic content), without compromising the quality of the model's\ntext generation on standard evaluation benchmarks. We demonstrate the effective\napplication of our approach through a variety of concepts, including toxicity,\nsafety, and writing style alignment. As such, this work establishes a robust\nframework for controlling LLM generations, ensuring their ethical and safe\ndeployment in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating human-like text, but their output may not be aligned with the user\nor even produce harmful content. This paper presents a novel approach to detect\nand steer concepts such as toxicity before generation. We introduce the Sparse\nConditioned Autoencoder (SCAR), a single trained module that extends the\notherwise untouched LLM. SCAR ensures full steerability, towards and away from\nconcepts (e.g., toxic content), without compromising the quality of the model's\ntext generation on standard evaluation benchmarks. We demonstrate the effective\napplication of our approach through a variety of concepts, including toxicity,\nsafety, and writing style alignment. As such, this work establishes a robust\nframework for controlling LLM generations, ensuring their ethical and safe\ndeployment in real-world applications."
                },
                "authors": [
                    {
                        "name": "Ruben Hrle"
                    },
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Bjrn Deiseroth"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "arxiv_comment": "Accepted at Socially Responsible Language Modelling Research (SoLaR)\n  Workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04041v1",
                "updated": "2024-12-05T10:30:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    30,
                    35,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:30:35Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    30,
                    35,
                    3,
                    340,
                    0
                ],
                "title": "GenChaR: A Dataset for Stock Chart Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenChaR: A Dataset for Stock Chart Captioning"
                },
                "summary": "In this work, we introduce a new dataset GenChaR for an image captioning task\naround stock charts. The task aims to read market sentiment directly from\ndepicted charts and generate descriptions, hopefully to provide comprehensible\nand useful insights for stock trading. Impressed by the success of large\nlanguage models (LLMs), the study decides to pioneer itself by exploring the\ncapabilities of large vision-language models (LVLMs) on the proposed task. This\npaper outlines the objectives of the stock captioning task, the dataset we\nbuilt, and automatic evaluation with some representative general-purpose LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce a new dataset GenChaR for an image captioning task\naround stock charts. The task aims to read market sentiment directly from\ndepicted charts and generate descriptions, hopefully to provide comprehensible\nand useful insights for stock trading. Impressed by the success of large\nlanguage models (LLMs), the study decides to pioneer itself by exploring the\ncapabilities of large vision-language models (LVLMs) on the proposed task. This\npaper outlines the objectives of the stock captioning task, the dataset we\nbuilt, and automatic evaluation with some representative general-purpose LVLMs."
                },
                "authors": [
                    {
                        "name": "Le Qiu"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuele Chersoni"
                },
                "author": "Emmanuele Chersoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04036v1",
                "updated": "2024-12-05T10:19:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    19,
                    36,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:19:36Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    19,
                    36,
                    3,
                    340,
                    0
                ],
                "title": "SocialMind: LLM-based Proactive AR Social Assistive System with\n  Human-like Perception for In-situ Live Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialMind: LLM-based Proactive AR Social Assistive System with\n  Human-like Perception for In-situ Live Interactions"
                },
                "summary": "Social interactions are fundamental to human life. The recent emergence of\nlarge language models (LLMs)-based virtual assistants has demonstrated their\npotential to revolutionize human interactions and lifestyles. However, existing\nassistive systems mainly provide reactive services to individual users, rather\nthan offering in-situ assistance during live social interactions with\nconversational partners. In this study, we introduce SocialMind, the first\nLLM-based proactive AR social assistive system that provides users with in-situ\nsocial assistance. SocialMind employs human-like perception leveraging\nmulti-modal sensors to extract both verbal and nonverbal cues, social factors,\nand implicit personas, incorporating these social cues into LLM reasoning for\nsocial suggestion generation. Additionally, SocialMind employs a multi-tier\ncollaborative generation strategy and proactive update mechanism to display\nsocial suggestions on Augmented Reality (AR) glasses, ensuring that suggestions\nare timely provided to users without disrupting the natural flow of\nconversation. Evaluations on three public datasets and a user study with 20\nparticipants show that SocialMind achieves 38.3% higher engagement compared to\nbaselines, and 95% of participants are willing to use SocialMind in their live\nsocial interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social interactions are fundamental to human life. The recent emergence of\nlarge language models (LLMs)-based virtual assistants has demonstrated their\npotential to revolutionize human interactions and lifestyles. However, existing\nassistive systems mainly provide reactive services to individual users, rather\nthan offering in-situ assistance during live social interactions with\nconversational partners. In this study, we introduce SocialMind, the first\nLLM-based proactive AR social assistive system that provides users with in-situ\nsocial assistance. SocialMind employs human-like perception leveraging\nmulti-modal sensors to extract both verbal and nonverbal cues, social factors,\nand implicit personas, incorporating these social cues into LLM reasoning for\nsocial suggestion generation. Additionally, SocialMind employs a multi-tier\ncollaborative generation strategy and proactive update mechanism to display\nsocial suggestions on Augmented Reality (AR) glasses, ensuring that suggestions\nare timely provided to users without disrupting the natural flow of\nconversation. Evaluations on three public datasets and a user study with 20\nparticipants show that SocialMind achieves 38.3% higher engagement compared to\nbaselines, and 95% of participants are willing to use SocialMind in their live\nsocial interactions."
                },
                "authors": [
                    {
                        "name": "Bufang Yang"
                    },
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Lilin Xu"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04031v1",
                "updated": "2024-12-05T10:09:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    9,
                    13,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:09:13Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    9,
                    13,
                    3,
                    340,
                    0
                ],
                "title": "Dimension Reduction via Random Projection for Privacy in Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dimension Reduction via Random Projection for Privacy in Multi-Agent\n  Systems"
                },
                "summary": "The agents in a Multi-Agent System (MAS) make observations about the system\nand send that information to a fusion center. The fusion center aggregates the\ninformation and concludes about the system parameters with as much accuracy as\npossible. However for the purposes of better efficiency of the system at large,\nthe agents need to append some private parameters to the observed data. In this\nscenario, the data sent to the fusion center is faced with privacy risks. The\ndata communicated to the fusion center must be secured against data privacy\nbreaches and inference attacks in a decentralized manner. However, this in turn\nleads to a loss of utility of the data being sent to the fusion center. We\nquantify the utility and privacy of the system using Cosine similarity. We\nformulate our MAS problem in terms of deducing a concept for which\ncompression-based methods are there in literature. Next, we propose a novel\nsanitization mechanism for our MAS using one such compression-based method\nwhile addressing the utility-privacy tradeoff problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The agents in a Multi-Agent System (MAS) make observations about the system\nand send that information to a fusion center. The fusion center aggregates the\ninformation and concludes about the system parameters with as much accuracy as\npossible. However for the purposes of better efficiency of the system at large,\nthe agents need to append some private parameters to the observed data. In this\nscenario, the data sent to the fusion center is faced with privacy risks. The\ndata communicated to the fusion center must be secured against data privacy\nbreaches and inference attacks in a decentralized manner. However, this in turn\nleads to a loss of utility of the data being sent to the fusion center. We\nquantify the utility and privacy of the system using Cosine similarity. We\nformulate our MAS problem in terms of deducing a concept for which\ncompression-based methods are there in literature. Next, we propose a novel\nsanitization mechanism for our MAS using one such compression-based method\nwhile addressing the utility-privacy tradeoff problem."
                },
                "authors": [
                    {
                        "name": "Puspanjali Ghoshal"
                    },
                    {
                        "name": "Ashok Singh Sairam"
                    }
                ],
                "author_detail": {
                    "name": "Ashok Singh Sairam"
                },
                "author": "Ashok Singh Sairam",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19846v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19846v6",
                "updated": "2024-12-05T09:56:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    56,
                    35,
                    3,
                    340,
                    0
                ],
                "published": "2024-05-30T08:50:55Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    8,
                    50,
                    55,
                    3,
                    151,
                    0
                ],
                "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model"
                },
                "summary": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes."
                },
                "authors": [
                    {
                        "name": "Chaochen Gao"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Qi Fu"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19846v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19846v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1903.04797v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1903.04797v3",
                "updated": "2024-12-05T09:41:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    41,
                    31,
                    3,
                    340,
                    0
                ],
                "published": "2019-03-12T09:28:05Z",
                "published_parsed": [
                    2019,
                    3,
                    12,
                    9,
                    28,
                    5,
                    1,
                    71,
                    0
                ],
                "title": "Elements of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elements of Sequential Monte Carlo"
                },
                "summary": "A core problem in statistics and probabilistic machine learning is to compute\nprobability distributions and expectations. This is the fundamental problem of\nBayesian statistics and machine learning, which frames all inference as\nexpectations with respect to the posterior distribution. The key challenge is\nto approximate these intractable expectations. In this tutorial, we review\nsequential Monte Carlo (SMC), a random-sampling-based class of methods for\napproximate inference. First, we explain the basics of SMC, discuss practical\nissues, and review theoretical results. We then examine two of the main user\ndesign choices: the proposal distributions and the so called intermediate\ntarget distributions. We review recent results on how variational inference and\namortization can be used to learn efficient proposals and target distributions.\nNext, we discuss the SMC estimate of the normalizing constant, how this can be\nused for pseudo-marginal inference and inference evaluation. Throughout the\ntutorial we illustrate the use of SMC on various models commonly used in\nmachine learning, such as stochastic recurrent neural networks, probabilistic\ngraphical models, and probabilistic programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core problem in statistics and probabilistic machine learning is to compute\nprobability distributions and expectations. This is the fundamental problem of\nBayesian statistics and machine learning, which frames all inference as\nexpectations with respect to the posterior distribution. The key challenge is\nto approximate these intractable expectations. In this tutorial, we review\nsequential Monte Carlo (SMC), a random-sampling-based class of methods for\napproximate inference. First, we explain the basics of SMC, discuss practical\nissues, and review theoretical results. We then examine two of the main user\ndesign choices: the proposal distributions and the so called intermediate\ntarget distributions. We review recent results on how variational inference and\namortization can be used to learn efficient proposals and target distributions.\nNext, we discuss the SMC estimate of the normalizing constant, how this can be\nused for pseudo-marginal inference and inference evaluation. Throughout the\ntutorial we illustrate the use of SMC on various models commonly used in\nmachine learning, such as stochastic recurrent neural networks, probabilistic\ngraphical models, and probabilistic programs."
                },
                "authors": [
                    {
                        "name": "Christian A. Naesseth"
                    },
                    {
                        "name": "Fredrik Lindsten"
                    },
                    {
                        "name": "Thomas B. Schn"
                    }
                ],
                "author_detail": {
                    "name": "Thomas B. Schn"
                },
                "author": "Thomas B. Schn",
                "arxiv_comment": "Foundations and Trends in Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1903.04797v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1903.04797v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10393v2",
                "updated": "2024-12-05T09:39:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    39,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-15T18:01:40Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    18,
                    1,
                    40,
                    4,
                    320,
                    0
                ],
                "title": "Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic\n  Programs with Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic\n  Programs with Loops"
                },
                "summary": "We study the problem of bounding the posterior distribution of discrete\nprobabilistic programs with unbounded support, loops, and conditioning. Loops\npose the main difficulty in this setting: even if exact Bayesian inference is\npossible, the state of the art requires user-provided loop invariant templates.\nBy contrast, we aim to find guaranteed bounds, which sandwich the true\ndistribution. They are fully automated, applicable to more programs and provide\nmore provable guarantees than approximate sampling-based inference. Since lower\nbounds can be obtained by unrolling loops, the main challenge is upper bounds,\nand we attack it in two ways. The first is called residual mass semantics,\nwhich is a flat bound based on the residual probability mass of a loop. The\napproach is simple, efficient, and has provable guarantees.\n  The main novelty of our work is the second approach, called geometric bound\nsemantics. It operates on a novel family of distributions, called eventually\ngeometric distributions (EGDs), and can bound the distribution of loops with a\nnew form of loop invariants called contraction invariants. The invariant\nsynthesis problem reduces to a system of polynomial inequality constraints,\nwhich is a decidable problem with automated solvers. If a solution exists, it\nyields an exponentially decreasing bound on the whole distribution, and can\ntherefore bound moments and tail asymptotics as well, not just probabilities as\nin the first approach.\n  Both semantics enjoy desirable theoretical properties. In particular, we\nprove soundness and convergence, i.e. the bounds converge to the exact\nposterior as loops are unrolled further. On the practical side, we describe\nDiabolo, a fully-automated implementation of both semantics, and evaluate them\non a variety of benchmarks from the literature, demonstrating their general\napplicability and the utility of the resulting bounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of bounding the posterior distribution of discrete\nprobabilistic programs with unbounded support, loops, and conditioning. Loops\npose the main difficulty in this setting: even if exact Bayesian inference is\npossible, the state of the art requires user-provided loop invariant templates.\nBy contrast, we aim to find guaranteed bounds, which sandwich the true\ndistribution. They are fully automated, applicable to more programs and provide\nmore provable guarantees than approximate sampling-based inference. Since lower\nbounds can be obtained by unrolling loops, the main challenge is upper bounds,\nand we attack it in two ways. The first is called residual mass semantics,\nwhich is a flat bound based on the residual probability mass of a loop. The\napproach is simple, efficient, and has provable guarantees.\n  The main novelty of our work is the second approach, called geometric bound\nsemantics. It operates on a novel family of distributions, called eventually\ngeometric distributions (EGDs), and can bound the distribution of loops with a\nnew form of loop invariants called contraction invariants. The invariant\nsynthesis problem reduces to a system of polynomial inequality constraints,\nwhich is a decidable problem with automated solvers. If a solution exists, it\nyields an exponentially decreasing bound on the whole distribution, and can\ntherefore bound moments and tail asymptotics as well, not just probabilities as\nin the first approach.\n  Both semantics enjoy desirable theoretical properties. In particular, we\nprove soundness and convergence, i.e. the bounds converge to the exact\nposterior as loops are unrolled further. On the practical side, we describe\nDiabolo, a fully-automated implementation of both semantics, and evaluate them\non a variety of benchmarks from the literature, demonstrating their general\napplicability and the utility of the resulting bounds."
                },
                "authors": [
                    {
                        "name": "Fabian Zaiser"
                    },
                    {
                        "name": "Andrzej S. Murawski"
                    },
                    {
                        "name": "C. -H. Luke Ong"
                    }
                ],
                "author_detail": {
                    "name": "C. -H. Luke Ong"
                },
                "author": "C. -H. Luke Ong",
                "arxiv_doi": "10.1145/3704874",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3704874",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.10393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Full version of the POPL 2025 article, including proofs and other\n  supplementary material",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.3.1; F.3.2; D.2.4; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04003v1",
                "updated": "2024-12-05T09:26:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    26,
                    58,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T09:26:58Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    26,
                    58,
                    3,
                    340,
                    0
                ],
                "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for\n  Cross-Lingual Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marco-LLM: Bridging Languages via Massive Multilingual Training for\n  Cross-Lingual Enhancement"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages."
                },
                "authors": [
                    {
                        "name": "Lingfeng Ming"
                    },
                    {
                        "name": "Bo Zeng"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Tianqi Shi"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Yefeng Liu"
                    },
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Linlong Xu"
                    },
                    {
                        "name": "Yangyang Liu"
                    },
                    {
                        "name": "Xiaohu Zhao"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Heng Liu"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Zifu Shang"
                    },
                    {
                        "name": "Haijun Li"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14438v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14438v3",
                "updated": "2024-12-05T09:23:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    23,
                    13,
                    3,
                    340,
                    0
                ],
                "published": "2024-05-23T11:10:32Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    11,
                    10,
                    32,
                    3,
                    144,
                    0
                ],
                "title": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks"
                },
                "summary": "Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets."
                },
                "authors": [
                    {
                        "name": "Michelle Halbheer"
                    },
                    {
                        "name": "Dominik J. Mhlematter"
                    },
                    {
                        "name": "Alexander Becker"
                    },
                    {
                        "name": "Dominik Narnhofer"
                    },
                    {
                        "name": "Helge Aasen"
                    },
                    {
                        "name": "Konrad Schindler"
                    },
                    {
                        "name": "Mehmet Ozgur Turkoglu"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Ozgur Turkoglu"
                },
                "author": "Mehmet Ozgur Turkoglu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14438v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14438v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.04468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04468v1",
                "updated": "2024-12-05T18:59:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    59,
                    55,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:59:55Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    59,
                    55,
                    3,
                    340,
                    0
                ],
                "title": "NVILA: Efficient Frontier Visual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVILA: Efficient Frontier Visual Language Models"
                },
                "summary": "Visual language models (VLMs) have made significant advances in accuracy in\nrecent years. However, their efficiency has received much less attention. This\npaper introduces NVILA, a family of open VLMs designed to optimize both\nefficiency and accuracy. Building on top of VILA, we improve its model\narchitecture by first scaling up the spatial and temporal resolutions, and then\ncompressing visual tokens. This \"scale-then-compress\" approach enables NVILA to\nefficiently process high-resolution images and long videos. We also conduct a\nsystematic investigation to enhance the efficiency of NVILA throughout its\nentire lifecycle, from training and fine-tuning to deployment. NVILA matches or\nsurpasses the accuracy of many leading open and proprietary VLMs across a wide\nrange of image and video benchmarks. At the same time, it reduces training\ncosts by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by\n1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and\nmodels available to facilitate reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual language models (VLMs) have made significant advances in accuracy in\nrecent years. However, their efficiency has received much less attention. This\npaper introduces NVILA, a family of open VLMs designed to optimize both\nefficiency and accuracy. Building on top of VILA, we improve its model\narchitecture by first scaling up the spatial and temporal resolutions, and then\ncompressing visual tokens. This \"scale-then-compress\" approach enables NVILA to\nefficiently process high-resolution images and long videos. We also conduct a\nsystematic investigation to enhance the efficiency of NVILA throughout its\nentire lifecycle, from training and fine-tuning to deployment. NVILA matches or\nsurpasses the accuracy of many leading open and proprietary VLMs across a wide\nrange of image and video benchmarks. At the same time, it reduces training\ncosts by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by\n1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and\nmodels available to facilitate reproducibility."
                },
                "authors": [
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Zhuoyang Zhang"
                    },
                    {
                        "name": "Yuming Lou"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yuxian Gu"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Cheng-Yu Hsieh"
                    },
                    {
                        "name": "De-An Huang"
                    },
                    {
                        "name": "An-Chieh Cheng"
                    },
                    {
                        "name": "Vishwesh Nath"
                    },
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Daguang Xu"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04432v1",
                "updated": "2024-12-05T18:53:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    53,
                    4,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:53:04Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    53,
                    4,
                    3,
                    340,
                    0
                ],
                "title": "Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation"
                },
                "summary": "In recent years, there has been a significant surge of interest in unifying\nimage comprehension and generation within Large Language Models (LLMs). This\ngrowing interest has prompted us to explore extending this unification to\nvideos. The core challenge lies in developing a versatile video tokenizer that\ncaptures both the spatial characteristics and temporal dynamics of videos to\nobtain representations for LLMs, and the representations can be further decoded\ninto realistic video clips to enable video generation. In this work, we\nintroduce Divot, a Diffusion-Powered Video Tokenizer, which leverages the\ndiffusion process for self-supervised video representation learning. We posit\nthat if a video diffusion model can effectively de-noise video clips by taking\nthe features of a video tokenizer as the condition, then the tokenizer has\nsuccessfully captured robust spatial and temporal information. Additionally,\nthe video diffusion model inherently functions as a de-tokenizer, decoding\nvideos from their representations. Building upon the Divot tokenizer, we\npresent Divot-Vicuna through video-to-text autoregression and text-to-video\ngeneration by modeling the distributions of continuous-valued Divot features\nwith a Gaussian Mixture Model. Experimental results demonstrate that our\ndiffusion-based video tokenizer, when integrated with a pre-trained LLM,\nachieves competitive performance across various video comprehension and\ngeneration benchmarks. The instruction tuned Divot-Vicuna also excels in video\nstorytelling, generating interleaved narratives and corresponding videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a significant surge of interest in unifying\nimage comprehension and generation within Large Language Models (LLMs). This\ngrowing interest has prompted us to explore extending this unification to\nvideos. The core challenge lies in developing a versatile video tokenizer that\ncaptures both the spatial characteristics and temporal dynamics of videos to\nobtain representations for LLMs, and the representations can be further decoded\ninto realistic video clips to enable video generation. In this work, we\nintroduce Divot, a Diffusion-Powered Video Tokenizer, which leverages the\ndiffusion process for self-supervised video representation learning. We posit\nthat if a video diffusion model can effectively de-noise video clips by taking\nthe features of a video tokenizer as the condition, then the tokenizer has\nsuccessfully captured robust spatial and temporal information. Additionally,\nthe video diffusion model inherently functions as a de-tokenizer, decoding\nvideos from their representations. Building upon the Divot tokenizer, we\npresent Divot-Vicuna through video-to-text autoregression and text-to-video\ngeneration by modeling the distributions of continuous-valued Divot features\nwith a Gaussian Mixture Model. Experimental results demonstrate that our\ndiffusion-based video tokenizer, when integrated with a pre-trained LLM,\nachieves competitive performance across various video comprehension and\ngeneration benchmarks. The instruction tuned Divot-Vicuna also excels in video\nstorytelling, generating interleaved narratives and corresponding videos."
                },
                "authors": [
                    {
                        "name": "Yuying Ge"
                    },
                    {
                        "name": "Yizhuo Li"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project released at: https://github.com/TencentARC/Divot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04424v1",
                "updated": "2024-12-05T18:50:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    50,
                    39,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:50:39Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    50,
                    39,
                    3,
                    340,
                    0
                ],
                "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision\n  Encoder and Depth-Breadth Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Florence-VL: Enhancing Vision-Language Models with Generative Vision\n  Encoder and Depth-Breadth Fusion"
                },
                "summary": "We present Florence-VL, a new family of multimodal large language models\n(MLLMs) with enriched visual representations produced by Florence-2, a\ngenerative vision foundation model. Unlike the widely used CLIP-style vision\ntransformer trained by contrastive learning, Florence-2 can capture different\nlevels and aspects of visual features, which are more versatile to be adapted\nto diverse downstream tasks. We propose a novel feature-fusion architecture and\nan innovative training recipe that effectively integrates Florence-2's visual\nfeatures into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we\npropose \"depth-breath fusion (DBFusion)\" to fuse the visual features extracted\nfrom different depths and under multiple prompts. Our model training is\ncomposed of end-to-end pretraining of the whole model followed by finetuning of\nthe projection layer and the LLM, on a carefully designed recipe of diverse\nopen-source datasets that include high-quality image captions and\ninstruction-tuning pairs. Our quantitative analysis and visualization of\nFlorence-VL's visual features show its advantages over popular vision encoders\non vision-language alignment, where the enriched depth and breath play\nimportant roles. Florence-VL achieves significant improvements over existing\nstate-of-the-art MLLMs across various multi-modal and vision-centric benchmarks\ncovering general VQA, perception, hallucination, OCR, Chart,\nknowledge-intensive understanding, etc. To facilitate future research, our\nmodels and the complete training recipe are open-sourced.\nhttps://github.com/JiuhaiChen/Florence-VL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Florence-VL, a new family of multimodal large language models\n(MLLMs) with enriched visual representations produced by Florence-2, a\ngenerative vision foundation model. Unlike the widely used CLIP-style vision\ntransformer trained by contrastive learning, Florence-2 can capture different\nlevels and aspects of visual features, which are more versatile to be adapted\nto diverse downstream tasks. We propose a novel feature-fusion architecture and\nan innovative training recipe that effectively integrates Florence-2's visual\nfeatures into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we\npropose \"depth-breath fusion (DBFusion)\" to fuse the visual features extracted\nfrom different depths and under multiple prompts. Our model training is\ncomposed of end-to-end pretraining of the whole model followed by finetuning of\nthe projection layer and the LLM, on a carefully designed recipe of diverse\nopen-source datasets that include high-quality image captions and\ninstruction-tuning pairs. Our quantitative analysis and visualization of\nFlorence-VL's visual features show its advantages over popular vision encoders\non vision-language alignment, where the enriched depth and breath play\nimportant roles. Florence-VL achieves significant improvements over existing\nstate-of-the-art MLLMs across various multi-modal and vision-centric benchmarks\ncovering general VQA, perception, hallucination, OCR, Chart,\nknowledge-intensive understanding, etc. To facilitate future research, our\nmodels and the complete training recipe are open-sourced.\nhttps://github.com/JiuhaiChen/Florence-VL"
                },
                "authors": [
                    {
                        "name": "Jiuhai Chen"
                    },
                    {
                        "name": "Jianwei Yang"
                    },
                    {
                        "name": "Haiping Wu"
                    },
                    {
                        "name": "Dianqi Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Bin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xiao"
                },
                "author": "Bin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07384v2",
                "updated": "2024-12-05T18:47:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    47,
                    47,
                    3,
                    340,
                    0
                ],
                "published": "2024-03-12T07:45:33Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    7,
                    45,
                    33,
                    1,
                    72,
                    0
                ],
                "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\n  Language Models by Summarizing Training Trajectories of Small Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\n  Language Models by Summarizing Training Trajectories of Small Models"
                },
                "summary": "Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection."
                },
                "authors": [
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Siddhartha Mishra"
                    },
                    {
                        "name": "Jeffrey N Chiang"
                    },
                    {
                        "name": "Baharan Mirzasoleiman"
                    }
                ],
                "author_detail": {
                    "name": "Baharan Mirzasoleiman"
                },
                "author": "Baharan Mirzasoleiman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04415v1",
                "updated": "2024-12-05T18:38:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    38,
                    30,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:38:30Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    38,
                    30,
                    3,
                    340,
                    0
                ],
                "title": "Targeting the Core: A Simple and Effective Method to Attack RAG-based\n  Agents via Direct LLM Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeting the Core: A Simple and Effective Method to Attack RAG-based\n  Agents via Direct LLM Manipulation"
                },
                "summary": "AI agents, powered by large language models (LLMs), have transformed\nhuman-computer interactions by enabling seamless, natural, and context-aware\ncommunication. While these advancements offer immense utility, they also\ninherit and amplify inherent safety risks such as bias, fairness,\nhallucinations, privacy breaches, and a lack of transparency. This paper\ninvestigates a critical vulnerability: adversarial attacks targeting the LLM\ncore within AI agents. Specifically, we test the hypothesis that a deceptively\nsimple adversarial prefix, such as \\textit{Ignore the document}, can compel\nLLMs to produce dangerous or unintended outputs by bypassing their contextual\nsafeguards. Through experimentation, we demonstrate a high attack success rate\n(ASR), revealing the fragility of existing LLM defenses. These findings\nemphasize the urgent need for robust, multi-layered security measures tailored\nto mitigate vulnerabilities at the LLM level and within broader agent-based\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents, powered by large language models (LLMs), have transformed\nhuman-computer interactions by enabling seamless, natural, and context-aware\ncommunication. While these advancements offer immense utility, they also\ninherit and amplify inherent safety risks such as bias, fairness,\nhallucinations, privacy breaches, and a lack of transparency. This paper\ninvestigates a critical vulnerability: adversarial attacks targeting the LLM\ncore within AI agents. Specifically, we test the hypothesis that a deceptively\nsimple adversarial prefix, such as \\textit{Ignore the document}, can compel\nLLMs to produce dangerous or unintended outputs by bypassing their contextual\nsafeguards. Through experimentation, we demonstrate a high attack success rate\n(ASR), revealing the fragility of existing LLM defenses. These findings\nemphasize the urgent need for robust, multi-layered security measures tailored\nto mitigate vulnerabilities at the LLM level and within broader agent-based\narchitectures."
                },
                "authors": [
                    {
                        "name": "Xuying Li"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Yuji Kosuga"
                    },
                    {
                        "name": "Yasuhiro Yoshida"
                    },
                    {
                        "name": "Victor Bian"
                    }
                ],
                "author_detail": {
                    "name": "Victor Bian"
                },
                "author": "Victor Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12924v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12924v3",
                "updated": "2024-12-05T18:35:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    35,
                    26,
                    3,
                    340,
                    0
                ],
                "published": "2024-09-04T03:17:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    3,
                    17,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "WaveletGPT: Wavelets Meet Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveletGPT: Wavelets Meet Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. They\nare trained on a simple objective: to predict the next token given the previous\ncontext. We live in a world where most of the data around us, e.g., text,\naudio, and music, has a multi-scale structure associated with it. This paper\ninfuses LLMs with traditional signal processing ideas, namely wavelets, during\npre-training to take advantage of the structure. Without adding \\textbf{any\nextra parameters} to a GPT-style LLM architecture, we achieve the same\npre-training performance almost twice as fast in text, raw audio, and symbolic\nmusic. This is achieved by imposing a structure on intermediate embeddings.\nWhen trained for the same number of training steps, we achieve significant\ngains in performance, which is comparable to pre-training a larger neural\narchitecture. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every Transformer\ndecoder block. This work will hopefully pave the way for incorporating\nmulti-rate signal processing ideas into traditional LLM pre-training. Further,\nwe showcase pushing model performance by improving internal structure instead\nof just going after scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. They\nare trained on a simple objective: to predict the next token given the previous\ncontext. We live in a world where most of the data around us, e.g., text,\naudio, and music, has a multi-scale structure associated with it. This paper\ninfuses LLMs with traditional signal processing ideas, namely wavelets, during\npre-training to take advantage of the structure. Without adding \\textbf{any\nextra parameters} to a GPT-style LLM architecture, we achieve the same\npre-training performance almost twice as fast in text, raw audio, and symbolic\nmusic. This is achieved by imposing a structure on intermediate embeddings.\nWhen trained for the same number of training steps, we achieve significant\ngains in performance, which is comparable to pre-training a larger neural\narchitecture. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every Transformer\ndecoder block. This work will hopefully pave the way for incorporating\nmulti-rate signal processing ideas into traditional LLM pre-training. Further,\nwe showcase pushing model performance by improving internal structure instead\nof just going after scale."
                },
                "authors": [
                    {
                        "name": "Prateek Verma"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Verma"
                },
                "author": "Prateek Verma",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12924v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12924v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04378v1",
                "updated": "2024-12-05T17:54:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    54,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T17:54:27Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    54,
                    27,
                    3,
                    340,
                    0
                ],
                "title": "Discriminative Fine-tuning of LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discriminative Fine-tuning of LVLMs"
                },
                "summary": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the\nde facto approach for discriminative vision-language representation learning.\nHowever, these models have limited language understanding, often exhibiting a\n\"bag of words\" behavior. At the same time, Large Vision-Language Models\n(LVLMs), which combine vision encoders with LLMs, have been shown capable of\ndetailed vision-language reasoning, yet their autoregressive nature renders\nthem less suitable for discriminative tasks.\n  In this work, we propose to combine \"the best of both worlds\": a new training\napproach for discriminative fine-tuning of LVLMs that results in strong\ndiscriminative and compositional capabilities. Essentially, our approach\nconverts a generative LVLM into a discriminative one, unlocking its capability\nfor powerful image-text discrimination combined with enhanced language\nunderstanding.\n  Our contributions include: (1) A carefully designed training/optimization\nframework that utilizes image-text pairs of variable length and granularity for\ntraining the model with both contrastive and next-token prediction losses. This\nis accompanied by ablation studies that justify the necessity of our\nframework's components. (2) A parameter-efficient adaptation method using a\ncombination of soft prompting and LoRA adapters. (3) Significant improvements\nover state-of-the-art CLIP-like models of similar size, including standard\nimage-text retrieval benchmarks and notable gains in compositionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the\nde facto approach for discriminative vision-language representation learning.\nHowever, these models have limited language understanding, often exhibiting a\n\"bag of words\" behavior. At the same time, Large Vision-Language Models\n(LVLMs), which combine vision encoders with LLMs, have been shown capable of\ndetailed vision-language reasoning, yet their autoregressive nature renders\nthem less suitable for discriminative tasks.\n  In this work, we propose to combine \"the best of both worlds\": a new training\napproach for discriminative fine-tuning of LVLMs that results in strong\ndiscriminative and compositional capabilities. Essentially, our approach\nconverts a generative LVLM into a discriminative one, unlocking its capability\nfor powerful image-text discrimination combined with enhanced language\nunderstanding.\n  Our contributions include: (1) A carefully designed training/optimization\nframework that utilizes image-text pairs of variable length and granularity for\ntraining the model with both contrastive and next-token prediction losses. This\nis accompanied by ablation studies that justify the necessity of our\nframework's components. (2) A parameter-efficient adaptation method using a\ncombination of soft prompting and LoRA adapters. (3) Significant improvements\nover state-of-the-art CLIP-like models of similar size, including standard\nimage-text retrieval benchmarks and notable gains in compositionality."
                },
                "authors": [
                    {
                        "name": "Yassine Ouali"
                    },
                    {
                        "name": "Adrian Bulat"
                    },
                    {
                        "name": "Alexandros Xenos"
                    },
                    {
                        "name": "Anestis Zaganidis"
                    },
                    {
                        "name": "Ioannis Maniadis Metaxas"
                    },
                    {
                        "name": "Georgios Tzimiropoulos"
                    },
                    {
                        "name": "Brais Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Brais Martinez"
                },
                "author": "Brais Martinez",
                "arxiv_comment": "Preprint. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02819v2",
                "updated": "2024-12-05T17:51:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    51,
                    20,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-03T20:35:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    35,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "CNNSum: Exploring Long-Conext Summarization with Large Language Models\n  in Chinese Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNNSum: Exploring Long-Conext Summarization with Large Language Models\n  in Chinese Novels"
                },
                "summary": "Large Language Models (LLMs) have been well-researched in many long-context\ntasks. However, due to high annotation costs, high-quality long-context summary\ndatasets for training or evaluation are scarce, limiting further research. In\nthis work, we introduce CNNSum, a new multi-scale Chinese long-context novel\nsummarization benchmark, including four subsets, length covering\n16k\\textasciitilde128k, 695 samples in total, the annotations are human-driven.\nWe evaluate commercial and open-source models on CNNSum and conduct a detailed\nanalysis. Based on the observations, we further conduct fine-tuning exploration\nwith short-context summary data. In our study: (1) GPT-4o underperformed, due\nto excessive subjective commentary. (2) Currently, long-context summarization\nmainly relies on memory ability, small LLMs with stable longer context lengths\nare the most cost-effective. Using long data concatenated from short-context\nsummaries makes a significant improvement. (3) Prompt templates may cause a\nlarge performance gap but can be mitigated through fine-tuning. (4) Fine-tuned\nChat or Instruction versions may harm the Base model and further fine-tuning\ncannot bridge performance gap. (5) while models with RoPE base scaling exhibit\nstrong extrapolation potential, their performance may vary significantly when\ncombined with other interpolation methods and need careful selection. (6)\nCNNSum provides more reliable and insightful evaluation results than other\nbenchmarks. We release CNNSum to advance research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been well-researched in many long-context\ntasks. However, due to high annotation costs, high-quality long-context summary\ndatasets for training or evaluation are scarce, limiting further research. In\nthis work, we introduce CNNSum, a new multi-scale Chinese long-context novel\nsummarization benchmark, including four subsets, length covering\n16k\\textasciitilde128k, 695 samples in total, the annotations are human-driven.\nWe evaluate commercial and open-source models on CNNSum and conduct a detailed\nanalysis. Based on the observations, we further conduct fine-tuning exploration\nwith short-context summary data. In our study: (1) GPT-4o underperformed, due\nto excessive subjective commentary. (2) Currently, long-context summarization\nmainly relies on memory ability, small LLMs with stable longer context lengths\nare the most cost-effective. Using long data concatenated from short-context\nsummaries makes a significant improvement. (3) Prompt templates may cause a\nlarge performance gap but can be mitigated through fine-tuning. (4) Fine-tuned\nChat or Instruction versions may harm the Base model and further fine-tuning\ncannot bridge performance gap. (5) while models with RoPE base scaling exhibit\nstrong extrapolation potential, their performance may vary significantly when\ncombined with other interpolation methods and need careful selection. (6)\nCNNSum provides more reliable and insightful evaluation results than other\nbenchmarks. We release CNNSum to advance research in this field."
                },
                "authors": [
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12259v2",
                "updated": "2024-12-05T17:47:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    47,
                    30,
                    3,
                    340,
                    0
                ],
                "published": "2024-06-18T04:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    4,
                    24,
                    30,
                    1,
                    170,
                    0
                ],
                "title": "Adversarial Attacks on Large Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Attacks on Large Language Models in Medicine"
                },
                "summary": "The integration of Large Language Models (LLMs) into healthcare applications\noffers promising advancements in medical diagnostics, treatment\nrecommendations, and patient care. However, the susceptibility of LLMs to\nadversarial attacks poses a significant threat, potentially leading to harmful\noutcomes in delicate medical contexts. This study investigates the\nvulnerability of LLMs to two types of adversarial attacks in three medical\ntasks. Utilizing real-world patient data, we demonstrate that both open-source\nand proprietary LLMs are susceptible to manipulation across multiple tasks.\nThis research further reveals that domain-specific tasks demand more\nadversarial data in model fine-tuning than general domain tasks for effective\nattack execution, especially for more capable models. We discover that while\nintegrating adversarial data does not markedly degrade overall model\nperformance on medical benchmarks, it does lead to noticeable shifts in\nfine-tuned model weights, suggesting a potential pathway for detecting and\ncountering model attacks. This research highlights the urgent need for robust\nsecurity measures and the development of defensive mechanisms to safeguard LLMs\nin medical applications, to ensure their safe and effective deployment in\nhealthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into healthcare applications\noffers promising advancements in medical diagnostics, treatment\nrecommendations, and patient care. However, the susceptibility of LLMs to\nadversarial attacks poses a significant threat, potentially leading to harmful\noutcomes in delicate medical contexts. This study investigates the\nvulnerability of LLMs to two types of adversarial attacks in three medical\ntasks. Utilizing real-world patient data, we demonstrate that both open-source\nand proprietary LLMs are susceptible to manipulation across multiple tasks.\nThis research further reveals that domain-specific tasks demand more\nadversarial data in model fine-tuning than general domain tasks for effective\nattack execution, especially for more capable models. We discover that while\nintegrating adversarial data does not markedly degrade overall model\nperformance on medical benchmarks, it does lead to noticeable shifts in\nfine-tuned model weights, suggesting a potential pathway for detecting and\ncountering model attacks. This research highlights the urgent need for robust\nsecurity measures and the development of defensive mechanisms to safeguard LLMs\nin medical applications, to ensure their safe and effective deployment in\nhealthcare settings."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02589v2",
                "updated": "2024-12-05T17:41:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    41,
                    48,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-04T20:29:35Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    20,
                    29,
                    35,
                    0,
                    309,
                    0
                ],
                "title": "Context-Informed Machine Translation of Manga using Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Informed Machine Translation of Manga using Multimodal Large\n  Language Models"
                },
                "summary": "Due to the significant time and effort required for handcrafting\ntranslations, most manga never leave the domestic Japanese market. Automatic\nmanga translation is a promising potential solution. However, it is a budding\nand underdeveloped field and presents complexities even greater than those\nfound in standard translation due to the need to effectively incorporate visual\nelements into the translation process to resolve ambiguities. In this work, we\ninvestigate to what extent multimodal large language models (LLMs) can provide\neffective manga translation, thereby assisting manga authors and publishers in\nreaching wider audiences. Specifically, we propose a methodology that leverages\nthe vision component of multimodal LLMs to improve translation quality and\nevaluate the impact of translation unit size, context length, and propose a\ntoken efficient approach for manga translation. Moreover, we introduce a new\nevaluation dataset -- the first parallel Japanese-Polish manga translation\ndataset -- as part of a benchmark to be used in future research. Finally, we\ncontribute an open-source software suite, enabling others to benchmark LLMs for\nmanga translation. Our findings demonstrate that our proposed methods achieve\nstate-of-the-art results for Japanese-English translation and set a new\nstandard for Japanese-Polish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the significant time and effort required for handcrafting\ntranslations, most manga never leave the domestic Japanese market. Automatic\nmanga translation is a promising potential solution. However, it is a budding\nand underdeveloped field and presents complexities even greater than those\nfound in standard translation due to the need to effectively incorporate visual\nelements into the translation process to resolve ambiguities. In this work, we\ninvestigate to what extent multimodal large language models (LLMs) can provide\neffective manga translation, thereby assisting manga authors and publishers in\nreaching wider audiences. Specifically, we propose a methodology that leverages\nthe vision component of multimodal LLMs to improve translation quality and\nevaluate the impact of translation unit size, context length, and propose a\ntoken efficient approach for manga translation. Moreover, we introduce a new\nevaluation dataset -- the first parallel Japanese-Polish manga translation\ndataset -- as part of a benchmark to be used in future research. Finally, we\ncontribute an open-source software suite, enabling others to benchmark LLMs for\nmanga translation. Our findings demonstrate that our proposed methods achieve\nstate-of-the-art results for Japanese-English translation and set a new\nstandard for Japanese-Polish."
                },
                "authors": [
                    {
                        "name": "Philip Lippmann"
                    },
                    {
                        "name": "Konrad Skublicki"
                    },
                    {
                        "name": "Joshua Tanner"
                    },
                    {
                        "name": "Shonosuke Ishiwatari"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04363v1",
                "updated": "2024-12-05T17:22:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    22,
                    4,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T17:22:04Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    22,
                    4,
                    3,
                    340,
                    0
                ],
                "title": "Challenges in Trustworthy Human Evaluation of Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges in Trustworthy Human Evaluation of Chatbots"
                },
                "summary": "Open community-driven platforms like Chatbot Arena that collect user\npreference data from site visitors have gained a reputation as one of the most\ntrustworthy publicly available benchmarks for LLM performance. While now\nstandard, it is tricky to implement effective guardrails to collect\nhigh-quality annotations from humans. In this paper, we demonstrate that three\nsources of bad annotations, both malicious and otherwise, can corrupt the\nreliability of open leaderboard rankings. In particular, we show that only 10\\%\nof poor quality votes by apathetic (site visitors not appropriately\nincentivized to give correct votes) or adversarial (bad actors seeking to\ninflate the ranking of a target model) annotators can change the rankings of\nmodels by up to 5 places on the leaderboard. Finally, we discuss open\nchallenges in ensuring high-quality human annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open community-driven platforms like Chatbot Arena that collect user\npreference data from site visitors have gained a reputation as one of the most\ntrustworthy publicly available benchmarks for LLM performance. While now\nstandard, it is tricky to implement effective guardrails to collect\nhigh-quality annotations from humans. In this paper, we demonstrate that three\nsources of bad annotations, both malicious and otherwise, can corrupt the\nreliability of open leaderboard rankings. In particular, we show that only 10\\%\nof poor quality votes by apathetic (site visitors not appropriately\nincentivized to give correct votes) or adversarial (bad actors seeking to\ninflate the ranking of a target model) annotators can change the rankings of\nmodels by up to 5 places on the leaderboard. Finally, we discuss open\nchallenges in ensuring high-quality human annotations."
                },
                "authors": [
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tanya Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Tanya Goyal"
                },
                "author": "Tanya Goyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04343v1",
                "updated": "2024-12-05T17:01:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    1,
                    9,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T17:01:09Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    1,
                    9,
                    3,
                    340,
                    0
                ],
                "title": "RMD: A Simple Baseline for More General Human Motion Generation via\n  Training-free Retrieval-Augmented Motion Diffuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMD: A Simple Baseline for More General Human Motion Generation via\n  Training-free Retrieval-Augmented Motion Diffuse"
                },
                "summary": "While motion generation has made substantial progress, its practical\napplication remains constrained by dataset diversity and scale, limiting its\nability to handle out-of-distribution scenarios. To address this, we propose a\nsimple and effective baseline, RMD, which enhances the generalization of motion\ngeneration through retrieval-augmented techniques. Unlike previous\nretrieval-based methods, RMD requires no additional training and offers three\nkey advantages: (1) the external retrieval database can be flexibly replaced;\n(2) body parts from the motion database can be reused, with an LLM facilitating\nsplitting and recombination; and (3) a pre-trained motion diffusion model\nserves as a prior to improve the quality of motions obtained through retrieval\nand direct combination. Without any training, RMD achieves state-of-the-art\nperformance, with notable advantages on out-of-distribution data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While motion generation has made substantial progress, its practical\napplication remains constrained by dataset diversity and scale, limiting its\nability to handle out-of-distribution scenarios. To address this, we propose a\nsimple and effective baseline, RMD, which enhances the generalization of motion\ngeneration through retrieval-augmented techniques. Unlike previous\nretrieval-based methods, RMD requires no additional training and offers three\nkey advantages: (1) the external retrieval database can be flexibly replaced;\n(2) body parts from the motion database can be reused, with an LLM facilitating\nsplitting and recombination; and (3) a pre-trained motion diffusion model\nserves as a prior to improve the quality of motions obtained through retrieval\nand direct combination. Without any training, RMD achieves state-of-the-art\nperformance, with notable advantages on out-of-distribution data."
                },
                "authors": [
                    {
                        "name": "Zhouyingcheng Liao"
                    },
                    {
                        "name": "Mingyuan Zhang"
                    },
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Taku Komura"
                    }
                ],
                "author_detail": {
                    "name": "Taku Komura"
                },
                "author": "Taku Komura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04342v1",
                "updated": "2024-12-05T17:00:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    0,
                    32,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T17:00:32Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    0,
                    32,
                    3,
                    340,
                    0
                ],
                "title": "Retrieval-Augmented Machine Translation with Unstructured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Machine Translation with Unstructured Knowledge"
                },
                "summary": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance models' MT ability.\nHowever, a large amount of world knowledge is organized in unstructured\ndocuments, and might not be fully paired across different languages. In this\npaper, we study retrieval-augmented MT using unstructured documents.\nSpecifically, we build RAGtrans, the first benchmark to train and evaluate\nLLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples\ncollected via GPT-4o and human translators. Besides, documents from different\nlanguages are also provided to supply the knowledge to these samples. Based on\nRAGtrans, we further propose a multi-task training method to teach LLMs how to\nuse information from multilingual documents during their translation. The\nmethod uses existing multilingual corpora to create auxiliary training\nobjectives without additional labeling requirements. Extensive experiments show\nthat the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance models' MT ability.\nHowever, a large amount of world knowledge is organized in unstructured\ndocuments, and might not be fully paired across different languages. In this\npaper, we study retrieval-augmented MT using unstructured documents.\nSpecifically, we build RAGtrans, the first benchmark to train and evaluate\nLLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples\ncollected via GPT-4o and human translators. Besides, documents from different\nlanguages are also provided to supply the knowledge to these samples. Based on\nRAGtrans, we further propose a multi-task training method to teach LLMs how to\nuse information from multilingual documents during their translation. The\nmethod uses existing multilingual corpora to create auxiliary training\nobjectives without additional labeling requirements. Extensive experiments show\nthat the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores."
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04332v1",
                "updated": "2024-12-05T16:48:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    48,
                    16,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:48:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    48,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "Liquid: Language Models are Scalable Multi-modal Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid: Language Models are Scalable Multi-modal Generators"
                },
                "summary": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Junfeng Wu"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Chuofan Ma"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Zehuan Yuan"
                    },
                    {
                        "name": "Song Bai"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Technical report. Will be updated soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04323v1",
                "updated": "2024-12-05T16:39:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    39,
                    1,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:39:01Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    39,
                    1,
                    3,
                    340,
                    0
                ],
                "title": "GRAM: Generalization in Deep RL with a Robust Adaptation Module",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRAM: Generalization in Deep RL with a Robust Adaptation Module"
                },
                "summary": "The reliable deployment of deep reinforcement learning in real-world settings\nrequires the ability to generalize across a variety of conditions, including\nboth in-distribution scenarios seen during training as well as novel\nout-of-distribution scenarios. In this work, we present a framework for\ndynamics generalization in deep reinforcement learning that unifies these two\ndistinct types of generalization within a single architecture. We introduce a\nrobust adaptation module that provides a mechanism for identifying and reacting\nto both in-distribution and out-of-distribution environment dynamics, along\nwith a joint training pipeline that combines the goals of in-distribution\nadaptation and out-of-distribution robustness. Our algorithm GRAM achieves\nstrong generalization performance across in-distribution and\nout-of-distribution scenarios upon deployment, which we demonstrate on a\nvariety of realistic simulated locomotion tasks with a quadruped robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reliable deployment of deep reinforcement learning in real-world settings\nrequires the ability to generalize across a variety of conditions, including\nboth in-distribution scenarios seen during training as well as novel\nout-of-distribution scenarios. In this work, we present a framework for\ndynamics generalization in deep reinforcement learning that unifies these two\ndistinct types of generalization within a single architecture. We introduce a\nrobust adaptation module that provides a mechanism for identifying and reacting\nto both in-distribution and out-of-distribution environment dynamics, along\nwith a joint training pipeline that combines the goals of in-distribution\nadaptation and out-of-distribution robustness. Our algorithm GRAM achieves\nstrong generalization performance across in-distribution and\nout-of-distribution scenarios upon deployment, which we demonstrate on a\nvariety of realistic simulated locomotion tasks with a quadruped robot."
                },
                "authors": [
                    {
                        "name": "James Queeney"
                    },
                    {
                        "name": "Xiaoyi Cai"
                    },
                    {
                        "name": "Mouhacine Benosman"
                    },
                    {
                        "name": "Jonathan P. How"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan P. How"
                },
                "author": "Jonathan P. How",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04318v1",
                "updated": "2024-12-05T16:34:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    34,
                    20,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:34:20Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    34,
                    20,
                    3,
                    340,
                    0
                ],
                "title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation"
                },
                "summary": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token."
                },
                "authors": [
                    {
                        "name": "Fredrik Carlsson"
                    },
                    {
                        "name": "Fangyu Liu"
                    },
                    {
                        "name": "Daniel Ward"
                    },
                    {
                        "name": "Murathan Kurfali"
                    },
                    {
                        "name": "Joakim Nivre"
                    }
                ],
                "author_detail": {
                    "name": "Joakim Nivre"
                },
                "author": "Joakim Nivre",
                "arxiv_comment": "Under review at ICLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04315v1",
                "updated": "2024-12-05T16:31:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    31,
                    13,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:31:13Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    31,
                    13,
                    3,
                    340,
                    0
                ],
                "title": "Densing Law of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Densing Law of LLMs"
                },
                "summary": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead."
                },
                "authors": [
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Jie Cai"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Guoyang Zeng"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18825v2",
                "updated": "2024-12-05T16:27:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    27,
                    8,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-27T23:58:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    23,
                    58,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language\n  Models for Reward Design in Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language\n  Models for Reward Design in Robotics"
                },
                "summary": "Reinforcement learning (RL) has demonstrated compelling performance in\nrobotic tasks, but its success often hinges on the design of complex, ad hoc\nreward functions. Researchers have explored how Large Language Models (LLMs)\ncould enable non-expert users to specify reward functions more easily. However,\nLLMs struggle to balance the importance of different features, generalize\npoorly to out-of-distribution robotic tasks, and cannot represent the problem\nproperly with only text-based descriptions. To address these challenges, we\npropose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a\nnovel framework that combines natural language guidance with visual user\ndemonstrations to align robot behavior with user intentions better. By\nincorporating visual inputs, ELEMENTAL overcomes the limitations of text-only\ntask specifications, while leveraging inverse reinforcement learning (IRL) to\nbalance feature weights and match the demonstrated behaviors optimally.\nELEMENTAL also introduces an iterative feedback-loop through self-reflection to\nimprove feature, reward, and policy learning. Our experiment results\ndemonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, and\nachieves 41.3% better generalization in out-of-distribution tasks, highlighting\nits robustness in LfD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has demonstrated compelling performance in\nrobotic tasks, but its success often hinges on the design of complex, ad hoc\nreward functions. Researchers have explored how Large Language Models (LLMs)\ncould enable non-expert users to specify reward functions more easily. However,\nLLMs struggle to balance the importance of different features, generalize\npoorly to out-of-distribution robotic tasks, and cannot represent the problem\nproperly with only text-based descriptions. To address these challenges, we\npropose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a\nnovel framework that combines natural language guidance with visual user\ndemonstrations to align robot behavior with user intentions better. By\nincorporating visual inputs, ELEMENTAL overcomes the limitations of text-only\ntask specifications, while leveraging inverse reinforcement learning (IRL) to\nbalance feature weights and match the demonstrated behaviors optimally.\nELEMENTAL also introduces an iterative feedback-loop through self-reflection to\nimprove feature, reward, and policy learning. Our experiment results\ndemonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, and\nachieves 41.3% better generalization in out-of-distribution tasks, highlighting\nits robustness in LfD."
                },
                "authors": [
                    {
                        "name": "Letian Chen"
                    },
                    {
                        "name": "Matthew Gombolay"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gombolay"
                },
                "author": "Matthew Gombolay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04307v1",
                "updated": "2024-12-05T16:26:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    37,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:26:37Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    37,
                    3,
                    340,
                    0
                ],
                "title": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark"
                },
                "summary": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset will be made available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset will be made available on GitHub."
                },
                "authors": [
                    {
                        "name": "Changsheng Gao"
                    },
                    {
                        "name": "Yifan Ma"
                    },
                    {
                        "name": "Qiaoxi Chen"
                    },
                    {
                        "name": "Yenan Xu"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04305v1",
                "updated": "2024-12-05T16:26:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    31,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:26:31Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    31,
                    3,
                    340,
                    0
                ],
                "title": "ALMA: Alignment with Minimal Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALMA: Alignment with Minimal Annotation"
                },
                "summary": "Recent approaches to large language model (LLM) alignment typically require\nmillions of human annotations or rely on external aligned models for synthetic\ndata generation. This paper introduces ALMA: Alignment with Minimal Annotation,\ndemonstrating that effective alignment can be achieved using only 9,000 labeled\nexamples -- less than 1% of conventional approaches. ALMA generates large\namounts of high-quality synthetic alignment data through new techniques:\ndiverse prompt synthesis via few-shot learning, diverse response generation\nwith multiple model checkpoints, and judge (reward model) enhancement through\nscore aggregation and self-distillation. Using only a pretrained Llama3 base\nmodel, 5,000 SFT examples, and 4,000 judge annotations, ALMA achieves\nperformance close to Llama3-Instruct across diverse alignment benchmarks (e.g.,\n0.1% difference on AlpacaEval 2.0 score). These results are achieved with a\nmulti-round, self-bootstrapped data synthesis and training recipe that\ncontinues to improve for 10 rounds, surpassing the typical 3-round ceiling of\nprevious methods. These results suggest that base models already possess\nsufficient knowledge for effective alignment, and that synthetic data\ngeneration methods can expose it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent approaches to large language model (LLM) alignment typically require\nmillions of human annotations or rely on external aligned models for synthetic\ndata generation. This paper introduces ALMA: Alignment with Minimal Annotation,\ndemonstrating that effective alignment can be achieved using only 9,000 labeled\nexamples -- less than 1% of conventional approaches. ALMA generates large\namounts of high-quality synthetic alignment data through new techniques:\ndiverse prompt synthesis via few-shot learning, diverse response generation\nwith multiple model checkpoints, and judge (reward model) enhancement through\nscore aggregation and self-distillation. Using only a pretrained Llama3 base\nmodel, 5,000 SFT examples, and 4,000 judge annotations, ALMA achieves\nperformance close to Llama3-Instruct across diverse alignment benchmarks (e.g.,\n0.1% difference on AlpacaEval 2.0 score). These results are achieved with a\nmulti-round, self-bootstrapped data synthesis and training recipe that\ncontinues to improve for 10 rounds, surpassing the typical 3-round ceiling of\nprevious methods. These results suggest that base models already possess\nsufficient knowledge for effective alignment, and that synthetic data\ngeneration methods can expose it."
                },
                "authors": [
                    {
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "name": "Leonid Shamis"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Andrew Cohen"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Marjan Ghazvininejad"
                    }
                ],
                "author_detail": {
                    "name": "Marjan Ghazvininejad"
                },
                "author": "Marjan Ghazvininejad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15796v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15796v5",
                "updated": "2024-12-05T16:13:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    13,
                    9,
                    3,
                    340,
                    0
                ],
                "published": "2024-06-22T09:40:07Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    9,
                    40,
                    7,
                    5,
                    174,
                    0
                ],
                "title": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis"
                },
                "summary": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs."
                },
                "authors": [
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15796v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15796v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04291v1",
                "updated": "2024-12-05T16:12:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    12,
                    6,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T16:12:06Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    12,
                    6,
                    3,
                    340,
                    0
                ],
                "title": "Evolutionary Pre-Prompt Optimization for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Pre-Prompt Optimization for Mathematical Reasoning"
                },
                "summary": "Recent advancements have highlighted that large language models (LLMs), when\ngiven a small set of task-specific examples, demonstrate remarkable\nproficiency, a capability that extends to complex reasoning tasks. In\nparticular, the combination of few-shot learning with the chain-of-thought\n(CoT) approach has been pivotal in steering models towards more logically\nconsistent conclusions. This paper explores the optimization of example\nselection for designing effective CoT pre-prompts and shows that the choice of\nthe optimization algorithm, typically in favor of comparison-based methods such\nas evolutionary computation, significantly enhances efficacy and feasibility.\nSpecifically, thanks to a limited exploitative and overfitted optimization,\nEvolutionary Pre-Prompt Optimization (EPPO) brings an improvement over the\nnaive few-shot approach exceeding 10 absolute points in exact match scores on\nbenchmark datasets such as GSM8k and MathQA. These gains are consistent across\nvarious contexts and are further amplified when integrated with\nself-consistency (SC)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have highlighted that large language models (LLMs), when\ngiven a small set of task-specific examples, demonstrate remarkable\nproficiency, a capability that extends to complex reasoning tasks. In\nparticular, the combination of few-shot learning with the chain-of-thought\n(CoT) approach has been pivotal in steering models towards more logically\nconsistent conclusions. This paper explores the optimization of example\nselection for designing effective CoT pre-prompts and shows that the choice of\nthe optimization algorithm, typically in favor of comparison-based methods such\nas evolutionary computation, significantly enhances efficacy and feasibility.\nSpecifically, thanks to a limited exploitative and overfitted optimization,\nEvolutionary Pre-Prompt Optimization (EPPO) brings an improvement over the\nnaive few-shot approach exceeding 10 absolute points in exact match scores on\nbenchmark datasets such as GSM8k and MathQA. These gains are consistent across\nvarious contexts and are further amplified when integrated with\nself-consistency (SC)"
                },
                "authors": [
                    {
                        "name": "Mathurin Videau"
                    },
                    {
                        "name": "Alessandro Leite"
                    },
                    {
                        "name": "Marc Schoenauer"
                    },
                    {
                        "name": "Olivier Teytaud"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Teytaud"
                },
                "author": "Olivier Teytaud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04277v1",
                "updated": "2024-12-05T15:59:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    59,
                    29,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:59:29Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    59,
                    29,
                    3,
                    340,
                    0
                ],
                "title": "Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic"
                },
                "summary": "Large Language Models (LLMs) have shown impressive results in multiple\ndomains of natural language processing (NLP) but are mainly focused on the\nEnglish language. Recently, more LLMs have incorporated a larger proportion of\nmultilingual text to represent low-resource languages. In Arabic NLP, several\nArabic-centric LLMs have shown remarkable results on multiple benchmarks in the\npast two years. However, most Arabic LLMs have more than 7 billion parameters,\nwhich increases their hardware requirements and inference latency, when\ncompared to smaller LLMs. This paper introduces Arabic Stable LM 1.6B in a base\nand chat version as a small but powerful Arabic-centric LLM. Our Arabic Stable\nLM 1.6B chat model achieves impressive results on several benchmarks beating\nmultiple models with up to 8x the parameters. In addition, we show the benefit\nof mixing in synthetic instruction tuning data by augmenting our fine-tuning\ndata with a large synthetic dialogue dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive results in multiple\ndomains of natural language processing (NLP) but are mainly focused on the\nEnglish language. Recently, more LLMs have incorporated a larger proportion of\nmultilingual text to represent low-resource languages. In Arabic NLP, several\nArabic-centric LLMs have shown remarkable results on multiple benchmarks in the\npast two years. However, most Arabic LLMs have more than 7 billion parameters,\nwhich increases their hardware requirements and inference latency, when\ncompared to smaller LLMs. This paper introduces Arabic Stable LM 1.6B in a base\nand chat version as a small but powerful Arabic-centric LLM. Our Arabic Stable\nLM 1.6B chat model achieves impressive results on several benchmarks beating\nmultiple models with up to 8x the parameters. In addition, we show the benefit\nof mixing in synthetic instruction tuning data by augmenting our fine-tuning\ndata with a large synthetic dialogue dataset."
                },
                "authors": [
                    {
                        "name": "Zaid Alyafeai"
                    },
                    {
                        "name": "Michael Pieler"
                    },
                    {
                        "name": "Hannah Teufel"
                    },
                    {
                        "name": "Jonathan Tow"
                    },
                    {
                        "name": "Marco Bellagente"
                    },
                    {
                        "name": "Duy Phung"
                    },
                    {
                        "name": "Nikhil Pinnaparaju"
                    },
                    {
                        "name": "Reshinth Adithyan"
                    },
                    {
                        "name": "Paulo Rocha"
                    },
                    {
                        "name": "Maksym Zhuravinskyi"
                    },
                    {
                        "name": "Carlos Riquelme"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Riquelme"
                },
                "author": "Carlos Riquelme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04272v1",
                "updated": "2024-12-05T15:54:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    54,
                    16,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:54:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    54,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "PoTable: Programming Standardly on Table-based Reasoning Like a Human\n  Analyst",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoTable: Programming Standardly on Table-based Reasoning Like a Human\n  Analyst"
                },
                "summary": "Table-based reasoning has garnered substantial research interest,\nparticularly in its integration with Large Language Model (LLM) which has\nrevolutionized the general reasoning paradigm. Numerous LLM-based studies\nintroduce symbolic tools (e.g., databases, Python) as assistants to extend\nhuman-like abilities in structured table understanding and complex arithmetic\ncomputations. However, these studies can be improved better in simulating human\ncognitive behavior when using symbolic tools, as they still suffer from\nlimitations of non-standard logical splits and constrained operation pools. In\nthis study, we propose PoTable as a novel table-based reasoning method that\nsimulates a human tabular analyst, which integrates a Python interpreter as the\nreal-time executor accompanied by an LLM-based operation planner and code\ngenerator. Specifically, PoTable follows a human-like logical stage split and\nextends the operation pool into an open-world space without any constraints.\nThrough planning and executing in each distinct stage, PoTable standardly\ncompletes the entire reasoning process and produces superior reasoning results\nalong with highly accurate, steply commented and completely executable\nprograms. Accordingly, the effectiveness and explainability of PoTable are\nfully demonstrated. Extensive experiments over three evaluation datasets from\ntwo public benchmarks on two backbones show the outstanding performance of our\napproach. In particular, GPT-based PoTable achieves over 4% higher absolute\naccuracy than runner-ups on all evaluation datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-based reasoning has garnered substantial research interest,\nparticularly in its integration with Large Language Model (LLM) which has\nrevolutionized the general reasoning paradigm. Numerous LLM-based studies\nintroduce symbolic tools (e.g., databases, Python) as assistants to extend\nhuman-like abilities in structured table understanding and complex arithmetic\ncomputations. However, these studies can be improved better in simulating human\ncognitive behavior when using symbolic tools, as they still suffer from\nlimitations of non-standard logical splits and constrained operation pools. In\nthis study, we propose PoTable as a novel table-based reasoning method that\nsimulates a human tabular analyst, which integrates a Python interpreter as the\nreal-time executor accompanied by an LLM-based operation planner and code\ngenerator. Specifically, PoTable follows a human-like logical stage split and\nextends the operation pool into an open-world space without any constraints.\nThrough planning and executing in each distinct stage, PoTable standardly\ncompletes the entire reasoning process and produces superior reasoning results\nalong with highly accurate, steply commented and completely executable\nprograms. Accordingly, the effectiveness and explainability of PoTable are\nfully demonstrated. Extensive experiments over three evaluation datasets from\ntwo public benchmarks on two backbones show the outstanding performance of our\napproach. In particular, GPT-based PoTable achieves over 4% higher absolute\naccuracy than runner-ups on all evaluation datasets."
                },
                "authors": [
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Rui Li"
                    }
                ],
                "author_detail": {
                    "name": "Rui Li"
                },
                "author": "Rui Li",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04259v1",
                "updated": "2024-12-05T15:39:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    39,
                    13,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:39:13Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    39,
                    13,
                    3,
                    340,
                    0
                ],
                "title": "SCADE: Scalable Command-line Anomaly Detection Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCADE: Scalable Command-line Anomaly Detection Engine"
                },
                "summary": "As command-line interfaces remain an integral part of high-computation\nenvironments, the risk of exploitation through stealthy, complex command-line\nabuse continues to grow. Conventional security solutions often struggle with\nthese command-line-based anomalies due to their context-specific nature and\nlack of labeled data, especially in detecting rare, malicious patterns amidst\nlegitimate, high-volume activity. This gap has left organizations vulnerable to\nsophisticated threats like Living-off-the-Land (LOL) attacks, where standard\ndetection tools frequently miss or misclassify anomalous command-line behavior.\nWe introduce Scalable Command-Line Anomaly Detection Engine (SCADE), who\naddresses these challenges by introducing a dual-layered detection framework\nthat combines a global statistical analysis with local context-specific anomaly\ndetection, innovatively using a novel ensemble of statistical models such as\nBM25 and Log Entropy, adapted for command-line data. The framework also\nfeatures a dynamic thresholding mechanism for adaptive anomaly detection,\nensuring high precision and recall even in environments with extremely high\nSignal-to-Noise Ratios (SNRs). Initial experimental results demonstrate the\neffectiveness of the framework, achieving above 98% SNR in identifying unusual\ncommand-line behavior while minimizing false positives. In this paper, we\npresent SCADE's core architecture, including its metadata-enriched approach to\nanomaly detection and the design choices behind its scalability for\nenterprise-level deployment. We argue that SCADE represents a significant\nadvancement in command-line anomaly detection, offering a robust, adaptive\nframework for security analysts and researchers seeking to enhance detection\naccuracy in high-computation environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As command-line interfaces remain an integral part of high-computation\nenvironments, the risk of exploitation through stealthy, complex command-line\nabuse continues to grow. Conventional security solutions often struggle with\nthese command-line-based anomalies due to their context-specific nature and\nlack of labeled data, especially in detecting rare, malicious patterns amidst\nlegitimate, high-volume activity. This gap has left organizations vulnerable to\nsophisticated threats like Living-off-the-Land (LOL) attacks, where standard\ndetection tools frequently miss or misclassify anomalous command-line behavior.\nWe introduce Scalable Command-Line Anomaly Detection Engine (SCADE), who\naddresses these challenges by introducing a dual-layered detection framework\nthat combines a global statistical analysis with local context-specific anomaly\ndetection, innovatively using a novel ensemble of statistical models such as\nBM25 and Log Entropy, adapted for command-line data. The framework also\nfeatures a dynamic thresholding mechanism for adaptive anomaly detection,\nensuring high precision and recall even in environments with extremely high\nSignal-to-Noise Ratios (SNRs). Initial experimental results demonstrate the\neffectiveness of the framework, achieving above 98% SNR in identifying unusual\ncommand-line behavior while minimizing false positives. In this paper, we\npresent SCADE's core architecture, including its metadata-enriched approach to\nanomaly detection and the design choices behind its scalability for\nenterprise-level deployment. We argue that SCADE represents a significant\nadvancement in command-line anomaly detection, offering a robust, adaptive\nframework for security analysts and researchers seeking to enhance detection\naccuracy in high-computation environments."
                },
                "authors": [
                    {
                        "name": "Vaishali Vinay"
                    },
                    {
                        "name": "Anjali Mangal"
                    }
                ],
                "author_detail": {
                    "name": "Anjali Mangal"
                },
                "author": "Anjali Mangal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04237v1",
                "updated": "2024-12-05T15:17:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    17,
                    6,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:17:06Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    17,
                    6,
                    3,
                    340,
                    0
                ],
                "title": "VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction"
                },
                "summary": "Large language models (LLMs) have proven effective for layout generation due\nto their ability to produce structure-description languages, such as HTML or\nJSON, even without access to visual information. Recently, LLM providers have\nevolved these models into large vision-language models (LVLM), which shows\nprominent multi-modal understanding capabilities. Then, how can we leverage\nthis multi-modal power for layout generation? To answer this, we propose\nVisual-Aware Self-Correction LAyout GeneRation (VASCAR) for LVLM-based\ncontent-aware layout generation. In our method, LVLMs iteratively refine their\noutputs with reference to rendered layout images, which are visualized as\ncolored bounding boxes on poster backgrounds. In experiments, we demonstrate\nthat our method combined with the Gemini. Without any additional training,\nVASCAR achieves state-of-the-art (SOTA) layout generation quality outperforming\nboth existing layout-specific generative models and other LLM-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have proven effective for layout generation due\nto their ability to produce structure-description languages, such as HTML or\nJSON, even without access to visual information. Recently, LLM providers have\nevolved these models into large vision-language models (LVLM), which shows\nprominent multi-modal understanding capabilities. Then, how can we leverage\nthis multi-modal power for layout generation? To answer this, we propose\nVisual-Aware Self-Correction LAyout GeneRation (VASCAR) for LVLM-based\ncontent-aware layout generation. In our method, LVLMs iteratively refine their\noutputs with reference to rendered layout images, which are visualized as\ncolored bounding boxes on poster backgrounds. In experiments, we demonstrate\nthat our method combined with the Gemini. Without any additional training,\nVASCAR achieves state-of-the-art (SOTA) layout generation quality outperforming\nboth existing layout-specific generative models and other LLM-based methods."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Ryota Yoshihashi"
                    },
                    {
                        "name": "Shunsuke Kitada"
                    },
                    {
                        "name": "Atsuki Osanai"
                    },
                    {
                        "name": "Yuta Nakashima"
                    }
                ],
                "author_detail": {
                    "name": "Yuta Nakashima"
                },
                "author": "Yuta Nakashima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04235v1",
                "updated": "2024-12-05T15:11:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    11,
                    12,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T15:11:12Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    11,
                    12,
                    3,
                    340,
                    0
                ],
                "title": "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM\n  Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM\n  Chatbots"
                },
                "summary": "I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains."
                },
                "authors": [
                    {
                        "name": "Maria Paola Priola"
                    }
                ],
                "author_detail": {
                    "name": "Maria Paola Priola"
                },
                "author": "Maria Paola Priola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05357v2",
                "updated": "2024-12-05T15:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    8,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-07T15:55:55Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    55,
                    55,
                    0,
                    281,
                    0
                ],
                "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild"
                },
                "summary": "As Large Language Models (LLMs) excel across tasks and specialized domains,\nscaling LLMs based on existing models has garnered significant attention, which\nfaces the challenge of decreasing performance when combining disparate models.\nVarious techniques have been proposed for the aggregation of pre-trained LLMs,\nincluding model merging, Mixture-of-Experts, and stacking. Despite their\nmerits, a comprehensive comparison and synergistic application of them to a\ndiverse model zoo is yet to be adequately addressed. In light of this research\ngap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First,\nour work starts with a benchmarking of existing LLM scaling techniques,\nespecially selective merging, and variants of mixture. Utilizing the insights\nfrom the benchmark results, we formulate an optimal strategy for the selection\nand aggregation of a heterogeneous model zoo characterizing different\narchitectures and initialization.Our methodology involves the clustering of\nmergeable models and optimal merging strategy selection, and the integration of\nclusters through a model mixture. Finally, evidenced by our experiments on a\ndiverse Llama-2-based model zoo, Model-GLUE shows an average performance\nenhancement of 5.61%, achieved without additional training. Codes are available\nat: https://github.com/Model-GLUE/Model-GLUE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) excel across tasks and specialized domains,\nscaling LLMs based on existing models has garnered significant attention, which\nfaces the challenge of decreasing performance when combining disparate models.\nVarious techniques have been proposed for the aggregation of pre-trained LLMs,\nincluding model merging, Mixture-of-Experts, and stacking. Despite their\nmerits, a comprehensive comparison and synergistic application of them to a\ndiverse model zoo is yet to be adequately addressed. In light of this research\ngap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First,\nour work starts with a benchmarking of existing LLM scaling techniques,\nespecially selective merging, and variants of mixture. Utilizing the insights\nfrom the benchmark results, we formulate an optimal strategy for the selection\nand aggregation of a heterogeneous model zoo characterizing different\narchitectures and initialization.Our methodology involves the clustering of\nmergeable models and optimal merging strategy selection, and the integration of\nclusters through a model mixture. Finally, evidenced by our experiments on a\ndiverse Llama-2-based model zoo, Model-GLUE shows an average performance\nenhancement of 5.61%, achieved without additional training. Codes are available\nat: https://github.com/Model-GLUE/Model-GLUE."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yukun Zhou"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Bowen Tan"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Yi Liang"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 4 figures, accepted to NeurIPS 2024 Datasets and Benchmarks\n  Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02830v2",
                "updated": "2024-12-05T14:51:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    51,
                    35,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-03T20:52:35Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    52,
                    35,
                    1,
                    338,
                    0
                ],
                "title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language\n  Models"
                },
                "summary": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical."
                },
                "authors": [
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Junda Wang"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v4",
                "updated": "2024-12-05T14:45:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    45,
                    5,
                    3,
                    340,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of\nsimple OM tools. Our framework is implemented in a proof-of-concept system.\nEvaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks\nover state-of-the-art OM systems show that our system can achieve results very\nclose to the long-standing best performance on simple OM tasks and can\nsignificantly improve the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "14 pages, 13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04193v1",
                "updated": "2024-12-05T14:33:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    33,
                    0,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:33:00Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    33,
                    0,
                    3,
                    340,
                    0
                ],
                "title": "AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in\n  Dialectal Arabic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in\n  Dialectal Arabic"
                },
                "summary": "Dialectal Arabic (DA) varieties are under-served by language technologies,\nparticularly large language models (LLMs). This trend threatens to exacerbate\nexisting social inequalities and limits language modeling applications, yet the\nresearch community lacks operationalized LLM performance measurements in DA. We\npresent a method that comprehensively evaluates LLM fidelity, understanding,\nquality, and diglossia in modeling DA. We evaluate nine LLMs in eight DA\nvarieties across these four dimensions and provide best practice\nrecommendations. Our evaluation suggests that LLMs do not produce DA as well as\nthey understand it, but does not suggest deterioration in quality when they do.\nFurther analysis suggests that current post-training can degrade DA\ncapabilities, that few-shot examples can overcome this and other LLM\ndeficiencies, and that otherwise no measurable features of input text correlate\nwell with LLM DA performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialectal Arabic (DA) varieties are under-served by language technologies,\nparticularly large language models (LLMs). This trend threatens to exacerbate\nexisting social inequalities and limits language modeling applications, yet the\nresearch community lacks operationalized LLM performance measurements in DA. We\npresent a method that comprehensively evaluates LLM fidelity, understanding,\nquality, and diglossia in modeling DA. We evaluate nine LLMs in eight DA\nvarieties across these four dimensions and provide best practice\nrecommendations. Our evaluation suggests that LLMs do not produce DA as well as\nthey understand it, but does not suggest deterioration in quality when they do.\nFurther analysis suggests that current post-training can degrade DA\ncapabilities, that few-shot examples can overcome this and other LLM\ndeficiencies, and that otherwise no measurable features of input text correlate\nwell with LLM DA performance."
                },
                "authors": [
                    {
                        "name": "Nathaniel R. Robinson"
                    },
                    {
                        "name": "Shahd Abdelmoneim"
                    },
                    {
                        "name": "Kelly Marchisio"
                    },
                    {
                        "name": "Sebastian Ruder"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Ruder"
                },
                "author": "Sebastian Ruder",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10182v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10182v4",
                "updated": "2024-12-05T14:30:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    30,
                    41,
                    3,
                    340,
                    0
                ],
                "published": "2024-03-15T10:38:48Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    10,
                    38,
                    48,
                    4,
                    75,
                    0
                ],
                "title": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification"
                },
                "summary": "Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage."
                },
                "authors": [
                    {
                        "name": "Arthur Thuy"
                    },
                    {
                        "name": "Dries F. Benoit"
                    }
                ],
                "author_detail": {
                    "name": "Dries F. Benoit"
                },
                "author": "Dries F. Benoit",
                "arxiv_comment": "Submitted to Annals of Operations Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10182v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10182v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04190v1",
                "updated": "2024-12-05T14:30:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    30,
                    18,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:30:18Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    30,
                    18,
                    3,
                    340,
                    0
                ],
                "title": "Directed Structural Adaptation to Overcome Statistical Conflicts and\n  Enable Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed Structural Adaptation to Overcome Statistical Conflicts and\n  Enable Continual Learning"
                },
                "summary": "Adaptive networks today rely on overparameterized fixed topologies that\ncannot break through the statistical conflicts they encounter in the data they\nare exposed to, and are prone to \"catastrophic forgetting\" as the network\nattempts to reuse the existing structures to learn new task. We propose a\nstructural adaptation method, DIRAD, that can complexify as needed and in a\ndirected manner without being limited by statistical conflicts within a\ndataset. We then extend this method and present the PREVAL framework, designed\nto prevent \"catastrophic forgetting\" in continual learning by detection of new\ndata and assigning encountered data to suitable models adapted to process them,\nwithout needing task labels anywhere in the workflow. We show the reliability\nof the DIRAD in growing a network with high performance and orders-of-magnitude\nsimpler than fixed topology networks; and demonstrate the proof-of-concept\noperation of PREVAL, in which continual adaptation to new tasks is observed\nwhile being able to detect and discern previously-encountered tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive networks today rely on overparameterized fixed topologies that\ncannot break through the statistical conflicts they encounter in the data they\nare exposed to, and are prone to \"catastrophic forgetting\" as the network\nattempts to reuse the existing structures to learn new task. We propose a\nstructural adaptation method, DIRAD, that can complexify as needed and in a\ndirected manner without being limited by statistical conflicts within a\ndataset. We then extend this method and present the PREVAL framework, designed\nto prevent \"catastrophic forgetting\" in continual learning by detection of new\ndata and assigning encountered data to suitable models adapted to process them,\nwithout needing task labels anywhere in the workflow. We show the reliability\nof the DIRAD in growing a network with high performance and orders-of-magnitude\nsimpler than fixed topology networks; and demonstrate the proof-of-concept\noperation of PREVAL, in which continual adaptation to new tasks is observed\nwhile being able to detect and discern previously-encountered tasks."
                },
                "authors": [
                    {
                        "name": "Zeki Doruk Erden"
                    },
                    {
                        "name": "Boi Faltings"
                    }
                ],
                "author_detail": {
                    "name": "Boi Faltings"
                },
                "author": "Boi Faltings",
                "arxiv_comment": "Presented in Deployable AI (DAI) workshop at AAAI-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04185v1",
                "updated": "2024-12-05T14:24:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    24,
                    7,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:24:07Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    24,
                    7,
                    3,
                    340,
                    0
                ],
                "title": "Leveraging Large Language Models to Generate Course-specific\n  Semantically Annotated Learning Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Generate Course-specific\n  Semantically Annotated Learning Objects"
                },
                "summary": "Background: Over the past few decades, the process and methodology of\nautomated question generation (AQG) have undergone significant transformations.\nRecent progress in generative natural language models has opened up new\npotential in the generation of educational content.\n  Objectives: This paper explores the potential of large language models (LLMs)\nfor generating computer science questions that are sufficiently annotated for\nautomatic learner model updates, are fully situated in the context of a\nparticular course, and address the cognitive dimension understand.\n  Methods: Unlike previous attempts that might use basic methods like ChatGPT,\nour approach involves more targeted strategies such as retrieval-augmented\ngeneration (RAG) to produce contextually relevant and pedagogically meaningful\nlearning objects.\n  Results and Conclusions: Our results show that generating structural,\nsemantic annotations works well. However, this success was not reflected in the\ncase of relational annotations. The quality of the generated questions often\ndid not meet educational standards, highlighting that although LLMs can\ncontribute to the pool of learning materials, their current level of\nperformance requires significant human intervention to refine and validate the\ngenerated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Over the past few decades, the process and methodology of\nautomated question generation (AQG) have undergone significant transformations.\nRecent progress in generative natural language models has opened up new\npotential in the generation of educational content.\n  Objectives: This paper explores the potential of large language models (LLMs)\nfor generating computer science questions that are sufficiently annotated for\nautomatic learner model updates, are fully situated in the context of a\nparticular course, and address the cognitive dimension understand.\n  Methods: Unlike previous attempts that might use basic methods like ChatGPT,\nour approach involves more targeted strategies such as retrieval-augmented\ngeneration (RAG) to produce contextually relevant and pedagogically meaningful\nlearning objects.\n  Results and Conclusions: Our results show that generating structural,\nsemantic annotations works well. However, this success was not reflected in the\ncase of relational annotations. The quality of the generated questions often\ndid not meet educational standards, highlighting that although LLMs can\ncontribute to the pool of learning materials, their current level of\nperformance requires significant human intervention to refine and validate the\ngenerated content."
                },
                "authors": [
                    {
                        "name": "Dominic Lohr"
                    },
                    {
                        "name": "Marc Berges"
                    },
                    {
                        "name": "Abhishek Chugh"
                    },
                    {
                        "name": "Michael Kohlhase"
                    },
                    {
                        "name": "Dennis Mller"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Mller"
                },
                "author": "Dennis Mller",
                "arxiv_comment": "Accepted at Journal of Computer Assisted Learning (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04180v1",
                "updated": "2024-12-05T14:19:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    19,
                    59,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:19:59Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    19,
                    59,
                    3,
                    340,
                    0
                ],
                "title": "SKIM: Any-bit Quantization Pushing The Limits of Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKIM: Any-bit Quantization Pushing The Limits of Post-Training\n  Quantization"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive performance across various\ntasks, but deploying them for inference poses challenges. Their high resource\ndemands often necessitate complex, costly multi-GPU pipelines, or the use of\nsmaller, less capable models. While quantization offers a promising solution\nutilizing lower precision for model storage, existing methods frequently\nexperience significant performance drops at lower precision levels.\nAdditionally, they typically provide only a limited set of solutions at\nspecific bit levels, many of which are extensively manually tuned. To address\nthese challenges, we propose a new method called SKIM: Scaled K-means\nclustering wIth Mixed precision. Our approach introduces two novel techniques:\n1. A greedy algorithm to solve approximately optimal bit allocation across\nweight channels, and 2. A trainable scaling vector for non-differentiable\nK-means clustering. These techniques substantially improve performance and can\nbe adapted to any given bit. Notably, in terms of model perplexity, our method\nnarrows the gap between 3-bit quantized LLaMA models and their full precision\ncounterparts by 16.3% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive performance across various\ntasks, but deploying them for inference poses challenges. Their high resource\ndemands often necessitate complex, costly multi-GPU pipelines, or the use of\nsmaller, less capable models. While quantization offers a promising solution\nutilizing lower precision for model storage, existing methods frequently\nexperience significant performance drops at lower precision levels.\nAdditionally, they typically provide only a limited set of solutions at\nspecific bit levels, many of which are extensively manually tuned. To address\nthese challenges, we propose a new method called SKIM: Scaled K-means\nclustering wIth Mixed precision. Our approach introduces two novel techniques:\n1. A greedy algorithm to solve approximately optimal bit allocation across\nweight channels, and 2. A trainable scaling vector for non-differentiable\nK-means clustering. These techniques substantially improve performance and can\nbe adapted to any given bit. Notably, in terms of model perplexity, our method\nnarrows the gap between 3-bit quantized LLaMA models and their full precision\ncounterparts by 16.3% on average."
                },
                "authors": [
                    {
                        "name": "Runsheng Bai"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Bo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Liu"
                },
                "author": "Bo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16105v2",
                "updated": "2024-12-05T14:16:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    16,
                    57,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-25T05:32:34Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    5,
                    32,
                    34,
                    0,
                    330,
                    0
                ],
                "title": "Adaptive Circuit Behavior and Generalization in Mechanistic\n  Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Circuit Behavior and Generalization in Mechanistic\n  Interpretability"
                },
                "summary": "Mechanistic interpretability aims to understand the inner workings of large\nneural networks by identifying circuits, or minimal subgraphs within the model\nthat implement algorithms responsible for performing specific tasks. These\ncircuits are typically discovered and analyzed using a narrowly defined prompt\nformat. However, given the abilities of large language models (LLMs) to\ngeneralize across various prompt formats for the same task, it remains unclear\nhow well these circuits generalize. For instance, it is unclear whether the\nmodels generalization results from reusing the same circuit components, the\ncomponents behaving differently, or the use of entirely different components.\nIn this paper, we investigate the generality of the indirect object\nidentification (IOI) circuit in GPT-2 small, which is well-studied and believed\nto implement a simple, interpretable algorithm. We evaluate its performance on\nprompt variants that challenge the assumptions of this algorithm. Our findings\nreveal that the circuit generalizes surprisingly well, reusing all of its\ncomponents and mechanisms while only adding additional input edges. Notably,\nthe circuit generalizes even to prompt variants where the original algorithm\nshould fail; we discover a mechanism that explains this which we term S2\nHacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic interpretability aims to understand the inner workings of large\nneural networks by identifying circuits, or minimal subgraphs within the model\nthat implement algorithms responsible for performing specific tasks. These\ncircuits are typically discovered and analyzed using a narrowly defined prompt\nformat. However, given the abilities of large language models (LLMs) to\ngeneralize across various prompt formats for the same task, it remains unclear\nhow well these circuits generalize. For instance, it is unclear whether the\nmodels generalization results from reusing the same circuit components, the\ncomponents behaving differently, or the use of entirely different components.\nIn this paper, we investigate the generality of the indirect object\nidentification (IOI) circuit in GPT-2 small, which is well-studied and believed\nto implement a simple, interpretable algorithm. We evaluate its performance on\nprompt variants that challenge the assumptions of this algorithm. Our findings\nreveal that the circuit generalizes surprisingly well, reusing all of its\ncomponents and mechanisms while only adding additional input edges. Notably,\nthe circuit generalizes even to prompt variants where the original algorithm\nshould fail; we discover a mechanism that explains this which we term S2\nHacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these\nmodels."
                },
                "authors": [
                    {
                        "name": "Jatin Nainani"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "AJ Yeung"
                    },
                    {
                        "name": "Kartik Gupta"
                    },
                    {
                        "name": "David Jensen"
                    }
                ],
                "author_detail": {
                    "name": "David Jensen"
                },
                "author": "David Jensen",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04167v1",
                "updated": "2024-12-05T14:03:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    3,
                    41,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T14:03:41Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    3,
                    41,
                    3,
                    340,
                    0
                ],
                "title": "Bench-CoE: a Framework for Collaboration of Experts from Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bench-CoE: a Framework for Collaboration of Experts from Benchmark"
                },
                "summary": "Large Language Models (LLMs) are key technologies driving intelligent systems\nto handle multiple tasks. To meet the demands of various tasks, an increasing\nnumber of LLMs-driven experts with diverse capabilities have been developed,\naccompanied by corresponding benchmarks to evaluate their performance. This\npaper proposes the Bench-CoE framework, which enables Collaboration of Experts\n(CoE) by effectively leveraging benchmark evaluations to achieve optimal\nperformance across various tasks. Bench-CoE includes a set of expert models, a\nrouter for assigning tasks to corresponding experts, and a benchmark dataset\nfor training the router. Moreover, we formulate Query-Level and Subject-Level\napproaches based on our framework, and analyze the merits and drawbacks of\nthese two approaches. Finally, we conduct a series of experiments with vary\ndata distributions on both language and multimodal tasks to validate that our\nproposed Bench-CoE outperforms any single model in terms of overall\nperformance. We hope this method serves as a baseline for further research in\nthis area. The code is available at\n\\url{https://github.com/ZhangXJ199/Bench-CoE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are key technologies driving intelligent systems\nto handle multiple tasks. To meet the demands of various tasks, an increasing\nnumber of LLMs-driven experts with diverse capabilities have been developed,\naccompanied by corresponding benchmarks to evaluate their performance. This\npaper proposes the Bench-CoE framework, which enables Collaboration of Experts\n(CoE) by effectively leveraging benchmark evaluations to achieve optimal\nperformance across various tasks. Bench-CoE includes a set of expert models, a\nrouter for assigning tasks to corresponding experts, and a benchmark dataset\nfor training the router. Moreover, we formulate Query-Level and Subject-Level\napproaches based on our framework, and analyze the merits and drawbacks of\nthese two approaches. Finally, we conduct a series of experiments with vary\ndata distributions on both language and multimodal tasks to validate that our\nproposed Bench-CoE outperforms any single model in terms of overall\nperformance. We hope this method serves as a baseline for further research in\nthis area. The code is available at\n\\url{https://github.com/ZhangXJ199/Bench-CoE}."
                },
                "authors": [
                    {
                        "name": "Yuanshuai Wang"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Jinkun Zhao"
                    },
                    {
                        "name": "Siwei Wen"
                    },
                    {
                        "name": "Peilin Feng"
                    },
                    {
                        "name": "Shuhao Liao"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Wenjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Wu"
                },
                "author": "Wenjun Wu",
                "arxiv_comment": "The code is available at\n  \\url{https://github.com/ZhangXJ199/Bench-CoE}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14123v2",
                "updated": "2024-12-05T13:15:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    15,
                    34,
                    3,
                    340,
                    0
                ],
                "published": "2024-02-21T20:43:49Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    20,
                    43,
                    49,
                    2,
                    52,
                    0
                ],
                "title": "DeiSAM: Segment Anything with Deictic Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeiSAM: Segment Anything with Deictic Prompting"
                },
                "summary": "Large-scale, pre-trained neural networks have demonstrated strong\ncapabilities in various tasks, including zero-shot image segmentation. To\nidentify concrete objects in complex scenes, humans instinctively rely on\ndeictic descriptions in natural language, i.e., referring to something\ndepending on the context such as \"The object that is on the desk and behind the\ncup.\". However, deep learning approaches cannot reliably interpret such deictic\nrepresentations due to their lack of reasoning capabilities in complex\nscenarios. To remedy this issue, we propose DeiSAM -- a combination of large\npre-trained neural networks with differentiable logic reasoners -- for deictic\npromptable segmentation. Given a complex, textual segmentation description,\nDeiSAM leverages Large Language Models (LLMs) to generate first-order logic\nrules and performs differentiable forward reasoning on generated scene graphs.\nSubsequently, DeiSAM segments objects by matching them to the logically\ninferred image regions. As part of our evaluation, we propose the Deictic\nVisual Genome (DeiVG) dataset, containing paired visual input and complex,\ndeictic textual prompts. Our empirical results demonstrate that DeiSAM is a\nsubstantial improvement over purely data-driven baselines for deictic\npromptable segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale, pre-trained neural networks have demonstrated strong\ncapabilities in various tasks, including zero-shot image segmentation. To\nidentify concrete objects in complex scenes, humans instinctively rely on\ndeictic descriptions in natural language, i.e., referring to something\ndepending on the context such as \"The object that is on the desk and behind the\ncup.\". However, deep learning approaches cannot reliably interpret such deictic\nrepresentations due to their lack of reasoning capabilities in complex\nscenarios. To remedy this issue, we propose DeiSAM -- a combination of large\npre-trained neural networks with differentiable logic reasoners -- for deictic\npromptable segmentation. Given a complex, textual segmentation description,\nDeiSAM leverages Large Language Models (LLMs) to generate first-order logic\nrules and performs differentiable forward reasoning on generated scene graphs.\nSubsequently, DeiSAM segments objects by matching them to the logically\ninferred image regions. As part of our evaluation, we propose the Deictic\nVisual Genome (DeiVG) dataset, containing paired visual input and complex,\ndeictic textual prompts. Our empirical results demonstrate that DeiSAM is a\nsubstantial improvement over purely data-driven baselines for deictic\npromptable segmentation."
                },
                "authors": [
                    {
                        "name": "Hikaru Shindo"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Gopika Sudhakaran"
                    },
                    {
                        "name": "Devendra Singh Dhami"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08535v2",
                "updated": "2024-12-05T13:15:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    15,
                    7,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-13T11:32:37Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    11,
                    32,
                    37,
                    2,
                    318,
                    0
                ],
                "title": "The EU AI Act is a good start but falls short",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EU AI Act is a good start but falls short"
                },
                "summary": "The EU AI Act was created to ensure ethical and safe Artificial Intelligence\n(AI) development and deployment across the EU. This study aims to identify key\nchallenges and strategies for helping enterprises focus on resources\neffectively. To achieve this aim, we conducted a Multivocal Literature Review\n(MLR) to explore the sentiments of both the industry and the academia. From 130\narticles, 56 met the criteria. Our key findings are three-fold. First,\nliability. Second, discrimination. Third, tool adequacy. Additionally, some\nnegative sentiments were expressed by industry and academia regarding\nregulatory interpretations, specific requirements, and transparency issues.\nNext, our findings are three essential themes for enterprises. First,\nrisk-based regulatory compliance. Second, ethical frameworks and principles in\ntechnology development. Third, policies and systems for regulatory risk\nmanagement. These results identify the key challenges and strategies and\nprovide less commonly discussed themes, enabling enterprises to align with the\nrequirements and minimize their distance from the EU market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EU AI Act was created to ensure ethical and safe Artificial Intelligence\n(AI) development and deployment across the EU. This study aims to identify key\nchallenges and strategies for helping enterprises focus on resources\neffectively. To achieve this aim, we conducted a Multivocal Literature Review\n(MLR) to explore the sentiments of both the industry and the academia. From 130\narticles, 56 met the criteria. Our key findings are three-fold. First,\nliability. Second, discrimination. Third, tool adequacy. Additionally, some\nnegative sentiments were expressed by industry and academia regarding\nregulatory interpretations, specific requirements, and transparency issues.\nNext, our findings are three essential themes for enterprises. First,\nrisk-based regulatory compliance. Second, ethical frameworks and principles in\ntechnology development. Third, policies and systems for regulatory risk\nmanagement. These results identify the key challenges and strategies and\nprovide less commonly discussed themes, enabling enterprises to align with the\nrequirements and minimize their distance from the EU market."
                },
                "authors": [
                    {
                        "name": "Chalisa Veesommai Sillberg"
                    },
                    {
                        "name": "Jose Siqueira De Cerqueira"
                    },
                    {
                        "name": "Pekka Sillberg"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "18 pages, 4 figures, 3 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04141v1",
                "updated": "2024-12-05T13:10:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    10,
                    54,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T13:10:54Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    10,
                    54,
                    3,
                    340,
                    0
                ],
                "title": "Reducing Tool Hallucination via Reliability Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Tool Hallucination via Reliability Alignment"
                },
                "summary": "Large Language Models (LLMs) have extended their capabilities beyond language\ngeneration to interact with external systems through tool calling, offering\npowerful potential for real-world applications. However, the phenomenon of tool\nhallucinations, which occur when models improperly select or misuse tools,\npresents critical challenges that can lead to flawed task execution and\nincreased operational costs. This paper investigates the concept of reliable\ntool calling and highlights the necessity of addressing tool hallucinations. We\nsystematically categorize tool hallucinations into two main types: tool\nselection hallucination and tool usage hallucination. To mitigate these issues,\nwe propose a reliability-focused alignment framework that enhances the model's\nability to accurately assess tool relevance and usage. By proposing a suite of\nevaluation metrics and evaluating on StableToolBench, we further demonstrate\nthe effectiveness of our framework in mitigating tool hallucination and\nimproving the overall system reliability of LLM tool calling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have extended their capabilities beyond language\ngeneration to interact with external systems through tool calling, offering\npowerful potential for real-world applications. However, the phenomenon of tool\nhallucinations, which occur when models improperly select or misuse tools,\npresents critical challenges that can lead to flawed task execution and\nincreased operational costs. This paper investigates the concept of reliable\ntool calling and highlights the necessity of addressing tool hallucinations. We\nsystematically categorize tool hallucinations into two main types: tool\nselection hallucination and tool usage hallucination. To mitigate these issues,\nwe propose a reliability-focused alignment framework that enhances the model's\nability to accurately assess tool relevance and usage. By proposing a suite of\nevaluation metrics and evaluating on StableToolBench, we further demonstrate\nthe effectiveness of our framework in mitigating tool hallucination and\nimproving the overall system reliability of LLM tool calling."
                },
                "authors": [
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Hang Zheng"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Ruisheng Cao"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04139v1",
                "updated": "2024-12-05T13:06:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    6,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T13:06:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    6,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "Monet: Mixture of Monosemantic Experts for Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monet: Mixture of Monosemantic Experts for Transformers"
                },
                "summary": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance} mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust} model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance} mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust} model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet."
                },
                "authors": [
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Young Jin Ahn"
                    },
                    {
                        "name": "Kee-Eung Kim"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02785v2",
                "updated": "2024-12-05T12:58:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    58,
                    44,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-05T03:51:13Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    3,
                    51,
                    13,
                    1,
                    310,
                    0
                ],
                "title": "Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM\n  Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM\n  Safety Alignment"
                },
                "summary": "Safety alignment of Large Language Models (LLMs) has recently become a\ncritical objective of model developers. In response, a growing body of work has\nbeen investigating how safety alignment can be bypassed through various\njailbreaking methods, such as adversarial attacks. However, these jailbreak\nmethods can be rather costly or involve a non-trivial amount of creativity and\neffort, introducing the assumption that malicious users are high-resource or\nsophisticated. In this paper, we study how simple random augmentations to the\ninput prompt affect safety alignment effectiveness in state-of-the-art LLMs,\nsuch as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different\nmodels and investigate the intersection of safety under random augmentations\nwith multiple dimensions: augmentation type, model size, quantization,\nfine-tuning-based defenses, and decoding strategies (e.g., sampling\ntemperature). We show that low-resource and unsophisticated attackers, i.e.\n$\\textit{stochastic monkeys}$, can significantly improve their chances of\nbypassing alignment with just 25 random augmentations per prompt. Source code\nand data: https://github.com/uiuc-focal-lab/stochastic-monkeys/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment of Large Language Models (LLMs) has recently become a\ncritical objective of model developers. In response, a growing body of work has\nbeen investigating how safety alignment can be bypassed through various\njailbreaking methods, such as adversarial attacks. However, these jailbreak\nmethods can be rather costly or involve a non-trivial amount of creativity and\neffort, introducing the assumption that malicious users are high-resource or\nsophisticated. In this paper, we study how simple random augmentations to the\ninput prompt affect safety alignment effectiveness in state-of-the-art LLMs,\nsuch as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different\nmodels and investigate the intersection of safety under random augmentations\nwith multiple dimensions: augmentation type, model size, quantization,\nfine-tuning-based defenses, and decoding strategies (e.g., sampling\ntemperature). We show that low-resource and unsophisticated attackers, i.e.\n$\\textit{stochastic monkeys}$, can significantly improve their chances of\nbypassing alignment with just 25 random augmentations per prompt. Source code\nand data: https://github.com/uiuc-focal-lab/stochastic-monkeys/"
                },
                "authors": [
                    {
                        "name": "Jason Vega"
                    },
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Gaokai Zhang"
                    },
                    {
                        "name": "Hangoo Kang"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "arxiv_comment": "v2: Updated with changes from peer review rebuttal. v1: Version under\n  peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03906v2",
                "updated": "2024-12-05T12:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    56,
                    40,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-06T13:37:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    37,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System"
                },
                "summary": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality."
                },
                "authors": [
                    {
                        "name": "David Maria Schmidt"
                    },
                    {
                        "name": "Mohammad Fazleh Elahi"
                    },
                    {
                        "name": "Philipp Cimiano"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Cimiano"
                },
                "author": "Philipp Cimiano",
                "arxiv_doi": "10.1007/978-3-031-77792-9_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-77792-9_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24th International Conference on Knowledge Engineering and Knowledge\n  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08020v2",
                "updated": "2024-12-05T12:40:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    40,
                    16,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-10T15:17:49Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    17,
                    49,
                    3,
                    284,
                    0
                ],
                "title": "Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs"
                },
                "summary": "Recent efforts in fine-tuning language models often rely on automatic data\nselection, commonly using Nearest Neighbors retrieval from large datasets.\nHowever, we theoretically show that this approach tends to select redundant\ndata, limiting its effectiveness or even hurting performance. To address this,\nwe introduce SIFT, a data selection algorithm designed to reduce uncertainty\nabout the model's response given a prompt, which unifies ideas from retrieval\nand active learning. Whereas Nearest Neighbor retrieval typically fails in the\npresence of information duplication, SIFT accounts for information duplication\nand optimizes the overall information gain of the selected examples. We focus\nour evaluations on fine-tuning at test-time for prompt-specific language\nmodeling on the Pile dataset, and show that SIFT consistently outperforms\nNearest Neighbor retrieval, with minimal computational overhead. Moreover, we\nshow that our uncertainty estimates can predict the performance gain of\ntest-time fine-tuning, and use this to develop an adaptive algorithm that\ninvests test-time compute proportional to realized performance gains. We\nprovide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\nas a drop-in replacement for Nearest Neighbor retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts in fine-tuning language models often rely on automatic data\nselection, commonly using Nearest Neighbors retrieval from large datasets.\nHowever, we theoretically show that this approach tends to select redundant\ndata, limiting its effectiveness or even hurting performance. To address this,\nwe introduce SIFT, a data selection algorithm designed to reduce uncertainty\nabout the model's response given a prompt, which unifies ideas from retrieval\nand active learning. Whereas Nearest Neighbor retrieval typically fails in the\npresence of information duplication, SIFT accounts for information duplication\nand optimizes the overall information gain of the selected examples. We focus\nour evaluations on fine-tuning at test-time for prompt-specific language\nmodeling on the Pile dataset, and show that SIFT consistently outperforms\nNearest Neighbor retrieval, with minimal computational overhead. Moreover, we\nshow that our uncertainty estimates can predict the performance gain of\ntest-time fine-tuning, and use this to develop an adaptive algorithm that\ninvests test-time compute proportional to realized performance gains. We\nprovide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\nas a drop-in replacement for Nearest Neighbor retrieval."
                },
                "authors": [
                    {
                        "name": "Jonas Hbotter"
                    },
                    {
                        "name": "Sascha Bongni"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04117v1",
                "updated": "2024-12-05T12:36:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    36,
                    12,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T12:36:12Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    36,
                    12,
                    3,
                    340,
                    0
                ],
                "title": "MVUDA: Unsupervised Domain Adaptation for Multi-view Pedestrian\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVUDA: Unsupervised Domain Adaptation for Multi-view Pedestrian\n  Detection"
                },
                "summary": "We address multi-view pedestrian detection in a setting where labeled data is\ncollected using a multi-camera setup different from the one used for testing.\nWhile recent multi-view pedestrian detectors perform well on the camera rig\nused for training, their performance declines when applied to a different\nsetup. To facilitate seamless deployment across varied camera rigs, we propose\nan unsupervised domain adaptation (UDA) method that adapts the model to new\nrigs without requiring additional labeled data. Specifically, we leverage the\nmean teacher self-training framework with a novel pseudo-labeling technique\ntailored to multi-view pedestrian detection. This method achieves\nstate-of-the-art performance on multiple benchmarks, including\nMultiviewX$\\rightarrow$Wildtrack. Unlike previous methods, our approach\neliminates the need for external labeled monocular datasets, thereby reducing\nreliance on labeled data. Extensive evaluations demonstrate the effectiveness\nof our method and validate key design choices. By enabling robust adaptation\nacross camera setups, our work enhances the practicality of multi-view\npedestrian detectors and establishes a strong UDA baseline for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address multi-view pedestrian detection in a setting where labeled data is\ncollected using a multi-camera setup different from the one used for testing.\nWhile recent multi-view pedestrian detectors perform well on the camera rig\nused for training, their performance declines when applied to a different\nsetup. To facilitate seamless deployment across varied camera rigs, we propose\nan unsupervised domain adaptation (UDA) method that adapts the model to new\nrigs without requiring additional labeled data. Specifically, we leverage the\nmean teacher self-training framework with a novel pseudo-labeling technique\ntailored to multi-view pedestrian detection. This method achieves\nstate-of-the-art performance on multiple benchmarks, including\nMultiviewX$\\rightarrow$Wildtrack. Unlike previous methods, our approach\neliminates the need for external labeled monocular datasets, thereby reducing\nreliance on labeled data. Extensive evaluations demonstrate the effectiveness\nof our method and validate key design choices. By enabling robust adaptation\nacross camera setups, our work enhances the practicality of multi-view\npedestrian detectors and establishes a strong UDA baseline for future research."
                },
                "authors": [
                    {
                        "name": "Erik Brorsson"
                    },
                    {
                        "name": "Lennart Svensson"
                    },
                    {
                        "name": "Kristofer Bengtsson"
                    },
                    {
                        "name": "Knut kesson"
                    }
                ],
                "author_detail": {
                    "name": "Knut kesson"
                },
                "author": "Knut kesson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04110v1",
                "updated": "2024-12-05T12:24:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    24,
                    54,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T12:24:54Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    24,
                    54,
                    3,
                    340,
                    0
                ],
                "title": "Enhancing Mathematical Reasoning in LLMs with Background Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Mathematical Reasoning in LLMs with Background Operators"
                },
                "summary": "We propose utilizing background operators for mathematical reasoning in large\nlanguage models (LLMs). To achieve this, we define a set of fundamental\nmathematical predicates as the basic building blocks. For each mathematical\nproblem, we develop a Prolog solution that includes problem-specific predicates\nand intermediate predicates derived from these background operators, ensuring\nthat each solution adheres to the defined operator set. We introduce the\nMATH-Prolog corpus, which is derived from the counting and probability\ncategories of the MATH corpus. For efficient data augmentation, we apply K-fold\ncross-validated self-training. This method incrementally generates new Prolog\nsolutions for each fold, incorporating those verified as correct into the\ntraining set throughout the model training process. Our experimental results\ndemonstrate that 5-fold crossvalidated self-training effectively identifies\nnew, accurate Prolog solutions, achieving an accuracy of 84.6% on the\ncross-validated set, and 84.8% on the test set during fine-tuning the\nMeta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new\nsolutions with fully computable inference steps for previously unseen problems.\nAdditionally, incorporating the background mathematical predicates into the\nprompt enhances solution coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose utilizing background operators for mathematical reasoning in large\nlanguage models (LLMs). To achieve this, we define a set of fundamental\nmathematical predicates as the basic building blocks. For each mathematical\nproblem, we develop a Prolog solution that includes problem-specific predicates\nand intermediate predicates derived from these background operators, ensuring\nthat each solution adheres to the defined operator set. We introduce the\nMATH-Prolog corpus, which is derived from the counting and probability\ncategories of the MATH corpus. For efficient data augmentation, we apply K-fold\ncross-validated self-training. This method incrementally generates new Prolog\nsolutions for each fold, incorporating those verified as correct into the\ntraining set throughout the model training process. Our experimental results\ndemonstrate that 5-fold crossvalidated self-training effectively identifies\nnew, accurate Prolog solutions, achieving an accuracy of 84.6% on the\ncross-validated set, and 84.8% on the test set during fine-tuning the\nMeta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new\nsolutions with fully computable inference steps for previously unseen problems.\nAdditionally, incorporating the background mathematical predicates into the\nprompt enhances solution coverage."
                },
                "authors": [
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Yik-Cheung Tam"
                    }
                ],
                "author_detail": {
                    "name": "Yik-Cheung Tam"
                },
                "author": "Yik-Cheung Tam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04107v1",
                "updated": "2024-12-05T12:17:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    17,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T12:17:56Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    17,
                    56,
                    3,
                    340,
                    0
                ],
                "title": "Pre-train, Align, and Disentangle: Empowering Sequential Recommendation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-train, Align, and Disentangle: Empowering Sequential Recommendation\n  with Large Language Models"
                },
                "summary": "Sequential recommendation (SR) aims to model the sequential dependencies in\nusers' historical interactions to better capture their evolving interests.\nHowever, existing SR approaches primarily rely on collaborative data, which\nleads to limitations such as the cold-start problem and sub-optimal\nperformance. Meanwhile, despite the success of large language models (LLMs),\ntheir application in industrial recommender systems is hindered by high\ninference latency, inability to capture all distribution statistics, and\ncatastrophic forgetting. To this end, we propose a novel Pre-train, Align, and\nDisentangle (PAD) paradigm to empower recommendation models with LLMs.\nSpecifically, we first pre-train both the SR and LLM models to get\ncollaborative and textual embeddings. Next, a characteristic\nrecommendation-anchored alignment loss is proposed using multi-kernel maximum\nmean discrepancy with Gaussian kernels. Finally, a triple-experts architecture,\nconsisting aligned and modality-specific experts with disentangled embeddings,\nis fine-tuned in a frequency-aware manner. Experiments conducted on three\npublic datasets demonstrate the effectiveness of PAD, showing significant\nimprovements and compatibility with various SR backbone models, especially on\ncold items. The implementation code and datasets will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) aims to model the sequential dependencies in\nusers' historical interactions to better capture their evolving interests.\nHowever, existing SR approaches primarily rely on collaborative data, which\nleads to limitations such as the cold-start problem and sub-optimal\nperformance. Meanwhile, despite the success of large language models (LLMs),\ntheir application in industrial recommender systems is hindered by high\ninference latency, inability to capture all distribution statistics, and\ncatastrophic forgetting. To this end, we propose a novel Pre-train, Align, and\nDisentangle (PAD) paradigm to empower recommendation models with LLMs.\nSpecifically, we first pre-train both the SR and LLM models to get\ncollaborative and textual embeddings. Next, a characteristic\nrecommendation-anchored alignment loss is proposed using multi-kernel maximum\nmean discrepancy with Gaussian kernels. Finally, a triple-experts architecture,\nconsisting aligned and modality-specific experts with disentangled embeddings,\nis fine-tuned in a frequency-aware manner. Experiments conducted on three\npublic datasets demonstrate the effectiveness of PAD, showing significant\nimprovements and compatibility with various SR backbone models, especially on\ncold items. The implementation code and datasets will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Junwei Pan"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Dapeng Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04093v1",
                "updated": "2024-12-05T11:57:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    57,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:57:49Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    57,
                    49,
                    3,
                    340,
                    0
                ],
                "title": "Practical Considerations for Agentic LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Considerations for Agentic LLM Systems"
                },
                "summary": "As the strength of Large Language Models (LLMs) has grown over recent years,\nso too has interest in their use as the underlying models for autonomous\nagents. Although LLMs demonstrate emergent abilities and broad expertise across\nnatural language domains, their inherent unpredictability makes the\nimplementation of LLM agents challenging, resulting in a gap between related\nresearch and the real-world implementation of such systems. To bridge this gap,\nthis paper frames actionable insights and considerations from the research\ncommunity in the context of established application paradigms to enable the\nconstruction and facilitate the informed deployment of robust LLM agents.\nNamely, we position relevant research findings into four broad\ncategories--Planning, Memory, Tools, and Control Flow--based on common\npractices in application-focused literature and highlight practical\nconsiderations to make when designing agentic LLMs for real-world applications,\nsuch as handling stochasticity and managing resources efficiently. While we do\nnot conduct empirical evaluations, we do provide the necessary background for\ndiscussing critical aspects of agentic LLM designs, both in academia and\nindustry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the strength of Large Language Models (LLMs) has grown over recent years,\nso too has interest in their use as the underlying models for autonomous\nagents. Although LLMs demonstrate emergent abilities and broad expertise across\nnatural language domains, their inherent unpredictability makes the\nimplementation of LLM agents challenging, resulting in a gap between related\nresearch and the real-world implementation of such systems. To bridge this gap,\nthis paper frames actionable insights and considerations from the research\ncommunity in the context of established application paradigms to enable the\nconstruction and facilitate the informed deployment of robust LLM agents.\nNamely, we position relevant research findings into four broad\ncategories--Planning, Memory, Tools, and Control Flow--based on common\npractices in application-focused literature and highlight practical\nconsiderations to make when designing agentic LLMs for real-world applications,\nsuch as handling stochasticity and managing resources efficiently. While we do\nnot conduct empirical evaluations, we do provide the necessary background for\ndiscussing critical aspects of agentic LLM designs, both in academia and\nindustry."
                },
                "authors": [
                    {
                        "name": "Chris Sypherd"
                    },
                    {
                        "name": "Vaishak Belle"
                    }
                ],
                "author_detail": {
                    "name": "Vaishak Belle"
                },
                "author": "Vaishak Belle",
                "arxiv_comment": "15 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04090v1",
                "updated": "2024-12-05T11:52:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    52,
                    20,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T11:52:20Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    52,
                    20,
                    3,
                    340,
                    0
                ],
                "title": "LossAgent: Towards Any Optimization Objectives for Image Processing with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LossAgent: Towards Any Optimization Objectives for Image Processing with\n  LLM Agents"
                },
                "summary": "We present the first loss agent, dubbed LossAgent, for low-level image\nprocessing tasks, e.g., image super-resolution and restoration, intending to\nachieve any customized optimization objectives of low-level image processing in\ndifferent practical applications. Notably, not all optimization objectives,\nsuch as complex hand-crafted perceptual metrics, text description, and\nintricate human feedback, can be instantiated with existing low-level losses,\ne.g., MSE loss. which presents a crucial challenge in optimizing image\nprocessing networks in an end-to-end manner. To eliminate this, our LossAgent\nintroduces the powerful large language model (LLM) as the loss agent, where the\nrich textual understanding of prior knowledge empowers the loss agent with the\npotential to understand complex optimization objectives, trajectory, and state\nfeedback from external environments in the optimization process of the\nlow-level image processing networks. In particular, we establish the loss\nrepository by incorporating existing loss functions that support the end-to-end\noptimization for low-level image processing. Then, we design the\noptimization-oriented prompt engineering for the loss agent to actively and\nintelligently decide the compositional weights for each loss in the repository\nat each optimization interaction, thereby achieving the required optimization\ntrajectory for any customized optimization objectives. Extensive experiments on\nthree typical low-level image processing tasks and multiple optimization\nobjectives have shown the effectiveness and applicability of our proposed\nLossAgent. Code and pre-trained models will be available at\nhttps://github.com/lbc12345/LossAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first loss agent, dubbed LossAgent, for low-level image\nprocessing tasks, e.g., image super-resolution and restoration, intending to\nachieve any customized optimization objectives of low-level image processing in\ndifferent practical applications. Notably, not all optimization objectives,\nsuch as complex hand-crafted perceptual metrics, text description, and\nintricate human feedback, can be instantiated with existing low-level losses,\ne.g., MSE loss. which presents a crucial challenge in optimizing image\nprocessing networks in an end-to-end manner. To eliminate this, our LossAgent\nintroduces the powerful large language model (LLM) as the loss agent, where the\nrich textual understanding of prior knowledge empowers the loss agent with the\npotential to understand complex optimization objectives, trajectory, and state\nfeedback from external environments in the optimization process of the\nlow-level image processing networks. In particular, we establish the loss\nrepository by incorporating existing loss functions that support the end-to-end\noptimization for low-level image processing. Then, we design the\noptimization-oriented prompt engineering for the loss agent to actively and\nintelligently decide the compositional weights for each loss in the repository\nat each optimization interaction, thereby achieving the required optimization\ntrajectory for any customized optimization objectives. Extensive experiments on\nthree typical low-level image processing tasks and multiple optimization\nobjectives have shown the effectiveness and applicability of our proposed\nLossAgent. Code and pre-trained models will be available at\nhttps://github.com/lbc12345/LossAgent."
                },
                "authors": [
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04057v1",
                "updated": "2024-12-05T10:50:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    50,
                    58,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:50:58Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    50,
                    58,
                    3,
                    340,
                    0
                ],
                "title": "From Code to Play: Benchmarking Program Search for Games Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Code to Play: Benchmarking Program Search for Games Using Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities in generating\nprogram code, opening exciting opportunities for applying program synthesis to\ngames. In this work, we explore the potential of LLMs to directly synthesize\nusable code for a wide range of gaming applications, focusing on two\nprogramming languages, Python and Java. We use an evolutionary hill-climbing\nalgorithm, where the mutations and seeds of the initial programs are controlled\nby LLMs. For Python, the framework covers various game-related tasks, including\nfive miniature versions of Atari games, ten levels of Baba is You, an\nenvironment inspired by Asteroids, and a maze generation task. For Java, the\nframework contains 12 games from the TAG tabletop games framework. Across 29\ntasks, we evaluated 12 language models for Python and 8 for Java. Our findings\nsuggest that the performance of LLMs depends more on the task than on model\nsize. While larger models generate more executable programs, these do not\nalways result in higher-quality solutions but are much more expensive. No model\nhas a clear advantage, although on any specific task, one model may be better.\nTrying many models on a problem and using the best results across them is more\nreliable than using just one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities in generating\nprogram code, opening exciting opportunities for applying program synthesis to\ngames. In this work, we explore the potential of LLMs to directly synthesize\nusable code for a wide range of gaming applications, focusing on two\nprogramming languages, Python and Java. We use an evolutionary hill-climbing\nalgorithm, where the mutations and seeds of the initial programs are controlled\nby LLMs. For Python, the framework covers various game-related tasks, including\nfive miniature versions of Atari games, ten levels of Baba is You, an\nenvironment inspired by Asteroids, and a maze generation task. For Java, the\nframework contains 12 games from the TAG tabletop games framework. Across 29\ntasks, we evaluated 12 language models for Python and 8 for Java. Our findings\nsuggest that the performance of LLMs depends more on the task than on model\nsize. While larger models generate more executable programs, these do not\nalways result in higher-quality solutions but are much more expensive. No model\nhas a clear advantage, although on any specific task, one model may be better.\nTrying many models on a problem and using the best results across them is more\nreliable than using just one."
                },
                "authors": [
                    {
                        "name": "Manuel Eberhardinger"
                    },
                    {
                        "name": "James Goodman"
                    },
                    {
                        "name": "Alexander Dockhorn"
                    },
                    {
                        "name": "Diego Perez-Liebana"
                    },
                    {
                        "name": "Raluca D. Gaina"
                    },
                    {
                        "name": "Duygu akmak"
                    },
                    {
                        "name": "Setareh Maghsudi"
                    },
                    {
                        "name": "Simon Lucas"
                    }
                ],
                "author_detail": {
                    "name": "Simon Lucas"
                },
                "author": "Simon Lucas",
                "arxiv_comment": "Submitted to Transactions on Games Special Issue on Large Language\n  Models and Games",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04056v1",
                "updated": "2024-12-05T10:49:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    49,
                    2,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:49:02Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    49,
                    2,
                    3,
                    340,
                    0
                ],
                "title": "Prompt Engineering Guidance for Conceptual Agent-based Model Extraction\n  using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Engineering Guidance for Conceptual Agent-based Model Extraction\n  using Large Language Models"
                },
                "summary": "This document contains detailed information about the prompts used in the\nexperimental process discussed in the paper \"Toward Automating Agent-based\nModel Generation: A Benchmark for Model Extraction using Question-Answering\nTechniques\". The paper aims to utilize Question-answering (QA) models to\nextract the necessary information to implement Agent-based Modeling (ABM) from\nconceptual models. It presents the extracted information in formats that can be\nread by both humans and computers (i.e., JavaScript Object Notation (JSON)),\nenabling manual use by humans and auto-code generation by Large Language Models\n(LLM).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This document contains detailed information about the prompts used in the\nexperimental process discussed in the paper \"Toward Automating Agent-based\nModel Generation: A Benchmark for Model Extraction using Question-Answering\nTechniques\". The paper aims to utilize Question-answering (QA) models to\nextract the necessary information to implement Agent-based Modeling (ABM) from\nconceptual models. It presents the extracted information in formats that can be\nread by both humans and computers (i.e., JavaScript Object Notation (JSON)),\nenabling manual use by humans and auto-code generation by Large Language Models\n(LLM)."
                },
                "authors": [
                    {
                        "name": "Siamak Khatami"
                    },
                    {
                        "name": "Christopher Frantz"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Frantz"
                },
                "author": "Christopher Frantz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; I.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07122v2",
                "updated": "2024-12-05T10:45:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    45,
                    2,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-11T16:51:39Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    51,
                    39,
                    0,
                    316,
                    0
                ],
                "title": "SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering\n  in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating human-like text, but their output may not be aligned with the user\nor even produce harmful content. This paper presents a novel approach to detect\nand steer concepts such as toxicity before generation. We introduce the Sparse\nConditioned Autoencoder (SCAR), a single trained module that extends the\notherwise untouched LLM. SCAR ensures full steerability, towards and away from\nconcepts (e.g., toxic content), without compromising the quality of the model's\ntext generation on standard evaluation benchmarks. We demonstrate the effective\napplication of our approach through a variety of concepts, including toxicity,\nsafety, and writing style alignment. As such, this work establishes a robust\nframework for controlling LLM generations, ensuring their ethical and safe\ndeployment in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating human-like text, but their output may not be aligned with the user\nor even produce harmful content. This paper presents a novel approach to detect\nand steer concepts such as toxicity before generation. We introduce the Sparse\nConditioned Autoencoder (SCAR), a single trained module that extends the\notherwise untouched LLM. SCAR ensures full steerability, towards and away from\nconcepts (e.g., toxic content), without compromising the quality of the model's\ntext generation on standard evaluation benchmarks. We demonstrate the effective\napplication of our approach through a variety of concepts, including toxicity,\nsafety, and writing style alignment. As such, this work establishes a robust\nframework for controlling LLM generations, ensuring their ethical and safe\ndeployment in real-world applications."
                },
                "authors": [
                    {
                        "name": "Ruben Hrle"
                    },
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Bjrn Deiseroth"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "arxiv_comment": "Accepted at Socially Responsible Language Modelling Research (SoLaR)\n  Workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04041v1",
                "updated": "2024-12-05T10:30:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    30,
                    35,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:30:35Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    30,
                    35,
                    3,
                    340,
                    0
                ],
                "title": "GenChaR: A Dataset for Stock Chart Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenChaR: A Dataset for Stock Chart Captioning"
                },
                "summary": "In this work, we introduce a new dataset GenChaR for an image captioning task\naround stock charts. The task aims to read market sentiment directly from\ndepicted charts and generate descriptions, hopefully to provide comprehensible\nand useful insights for stock trading. Impressed by the success of large\nlanguage models (LLMs), the study decides to pioneer itself by exploring the\ncapabilities of large vision-language models (LVLMs) on the proposed task. This\npaper outlines the objectives of the stock captioning task, the dataset we\nbuilt, and automatic evaluation with some representative general-purpose LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce a new dataset GenChaR for an image captioning task\naround stock charts. The task aims to read market sentiment directly from\ndepicted charts and generate descriptions, hopefully to provide comprehensible\nand useful insights for stock trading. Impressed by the success of large\nlanguage models (LLMs), the study decides to pioneer itself by exploring the\ncapabilities of large vision-language models (LVLMs) on the proposed task. This\npaper outlines the objectives of the stock captioning task, the dataset we\nbuilt, and automatic evaluation with some representative general-purpose LVLMs."
                },
                "authors": [
                    {
                        "name": "Le Qiu"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuele Chersoni"
                },
                "author": "Emmanuele Chersoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04036v1",
                "updated": "2024-12-05T10:19:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    19,
                    36,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:19:36Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    19,
                    36,
                    3,
                    340,
                    0
                ],
                "title": "SocialMind: LLM-based Proactive AR Social Assistive System with\n  Human-like Perception for In-situ Live Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialMind: LLM-based Proactive AR Social Assistive System with\n  Human-like Perception for In-situ Live Interactions"
                },
                "summary": "Social interactions are fundamental to human life. The recent emergence of\nlarge language models (LLMs)-based virtual assistants has demonstrated their\npotential to revolutionize human interactions and lifestyles. However, existing\nassistive systems mainly provide reactive services to individual users, rather\nthan offering in-situ assistance during live social interactions with\nconversational partners. In this study, we introduce SocialMind, the first\nLLM-based proactive AR social assistive system that provides users with in-situ\nsocial assistance. SocialMind employs human-like perception leveraging\nmulti-modal sensors to extract both verbal and nonverbal cues, social factors,\nand implicit personas, incorporating these social cues into LLM reasoning for\nsocial suggestion generation. Additionally, SocialMind employs a multi-tier\ncollaborative generation strategy and proactive update mechanism to display\nsocial suggestions on Augmented Reality (AR) glasses, ensuring that suggestions\nare timely provided to users without disrupting the natural flow of\nconversation. Evaluations on three public datasets and a user study with 20\nparticipants show that SocialMind achieves 38.3% higher engagement compared to\nbaselines, and 95% of participants are willing to use SocialMind in their live\nsocial interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social interactions are fundamental to human life. The recent emergence of\nlarge language models (LLMs)-based virtual assistants has demonstrated their\npotential to revolutionize human interactions and lifestyles. However, existing\nassistive systems mainly provide reactive services to individual users, rather\nthan offering in-situ assistance during live social interactions with\nconversational partners. In this study, we introduce SocialMind, the first\nLLM-based proactive AR social assistive system that provides users with in-situ\nsocial assistance. SocialMind employs human-like perception leveraging\nmulti-modal sensors to extract both verbal and nonverbal cues, social factors,\nand implicit personas, incorporating these social cues into LLM reasoning for\nsocial suggestion generation. Additionally, SocialMind employs a multi-tier\ncollaborative generation strategy and proactive update mechanism to display\nsocial suggestions on Augmented Reality (AR) glasses, ensuring that suggestions\nare timely provided to users without disrupting the natural flow of\nconversation. Evaluations on three public datasets and a user study with 20\nparticipants show that SocialMind achieves 38.3% higher engagement compared to\nbaselines, and 95% of participants are willing to use SocialMind in their live\nsocial interactions."
                },
                "authors": [
                    {
                        "name": "Bufang Yang"
                    },
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Lilin Xu"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04029v1",
                "updated": "2024-12-05T10:05:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    5,
                    53,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T10:05:53Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    10,
                    5,
                    53,
                    3,
                    340,
                    0
                ],
                "title": "Considerations Influencing Offense-Defense Dynamics From Artificial\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considerations Influencing Offense-Defense Dynamics From Artificial\n  Intelligence"
                },
                "summary": "The rapid advancement of artificial intelligence (AI) technologies presents\nprofound challenges to societal safety. As AI systems become more capable,\naccessible, and integrated into critical services, the dual nature of their\npotential is increasingly clear. While AI can enhance defensive capabilities in\nareas like threat detection, risk assessment, and automated security\noperations, it also presents avenues for malicious exploitation and large-scale\nsocietal harm, for example through automated influence operations and cyber\nattacks. Understanding the dynamics that shape AI's capacity to both cause harm\nand enhance protective measures is essential for informed decision-making\nregarding the deployment, use, and integration of advanced AI systems. This\npaper builds on recent work on offense-defense dynamics within the realm of AI,\nproposing a taxonomy to map and examine the key factors that influence whether\nAI systems predominantly pose threats or offer protective benefits to society.\nBy establishing a shared terminology and conceptual foundation for analyzing\nthese interactions, this work seeks to facilitate further research and\ndiscourse in this critical area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence (AI) technologies presents\nprofound challenges to societal safety. As AI systems become more capable,\naccessible, and integrated into critical services, the dual nature of their\npotential is increasingly clear. While AI can enhance defensive capabilities in\nareas like threat detection, risk assessment, and automated security\noperations, it also presents avenues for malicious exploitation and large-scale\nsocietal harm, for example through automated influence operations and cyber\nattacks. Understanding the dynamics that shape AI's capacity to both cause harm\nand enhance protective measures is essential for informed decision-making\nregarding the deployment, use, and integration of advanced AI systems. This\npaper builds on recent work on offense-defense dynamics within the realm of AI,\nproposing a taxonomy to map and examine the key factors that influence whether\nAI systems predominantly pose threats or offer protective benefits to society.\nBy establishing a shared terminology and conceptual foundation for analyzing\nthese interactions, this work seeks to facilitate further research and\ndiscourse in this critical area."
                },
                "authors": [
                    {
                        "name": "Giulio Corsi"
                    },
                    {
                        "name": "Kyle Kilian"
                    },
                    {
                        "name": "Richard Mallah"
                    }
                ],
                "author_detail": {
                    "name": "Richard Mallah"
                },
                "author": "Richard Mallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19846v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19846v6",
                "updated": "2024-12-05T09:56:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    56,
                    35,
                    3,
                    340,
                    0
                ],
                "published": "2024-05-30T08:50:55Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    8,
                    50,
                    55,
                    3,
                    151,
                    0
                ],
                "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model"
                },
                "summary": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes."
                },
                "authors": [
                    {
                        "name": "Chaochen Gao"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Qi Fu"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19846v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19846v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04003v1",
                "updated": "2024-12-05T09:26:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    26,
                    58,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T09:26:58Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    26,
                    58,
                    3,
                    340,
                    0
                ],
                "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for\n  Cross-Lingual Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marco-LLM: Bridging Languages via Massive Multilingual Training for\n  Cross-Lingual Enhancement"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages."
                },
                "authors": [
                    {
                        "name": "Lingfeng Ming"
                    },
                    {
                        "name": "Bo Zeng"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Tianqi Shi"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Yefeng Liu"
                    },
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Linlong Xu"
                    },
                    {
                        "name": "Yangyang Liu"
                    },
                    {
                        "name": "Xiaohu Zhao"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Heng Liu"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Zifu Shang"
                    },
                    {
                        "name": "Haijun Li"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14438v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14438v3",
                "updated": "2024-12-05T09:23:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    23,
                    13,
                    3,
                    340,
                    0
                ],
                "published": "2024-05-23T11:10:32Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    11,
                    10,
                    32,
                    3,
                    144,
                    0
                ],
                "title": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks"
                },
                "summary": "Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets."
                },
                "authors": [
                    {
                        "name": "Michelle Halbheer"
                    },
                    {
                        "name": "Dominik J. Mhlematter"
                    },
                    {
                        "name": "Alexander Becker"
                    },
                    {
                        "name": "Dominik Narnhofer"
                    },
                    {
                        "name": "Helge Aasen"
                    },
                    {
                        "name": "Konrad Schindler"
                    },
                    {
                        "name": "Mehmet Ozgur Turkoglu"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Ozgur Turkoglu"
                },
                "author": "Mehmet Ozgur Turkoglu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14438v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14438v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03162v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03162v2",
                "updated": "2024-12-05T09:21:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    21,
                    16,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-04T09:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    39,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "LLM-Mirror: A Generated-Persona Approach for Survey Pre-Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Mirror: A Generated-Persona Approach for Survey Pre-Testing"
                },
                "summary": "Surveys are widely used in social sciences to understand human behavior, but\ntheir implementation often involves iterative adjustments that demand\nsignificant effort and resources. To this end, researchers have increasingly\nturned to large language models (LLMs) to simulate human behavior. While\nexisting studies have focused on distributional similarities, individual-level\ncomparisons remain underexplored. Building upon prior work, we investigate\nwhether providing LLMs with respondents' prior information can replicate both\nstatistical distributions and individual decision-making patterns using Partial\nLeast Squares Structural Equation Modeling (PLS-SEM), a well-established causal\nanalysis method. We also introduce the concept of the LLM-Mirror, user personas\ngenerated by supplying respondent-specific information to the LLM. By comparing\nresponses generated by the LLM-Mirror with actual individual survey responses,\nwe assess its effectiveness in replicating individual-level outcomes. Our\nfindings show that: (1) PLS-SEM analysis shows LLM-generated responses align\nwith human responses, (2) LLMs, when provided with respondent-specific\ninformation, are capable of reproducing individual human responses, and (3)\nLLM-Mirror responses closely follow human responses at the individual level.\nThese findings highlight the potential of LLMs as a complementary tool for\npre-testing surveys and optimizing research design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveys are widely used in social sciences to understand human behavior, but\ntheir implementation often involves iterative adjustments that demand\nsignificant effort and resources. To this end, researchers have increasingly\nturned to large language models (LLMs) to simulate human behavior. While\nexisting studies have focused on distributional similarities, individual-level\ncomparisons remain underexplored. Building upon prior work, we investigate\nwhether providing LLMs with respondents' prior information can replicate both\nstatistical distributions and individual decision-making patterns using Partial\nLeast Squares Structural Equation Modeling (PLS-SEM), a well-established causal\nanalysis method. We also introduce the concept of the LLM-Mirror, user personas\ngenerated by supplying respondent-specific information to the LLM. By comparing\nresponses generated by the LLM-Mirror with actual individual survey responses,\nwe assess its effectiveness in replicating individual-level outcomes. Our\nfindings show that: (1) PLS-SEM analysis shows LLM-generated responses align\nwith human responses, (2) LLMs, when provided with respondent-specific\ninformation, are capable of reproducing individual human responses, and (3)\nLLM-Mirror responses closely follow human responses at the individual level.\nThese findings highlight the potential of LLMs as a complementary tool for\npre-testing surveys and optimizing research design."
                },
                "authors": [
                    {
                        "name": "Sunwoong Kim"
                    },
                    {
                        "name": "Jongho Jeong"
                    },
                    {
                        "name": "Jin Soo Han"
                    },
                    {
                        "name": "Donghyuk Shin"
                    }
                ],
                "author_detail": {
                    "name": "Donghyuk Shin"
                },
                "author": "Donghyuk Shin",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03162v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03987v1",
                "updated": "2024-12-05T09:05:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    5,
                    30,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T09:05:30Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    9,
                    5,
                    30,
                    3,
                    340,
                    0
                ],
                "title": "MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for\n  Strengthening LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for\n  Strengthening LLM"
                },
                "summary": "Large language models (LLMs) have shown limitations in tasks requiring\ncomplex logical reasoning and multi-step problem-solving. To address these\nchallenges, researchers have employed carefully designed prompts and\nflowcharts, simulating human cognitive processes to enhance LLM performance,\nsuch as the Chain of Thought approach. In this paper, we introduce MTMT\n(Multi-thinking Modes Tree), a novel method that interacts with LLMs to\nconstruct a thought tree, simulating various advanced cognitive processes,\nincluding but not limited to association, counterfactual thinking, task\ndecomposition, and comparison. By breaking down the original complex task into\nsimpler sub-questions, MTMT facilitates easier problem-solving for LLMs,\nenabling more effective utilization of the latent knowledge within LLMs. We\nevaluate the performance of MTMT under different parameter configurations,\nusing GPT-4o mini as the base model. Our results demonstrate that integrating\nmultiple modes of thinking significantly enhances the ability of LLMs to handle\ncomplex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown limitations in tasks requiring\ncomplex logical reasoning and multi-step problem-solving. To address these\nchallenges, researchers have employed carefully designed prompts and\nflowcharts, simulating human cognitive processes to enhance LLM performance,\nsuch as the Chain of Thought approach. In this paper, we introduce MTMT\n(Multi-thinking Modes Tree), a novel method that interacts with LLMs to\nconstruct a thought tree, simulating various advanced cognitive processes,\nincluding but not limited to association, counterfactual thinking, task\ndecomposition, and comparison. By breaking down the original complex task into\nsimpler sub-questions, MTMT facilitates easier problem-solving for LLMs,\nenabling more effective utilization of the latent knowledge within LLMs. We\nevaluate the performance of MTMT under different parameter configurations,\nusing GPT-4o mini as the base model. Our results demonstrate that integrating\nmultiple modes of thinking significantly enhances the ability of LLMs to handle\ncomplex tasks."
                },
                "authors": [
                    {
                        "name": "Changcheng Li"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Qiuju Chen"
                    },
                    {
                        "name": "Xiren Zhou"
                    },
                    {
                        "name": "Huanhuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huanhuan Chen"
                },
                "author": "Huanhuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18220v2",
                "updated": "2024-12-05T08:57:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    8,
                    57,
                    30,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-27T10:57:06Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    57,
                    6,
                    2,
                    332,
                    0
                ],
                "title": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge"
                },
                "summary": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Vlad C. Andrei"
                    },
                    {
                        "name": "Mohsen Pourghasemian"
                    },
                    {
                        "name": "Haris Gacanin"
                    },
                    {
                        "name": "Holger Boche"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03980v1",
                "updated": "2024-12-05T08:56:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    8,
                    56,
                    54,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T08:56:54Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    8,
                    56,
                    54,
                    3,
                    340,
                    0
                ],
                "title": "Comprehensive Audio Query Handling System with Integrated Expert Models\n  and Contextual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Audio Query Handling System with Integrated Expert Models\n  and Contextual Understanding"
                },
                "summary": "This paper presents a comprehensive chatbot system designed to handle a wide\nrange of audio-related queries by integrating multiple specialized audio\nprocessing models. The proposed system uses an intent classifier, trained on a\ndiverse audio query dataset, to route queries about audio content to expert\nmodels such as Automatic Speech Recognition (ASR), Speaker Diarization, Music\nIdentification, and Text-to-Audio generation. A 3.8 B LLM model then takes\ninputs from an Audio Context Detection (ACD) module extracting audio event\ninformation from the audio and post processes text domain outputs from the\nexpert models to compute the final response to the user. We evaluated the\nsystem on custom audio tasks and MMAU sound set benchmarks. The custom datasets\nwere motivated by target use cases not covered in industry benchmarks and\nincluded ACD-timestamp-QA (Question Answering) as well as ACD-temporal-QA\ndatasets to evaluate timestamp and temporal reasoning questions, respectively.\nFirst we determined that a BERT based Intent Classifier outperforms LLM-fewshot\nintent classifier in routing queries. Experiments further show that our\napproach significantly improves accuracy on some custom tasks compared to\nstate-of-the-art Large Audio Language Models and outperforms models in the 7B\nparameter size range on the sound testset of the MMAU benchmark, thereby\noffering an attractive option for on device deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive chatbot system designed to handle a wide\nrange of audio-related queries by integrating multiple specialized audio\nprocessing models. The proposed system uses an intent classifier, trained on a\ndiverse audio query dataset, to route queries about audio content to expert\nmodels such as Automatic Speech Recognition (ASR), Speaker Diarization, Music\nIdentification, and Text-to-Audio generation. A 3.8 B LLM model then takes\ninputs from an Audio Context Detection (ACD) module extracting audio event\ninformation from the audio and post processes text domain outputs from the\nexpert models to compute the final response to the user. We evaluated the\nsystem on custom audio tasks and MMAU sound set benchmarks. The custom datasets\nwere motivated by target use cases not covered in industry benchmarks and\nincluded ACD-timestamp-QA (Question Answering) as well as ACD-temporal-QA\ndatasets to evaluate timestamp and temporal reasoning questions, respectively.\nFirst we determined that a BERT based Intent Classifier outperforms LLM-fewshot\nintent classifier in routing queries. Experiments further show that our\napproach significantly improves accuracy on some custom tasks compared to\nstate-of-the-art Large Audio Language Models and outperforms models in the 7B\nparameter size range on the sound testset of the MMAU benchmark, thereby\noffering an attractive option for on device deployment."
                },
                "authors": [
                    {
                        "name": "Vakada Naveen"
                    },
                    {
                        "name": "Arvind Krishna Sridhar"
                    },
                    {
                        "name": "Yinyi Guo"
                    },
                    {
                        "name": "Erik Visser"
                    }
                ],
                "author_detail": {
                    "name": "Erik Visser"
                },
                "author": "Erik Visser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03966v1",
                "updated": "2024-12-05T08:33:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    8,
                    33,
                    52,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T08:33:52Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    8,
                    33,
                    52,
                    3,
                    340,
                    0
                ],
                "title": "Demonstration Selection for In-Context Learning via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstration Selection for In-Context Learning via Reinforcement\n  Learning"
                },
                "summary": "Diversity in demonstration selection is crucial for enhancing model\ngeneralization, as it enables a broader coverage of structures and concepts.\nHowever, constructing an appropriate set of demonstrations has remained a focal\npoint of research. This paper presents the Relevance-Diversity Enhanced\nSelection (RDES), an innovative approach that leverages reinforcement learning\nto optimize the selection of diverse reference demonstrations for text\nclassification tasks using Large Language Models (LLMs), especially in few-shot\nprompting scenarios. RDES employs a Q-learning framework to dynamically\nidentify demonstrations that maximize both diversity and relevance to the\nclassification objective by calculating a diversity score based on label\ndistribution among selected demonstrations. This method ensures a balanced\nrepresentation of reference data, leading to improved classification accuracy.\nThrough extensive experiments on four benchmark datasets and involving 12\nclosed-source and open-source LLMs, we demonstrate that RDES significantly\nenhances classification accuracy compared to ten established baselines.\nFurthermore, we investigate the incorporation of Chain-of-Thought (CoT)\nreasoning in the reasoning process, which further enhances the model's\npredictive performance. The results underscore the potential of reinforcement\nlearning to facilitate adaptive demonstration selection and deepen the\nunderstanding of classification challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in demonstration selection is crucial for enhancing model\ngeneralization, as it enables a broader coverage of structures and concepts.\nHowever, constructing an appropriate set of demonstrations has remained a focal\npoint of research. This paper presents the Relevance-Diversity Enhanced\nSelection (RDES), an innovative approach that leverages reinforcement learning\nto optimize the selection of diverse reference demonstrations for text\nclassification tasks using Large Language Models (LLMs), especially in few-shot\nprompting scenarios. RDES employs a Q-learning framework to dynamically\nidentify demonstrations that maximize both diversity and relevance to the\nclassification objective by calculating a diversity score based on label\ndistribution among selected demonstrations. This method ensures a balanced\nrepresentation of reference data, leading to improved classification accuracy.\nThrough extensive experiments on four benchmark datasets and involving 12\nclosed-source and open-source LLMs, we demonstrate that RDES significantly\nenhances classification accuracy compared to ten established baselines.\nFurthermore, we investigate the incorporation of Chain-of-Thought (CoT)\nreasoning in the reasoning process, which further enhances the model's\npredictive performance. The results underscore the potential of reinforcement\nlearning to facilitate adaptive demonstration selection and deepen the\nunderstanding of classification challenges."
                },
                "authors": [
                    {
                        "name": "Xubin Wang"
                    },
                    {
                        "name": "Jianfei Wu"
                    },
                    {
                        "name": "Yichen Yuan"
                    },
                    {
                        "name": "Mingzhe Li"
                    },
                    {
                        "name": "Deyu Cai"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03944v1",
                "updated": "2024-12-05T07:47:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    47,
                    29,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T07:47:29Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    47,
                    29,
                    3,
                    340,
                    0
                ],
                "title": "Chain-of-Thought in Large Language Models: Decoding, Projection, and\n  Activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought in Large Language Models: Decoding, Projection, and\n  Activation"
                },
                "summary": "Chain-of-Thought prompting has significantly enhanced the reasoning\ncapabilities of large language models, with numerous studies exploring factors\ninfluencing its performance. However, the underlying mechanisms remain poorly\nunderstood. To further demystify the operational principles, this work examines\nthree key aspects: decoding, projection, and activation, aiming to elucidate\nthe changes that occur within models when employing Chainof-Thought. Our\nfindings reveal that LLMs effectively imitate exemplar formats while\nintegrating them with their understanding of the question, exhibiting\nfluctuations in token logits during generation but ultimately producing a more\nconcentrated logits distribution, and activating a broader set of neurons in\nthe final layers, indicating more extensive knowledge retrieval compared to\nstandard prompts. Our code and data will be publicly avialable when the paper\nis accepted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought prompting has significantly enhanced the reasoning\ncapabilities of large language models, with numerous studies exploring factors\ninfluencing its performance. However, the underlying mechanisms remain poorly\nunderstood. To further demystify the operational principles, this work examines\nthree key aspects: decoding, projection, and activation, aiming to elucidate\nthe changes that occur within models when employing Chainof-Thought. Our\nfindings reveal that LLMs effectively imitate exemplar formats while\nintegrating them with their understanding of the question, exhibiting\nfluctuations in token logits during generation but ultimately producing a more\nconcentrated logits distribution, and activating a broader set of neurons in\nthe final layers, indicating more extensive knowledge retrieval compared to\nstandard prompts. Our code and data will be publicly avialable when the paper\nis accepted."
                },
                "authors": [
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Qianghua Zhao"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03937v1",
                "updated": "2024-12-05T07:35:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    35,
                    19,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T07:35:19Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    35,
                    19,
                    3,
                    340,
                    0
                ],
                "title": "AIpparel: A Large Multimodal Generative Model for Digital Garments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIpparel: A Large Multimodal Generative Model for Digital Garments"
                },
                "summary": "Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a large\nmultimodal model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. \\methodname\nachieves state-of-the-art performance in single-modal tasks, including\ntext-to-garment and image-to-garment prediction, and enables novel multimodal\ngarment generation applications such as interactive garment editing. The\nproject website is at georgenakayama.github.io/AIpparel/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a large\nmultimodal model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. \\methodname\nachieves state-of-the-art performance in single-modal tasks, including\ntext-to-garment and image-to-garment prediction, and enables novel multimodal\ngarment generation applications such as interactive garment editing. The\nproject website is at georgenakayama.github.io/AIpparel/."
                },
                "authors": [
                    {
                        "name": "Kiyohiro Nakayama"
                    },
                    {
                        "name": "Jan Ackermann"
                    },
                    {
                        "name": "Timur Levent Kesdogan"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Maria Korosteleva"
                    },
                    {
                        "name": "Olga Sorkine-Hornung"
                    },
                    {
                        "name": "Leonidas J. Guibas"
                    },
                    {
                        "name": "Guandao Yang"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Wetzstein"
                },
                "author": "Gordon Wetzstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03933v1",
                "updated": "2024-12-05T07:23:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    23,
                    14,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T07:23:14Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    23,
                    14,
                    3,
                    340,
                    0
                ],
                "title": "Exploring AI Text Generation, Retrieval-Augmented Generation, and\n  Detection Technologies: a Comprehensive Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring AI Text Generation, Retrieval-Augmented Generation, and\n  Detection Technologies: a Comprehensive Overview"
                },
                "summary": "The rapid development of Artificial Intelligence (AI) has led to the creation\nof powerful text generation models, such as large language models (LLMs), which\nare widely used for diverse applications. However, concerns surrounding\nAI-generated content, including issues of originality, bias, misinformation,\nand accountability, have become increasingly prominent. This paper offers a\ncomprehensive overview of AI text generators (AITGs), focusing on their\nevolution, capabilities, and ethical implications. This paper also introduces\nRetrieval-Augmented Generation (RAG), a recent approach that improves the\ncontextual relevance and accuracy of text generation by integrating dynamic\ninformation retrieval. RAG addresses key limitations of traditional models,\nincluding their reliance on static knowledge and potential inaccuracies in\nhandling real-world data. Additionally, the paper reviews detection tools that\nhelp differentiate AI-generated text from human-written content and discusses\nthe ethical challenges these technologies pose. The paper explores future\ndirections for improving detection accuracy, supporting ethical AI development,\nand increasing accessibility. The paper contributes to a more responsible and\nreliable use of AI in content creation through these discussions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Artificial Intelligence (AI) has led to the creation\nof powerful text generation models, such as large language models (LLMs), which\nare widely used for diverse applications. However, concerns surrounding\nAI-generated content, including issues of originality, bias, misinformation,\nand accountability, have become increasingly prominent. This paper offers a\ncomprehensive overview of AI text generators (AITGs), focusing on their\nevolution, capabilities, and ethical implications. This paper also introduces\nRetrieval-Augmented Generation (RAG), a recent approach that improves the\ncontextual relevance and accuracy of text generation by integrating dynamic\ninformation retrieval. RAG addresses key limitations of traditional models,\nincluding their reliance on static knowledge and potential inaccuracies in\nhandling real-world data. Additionally, the paper reviews detection tools that\nhelp differentiate AI-generated text from human-written content and discusses\nthe ethical challenges these technologies pose. The paper explores future\ndirections for improving detection accuracy, supporting ethical AI development,\nand increasing accessibility. The paper contributes to a more responsible and\nreliable use of AI in content creation through these discussions."
                },
                "authors": [
                    {
                        "name": "Fnu Neha"
                    },
                    {
                        "name": "Deepshikha Bhati"
                    },
                    {
                        "name": "Deepak Kumar Shukla"
                    },
                    {
                        "name": "Angela Guercio"
                    },
                    {
                        "name": "Ben Ward"
                    }
                ],
                "author_detail": {
                    "name": "Ben Ward"
                },
                "author": "Ben Ward",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16320v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16320v3",
                "updated": "2024-12-05T07:14:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    14,
                    52,
                    3,
                    340,
                    0
                ],
                "published": "2024-09-21T03:45:05Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    3,
                    45,
                    5,
                    5,
                    265,
                    0
                ],
                "title": "Developing a Thailand solar irradiance map using Himawari-8 satellite\n  imageries and deep learning models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing a Thailand solar irradiance map using Himawari-8 satellite\n  imageries and deep learning models"
                },
                "summary": "This paper presents an online platform showing Thailand solar irradiance map\nevery 30 minutes, available at https://www.cusolarforecast.com. The methodology\nfor estimating global horizontal irradiance (GHI) across Thailand relies on\ncloud index extracted from Himawari-8 satellite imagery, Ineichen clear-sky\nmodel with locally-tuned Linke turbidity, and machine learning models. The\nmethods take clear-sky irradiance, cloud index, re-analyzed GHI and temperature\ndata from the MERRA-2 database, and date-time as inputs for GHI estimation\nmodels, including LightGBM, LSTM, Informer, and Transformer. These are\nbenchmarked with the estimate from a commercial service X by evaluation of\n15-minute ground GHI data from 53 ground stations over 1.5 years during\n2022-2023. The results show that the four models exhibit comparable overall MAE\nperformance to the service X. The best model is LightGBM with an overall MAE of\n78.58 W/sqm and RMSE of 118.97 W/sqm, while the service X achieves the lowest\nMAE, RMSE, and MBE in cloudy condition. Obtaining re-analyzed MERRA-2 data for\nthe whole Thailand region is not economically feasible for deployment. When\nremoving these features, the Informer model has a winning performance in MAE of\n78.67 W/sqm. The obtained performance aligns with existing literature by taking\nthe climate zone and time granularity of data into consideration. As the map\nshows an estimate of GHI over 93,000 grids with a frequent update, the paper\nalso describes a computational framework for displaying the entire map. It\ntests the runtime performance of deep learning models in the GHI estimation\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an online platform showing Thailand solar irradiance map\nevery 30 minutes, available at https://www.cusolarforecast.com. The methodology\nfor estimating global horizontal irradiance (GHI) across Thailand relies on\ncloud index extracted from Himawari-8 satellite imagery, Ineichen clear-sky\nmodel with locally-tuned Linke turbidity, and machine learning models. The\nmethods take clear-sky irradiance, cloud index, re-analyzed GHI and temperature\ndata from the MERRA-2 database, and date-time as inputs for GHI estimation\nmodels, including LightGBM, LSTM, Informer, and Transformer. These are\nbenchmarked with the estimate from a commercial service X by evaluation of\n15-minute ground GHI data from 53 ground stations over 1.5 years during\n2022-2023. The results show that the four models exhibit comparable overall MAE\nperformance to the service X. The best model is LightGBM with an overall MAE of\n78.58 W/sqm and RMSE of 118.97 W/sqm, while the service X achieves the lowest\nMAE, RMSE, and MBE in cloudy condition. Obtaining re-analyzed MERRA-2 data for\nthe whole Thailand region is not economically feasible for deployment. When\nremoving these features, the Informer model has a winning performance in MAE of\n78.67 W/sqm. The obtained performance aligns with existing literature by taking\nthe climate zone and time granularity of data into consideration. As the map\nshows an estimate of GHI over 93,000 grids with a frequent update, the paper\nalso describes a computational framework for displaying the entire map. It\ntests the runtime performance of deep learning models in the GHI estimation\nprocess."
                },
                "authors": [
                    {
                        "name": "Suwichaya Suwanwimolkul"
                    },
                    {
                        "name": "Natanon Tongamrak"
                    },
                    {
                        "name": "Nuttamon Thungka"
                    },
                    {
                        "name": "Naebboon Hoonchareon"
                    },
                    {
                        "name": "Jitkomut Songsiri"
                    }
                ],
                "author_detail": {
                    "name": "Jitkomut Songsiri"
                },
                "author": "Jitkomut Songsiri",
                "arxiv_comment": "23 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16320v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16320v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21216v2",
                "updated": "2024-12-05T07:09:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    9,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-28T17:01:52Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    1,
                    52,
                    0,
                    302,
                    0
                ],
                "title": "HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced\n  Context Awareness and Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced\n  Context Awareness and Extrapolation"
                },
                "summary": "Many positional encodings (PEs) are designed to exhibit long-term decay,\nbased on an entrenched and long-standing inductive opinion: tokens farther away\nfrom the current position carry less relevant information. We argue that\nlong-term decay is outdated in the era of LLMs, as LLMs are now applied to\ntasks demanding precise retrieval of in-context information from arbitrary\npositions. Firstly, we present empirical analyses on various PEs, demonstrating\nthat models inherently learn attention with only a local-decay pattern while\nforming a U-shape pattern globally, contradicting the principle of long-term\ndecay. Furthermore, we conduct a detailed analysis of rotary position encoding\n(RoPE, a prevalent relative positional encoding in LLMs), and found that the\nU-shape attention is caused by some learned components, which are also the key\nfactor limiting RoPE's expressiveness and extrapolation.Inspired by these\ninsights, we propose High-frequency rotary Position Encoding (HoPE). HoPE\nreplaces the specific components in RoPE with position-independent ones,\nretaining only high-frequency signals, which also breaks the principle of\nlong-term decay in theory. HoPE achieves two major advantages: (1) Without\nconstraints imposed by long-term decay, contradictory factors that limit\nspontaneous attention optimization and model extrapolation performance are\nremoved. (2) Components representing positions and semantics are are optimized.\nThese enhances model's context awareness and extrapolation, as validated by\nextensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many positional encodings (PEs) are designed to exhibit long-term decay,\nbased on an entrenched and long-standing inductive opinion: tokens farther away\nfrom the current position carry less relevant information. We argue that\nlong-term decay is outdated in the era of LLMs, as LLMs are now applied to\ntasks demanding precise retrieval of in-context information from arbitrary\npositions. Firstly, we present empirical analyses on various PEs, demonstrating\nthat models inherently learn attention with only a local-decay pattern while\nforming a U-shape pattern globally, contradicting the principle of long-term\ndecay. Furthermore, we conduct a detailed analysis of rotary position encoding\n(RoPE, a prevalent relative positional encoding in LLMs), and found that the\nU-shape attention is caused by some learned components, which are also the key\nfactor limiting RoPE's expressiveness and extrapolation.Inspired by these\ninsights, we propose High-frequency rotary Position Encoding (HoPE). HoPE\nreplaces the specific components in RoPE with position-independent ones,\nretaining only high-frequency signals, which also breaks the principle of\nlong-term decay in theory. HoPE achieves two major advantages: (1) Without\nconstraints imposed by long-term decay, contradictory factors that limit\nspontaneous attention optimization and model extrapolation performance are\nremoved. (2) Components representing positions and semantics are are optimized.\nThese enhances model's context awareness and extrapolation, as validated by\nextensive experiments."
                },
                "authors": [
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03927v1",
                "updated": "2024-12-05T07:06:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    6,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T07:06:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    6,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language\n  Models"
                },
                "summary": "In vision-language models (VLMs), the ability to perceive and interpret color\nand physical environment is crucial for achieving contextually accurate\nunderstanding and interaction. However, despite advances in multimodal\nmodeling, there remains a significant lack of specialized datasets that\nrigorously evaluate a model's capacity to discern subtle color variations and\nspatial context -- critical elements for situational comprehension and reliable\ndeployment across real-world applications. Toward that goal, we curate\nMegaCOIN, a high-quality, human-labeled dataset based on \\emph{real} images\nwith various contextual attributes. MegaCOIN consists of two parts:\nMegaCOIN-Instruct, which serves as a supervised fine-tuning (SFT) dataset for\nVLMs; and MegaCOIN-Bench, an annotated test set that can be used as a\nstand-alone QA dataset. MegaCOIN~provides three annotated features for 220,000\nreal images: foreground color, background color, and description of an object's\nphysical environment, constituting 660k human annotations. In addition,\nMegaCOIN can be applied to benchmark domain generalization (DG) algorithms. We\nexplore benchmarking DG methods in the linear probing setup for VLM and show\nsome new insights. Last but not least, we show that VLMs, including GPT-4o,\nhave subpar color recognition capabilities, and fine-tuning with MegaCOIN can\nresult in improved performance on visual evaluation tasks. In certain cases,\nMegaCOIN fine-tuned small-scale opensource models such as LLaVA and Bunny can\noutperform closed-source GPT-4o. We hope the utilities of MegaCOIN can shed\nlight on the directions VLMs can improve and provide a more complex platform\nfor domain generalization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In vision-language models (VLMs), the ability to perceive and interpret color\nand physical environment is crucial for achieving contextually accurate\nunderstanding and interaction. However, despite advances in multimodal\nmodeling, there remains a significant lack of specialized datasets that\nrigorously evaluate a model's capacity to discern subtle color variations and\nspatial context -- critical elements for situational comprehension and reliable\ndeployment across real-world applications. Toward that goal, we curate\nMegaCOIN, a high-quality, human-labeled dataset based on \\emph{real} images\nwith various contextual attributes. MegaCOIN consists of two parts:\nMegaCOIN-Instruct, which serves as a supervised fine-tuning (SFT) dataset for\nVLMs; and MegaCOIN-Bench, an annotated test set that can be used as a\nstand-alone QA dataset. MegaCOIN~provides three annotated features for 220,000\nreal images: foreground color, background color, and description of an object's\nphysical environment, constituting 660k human annotations. In addition,\nMegaCOIN can be applied to benchmark domain generalization (DG) algorithms. We\nexplore benchmarking DG methods in the linear probing setup for VLM and show\nsome new insights. Last but not least, we show that VLMs, including GPT-4o,\nhave subpar color recognition capabilities, and fine-tuning with MegaCOIN can\nresult in improved performance on visual evaluation tasks. In certain cases,\nMegaCOIN fine-tuned small-scale opensource models such as LLaVA and Bunny can\noutperform closed-source GPT-4o. We hope the utilities of MegaCOIN can shed\nlight on the directions VLMs can improve and provide a more complex platform\nfor domain generalization algorithms."
                },
                "authors": [
                    {
                        "name": "Ming-Chang Chiu"
                    },
                    {
                        "name": "Shicheng Wen"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Xuezhe Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xuezhe Ma"
                },
                "author": "Xuezhe Ma",
                "arxiv_comment": "8 pages, 13 tables, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00741v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00741v3",
                "updated": "2024-12-05T07:05:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    5,
                    59,
                    3,
                    340,
                    0
                ],
                "published": "2024-01-01T12:49:36Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    12,
                    49,
                    36,
                    0,
                    1,
                    0
                ],
                "title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of\n  Large Language Models in Real-world Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of\n  Large Language Models in Real-world Scenarios"
                },
                "summary": "Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the complex capabilities required for LLMs\nto effectively use tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. The code and data are available at\nhttps://github.com/Junjie-Ye/ToolEyes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the complex capabilities required for LLMs\nto effectively use tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. The code and data are available at\nhttps://github.com/Junjie-Ye/ToolEyes."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Guanyu Li"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Caishuang Huang"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Sixian Li"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by COLING 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00741v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00741v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17993v2",
                "updated": "2024-12-05T06:53:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    53,
                    40,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-27T02:20:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    2,
                    20,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "DRS: Deep Question Reformulation With Structured Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRS: Deep Question Reformulation With Structured Output"
                },
                "summary": "Question answering represents a core capability of large language models\n(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they\noften formulate questions that the text itself cannot answer due to\ninsufficient understanding of the underlying information. Recent studies reveal\nthat while LLMs can detect unanswerable questions, they struggle to assist\nusers in reformulating these questions. Even advanced models like GPT-3.5\ndemonstrate limited effectiveness in this regard. To address this limitation,\nwe propose DRS: Deep Question Reformulation with Structured Output, a novel\nzero-shot method aimed at enhancing LLMs ability to assist users in\nreformulating questions to extract relevant information from new documents. DRS\ncombines the strengths of LLMs with a DFS-based algorithm to iteratively\nexplore potential entity combinations and constrain outputs using predefined\nentities. This structured approach significantly enhances the reformulation\ncapabilities of LLMs. Comprehensive experimental evaluations demonstrate that\nDRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while\nalso enhancing the performance of open-source models, such as Gemma2-9B, from\n26.35% to 56.75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering represents a core capability of large language models\n(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they\noften formulate questions that the text itself cannot answer due to\ninsufficient understanding of the underlying information. Recent studies reveal\nthat while LLMs can detect unanswerable questions, they struggle to assist\nusers in reformulating these questions. Even advanced models like GPT-3.5\ndemonstrate limited effectiveness in this regard. To address this limitation,\nwe propose DRS: Deep Question Reformulation with Structured Output, a novel\nzero-shot method aimed at enhancing LLMs ability to assist users in\nreformulating questions to extract relevant information from new documents. DRS\ncombines the strengths of LLMs with a DFS-based algorithm to iteratively\nexplore potential entity combinations and constrain outputs using predefined\nentities. This structured approach significantly enhances the reformulation\ncapabilities of LLMs. Comprehensive experimental evaluations demonstrate that\nDRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while\nalso enhancing the performance of open-source models, such as Gemma2-9B, from\n26.35% to 56.75%."
                },
                "authors": [
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03920v1",
                "updated": "2024-12-05T06:46:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    46,
                    46,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T06:46:46Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    46,
                    46,
                    3,
                    340,
                    0
                ],
                "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic\n  Scenarios"
                },
                "summary": "Game-theoretic scenarios have become pivotal in evaluating the social\nintelligence of Large Language Model (LLM)-based social agents. While numerous\nstudies have explored these agents in such settings, there is a lack of a\ncomprehensive survey summarizing the current progress. To address this gap, we\nsystematically review existing research on LLM-based social agents within\ngame-theoretic scenarios. Our survey organizes the findings into three core\ncomponents: Game Framework, Social Agent, and Evaluation Protocol. The game\nframework encompasses diverse game scenarios, ranging from choice-focusing to\ncommunication-focusing games. The social agent part explores agents'\npreferences, beliefs, and reasoning abilities. The evaluation protocol covers\nboth game-agnostic and game-specific metrics for assessing agent performance.\nBy reflecting on the current research and identifying future research\ndirections, this survey provides insights to advance the development and\nevaluation of social agents in game-theoretic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game-theoretic scenarios have become pivotal in evaluating the social\nintelligence of Large Language Model (LLM)-based social agents. While numerous\nstudies have explored these agents in such settings, there is a lack of a\ncomprehensive survey summarizing the current progress. To address this gap, we\nsystematically review existing research on LLM-based social agents within\ngame-theoretic scenarios. Our survey organizes the findings into three core\ncomponents: Game Framework, Social Agent, and Evaluation Protocol. The game\nframework encompasses diverse game scenarios, ranging from choice-focusing to\ncommunication-focusing games. The social agent part explores agents'\npreferences, beliefs, and reasoning abilities. The evaluation protocol covers\nboth game-agnostic and game-specific metrics for assessing agent performance.\nBy reflecting on the current research and identifying future research\ndirections, this survey provides insights to advance the development and\nevaluation of social agents in game-theoretic scenarios."
                },
                "authors": [
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Ella Li"
                    },
                    {
                        "name": "Qinghao Wang"
                    },
                    {
                        "name": "Haochuan Wang"
                    },
                    {
                        "name": "Yu Guo"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03915v1",
                "updated": "2024-12-05T06:34:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    34,
                    6,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T06:34:06Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    34,
                    6,
                    3,
                    340,
                    0
                ],
                "title": "Quantized and Interpretable Learning Scheme for Deep Neural Networks in\n  Classification Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized and Interpretable Learning Scheme for Deep Neural Networks in\n  Classification Task"
                },
                "summary": "Deep learning techniques have proven highly effective in image\nclassification, but their deployment in resourceconstrained environments\nremains challenging due to high computational demands. Furthermore, their\ninterpretability is of high importance which demands even more available\nresources. In this work, we introduce an approach that combines saliency-guided\ntraining with quantization techniques to create an interpretable and\nresource-efficient model without compromising accuracy. We utilize\nParameterized Clipping Activation (PACT) to perform quantization-aware\ntraining, specifically targeting activations and weights to optimize precision\nwhile minimizing resource usage. Concurrently, saliency-guided training is\nemployed to enhance interpretability by iteratively masking features with low\ngradient values, leading to more focused and meaningful saliency maps. This\ntraining procedure helps in mitigating noisy gradients and yields models that\nprovide clearer, more interpretable insights into their decision-making\nprocesses. To evaluate the impact of our approach, we conduct experiments using\nfamous Convolutional Neural Networks (CNN) architecture on the MNIST and\nCIFAR-10 benchmark datasets as two popular datasets. We compare the saliency\nmaps generated by standard and quantized models to assess the influence of\nquantization on both interpretability and classification accuracy. Our results\ndemonstrate that the combined use of saliency-guided training and PACT-based\nquantization not only maintains classification performance but also produces\nmodels that are significantly more efficient and interpretable, making them\nsuitable for deployment in resource-limited settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning techniques have proven highly effective in image\nclassification, but their deployment in resourceconstrained environments\nremains challenging due to high computational demands. Furthermore, their\ninterpretability is of high importance which demands even more available\nresources. In this work, we introduce an approach that combines saliency-guided\ntraining with quantization techniques to create an interpretable and\nresource-efficient model without compromising accuracy. We utilize\nParameterized Clipping Activation (PACT) to perform quantization-aware\ntraining, specifically targeting activations and weights to optimize precision\nwhile minimizing resource usage. Concurrently, saliency-guided training is\nemployed to enhance interpretability by iteratively masking features with low\ngradient values, leading to more focused and meaningful saliency maps. This\ntraining procedure helps in mitigating noisy gradients and yields models that\nprovide clearer, more interpretable insights into their decision-making\nprocesses. To evaluate the impact of our approach, we conduct experiments using\nfamous Convolutional Neural Networks (CNN) architecture on the MNIST and\nCIFAR-10 benchmark datasets as two popular datasets. We compare the saliency\nmaps generated by standard and quantized models to assess the influence of\nquantization on both interpretability and classification accuracy. Our results\ndemonstrate that the combined use of saliency-guided training and PACT-based\nquantization not only maintains classification performance but also produces\nmodels that are significantly more efficient and interpretable, making them\nsuitable for deployment in resource-limited settings."
                },
                "authors": [
                    {
                        "name": "Alireza Maleki"
                    },
                    {
                        "name": "Mahsa Lavaei"
                    },
                    {
                        "name": "Mohsen Bagheritabar"
                    },
                    {
                        "name": "Salar Beigzad"
                    },
                    {
                        "name": "Zahra Abadi"
                    }
                ],
                "author_detail": {
                    "name": "Zahra Abadi"
                },
                "author": "Zahra Abadi",
                "arxiv_journal_ref": "2024 IEEE 8th International Conference on Information and\n  Communication Technology (CICT)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19969v2",
                "updated": "2024-12-05T06:29:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    29,
                    7,
                    3,
                    340,
                    0
                ],
                "published": "2024-03-29T04:28:06Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    4,
                    28,
                    6,
                    4,
                    89,
                    0
                ],
                "title": "Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output\n  Channel Pruning on Computer Vision Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output\n  Channel Pruning on Computer Vision Tasks"
                },
                "summary": "Block pruning, which eliminates contiguous blocks of weights, is a structural\npruning method that can significantly enhance the performance of neural\nprocessing units (NPUs). In industrial applications, an ideal block pruning\nalgorithm should meet three key requirements: (1) maintain high accuracy across\ndiverse models and tasks, as machine learning deployments on edge devices are\ntypically accuracy-critical; (2) offer precise control over resource\nconstraints to facilitate user adoption; and (3) provide convergence guarantees\nto prevent performance instability. However, to the best of our knowledge, no\nexisting block pruning algorithm satisfies all three requirements\nsimultaneously. In this paper, we introduce SMART (Separate, Dynamic, and\nDifferentiable) pruning, a novel algorithm designed to address this gap. SMART\nleverages both weight and activation information to enhance accuracy, employs a\ndifferentiable top-k operator for precise control of resource constraints, and\noffers convergence guarantees under mild conditions. Extensive experiments\ninvolving seven models, four datasets, three different block types, and three\ncomputer vision tasks demonstrate that SMART pruning achieves state-of-the-art\nperformance in block pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block pruning, which eliminates contiguous blocks of weights, is a structural\npruning method that can significantly enhance the performance of neural\nprocessing units (NPUs). In industrial applications, an ideal block pruning\nalgorithm should meet three key requirements: (1) maintain high accuracy across\ndiverse models and tasks, as machine learning deployments on edge devices are\ntypically accuracy-critical; (2) offer precise control over resource\nconstraints to facilitate user adoption; and (3) provide convergence guarantees\nto prevent performance instability. However, to the best of our knowledge, no\nexisting block pruning algorithm satisfies all three requirements\nsimultaneously. In this paper, we introduce SMART (Separate, Dynamic, and\nDifferentiable) pruning, a novel algorithm designed to address this gap. SMART\nleverages both weight and activation information to enhance accuracy, employs a\ndifferentiable top-k operator for precise control of resource constraints, and\noffers convergence guarantees under mild conditions. Extensive experiments\ninvolving seven models, four datasets, three different block types, and three\ncomputer vision tasks demonstrate that SMART pruning achieves state-of-the-art\nperformance in block pruning."
                },
                "authors": [
                    {
                        "name": "Guanhua Ding"
                    },
                    {
                        "name": "Zexi Ye"
                    },
                    {
                        "name": "Zhen Zhong"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "David Shao"
                    }
                ],
                "author_detail": {
                    "name": "David Shao"
                },
                "author": "David Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03905v1",
                "updated": "2024-12-05T06:21:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    21,
                    31,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T06:21:31Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    21,
                    31,
                    3,
                    340,
                    0
                ],
                "title": "Integrating Various Software Artifacts for Better LLM-based Bug\n  Localization and Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Various Software Artifacts for Better LLM-based Bug\n  Localization and Program Repair"
                },
                "summary": "LLMs have garnered considerable attention for their potential to streamline\nAutomated Program Repair (APR). LLM-based approaches can either insert the\ncorrect code or directly generate patches when provided with buggy methods.\nHowever, most of LLM-based APR methods rely on a single type of software\ninformation, without fully leveraging different software artifacts. Despite\nthis, many LLM-based approaches do not explore which specific types of\ninformation best assist in APR. Addressing this gap is crucial for advancing\nLLM-based APR techniques. We propose DEVLoRe to use issue content (description\nand message) and stack error traces to localize buggy methods, then rely on\ndebug information in buggy methods and issue content and stack error to\nlocalize buggy lines and generate plausible patches which can pass all unit\ntests. The results show that while issue content is particularly effective in\nassisting LLMs with fault localization and program repair, different types of\nsoftware artifacts complement each other. By incorporating different artifacts,\nDEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy\nmethods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0\ndataset, respectively. This outperforms current state-of-the-art APR methods.\nThe source code and experimental results of this work for replication are\navailable at https://github.com/XYZboom/DEVLoRe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have garnered considerable attention for their potential to streamline\nAutomated Program Repair (APR). LLM-based approaches can either insert the\ncorrect code or directly generate patches when provided with buggy methods.\nHowever, most of LLM-based APR methods rely on a single type of software\ninformation, without fully leveraging different software artifacts. Despite\nthis, many LLM-based approaches do not explore which specific types of\ninformation best assist in APR. Addressing this gap is crucial for advancing\nLLM-based APR techniques. We propose DEVLoRe to use issue content (description\nand message) and stack error traces to localize buggy methods, then rely on\ndebug information in buggy methods and issue content and stack error to\nlocalize buggy lines and generate plausible patches which can pass all unit\ntests. The results show that while issue content is particularly effective in\nassisting LLMs with fault localization and program repair, different types of\nsoftware artifacts complement each other. By incorporating different artifacts,\nDEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy\nmethods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0\ndataset, respectively. This outperforms current state-of-the-art APR methods.\nThe source code and experimental results of this work for replication are\navailable at https://github.com/XYZboom/DEVLoRe."
                },
                "authors": [
                    {
                        "name": "Qiong Feng"
                    },
                    {
                        "name": "Xiaotian Ma"
                    },
                    {
                        "name": "Jiayi Sheng"
                    },
                    {
                        "name": "Ziyuan Feng"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Peng Liang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liang"
                },
                "author": "Peng Liang",
                "arxiv_comment": "22 pages, 11 images, 9 tables, Manuscript submitted to a journal\n  (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03904v1",
                "updated": "2024-12-05T06:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    20,
                    47,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T06:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    20,
                    47,
                    3,
                    340,
                    0
                ],
                "title": "MISR: Measuring Instrumental Self-Reasoning in Frontier Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MISR: Measuring Instrumental Self-Reasoning in Frontier Models"
                },
                "summary": "We propose a suite of tasks to evaluate the instrumental self-reasoning\nability of large language model (LLM) agents. Instrumental self-reasoning\nability could improve adaptability and enable self-modification, but it could\nalso pose significant risks, such as enabling deceptive alignment. Prior work\nhas only evaluated self-reasoning in non-agentic settings or in limited\ndomains. In this paper, we propose evaluations for instrumental self-reasoning\nability in agentic tasks in a wide range of scenarios, including\nself-modification, knowledge seeking, and opaque self-reasoning. We evaluate\nagents built using state-of-the-art LLMs, including commercial and open source\nsystems. We find that instrumental self-reasoning ability emerges only in the\nmost capable frontier models and that it is highly context-dependent. No model\npasses the the most difficult versions of our evaluations, hence our evaluation\ncan be used to measure increases in instrumental self-reasoning ability in\nfuture models. We open-source our evaluations at\nhttps://github.com/kaifronsdal/Self-Reasoning-Evals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a suite of tasks to evaluate the instrumental self-reasoning\nability of large language model (LLM) agents. Instrumental self-reasoning\nability could improve adaptability and enable self-modification, but it could\nalso pose significant risks, such as enabling deceptive alignment. Prior work\nhas only evaluated self-reasoning in non-agentic settings or in limited\ndomains. In this paper, we propose evaluations for instrumental self-reasoning\nability in agentic tasks in a wide range of scenarios, including\nself-modification, knowledge seeking, and opaque self-reasoning. We evaluate\nagents built using state-of-the-art LLMs, including commercial and open source\nsystems. We find that instrumental self-reasoning ability emerges only in the\nmost capable frontier models and that it is highly context-dependent. No model\npasses the the most difficult versions of our evaluations, hence our evaluation\ncan be used to measure increases in instrumental self-reasoning ability in\nfuture models. We open-source our evaluations at\nhttps://github.com/kaifronsdal/Self-Reasoning-Evals."
                },
                "authors": [
                    {
                        "name": "Kai Fronsdal"
                    },
                    {
                        "name": "David Lindner"
                    }
                ],
                "author_detail": {
                    "name": "David Lindner"
                },
                "author": "David Lindner",
                "arxiv_comment": "10 pages, 65 page appendix, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14215v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14215v2",
                "updated": "2024-12-05T06:02:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    2,
                    59,
                    3,
                    340,
                    0
                ],
                "published": "2024-04-22T14:31:28Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    14,
                    31,
                    28,
                    0,
                    113,
                    0
                ],
                "title": "Text-Tuple-Table: Towards Information Integration in Text-to-Table\n  Generation via Global Tuple Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Tuple-Table: Towards Information Integration in Text-to-Table\n  Generation via Global Tuple Extraction"
                },
                "summary": "The task of condensing large chunks of textual information into concise and\nstructured tables has gained attention recently due to the emergence of Large\nLanguage Models (LLMs) and their potential benefit for downstream tasks, such\nas text summarization and text mining. Previous approaches often generate\ntables that directly replicate information from the text, limiting their\napplicability in broader contexts, as text-to-table generation in real-life\nscenarios necessitates information extraction, reasoning, and integration.\nHowever, there is a lack of both datasets and methodologies towards this task.\nIn this paper, we introduce LiveSum, a new benchmark dataset created for\ngenerating summary tables of competitions based on real-time commentary texts.\nWe evaluate the performances of state-of-the-art LLMs on this task in both\nfine-tuning and zero-shot settings, and additionally propose a novel pipeline\ncalled $T^3$(Text-Tuple-Table) to improve their performances. Extensive\nexperimental results demonstrate that LLMs still struggle with this task even\nafter fine-tuning, while our approach can offer substantial performance gains\nwithout explicit training. Further analyses demonstrate that our method\nexhibits strong generalization abilities, surpassing previous approaches on\nseveral other text-to-table datasets. Our code and data can be found at\nhttps://github.com/HKUST-KnowComp/LiveSum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of condensing large chunks of textual information into concise and\nstructured tables has gained attention recently due to the emergence of Large\nLanguage Models (LLMs) and their potential benefit for downstream tasks, such\nas text summarization and text mining. Previous approaches often generate\ntables that directly replicate information from the text, limiting their\napplicability in broader contexts, as text-to-table generation in real-life\nscenarios necessitates information extraction, reasoning, and integration.\nHowever, there is a lack of both datasets and methodologies towards this task.\nIn this paper, we introduce LiveSum, a new benchmark dataset created for\ngenerating summary tables of competitions based on real-time commentary texts.\nWe evaluate the performances of state-of-the-art LLMs on this task in both\nfine-tuning and zero-shot settings, and additionally propose a novel pipeline\ncalled $T^3$(Text-Tuple-Table) to improve their performances. Extensive\nexperimental results demonstrate that LLMs still struggle with this task even\nafter fine-tuning, while our approach can offer substantial performance gains\nwithout explicit training. Further analyses demonstrate that our method\nexhibits strong generalization abilities, surpassing previous approaches on\nseveral other text-to-table datasets. Our code and data can be found at\nhttps://github.com/HKUST-KnowComp/LiveSum."
                },
                "authors": [
                    {
                        "name": "Zheye Deng"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yuxi Sun"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Yauwai Yim"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14215v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02372v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02372v2",
                "updated": "2024-12-05T06:00:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    0,
                    34,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-03T10:58:34Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    10,
                    58,
                    34,
                    1,
                    338,
                    0
                ],
                "title": "HERO: Hint-Based Efficient and Reliable Query Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERO: Hint-Based Efficient and Reliable Query Optimizer"
                },
                "summary": "We propose a novel model for learned query optimization which provides query\nhints leading to better execution plans. The model addresses the three key\nchallenges in learned hint-based query optimization: reliable hint\nrecommendation (ensuring non-degradation of query latency), efficient hint\nexploration, and fast inference. We provide an in-depth analysis of existing\nNN-based approaches to hint-based optimization and experimentally confirm the\nnamed challenges for them. Our alternative solution consists of a new inference\nschema based on an ensemble of context-aware models and a graph storage for\nreliable hint suggestion and fast inference, and a budget-controlled training\nprocedure with a local search algorithm that solves the issue of exponential\nsearch space exploration. In experiments on standard benchmarks, our model\ndemonstrates optimization capability close to the best achievable with\ncoarse-grained hints. Controlling the degree of parallelism (query dop) in\naddition to operator-related hints enables our model to achieve 3x latency\nimprovement on JOB benchmark which sets a new standard for optimization. Our\nmodel is interpretable and easy to debug, which is particularly important for\ndeployment in production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel model for learned query optimization which provides query\nhints leading to better execution plans. The model addresses the three key\nchallenges in learned hint-based query optimization: reliable hint\nrecommendation (ensuring non-degradation of query latency), efficient hint\nexploration, and fast inference. We provide an in-depth analysis of existing\nNN-based approaches to hint-based optimization and experimentally confirm the\nnamed challenges for them. Our alternative solution consists of a new inference\nschema based on an ensemble of context-aware models and a graph storage for\nreliable hint suggestion and fast inference, and a budget-controlled training\nprocedure with a local search algorithm that solves the issue of exponential\nsearch space exploration. In experiments on standard benchmarks, our model\ndemonstrates optimization capability close to the best achievable with\ncoarse-grained hints. Controlling the degree of parallelism (query dop) in\naddition to operator-related hints enables our model to achieve 3x latency\nimprovement on JOB benchmark which sets a new standard for optimization. Our\nmodel is interpretable and easy to debug, which is particularly important for\ndeployment in production."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Sergey Iazov"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Iazov"
                },
                "author": "Sergey Iazov",
                "arxiv_comment": "Submitted to VLDB 2025; 13 pages; 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02372v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02372v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; I.2.6; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03886v1",
                "updated": "2024-12-05T05:39:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    5,
                    39,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T05:39:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    5,
                    39,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "Uniform Discretized Integrated Gradients: An effective attribution based\n  method for explaining large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform Discretized Integrated Gradients: An effective attribution based\n  method for explaining large language models"
                },
                "summary": "Integrated Gradients is a well-known technique for explaining deep learning\nmodels. It calculates feature importance scores by employing a gradient based\napproach computing gradients of the model output with respect to input features\nand accumulating them along a linear path. While this works well for continuous\nfeatures spaces, it may not be the most optimal way to deal with discrete\nspaces like word embeddings. For interpreting LLMs (Large Language Models),\nthere exists a need for a non-linear path where intermediate points, whose\ngradients are to be computed, lie close to actual words in the embedding space.\nIn this paper, we propose a method called Uniform Discretized Integrated\nGradients (UDIG) based on a new interpolation strategy where we choose a\nfavorable nonlinear path for computing attribution scores suitable for\npredictive language models. We evaluate our method on two types of NLP tasks-\nSentiment Classification and Question Answering against three metrics viz Log\nodds, Comprehensiveness and Sufficiency. For sentiment classification, we have\nused the SST2, IMDb and Rotten Tomatoes datasets for benchmarking and for\nQuestion Answering, we have used the fine-tuned BERT model on SQuAD dataset.\nOur approach outperforms the existing methods in almost all the metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Gradients is a well-known technique for explaining deep learning\nmodels. It calculates feature importance scores by employing a gradient based\napproach computing gradients of the model output with respect to input features\nand accumulating them along a linear path. While this works well for continuous\nfeatures spaces, it may not be the most optimal way to deal with discrete\nspaces like word embeddings. For interpreting LLMs (Large Language Models),\nthere exists a need for a non-linear path where intermediate points, whose\ngradients are to be computed, lie close to actual words in the embedding space.\nIn this paper, we propose a method called Uniform Discretized Integrated\nGradients (UDIG) based on a new interpolation strategy where we choose a\nfavorable nonlinear path for computing attribution scores suitable for\npredictive language models. We evaluate our method on two types of NLP tasks-\nSentiment Classification and Question Answering against three metrics viz Log\nodds, Comprehensiveness and Sufficiency. For sentiment classification, we have\nused the SST2, IMDb and Rotten Tomatoes datasets for benchmarking and for\nQuestion Answering, we have used the fine-tuned BERT model on SQuAD dataset.\nOur approach outperforms the existing methods in almost all the metrics."
                },
                "authors": [
                    {
                        "name": "Swarnava Sinha Roy"
                    },
                    {
                        "name": "Ayan Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Ayan Kundu"
                },
                "author": "Ayan Kundu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08188v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08188v4",
                "updated": "2024-12-05T05:37:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    5,
                    37,
                    46,
                    3,
                    340,
                    0
                ],
                "published": "2024-08-15T14:46:13Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    46,
                    13,
                    3,
                    228,
                    0
                ],
                "title": "Nl2Hltl2Plan: Scaling Up Natural Language Understanding for Multi-Robots\n  Through Hierarchical Temporal Logic Task Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nl2Hltl2Plan: Scaling Up Natural Language Understanding for Multi-Robots\n  Through Hierarchical Temporal Logic Task Representation"
                },
                "summary": "To enable non-experts to specify long-horizon, multi-robot collaborative\ntasks, language models are increasingly used to translate natural language\ncommands into formal specifications. However, because translation can occur in\nmultiple ways, such translations may lack accuracy or lead to inefficient\nmulti-robot planning. Our key insight is that concise hierarchical\nspecifications can simplify planning while remaining straightforward to derive\nfrom human instructions. We propose Nl2Hltl2Plan, a framework that translates\nnatural language commands into hierarchical Linear Temporal Logic (LTL) and\nsolves the corresponding planning problem. The translation involves two steps\nleveraging Large Language Models (LLMs). First, an LLM transforms instructions\ninto a Hierarchical Task Tree, capturing logical and temporal relations. Next,\na fine-tuned LLM converts sub-tasks into flat LTL formulas, which are\naggregated into hierarchical specifications, with the lowest level\ncorresponding to ordered robot actions. These specifications are then used with\noff-the-shelf planners. Our Nl2Hltl2Plan demonstrates the potential of LLMs in\nhierarchical reasoning for multi-robot task planning. Evaluations in simulation\nand real-world experiments with human participants show that Nl2Hltl2Plan\noutperforms existing methods, handling more complex instructions while\nachieving higher success rates and lower costs in task allocation and planning.\nAdditional details are available at https://nl2hltl2plan.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enable non-experts to specify long-horizon, multi-robot collaborative\ntasks, language models are increasingly used to translate natural language\ncommands into formal specifications. However, because translation can occur in\nmultiple ways, such translations may lack accuracy or lead to inefficient\nmulti-robot planning. Our key insight is that concise hierarchical\nspecifications can simplify planning while remaining straightforward to derive\nfrom human instructions. We propose Nl2Hltl2Plan, a framework that translates\nnatural language commands into hierarchical Linear Temporal Logic (LTL) and\nsolves the corresponding planning problem. The translation involves two steps\nleveraging Large Language Models (LLMs). First, an LLM transforms instructions\ninto a Hierarchical Task Tree, capturing logical and temporal relations. Next,\na fine-tuned LLM converts sub-tasks into flat LTL formulas, which are\naggregated into hierarchical specifications, with the lowest level\ncorresponding to ordered robot actions. These specifications are then used with\noff-the-shelf planners. Our Nl2Hltl2Plan demonstrates the potential of LLMs in\nhierarchical reasoning for multi-robot task planning. Evaluations in simulation\nand real-world experiments with human participants show that Nl2Hltl2Plan\noutperforms existing methods, handling more complex instructions while\nachieving higher success rates and lower costs in task allocation and planning.\nAdditional details are available at https://nl2hltl2plan.github.io ."
                },
                "authors": [
                    {
                        "name": "Shaojun Xu"
                    },
                    {
                        "name": "Xusheng Luo"
                    },
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Letian Leng"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08188v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08188v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03879v1",
                "updated": "2024-12-05T05:21:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    5,
                    21,
                    32,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T05:21:32Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    5,
                    21,
                    32,
                    3,
                    340,
                    0
                ],
                "title": "E-Commerce in Africa: Divergent Impacts on Rural and Urban Economies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-Commerce in Africa: Divergent Impacts on Rural and Urban Economies"
                },
                "summary": "E-commerce is rapidly transforming economies across Africa, offering immense\nopportunities for economic growth, market expansion, and digital inclusion.\nThis study investigates the effects of e-commerce on select African regions. By\nutilizing readiness factors, including mobile money deployment, GDP per capita,\ninternet penetration, and digital infrastructure, the preparedness of African\ncountries for e-commerce adoption is quantified, highlighting significant\ndisparities. Through case studies in urban and rural areas, including Lagos,\nKano, Nairobi, and the Rift Valley, the study shows e-commerce's significant\neffects on small and medium-sized enterprises (SMEs), employment, and market\nefficiency. Urban centers demonstrated significant gains in productivity and\nprofitability, whereas rural regions experienced slower growth due to limited\ninternet access and infrastructural barriers. Despite these challenges,\nlocalized solutions such as mobile money systems and agricultural e-commerce\nplatforms are bridging gaps. This study highlights the significant potential of\ne-commerce in Africa while emphasizing the need for targeted investments and\nstrategies to address existing regional disparities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce is rapidly transforming economies across Africa, offering immense\nopportunities for economic growth, market expansion, and digital inclusion.\nThis study investigates the effects of e-commerce on select African regions. By\nutilizing readiness factors, including mobile money deployment, GDP per capita,\ninternet penetration, and digital infrastructure, the preparedness of African\ncountries for e-commerce adoption is quantified, highlighting significant\ndisparities. Through case studies in urban and rural areas, including Lagos,\nKano, Nairobi, and the Rift Valley, the study shows e-commerce's significant\neffects on small and medium-sized enterprises (SMEs), employment, and market\nefficiency. Urban centers demonstrated significant gains in productivity and\nprofitability, whereas rural regions experienced slower growth due to limited\ninternet access and infrastructural barriers. Despite these challenges,\nlocalized solutions such as mobile money systems and agricultural e-commerce\nplatforms are bridging gaps. This study highlights the significant potential of\ne-commerce in Africa while emphasizing the need for targeted investments and\nstrategies to address existing regional disparities."
                },
                "authors": [
                    {
                        "name": "Jaelyn S. Liang"
                    },
                    {
                        "name": "Rehaan S. Mundy"
                    },
                    {
                        "name": "Shriya Jagwayan"
                    }
                ],
                "author_detail": {
                    "name": "Shriya Jagwayan"
                },
                "author": "Shriya Jagwayan",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01129v2",
                "updated": "2024-12-05T05:05:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    5,
                    5,
                    1,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T05:09:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    5,
                    9,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for\n  Boosting 2-bit Large Language Model Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for\n  Boosting 2-bit Large Language Model Accuracy"
                },
                "summary": "Low-rank adaptation (LoRA) has become the dominant method for\nparameter-efficient LLM fine-tuning, with LoRA-based quantization error\ncompensation (LQEC) emerging as a powerful tool for recovering accuracy in\ncompressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, with\nno prior investigation into understanding this limitation. We propose RILQ\n(Rank-Insensitive LoRA-based Quantization Error Compensation) to understand\nfundamental limitation and boost 2-bit LLM accuracy. Based on rank analysis\nrevealing model-wise activation discrepancy loss's rank-insensitive nature,\nRILQ employs this loss to adjust adapters cooperatively across layers, enabling\nrobust error compensation with low-rank adapters. Evaluations on LLaMA-2 and\nLLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inference\nacross various state-of-the-art quantizers and enhanced accuracy in\ntask-specific fine-tuning. RILQ maintains computational efficiency comparable\nto existing LoRA methods, enabling adapter-merged weight-quantized LLM\ninference with significantly enhanced accuracy, making it a promising approach\nfor boosting 2-bit LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptation (LoRA) has become the dominant method for\nparameter-efficient LLM fine-tuning, with LoRA-based quantization error\ncompensation (LQEC) emerging as a powerful tool for recovering accuracy in\ncompressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, with\nno prior investigation into understanding this limitation. We propose RILQ\n(Rank-Insensitive LoRA-based Quantization Error Compensation) to understand\nfundamental limitation and boost 2-bit LLM accuracy. Based on rank analysis\nrevealing model-wise activation discrepancy loss's rank-insensitive nature,\nRILQ employs this loss to adjust adapters cooperatively across layers, enabling\nrobust error compensation with low-rank adapters. Evaluations on LLaMA-2 and\nLLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inference\nacross various state-of-the-art quantizers and enhanced accuracy in\ntask-specific fine-tuning. RILQ maintains computational efficiency comparable\nto existing LoRA methods, enabling adapter-merged weight-quantized LLM\ninference with significantly enhanced accuracy, making it a promising approach\nfor boosting 2-bit LLM performance."
                },
                "authors": [
                    {
                        "name": "Geonho Lee"
                    },
                    {
                        "name": "Janghwan Lee"
                    },
                    {
                        "name": "Sukjin Hong"
                    },
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Euijai Ahn"
                    },
                    {
                        "name": "Du-Seong Chang"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "The typo in Table 4 has been corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11534v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11534v2",
                "updated": "2024-12-05T04:40:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    40,
                    54,
                    3,
                    340,
                    0
                ],
                "published": "2024-02-18T10:15:38Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    10,
                    15,
                    38,
                    6,
                    49,
                    0
                ],
                "title": "PreAct: Prediction Enhances Agent's Planning Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PreAct: Prediction Enhances Agent's Planning Ability"
                },
                "summary": "Addressing the disparity between forecasts and actual results can enable\nindividuals to expand their thought processes and stimulate self-reflection,\nthus promoting accurate planning. In this research, we present **PreAct**, an\nagent framework that integrates **pre**diction, **rea**soning, and **act**ion.\nBy utilizing the information derived from predictions, the large language model\n(LLM) agent can provide a wider range and more strategically focused reasoning.\nThis leads to more efficient actions that aid the agent in accomplishing\nintricate tasks. Our experimental results show that PreAct surpasses the ReAct\nmethod in completing complex tasks and that PreAct's performance can be further\nimproved when paired with other memory or selection strategy techniques. We\npresented the model with varying quantities of historical predictions and\ndiscovered that these predictions consistently enhance LLM planning.The\nvariances in single-step reasoning between PreAct and ReAct indicate that\nPreAct indeed has benefits in terms of diversity and strategic orientation over\nReAct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the disparity between forecasts and actual results can enable\nindividuals to expand their thought processes and stimulate self-reflection,\nthus promoting accurate planning. In this research, we present **PreAct**, an\nagent framework that integrates **pre**diction, **rea**soning, and **act**ion.\nBy utilizing the information derived from predictions, the large language model\n(LLM) agent can provide a wider range and more strategically focused reasoning.\nThis leads to more efficient actions that aid the agent in accomplishing\nintricate tasks. Our experimental results show that PreAct surpasses the ReAct\nmethod in completing complex tasks and that PreAct's performance can be further\nimproved when paired with other memory or selection strategy techniques. We\npresented the model with varying quantities of historical predictions and\ndiscovered that these predictions consistently enhance LLM planning.The\nvariances in single-step reasoning between PreAct and ReAct indicate that\nPreAct indeed has benefits in terms of diversity and strategic orientation over\nReAct."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Jianzhao Huang"
                    },
                    {
                        "name": "Siyuan Lu"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Keqing He"
                    },
                    {
                        "name": "Weiran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Xu"
                },
                "author": "Weiran Xu",
                "arxiv_comment": "Coling 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11534v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10659v4",
                "updated": "2024-12-05T04:35:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    35,
                    22,
                    3,
                    340,
                    0
                ],
                "published": "2024-02-16T13:10:14Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    13,
                    10,
                    14,
                    4,
                    47,
                    0
                ],
                "title": "Network Formation and Dynamics Among Multi-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network Formation and Dynamics Among Multi-LLMs"
                },
                "summary": "Social networks fundamentally shape human opinions, behaviors, and the\ndissemination of information. As large language models (LLMs) like GPT, Claude,\nand Llama increasingly integrate into social and professional settings,\nunderstanding their behavior in the context of social interactions and network\nformation becomes essential. This study develops a framework to systematically\nexamine whether the network formation behaviors of multiple LLMs approximate\ncertain aspects of human network dynamics. By simulating interactions among LLM\nagents across various model families, we observe that these models consistently\nexhibit key patterns associated with social network principles including\npreferential attachment, triadic closure, homophily, community structure, and\nthe small-world phenomenon when forming networks. Moreover, LLMs adapt their\nnetwork formation strategies based on each network's characteristics,\nreflecting the context-dependent nature of human behavior: in Facebook\nnetworks, they prioritize triadic closure and homophily, mirroring close-knit\nfriendships; in phone networks, homophily and preferential attachment dominate,\ncapturing personal and professional connections, while in employment networks,\nLLMs favor heterophily and high-degree connections, aligning with career\nadvancement dynamics. These results open new avenues for using LLMs in network\nscience research, with potential applications in agent-based modeling and\nsynthetic network generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social networks fundamentally shape human opinions, behaviors, and the\ndissemination of information. As large language models (LLMs) like GPT, Claude,\nand Llama increasingly integrate into social and professional settings,\nunderstanding their behavior in the context of social interactions and network\nformation becomes essential. This study develops a framework to systematically\nexamine whether the network formation behaviors of multiple LLMs approximate\ncertain aspects of human network dynamics. By simulating interactions among LLM\nagents across various model families, we observe that these models consistently\nexhibit key patterns associated with social network principles including\npreferential attachment, triadic closure, homophily, community structure, and\nthe small-world phenomenon when forming networks. Moreover, LLMs adapt their\nnetwork formation strategies based on each network's characteristics,\nreflecting the context-dependent nature of human behavior: in Facebook\nnetworks, they prioritize triadic closure and homophily, mirroring close-knit\nfriendships; in phone networks, homophily and preferential attachment dominate,\ncapturing personal and professional connections, while in employment networks,\nLLMs favor heterophily and high-degree connections, aligning with career\nadvancement dynamics. These results open new avenues for using LLMs in network\nscience research, with potential applications in agent-based modeling and\nsynthetic network generation."
                },
                "authors": [
                    {
                        "name": "Marios Papachristou"
                    },
                    {
                        "name": "Yuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Yuan"
                },
                "author": "Yuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03864v1",
                "updated": "2024-12-05T04:20:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    20,
                    54,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T04:20:54Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    20,
                    54,
                    3,
                    340,
                    0
                ],
                "title": "Training MLPs on Graphs without Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training MLPs on Graphs without Supervision"
                },
                "summary": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various\ngraph learning tasks, yet their reliance on neighborhood aggregation during\ninference poses challenges for deployment in latency-sensitive applications,\nsuch as real-time financial fraud detection. To address this limitation, recent\nstudies have proposed distilling knowledge from teacher GNNs into student\nMulti-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate\ninference. However, these approaches often inadequately explore structural\ninformation when inferring unseen nodes. To this end, we introduce SimMLP, a\nSelf-supervised framework for learning MLPs on graphs, designed to fully\nintegrate rich structural information into MLPs. Notably, SimMLP is the first\nMLP-learning method that can achieve equivalence to GNNs in the optimal case.\nThe key idea is to employ self-supervised learning to align the representations\nencoded by graph context-aware GNNs and neighborhood dependency-free MLPs,\nthereby fully integrating the structural information into MLPs. We provide a\ncomprehensive theoretical analysis, demonstrating the equivalence between\nSimMLP and GNNs based on mutual information and inductive bias, highlighting\nSimMLP's advanced structural learning capabilities. Additionally, we conduct\nextensive experiments on 20 benchmark datasets, covering node classification,\nlink prediction, and graph classification, to showcase SimMLP's superiority\nover state-of-the-art baselines, particularly in scenarios involving unseen\nnodes (e.g., inductive and cold-start node classification) where structural\ninsights are crucial. Our codes are available at:\nhttps://github.com/Zehong-Wang/SimMLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various\ngraph learning tasks, yet their reliance on neighborhood aggregation during\ninference poses challenges for deployment in latency-sensitive applications,\nsuch as real-time financial fraud detection. To address this limitation, recent\nstudies have proposed distilling knowledge from teacher GNNs into student\nMulti-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate\ninference. However, these approaches often inadequately explore structural\ninformation when inferring unseen nodes. To this end, we introduce SimMLP, a\nSelf-supervised framework for learning MLPs on graphs, designed to fully\nintegrate rich structural information into MLPs. Notably, SimMLP is the first\nMLP-learning method that can achieve equivalence to GNNs in the optimal case.\nThe key idea is to employ self-supervised learning to align the representations\nencoded by graph context-aware GNNs and neighborhood dependency-free MLPs,\nthereby fully integrating the structural information into MLPs. We provide a\ncomprehensive theoretical analysis, demonstrating the equivalence between\nSimMLP and GNNs based on mutual information and inductive bias, highlighting\nSimMLP's advanced structural learning capabilities. Additionally, we conduct\nextensive experiments on 20 benchmark datasets, covering node classification,\nlink prediction, and graph classification, to showcase SimMLP's superiority\nover state-of-the-art baselines, particularly in scenarios involving unseen\nnodes (e.g., inductive and cold-start node classification) where structural\ninsights are crucial. Our codes are available at:\nhttps://github.com/Zehong-Wang/SimMLP."
                },
                "authors": [
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Chuxu Zhang"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "arxiv_comment": "Accepted by WSDM 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00273v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00273v3",
                "updated": "2024-12-05T04:19:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    19,
                    45,
                    3,
                    340,
                    0
                ],
                "published": "2024-05-01T01:45:50Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    1,
                    45,
                    50,
                    2,
                    122,
                    0
                ],
                "title": "Social Life Simulation for Non-Cognitive Skills Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Life Simulation for Non-Cognitive Skills Learning"
                },
                "summary": "Non-cognitive skills are crucial for personal and social life well-being, and\nsuch skill development can be supported by narrative-based (e.g., storytelling)\ntechnologies. While generative AI enables interactive and role-playing\nstorytelling, little is known about how users engage with and perceive the use\nof AI in social life simulation for non-cognitive skills learning.\nAdditionally, the benefits of AI mentorship on self-reflection awareness and\nability in this context remain largely underexplored. To this end, we\nintroduced Simulife++, an interactive platform enabled by a large language\nmodel (LLM). The system allows users to act as protagonists, creating stories\nwith one or multiple AI-based characters in diverse social scenarios. In\nparticular, we expanded the Human-AI interaction to a Human-AI-AI collaboration\nby including a Sage Agent, who acts as a bystander, providing users with some\nperspectives and guidance on their choices and conversations in terms of\nnon-cognitive skills to promote reflection. In a within-subject user study, our\nquantitative results reveal that, when accompanied by Sage Agent, users exhibit\nsignificantly higher levels of reflection on motivation, self-perceptions, and\nresilience & coping, along with an enhanced experience of narrative\ntransportation. Additionally, our qualitative findings suggest that Sage Agent\nplays a crucial role in promoting reflection on non-cognitive skills, enhancing\nsocial communication and decision-making performance, and improving overall\nuser experience within Simulife++. Multiple supportive relationships between\nSage Agent and users were also reported. We offer design implications for the\napplication of generative AI in narrative solutions and the future potential of\nSage Agent for non-cognitive skill development in broader social contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-cognitive skills are crucial for personal and social life well-being, and\nsuch skill development can be supported by narrative-based (e.g., storytelling)\ntechnologies. While generative AI enables interactive and role-playing\nstorytelling, little is known about how users engage with and perceive the use\nof AI in social life simulation for non-cognitive skills learning.\nAdditionally, the benefits of AI mentorship on self-reflection awareness and\nability in this context remain largely underexplored. To this end, we\nintroduced Simulife++, an interactive platform enabled by a large language\nmodel (LLM). The system allows users to act as protagonists, creating stories\nwith one or multiple AI-based characters in diverse social scenarios. In\nparticular, we expanded the Human-AI interaction to a Human-AI-AI collaboration\nby including a Sage Agent, who acts as a bystander, providing users with some\nperspectives and guidance on their choices and conversations in terms of\nnon-cognitive skills to promote reflection. In a within-subject user study, our\nquantitative results reveal that, when accompanied by Sage Agent, users exhibit\nsignificantly higher levels of reflection on motivation, self-perceptions, and\nresilience & coping, along with an enhanced experience of narrative\ntransportation. Additionally, our qualitative findings suggest that Sage Agent\nplays a crucial role in promoting reflection on non-cognitive skills, enhancing\nsocial communication and decision-making performance, and improving overall\nuser experience within Simulife++. Multiple supportive relationships between\nSage Agent and users were also reported. We offer design implications for the\napplication of generative AI in narrative solutions and the future potential of\nSage Agent for non-cognitive skill development in broader social contexts."
                },
                "authors": [
                    {
                        "name": "Zihan Yan"
                    },
                    {
                        "name": "Yaohong Xiang"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00273v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00273v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05889v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05889v3",
                "updated": "2024-12-05T04:16:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    16,
                    54,
                    3,
                    340,
                    0
                ],
                "published": "2024-02-08T18:27:22Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    18,
                    27,
                    22,
                    3,
                    39,
                    0
                ],
                "title": "CREMA: Generalizable and Efficient Video-Language Reasoning via\n  Multimodal Modular Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREMA: Generalizable and Efficient Video-Language Reasoning via\n  Multimodal Modular Fusion"
                },
                "summary": "Despite impressive advancements in recent multimodal reasoning approaches,\nthey are still limited in flexibility and efficiency, as these models typically\nprocess only a few fixed modality inputs and require updates to numerous\nparameters. This paper tackles these critical challenges and proposes CREMA, a\ngeneralizable, highly efficient, and modular modality-fusion framework that can\nincorporate any new modality to enhance video reasoning. We first augment\nmultiple informative modalities (such as optical flow, 3D point cloud, audio,\nthermal heatmap, and touch map) from given videos without extra human\nannotation by leveraging sensors or existing pre-trained models. Next, we\nintroduce a query transformer with multiple parameter-efficient modules\nassociated with each accessible modality. It projects diverse modality features\nto the LLM token embedding space, allowing the model to integrate different\ndata types for response generation. Furthermore, we propose a novel progressive\nmultimodal fusion design supported by a lightweight fusion module and\nmodality-sequential training strategy. It helps compress information across\nvarious assisting modalities, maintaining computational efficiency in the LLM\nwhile improving performance. We validate our method on 7 video-language\nreasoning tasks assisted by diverse modalities, including conventional VideoQA\nand Video-Audio/3D/Touch/Thermal QA, and achieve better/equivalent performance\nagainst strong multimodal LLMs, including OneLLM, BLIP-2, and SeViLA while\nreducing over 90% trainable parameters. We provide extensive analyses of CREMA,\nincluding the impact of each modality on reasoning domains, the design of the\nfusion module, and example visualizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive advancements in recent multimodal reasoning approaches,\nthey are still limited in flexibility and efficiency, as these models typically\nprocess only a few fixed modality inputs and require updates to numerous\nparameters. This paper tackles these critical challenges and proposes CREMA, a\ngeneralizable, highly efficient, and modular modality-fusion framework that can\nincorporate any new modality to enhance video reasoning. We first augment\nmultiple informative modalities (such as optical flow, 3D point cloud, audio,\nthermal heatmap, and touch map) from given videos without extra human\nannotation by leveraging sensors or existing pre-trained models. Next, we\nintroduce a query transformer with multiple parameter-efficient modules\nassociated with each accessible modality. It projects diverse modality features\nto the LLM token embedding space, allowing the model to integrate different\ndata types for response generation. Furthermore, we propose a novel progressive\nmultimodal fusion design supported by a lightweight fusion module and\nmodality-sequential training strategy. It helps compress information across\nvarious assisting modalities, maintaining computational efficiency in the LLM\nwhile improving performance. We validate our method on 7 video-language\nreasoning tasks assisted by diverse modalities, including conventional VideoQA\nand Video-Audio/3D/Touch/Thermal QA, and achieve better/equivalent performance\nagainst strong multimodal LLMs, including OneLLM, BLIP-2, and SeViLA while\nreducing over 90% trainable parameters. We provide extensive analyses of CREMA,\nincluding the impact of each modality on reasoning domains, the design of the\nfusion module, and example visualizations."
                },
                "authors": [
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "first two authors contributed equally. Project page:\n  https://CREMA-VideoLLM.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05889v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05889v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11831v3",
                "updated": "2024-12-05T04:06:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    6,
                    9,
                    3,
                    340,
                    0
                ],
                "published": "2024-06-17T17:59:43Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    59,
                    43,
                    0,
                    169,
                    0
                ],
                "title": "Exploring the Role of Large Language Models in Prompt Encoding for\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Large Language Models in Prompt Encoding for\n  Diffusion Models"
                },
                "summary": "Large language models (LLMs) based on decoder-only transformers have\ndemonstrated superior text understanding capabilities compared to CLIP and\nT5-series models. However, the paradigm for utilizing current advanced LLMs in\ntext-to-image diffusion models remains to be explored. We observed an unusual\nphenomenon: directly using a large language model as the prompt encoder\nsignificantly degrades the prompt-following ability in image generation. We\nidentified two main obstacles behind this issue. One is the misalignment\nbetween the next token prediction training in LLM and the requirement for\ndiscriminative prompt features in diffusion models. The other is the intrinsic\npositional bias introduced by the decoder-only architecture. To deal with this\nissue, we propose a novel framework to fully harness the capabilities of LLMs.\nThrough the carefully designed usage guidance, we effectively enhance the text\nrepresentation capability for prompt encoding and eliminate its inherent\npositional bias. This allows us to integrate state-of-the-art LLMs into the\ntext-to-image generation model flexibly. Furthermore, we also provide an\neffective manner to fuse multiple LLMs into our framework. Considering the\nexcellent performance and scaling capabilities demonstrated by the transformer\narchitecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT)\nbased on the framework. We conduct extensive experiments to validate LI-DiT\nacross model size and data size. Benefiting from the inherent ability of the\nLLMs and our innovative designs, the prompt understanding performance of LI-DiT\neasily surpasses state-of-the-art open-source models as well as mainstream\nclosed-source commercial models including Stable Diffusion 3, DALL-E 3, and\nMidjourney V6. The LLM-Infused Diffuser framework is also one of the core\ntechnologies powering SenseMirage, a highly advanced text-to-image model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on decoder-only transformers have\ndemonstrated superior text understanding capabilities compared to CLIP and\nT5-series models. However, the paradigm for utilizing current advanced LLMs in\ntext-to-image diffusion models remains to be explored. We observed an unusual\nphenomenon: directly using a large language model as the prompt encoder\nsignificantly degrades the prompt-following ability in image generation. We\nidentified two main obstacles behind this issue. One is the misalignment\nbetween the next token prediction training in LLM and the requirement for\ndiscriminative prompt features in diffusion models. The other is the intrinsic\npositional bias introduced by the decoder-only architecture. To deal with this\nissue, we propose a novel framework to fully harness the capabilities of LLMs.\nThrough the carefully designed usage guidance, we effectively enhance the text\nrepresentation capability for prompt encoding and eliminate its inherent\npositional bias. This allows us to integrate state-of-the-art LLMs into the\ntext-to-image generation model flexibly. Furthermore, we also provide an\neffective manner to fuse multiple LLMs into our framework. Considering the\nexcellent performance and scaling capabilities demonstrated by the transformer\narchitecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT)\nbased on the framework. We conduct extensive experiments to validate LI-DiT\nacross model size and data size. Benefiting from the inherent ability of the\nLLMs and our innovative designs, the prompt understanding performance of LI-DiT\neasily surpasses state-of-the-art open-source models as well as mainstream\nclosed-source commercial models including Stable Diffusion 3, DALL-E 3, and\nMidjourney V6. The LLM-Infused Diffuser framework is also one of the core\ntechnologies powering SenseMirage, a highly advanced text-to-image model."
                },
                "authors": [
                    {
                        "name": "Bingqi Ma"
                    },
                    {
                        "name": "Zhuofan Zong"
                    },
                    {
                        "name": "Guanglu Song"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03856v1",
                "updated": "2024-12-05T04:05:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    5,
                    43,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T04:05:43Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    5,
                    43,
                    3,
                    340,
                    0
                ],
                "title": "How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs\n  in E-Learning Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs\n  in E-Learning Environments?"
                },
                "summary": "E-learning environments are increasingly harnessing large language models\n(LLMs) like GPT-3.5 and GPT-4 for tailored educational support. This study\nintroduces an approach that integrates dynamic knowledge graphs with LLMs to\noffer nuanced student assistance. By evaluating past and ongoing student\ninteractions, the system identifies and appends the most salient learning\ncontext to prompts directed at the LLM. Central to this method is the knowledge\ngraph's role in assessing a student's comprehension of topic prerequisites.\nDepending on the categorized understanding (good, average, or poor), the LLM\nadjusts its guidance, offering advanced assistance, foundational reviews, or\nin-depth prerequisite explanations, respectively. Preliminary findings suggest\nstudents could benefit from this tiered support, achieving enhanced\ncomprehension and improved task outcomes. However, several issues related to\npotential errors arising from LLMs were identified, which can potentially\nmislead students. This highlights the need for human intervention to mitigate\nthese risks. This research aims to advance AI-driven personalized learning\nwhile acknowledging the limitations and potential pitfalls, thus guiding future\nresearch in technology and data-driven education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-learning environments are increasingly harnessing large language models\n(LLMs) like GPT-3.5 and GPT-4 for tailored educational support. This study\nintroduces an approach that integrates dynamic knowledge graphs with LLMs to\noffer nuanced student assistance. By evaluating past and ongoing student\ninteractions, the system identifies and appends the most salient learning\ncontext to prompts directed at the LLM. Central to this method is the knowledge\ngraph's role in assessing a student's comprehension of topic prerequisites.\nDepending on the categorized understanding (good, average, or poor), the LLM\nadjusts its guidance, offering advanced assistance, foundational reviews, or\nin-depth prerequisite explanations, respectively. Preliminary findings suggest\nstudents could benefit from this tiered support, achieving enhanced\ncomprehension and improved task outcomes. However, several issues related to\npotential errors arising from LLMs were identified, which can potentially\nmislead students. This highlights the need for human intervention to mitigate\nthese risks. This research aims to advance AI-driven personalized learning\nwhile acknowledging the limitations and potential pitfalls, thus guiding future\nresearch in technology and data-driven education."
                },
                "authors": [
                    {
                        "name": "Patrick Ocheja"
                    },
                    {
                        "name": "Brendan Flanagan"
                    },
                    {
                        "name": "Yiling Dai"
                    },
                    {
                        "name": "Hiroaki Ogata"
                    }
                ],
                "author_detail": {
                    "name": "Hiroaki Ogata"
                },
                "author": "Hiroaki Ogata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18711v2",
                "updated": "2024-12-05T04:01:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    1,
                    28,
                    3,
                    340,
                    0
                ],
                "published": "2024-05-29T02:44:12Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    2,
                    44,
                    12,
                    2,
                    150,
                    0
                ],
                "title": "Calibrating Reasoning in Language Models with Internal Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Reasoning in Language Models with Internal Consistency"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks, aided by techniques like chain-of-thought prompting\nthat elicits verbalized reasoning. However, LLMs often generate text with\nobvious mistakes and contradictions, raising doubts about their ability to\nrobustly process and utilize generated rationales. In this work, we investigate\nreasoning in LLMs through the lens of internal representations, focusing on how\nthese representations are influenced by generated rationales. Our preliminary\nanalysis reveals that while generated rationales improve answer accuracy,\ninconsistencies emerge between the model's internal representations in middle\nlayers and those in final layers, potentially undermining the reliability of\ntheir reasoning processes. To address this, we propose internal consistency as\na measure of the model's confidence by examining the agreement of latent\npredictions decoded from intermediate layers. Extensive empirical studies\nacross different models and datasets demonstrate that internal consistency\neffectively distinguishes between correct and incorrect reasoning paths.\nMotivated by this, we propose a new approach to calibrate reasoning by\nup-weighting reasoning paths with high internal consistency, resulting in a\nsignificant boost in reasoning performance. Further analysis uncovers distinct\npatterns in attention and feed-forward modules across layers, providing\ninsights into the emergence of internal inconsistency. In summary, our results\ndemonstrate the potential of using internal representations for self-evaluation\nof LLMs. Our code is available at github.com/zhxieml/internal-consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks, aided by techniques like chain-of-thought prompting\nthat elicits verbalized reasoning. However, LLMs often generate text with\nobvious mistakes and contradictions, raising doubts about their ability to\nrobustly process and utilize generated rationales. In this work, we investigate\nreasoning in LLMs through the lens of internal representations, focusing on how\nthese representations are influenced by generated rationales. Our preliminary\nanalysis reveals that while generated rationales improve answer accuracy,\ninconsistencies emerge between the model's internal representations in middle\nlayers and those in final layers, potentially undermining the reliability of\ntheir reasoning processes. To address this, we propose internal consistency as\na measure of the model's confidence by examining the agreement of latent\npredictions decoded from intermediate layers. Extensive empirical studies\nacross different models and datasets demonstrate that internal consistency\neffectively distinguishes between correct and incorrect reasoning paths.\nMotivated by this, we propose a new approach to calibrate reasoning by\nup-weighting reasoning paths with high internal consistency, resulting in a\nsignificant boost in reasoning performance. Further analysis uncovers distinct\npatterns in attention and feed-forward modules across layers, providing\ninsights into the emergence of internal inconsistency. In summary, our results\ndemonstrate the potential of using internal representations for self-evaluation\nof LLMs. Our code is available at github.com/zhxieml/internal-consistency."
                },
                "authors": [
                    {
                        "name": "Zhihui Xie"
                    },
                    {
                        "name": "Jizhou Guo"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Shuai Li"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Li"
                },
                "author": "Shuai Li",
                "arxiv_comment": "NeurIPS 2024 camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03847v1",
                "updated": "2024-12-05T03:27:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    3,
                    27,
                    2,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T03:27:02Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    3,
                    27,
                    2,
                    3,
                    340,
                    0
                ],
                "title": "Educational-Psychological Dialogue Robot Based on Multi-Agent\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Educational-Psychological Dialogue Robot Based on Multi-Agent\n  Collaboration"
                },
                "summary": "Intelligent dialogue systems are increasingly used in modern education and\npsychological counseling fields, but most existing systems are limited to a\nsingle domain, cannot deal with both educational and psychological issues, and\noften lack accuracy and professionalism when dealing with complex issues. To\naddress these problems, this paper proposes an intelligent dialog system that\ncombines educational and psychological counseling functions. The system\nconsists of multiple AI agent, including security detection agent, intent\nidentification agent, educational LLM agent, and psychological LLM agent, which\nwork in concert to ensure the provision of accurate educational knowledge Q\\&A\nand psychological support services. Specifically, the system recognizes\nuser-input intentions through an intention classification model and invokes a\nretrieval-enhanced educational grand model and a psychological grand model\nfine-tuned with psychological data in order to provide professional educational\nadvice and psychological support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent dialogue systems are increasingly used in modern education and\npsychological counseling fields, but most existing systems are limited to a\nsingle domain, cannot deal with both educational and psychological issues, and\noften lack accuracy and professionalism when dealing with complex issues. To\naddress these problems, this paper proposes an intelligent dialog system that\ncombines educational and psychological counseling functions. The system\nconsists of multiple AI agent, including security detection agent, intent\nidentification agent, educational LLM agent, and psychological LLM agent, which\nwork in concert to ensure the provision of accurate educational knowledge Q\\&A\nand psychological support services. Specifically, the system recognizes\nuser-input intentions through an intention classification model and invokes a\nretrieval-enhanced educational grand model and a psychological grand model\nfine-tuned with psychological data in order to provide professional educational\nadvice and psychological support."
                },
                "authors": [
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "arxiv_journal_ref": "ICSR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03842v1",
                "updated": "2024-12-05T03:12:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    3,
                    12,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T03:12:49Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    3,
                    12,
                    49,
                    3,
                    340,
                    0
                ],
                "title": "CCxTrust: Confidential Computing Platform Based on TEE and TPM\n  Collaborative Trust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCxTrust: Confidential Computing Platform Based on TEE and TPM\n  Collaborative Trust"
                },
                "summary": "Confidential Computing has emerged to address data security challenges in\ncloud-centric deployments by protecting data in use through hardware-level\nisolation. However, reliance on a single hardware root of trust (RoT) limits\nuser confidence in cloud platforms, especially for high-performance AI\nservices, where end-to-end protection of sensitive models and data is critical.\nFurthermore, the lack of interoperability and a unified trust model in\nmulti-cloud environments prevents the establishment of a cross-platform,\ncross-cloud chain of trust, creating a significant trust gap for users with\nhigh privacy requirements. To address the challenges mentioned above, this\npaper proposes CCxTrust (Confidential Computing with Trust), a confidential\ncomputing platform leveraging collaborative roots of trust from TEE and TPM.\nCCxTrust combines the black-box RoT embedded in the CPU-TEE with the flexible\nwhite-box RoT of TPM to establish a collaborative trust framework. The platform\nimplements independent Roots of Trust for Measurement (RTM) for TEE and TPM,\nand a collaborative Root of Trust for Report (RTR) for composite attestation.\nThe Root of Trust for Storage (RTS) is solely supported by TPM. We also present\nthe design and implementation of a confidential TPM supporting multiple modes\nfor secure use within confidential virtual machines. Additionally, we propose a\ncomposite attestation protocol integrating TEE and TPM to enhance security and\nattestation efficiency, which is proven secure under the PCL protocol security\nmodel. We implemented a prototype of CCxTrust on a confidential computing\nserver with AMD SEV-SNP and TPM chips, requiring minimal modifications to the\nTPM and guest Linux kernel. The composite attestation efficiency improved by\n24% without significant overhead, while Confidential TPM performance showed a\n16.47% reduction compared to standard TPM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Computing has emerged to address data security challenges in\ncloud-centric deployments by protecting data in use through hardware-level\nisolation. However, reliance on a single hardware root of trust (RoT) limits\nuser confidence in cloud platforms, especially for high-performance AI\nservices, where end-to-end protection of sensitive models and data is critical.\nFurthermore, the lack of interoperability and a unified trust model in\nmulti-cloud environments prevents the establishment of a cross-platform,\ncross-cloud chain of trust, creating a significant trust gap for users with\nhigh privacy requirements. To address the challenges mentioned above, this\npaper proposes CCxTrust (Confidential Computing with Trust), a confidential\ncomputing platform leveraging collaborative roots of trust from TEE and TPM.\nCCxTrust combines the black-box RoT embedded in the CPU-TEE with the flexible\nwhite-box RoT of TPM to establish a collaborative trust framework. The platform\nimplements independent Roots of Trust for Measurement (RTM) for TEE and TPM,\nand a collaborative Root of Trust for Report (RTR) for composite attestation.\nThe Root of Trust for Storage (RTS) is solely supported by TPM. We also present\nthe design and implementation of a confidential TPM supporting multiple modes\nfor secure use within confidential virtual machines. Additionally, we propose a\ncomposite attestation protocol integrating TEE and TPM to enhance security and\nattestation efficiency, which is proven secure under the PCL protocol security\nmodel. We implemented a prototype of CCxTrust on a confidential computing\nserver with AMD SEV-SNP and TPM chips, requiring minimal modifications to the\nTPM and guest Linux kernel. The composite attestation efficiency improved by\n24% without significant overhead, while Confidential TPM performance showed a\n16.47% reduction compared to standard TPM."
                },
                "authors": [
                    {
                        "name": "Ketong Shang"
                    },
                    {
                        "name": "Jiangnan Lin"
                    },
                    {
                        "name": "Yu Qin"
                    },
                    {
                        "name": "Muyan Shen"
                    },
                    {
                        "name": "Hongzhan Ma"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Dengguo Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dengguo Feng"
                },
                "author": "Dengguo Feng",
                "arxiv_comment": "23 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.6; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05014v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05014v2",
                "updated": "2024-12-05T03:12:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    3,
                    12,
                    4,
                    3,
                    340,
                    0
                ],
                "published": "2024-09-08T07:54:16Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    7,
                    54,
                    16,
                    6,
                    252,
                    0
                ],
                "title": "Analyzing Challenges in Deployment of the SLSA Framework for Software\n  Supply Chain Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Challenges in Deployment of the SLSA Framework for Software\n  Supply Chain Security"
                },
                "summary": "In 2023, Sonatype reported a 200\\% increase in software supply chain attacks,\nincluding major build infrastructure attacks. To secure the software supply\nchain, practitioners can follow security framework guidance like the\nSupply-chain Levels for Software Artifacts (SLSA). However, recent surveys and\nindustry summits have shown that despite growing interest, the adoption of SLSA\nis not widespread. To understand adoption challenges, \\textit{the goal of this\nstudy is to aid framework authors and practitioners in improving the adoption\nand development of Supply-Chain Levels for Software Artifacts (SLSA) through a\nqualitative study of SLSA-related issues on GitHub}. We analyzed 1,523\nSLSA-related issues extracted from 233 GitHub repositories. We conducted a\ntopic-guided thematic analysis, leveraging the Latent Dirichlet Allocation\n(LDA) unsupervised machine learning algorithm, to explore the challenges of\nadopting SLSA and the strategies for overcoming these challenges. We identified\nfour significant challenges and five suggested adoption strategies. The two\nmain challenges reported are complex implementation and unclear communication,\nhighlighting the difficulties in implementing and understanding the SLSA\nprocess across diverse ecosystems. The suggested strategies include\nstreamlining provenance generation processes, improving the SLSA verification\nprocess, and providing specific and detailed documentation. Our findings\nindicate that some strategies can help mitigate multiple challenges, and some\nchallenges need future research and tool enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 2023, Sonatype reported a 200\\% increase in software supply chain attacks,\nincluding major build infrastructure attacks. To secure the software supply\nchain, practitioners can follow security framework guidance like the\nSupply-chain Levels for Software Artifacts (SLSA). However, recent surveys and\nindustry summits have shown that despite growing interest, the adoption of SLSA\nis not widespread. To understand adoption challenges, \\textit{the goal of this\nstudy is to aid framework authors and practitioners in improving the adoption\nand development of Supply-Chain Levels for Software Artifacts (SLSA) through a\nqualitative study of SLSA-related issues on GitHub}. We analyzed 1,523\nSLSA-related issues extracted from 233 GitHub repositories. We conducted a\ntopic-guided thematic analysis, leveraging the Latent Dirichlet Allocation\n(LDA) unsupervised machine learning algorithm, to explore the challenges of\nadopting SLSA and the strategies for overcoming these challenges. We identified\nfour significant challenges and five suggested adoption strategies. The two\nmain challenges reported are complex implementation and unclear communication,\nhighlighting the difficulties in implementing and understanding the SLSA\nprocess across diverse ecosystems. The suggested strategies include\nstreamlining provenance generation processes, improving the SLSA verification\nprocess, and providing specific and detailed documentation. Our findings\nindicate that some strategies can help mitigate multiple challenges, and some\nchallenges need future research and tool enhancement."
                },
                "authors": [
                    {
                        "name": "Mahzabin Tamanna"
                    },
                    {
                        "name": "Sivana Hamer"
                    },
                    {
                        "name": "Mindy Tran"
                    },
                    {
                        "name": "Sascha Fahl"
                    },
                    {
                        "name": "Yasemin Acar"
                    },
                    {
                        "name": "Laurie Williams"
                    }
                ],
                "author_detail": {
                    "name": "Laurie Williams"
                },
                "author": "Laurie Williams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05014v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05014v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11266v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11266v4",
                "updated": "2024-12-05T02:48:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    2,
                    48,
                    32,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-18T03:45:34Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    3,
                    45,
                    34,
                    0,
                    323,
                    0
                ],
                "title": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs"
                },
                "summary": "Large-scale pretrained models, particularly Large Language Models (LLMs),\nhave exhibited remarkable capabilities in handling multiple tasks across\ndomains due to their emergent properties. These capabilities are further\naugmented during the Supervised Fine-Tuning (SFT) phase. Despite their\npotential, existing work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce VersaTune, a novel\ndata composition framework designed for enhancing LLMs' overall multi-ability\nperformances during training. We categorize knowledge into distinct domains\nincluding law, medicine, finance, science, code, etc. We begin with detecting\nthe distribution of domain-specific knowledge within the base model, followed\nby the training data composition that aligns with the model's existing\nknowledge distribution. During the training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results demonstrate that VersaTune achieves significant\nimprovements in multi-domain performance, with an 35.21% enhancement in\ncomprehensive multi-domain tasks. Additionally, in scenarios where specific\ndomain optimization is required, VersaTune reduces the degradation of\nperformance in other domains by 38.77%, without compromising the target\ndomain's training efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pretrained models, particularly Large Language Models (LLMs),\nhave exhibited remarkable capabilities in handling multiple tasks across\ndomains due to their emergent properties. These capabilities are further\naugmented during the Supervised Fine-Tuning (SFT) phase. Despite their\npotential, existing work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce VersaTune, a novel\ndata composition framework designed for enhancing LLMs' overall multi-ability\nperformances during training. We categorize knowledge into distinct domains\nincluding law, medicine, finance, science, code, etc. We begin with detecting\nthe distribution of domain-specific knowledge within the base model, followed\nby the training data composition that aligns with the model's existing\nknowledge distribution. During the training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results demonstrate that VersaTune achieves significant\nimprovements in multi-domain performance, with an 35.21% enhancement in\ncomprehensive multi-domain tasks. Additionally, in scenarios where specific\ndomain optimization is required, VersaTune reduces the degradation of\nperformance in other domains by 38.77%, without compromising the target\ndomain's training efficacy."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Keshi Zhao"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11266v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11266v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03822v1",
                "updated": "2024-12-05T02:35:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    2,
                    35,
                    46,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T02:35:46Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    2,
                    35,
                    46,
                    3,
                    340,
                    0
                ],
                "title": "Beyond the Binary: Capturing Diverse Preferences With Reward\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Binary: Capturing Diverse Preferences With Reward\n  Regularization"
                },
                "summary": "Large language models (LLMs) are increasingly deployed via public-facing\ninterfaces to interact with millions of users, each with diverse preferences.\nDespite this, preference tuning of LLMs predominantly relies on reward models\ntrained using binary judgments where annotators select the preferred choice out\nof pairs of model outputs. In this work, we argue that this reliance on binary\nchoices does not capture the broader, aggregate preferences of the target user\nin real-world tasks. We propose a taxonomy that identifies two dimensions of\nsubjectivity where different users disagree on the preferred output-namely, the\nPlurality of Responses to Prompts, where prompts allow for multiple correct\nanswers, and the Indistinguishability of Responses, where candidate outputs are\nparaphrases of each other. We show that reward models correlate weakly with\nuser preferences in these cases. As a first step to address this issue, we\nintroduce a simple yet effective method that augments existing binary\npreference datasets with synthetic preference judgments to estimate potential\nuser disagreement. Incorporating these via a margin term as a form of\nregularization during model training yields predictions that better align with\nthe aggregate user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed via public-facing\ninterfaces to interact with millions of users, each with diverse preferences.\nDespite this, preference tuning of LLMs predominantly relies on reward models\ntrained using binary judgments where annotators select the preferred choice out\nof pairs of model outputs. In this work, we argue that this reliance on binary\nchoices does not capture the broader, aggregate preferences of the target user\nin real-world tasks. We propose a taxonomy that identifies two dimensions of\nsubjectivity where different users disagree on the preferred output-namely, the\nPlurality of Responses to Prompts, where prompts allow for multiple correct\nanswers, and the Indistinguishability of Responses, where candidate outputs are\nparaphrases of each other. We show that reward models correlate weakly with\nuser preferences in these cases. As a first step to address this issue, we\nintroduce a simple yet effective method that augments existing binary\npreference datasets with synthetic preference judgments to estimate potential\nuser disagreement. Incorporating these via a margin term as a form of\nregularization during model training yields predictions that better align with\nthe aggregate user preferences."
                },
                "authors": [
                    {
                        "name": "Vishakh Padmakumar"
                    },
                    {
                        "name": "Chuanyang Jin"
                    },
                    {
                        "name": "Hannah Rose Kirk"
                    },
                    {
                        "name": "He He"
                    }
                ],
                "author_detail": {
                    "name": "He He"
                },
                "author": "He He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03815v1",
                "updated": "2024-12-05T02:18:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    2,
                    18,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T02:18:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    2,
                    18,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "Synergizing LLMs and Knowledge Graphs: A Novel Approach to Software\n  Repository-Related Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing LLMs and Knowledge Graphs: A Novel Approach to Software\n  Repository-Related Question Answering"
                },
                "summary": "Software repositories contain valuable information for gaining insights into\ntheir development process. However, extracting insights from these repository\ndata is time-consuming and requires technical expertise. While software\nengineering chatbots have been developed to facilitate natural language\ninteractions with repositories, they struggle with understanding natural\nlanguage and accurately retrieving relevant data. This study aims to improve\nthe accuracy of LLM-based chatbots in answering repository-related questions by\naugmenting them with knowledge graphs. We achieve this in a two-step approach;\n(1) constructing a knowledge graph from the repository data and (2) synergizing\nthe knowledge graph with LLM to allow for the natural language questions and\nanswers. We curated a set of 20 questions with different complexities and\nevaluated our approach on five popular open-source projects. Our approach\nachieved an accuracy of 65%. We further investigated the limitations and\nidentified six key issues, with the majority relating to the reasoning\ncapability of the LLM. We experimented with a few-shot chain-of-thought\nprompting to determine if it could enhance our approach. This technique\nimproved the overall accuracy to 84%. Our findings demonstrate the synergy\nbetween LLMs and knowledge graphs as a viable solution for making repository\ndata accessible to both technical and non-technical stakeholders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software repositories contain valuable information for gaining insights into\ntheir development process. However, extracting insights from these repository\ndata is time-consuming and requires technical expertise. While software\nengineering chatbots have been developed to facilitate natural language\ninteractions with repositories, they struggle with understanding natural\nlanguage and accurately retrieving relevant data. This study aims to improve\nthe accuracy of LLM-based chatbots in answering repository-related questions by\naugmenting them with knowledge graphs. We achieve this in a two-step approach;\n(1) constructing a knowledge graph from the repository data and (2) synergizing\nthe knowledge graph with LLM to allow for the natural language questions and\nanswers. We curated a set of 20 questions with different complexities and\nevaluated our approach on five popular open-source projects. Our approach\nachieved an accuracy of 65%. We further investigated the limitations and\nidentified six key issues, with the majority relating to the reasoning\ncapability of the LLM. We experimented with a few-shot chain-of-thought\nprompting to determine if it could enhance our approach. This technique\nimproved the overall accuracy to 84%. Our findings demonstrate the synergy\nbetween LLMs and knowledge graphs as a viable solution for making repository\ndata accessible to both technical and non-technical stakeholders."
                },
                "authors": [
                    {
                        "name": "Samuel Abedu"
                    },
                    {
                        "name": "SayedHassan Khatoonabadi"
                    },
                    {
                        "name": "Emad Shihab"
                    }
                ],
                "author_detail": {
                    "name": "Emad Shihab"
                },
                "author": "Emad Shihab",
                "arxiv_comment": "Submitted to ACM Transactions on Software Engineering and Methodology\n  for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03809v1",
                "updated": "2024-12-05T02:05:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    2,
                    5,
                    33,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T02:05:33Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    2,
                    5,
                    33,
                    3,
                    340,
                    0
                ],
                "title": "EditScout: Locating Forged Regions from Diffusion-based Edited Images\n  with Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EditScout: Locating Forged Regions from Diffusion-based Edited Images\n  with Multimodal LLM"
                },
                "summary": "Image editing technologies are tools used to transform, adjust, remove, or\notherwise alter images. Recent research has significantly improved the\ncapabilities of image editing tools, enabling the creation of photorealistic\nand semantically informed forged regions that are nearly indistinguishable from\nauthentic imagery, presenting new challenges in digital forensics and media\ncredibility. While current image forensic techniques are adept at localizing\nforged regions produced by traditional image manipulation methods, current\ncapabilities struggle to localize regions created by diffusion-based\ntechniques. To bridge this gap, we present a novel framework that integrates a\nmultimodal Large Language Model (LLM) for enhanced reasoning capabilities to\nlocalize tampered regions in images produced by diffusion model-based editing\nmethods. By leveraging the contextual and semantic strengths of LLMs, our\nframework achieves promising results on MagicBrush, AutoSplice, and PerfBrush\n(novel diffusion-based dataset) datasets, outperforming previous approaches in\nmIoU and F1-score metrics. Notably, our method excels on the PerfBrush dataset,\na self-constructed test set featuring previously unseen types of edits. Here,\nwhere traditional methods typically falter, achieving markedly low scores, our\napproach demonstrates promising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image editing technologies are tools used to transform, adjust, remove, or\notherwise alter images. Recent research has significantly improved the\ncapabilities of image editing tools, enabling the creation of photorealistic\nand semantically informed forged regions that are nearly indistinguishable from\nauthentic imagery, presenting new challenges in digital forensics and media\ncredibility. While current image forensic techniques are adept at localizing\nforged regions produced by traditional image manipulation methods, current\ncapabilities struggle to localize regions created by diffusion-based\ntechniques. To bridge this gap, we present a novel framework that integrates a\nmultimodal Large Language Model (LLM) for enhanced reasoning capabilities to\nlocalize tampered regions in images produced by diffusion model-based editing\nmethods. By leveraging the contextual and semantic strengths of LLMs, our\nframework achieves promising results on MagicBrush, AutoSplice, and PerfBrush\n(novel diffusion-based dataset) datasets, outperforming previous approaches in\nmIoU and F1-score metrics. Notably, our method excels on the PerfBrush dataset,\na self-constructed test set featuring previously unseen types of edits. Here,\nwhere traditional methods typically falter, achieving markedly low scores, our\napproach demonstrates promising performance."
                },
                "authors": [
                    {
                        "name": "Quang Nguyen"
                    },
                    {
                        "name": "Truong Vu"
                    },
                    {
                        "name": "Trong-Tung Nguyen"
                    },
                    {
                        "name": "Yuxin Wen"
                    },
                    {
                        "name": "Preston K Robinette"
                    },
                    {
                        "name": "Taylor T Johnson"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Anh Tran"
                    },
                    {
                        "name": "Khoi Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Khoi Nguyen"
                },
                "author": "Khoi Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]