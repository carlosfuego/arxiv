[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20787v2",
                "updated": "2025-10-24T16:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    56,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T17:53:03Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    17,
                    53,
                    3,
                    3,
                    296,
                    0
                ],
                "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction"
                },
                "summary": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v2",
                "updated": "2025-10-24T14:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    55,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22922v2",
                "updated": "2025-10-24T11:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-28T15:15:31Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    15,
                    15,
                    31,
                    5,
                    179,
                    0
                ],
                "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling"
                },
                "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass."
                },
                "authors": [
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v6",
                "updated": "2025-10-24T08:41:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    41,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v2",
                "updated": "2025-10-24T08:35:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    35,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v2",
                "updated": "2025-10-24T05:39:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    5,
                    39,
                    3,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v2",
                "updated": "2025-10-24T04:48:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    48,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v5",
                "updated": "2025-10-23T23:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    23,
                    35,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page:\n  https://github.com/tensorgi/TPA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v2",
                "updated": "2025-10-23T21:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    31,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v3",
                "updated": "2025-10-23T19:45:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    19,
                    45,
                    39,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    },
                    {
                        "name": "Panagiotis Karras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Karras"
                },
                "author": "Panagiotis Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v3",
                "updated": "2025-10-23T18:52:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    18,
                    52,
                    25,
                    3,
                    296,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20707v1",
                "updated": "2025-10-23T16:17:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T16:17:47Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models"
                },
                "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Yuchao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/MixKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v2",
                "updated": "2025-10-23T15:26:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    15,
                    26,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "arxiv_comment": "Accepted to ALENEX`26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v4",
                "updated": "2025-10-23T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    14,
                    23,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "35 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v1",
                "updated": "2025-10-23T12:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif rdk"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20878v1",
                "updated": "2025-10-23T12:28:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:28:58Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement"
                },
                "summary": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Yixue Yang"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "13 pages,16 figures,2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; E.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v4",
                "updated": "2025-10-25T00:33:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    0,
                    33,
                    14,
                    5,
                    298,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20400v1",
                "updated": "2025-10-23T10:06:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:06:48Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels"
                },
                "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline."
                },
                "authors": [
                    {
                        "name": "Rubn Langarita"
                    },
                    {
                        "name": "Jess Alastruey-Bened"
                    },
                    {
                        "name": "Pablo Ibez-Marn"
                    },
                    {
                        "name": "Santiago Marco-Sola"
                    },
                    {
                        "name": "Miquel Moret"
                    },
                    {
                        "name": "Adri Armejach"
                    }
                ],
                "author_detail": {
                    "name": "Adri Armejach"
                },
                "author": "Adri Armejach",
                "arxiv_comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v2",
                "updated": "2025-10-23T09:55:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    55,
                    50,
                    3,
                    296,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v2",
                "updated": "2025-10-23T09:09:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    9,
                    15,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v2",
                "updated": "2025-10-23T08:29:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    29,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20230v1",
                "updated": "2025-10-23T05:22:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T05:22:09Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "title": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$"
                },
                "summary": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Chenchao Xu"
                    },
                    {
                        "name": "Zhimian Wu"
                    },
                    {
                        "name": "Huachen Rao"
                    },
                    {
                        "name": "Zhaoyang Shan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Guanghan Cao"
                    },
                    {
                        "name": "Michael Smidman"
                    },
                    {
                        "name": "Ming Shi"
                    },
                    {
                        "name": "Huiqiu Yuan"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Xianhui Chen"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Yu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yu Song"
                },
                "author": "Yu Song",
                "arxiv_comment": "submitted to journal in July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v4",
                "updated": "2025-10-23T00:47:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    47,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "Accepted to ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v2",
                "updated": "2025-10-23T00:40:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    40,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v2",
                "updated": "2025-10-22T23:56:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    56,
                    45,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    }
                ],
                "author_detail": {
                    "name": "Fariha Tasmin"
                },
                "author": "Fariha Tasmin",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19875v1",
                "updated": "2025-10-22T09:42:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:42:29Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention"
                },
                "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "Jos Luis Redondo Garca"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Konstantina Palla"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19264v1",
                "updated": "2025-10-22T05:47:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T05:47:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPRAD: LLM-Assisted PRotocol Attack Discovery"
                },
                "summary": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."
                },
                "authors": [
                    {
                        "name": "R. Can Aygun"
                    },
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    },
                    {
                        "name": "Leonard Kleinrock"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Kleinrock"
                },
                "arxiv_affiliation": "UCLA",
                "author": "Leonard Kleinrock",
                "arxiv_comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)",
                "arxiv_journal_ref": "Published in IFIP Networking 2025 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19183v1",
                "updated": "2025-10-22T02:41:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:41:07Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning"
                },
                "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Fengyuan Sun"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Xinhao Xu"
                    },
                    {
                        "name": "Dandan Zheng"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19171v1",
                "updated": "2025-10-22T02:09:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:09:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG"
                },
                "summary": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference."
                },
                "authors": [
                    {
                        "name": "Jihwan Bang"
                    },
                    {
                        "name": "Juntae Lee"
                    },
                    {
                        "name": "Seunghan Yang"
                    },
                    {
                        "name": "Sungha Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungha Choi"
                },
                "author": "Sungha Choi",
                "arxiv_comment": "Accepted at NeurIPS 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v3",
                "updated": "2025-10-21T22:37:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    22,
                    37,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v3",
                "updated": "2025-10-21T21:07:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    21,
                    7,
                    17,
                    1,
                    294,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "arxiv_doi": "10.1145/3772052.3772208",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772208",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the version accepted to ACM SoCC 2025. The title has been\n  updated to match the published version",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15878v2",
                "updated": "2025-10-21T16:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    32,
                    50,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-21T16:10:26Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    10,
                    26,
                    3,
                    233,
                    0
                ],
                "title": "Putting the Context back into Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putting the Context back into Memory"
                },
                "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices."
                },
                "authors": [
                    {
                        "name": "David A. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "David A. Roberts"
                },
                "author": "David A. Roberts",
                "arxiv_comment": "Fixed errors in paragraph numbering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18716v1",
                "updated": "2025-10-21T15:17:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:17:37Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Siyong Jian"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14576v3",
                "updated": "2025-10-21T15:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    13,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2024-01-26T00:27:00Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    0,
                    27,
                    0,
                    4,
                    26,
                    0
                ],
                "title": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints"
                },
                "summary": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings."
                },
                "authors": [
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Artur Podobas"
                    },
                    {
                        "name": "Niclas Jansson"
                    },
                    {
                        "name": "Stefano Markidis"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "arxiv_doi": "10.1145/3772052.3772212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SoCC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v1",
                "updated": "2025-10-21T12:39:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18546v1",
                "updated": "2025-10-21T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval"
                },
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v2",
                "updated": "2025-10-21T10:33:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    33,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching"
                },
                "summary": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v2",
                "updated": "2025-10-21T10:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    8,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v1",
                "updated": "2025-10-21T10:00:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18413v1",
                "updated": "2025-10-21T08:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T08:44:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference"
                },
                "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity."
                },
                "authors": [
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Guo-Qing Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    },
                    {
                        "name": "Ran Zhu"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v5",
                "updated": "2025-10-21T06:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    47,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was first submitted for review on Sept. 5, 2024, and the\n  initial version was uploaded to Arxiv on Sept. 30, 2024. The latest version\n  has accepted for publication by IEEE Transactions on Information Forensics\n  and Security (TIFS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v3",
                "updated": "2025-10-21T06:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    30,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Steffen Christgau"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18269v1",
                "updated": "2025-10-21T03:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T03:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "title": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding"
                },
                "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth."
                },
                "authors": [
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17777v1",
                "updated": "2025-10-20T17:35:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:35:47Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference"
                },
                "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
                },
                "authors": [
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Konstantinos N. Plataniotis"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17238v1",
                "updated": "2025-10-20T07:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17098v1",
                "updated": "2025-10-20T02:04:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T02:04:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models"
                },
                "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."
                },
                "authors": [
                    {
                        "name": "Elias Hossain"
                    },
                    {
                        "name": "Swayamjit Saha"
                    },
                    {
                        "name": "Somshubhra Roy"
                    },
                    {
                        "name": "Ravi Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prasad"
                },
                "author": "Ravi Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17045v1",
                "updated": "2025-10-19T23:17:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T23:17:13Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "title": "Video Reasoning without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Reasoning without Training"
                },
                "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model."
                },
                "authors": [
                    {
                        "name": "Deepak Sridhar"
                    },
                    {
                        "name": "Kartikeya Bhardwaj"
                    },
                    {
                        "name": "Jeya Pradha Jeyaraj"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    },
                    {
                        "name": "Ankita Nayak"
                    },
                    {
                        "name": "Harris Teague"
                    }
                ],
                "author_detail": {
                    "name": "Harris Teague"
                },
                "author": "Harris Teague",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16871v1",
                "updated": "2025-10-19T15:13:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T15:13:25Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "title": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy"
                },
                "summary": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16805v1",
                "updated": "2025-10-19T12:16:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T12:16:40Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "title": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects"
                },
                "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models."
                },
                "authors": [
                    {
                        "name": "Mariam Rakka"
                    },
                    {
                        "name": "Marios Fournarakis"
                    },
                    {
                        "name": "Olga Krestinskaya"
                    },
                    {
                        "name": "Jinane Bazzi"
                    },
                    {
                        "name": "Khaled N. Salama"
                    },
                    {
                        "name": "Fadi Kurdahi"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "arxiv_comment": "46 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v2",
                "updated": "2025-10-18T11:29:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    11,
                    29,
                    52,
                    5,
                    291,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v2",
                "updated": "2025-10-18T06:04:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    6,
                    4,
                    53,
                    5,
                    291,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v3",
                "updated": "2025-10-18T02:48:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    2,
                    48,
                    35,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16292v1",
                "updated": "2025-10-18T01:31:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T01:31:14Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Haiyu Wang"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "Accepted as Spotlight paper by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16276v1",
                "updated": "2025-10-18T00:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T00:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "title": "What Limits Agentic Systems Efficiency?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Limits Agentic Systems Efficiency?"
                },
                "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance."
                },
                "authors": [
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Anand Jayarajan"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "27 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15590v1",
                "updated": "2025-10-17T12:38:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T12:38:25Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "title": "A single optically detectable tumbling spin in silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single optically detectable tumbling spin in silicon"
                },
                "summary": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial."
                },
                "authors": [
                    {
                        "name": "Flix Cache"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Frdric Mazen"
                    },
                    {
                        "name": "Frdric Milsi"
                    },
                    {
                        "name": "Sbastien Kerdils"
                    },
                    {
                        "name": "Isabelle Robert-Philip"
                    },
                    {
                        "name": "Jean-Michel Grard"
                    },
                    {
                        "name": "Guillaume Cassabois"
                    },
                    {
                        "name": "Vincent Jacques"
                    },
                    {
                        "name": "Anas Drau"
                    }
                ],
                "author_detail": {
                    "name": "Anas Drau"
                },
                "author": "Anas Drau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15413v1",
                "updated": "2025-10-17T08:07:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T08:07:27Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "title": "FHE-SQL: Fully Homomorphic Encrypted SQL Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL: Fully Homomorphic Encrypted SQL Database"
                },
                "summary": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework."
                },
                "authors": [
                    {
                        "name": "Po-Yu Tseng"
                    },
                    {
                        "name": "Po-Chu Hsu"
                    },
                    {
                        "name": "Shih-Wei Liao"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Wei Liao"
                },
                "author": "Shih-Wei Liao",
                "arxiv_comment": "12 pages, 1 figures, Keywords: Fully Homomorphic Encryption, Private\n  Information Retrieval, Encrypted Databases, Privacy-Preserving Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v4",
                "updated": "2025-10-17T06:54:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    54,
                    10,
                    4,
                    290,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "NeurIPS 2025 Camera-ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v2",
                "updated": "2025-10-17T06:45:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    45,
                    17,
                    4,
                    290,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Pre-trained Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Pre-trained Large Language Models"
                },
                "summary": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "A version accepted by TACL, prior to its publication by MIT Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15152v1",
                "updated": "2025-10-16T21:22:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T21:22:16Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "title": "Tail-Optimized Caching for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tail-Optimized Caching for LLM Inference"
                },
                "summary": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments."
                },
                "authors": [
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Yueying Li"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15095v1",
                "updated": "2025-10-16T19:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T19:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "title": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs"
                },
                "summary": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing."
                },
                "authors": [
                    {
                        "name": "Md Sabbir Hossain Polak"
                    },
                    {
                        "name": "David Troendle"
                    },
                    {
                        "name": "Byunghyun Jang"
                    }
                ],
                "author_detail": {
                    "name": "Byunghyun Jang"
                },
                "author": "Byunghyun Jang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14973v1",
                "updated": "2025-10-16T17:59:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "https://vila-lab.github.io/elastic-cache-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14891v1",
                "updated": "2025-10-16T17:10:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:10:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "A Performance Portable Matrix Free Dense MTTKRP in GenTen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Performance Portable Matrix Free Dense MTTKRP in GenTen"
                },
                "summary": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter."
                },
                "authors": [
                    {
                        "name": "Gabriel Kosmacher"
                    },
                    {
                        "name": "Eric T. Phipps"
                    },
                    {
                        "name": "Sivasankaran Rajamanickam"
                    }
                ],
                "author_detail": {
                    "name": "Sivasankaran Rajamanickam"
                },
                "author": "Sivasankaran Rajamanickam",
                "arxiv_comment": "10 pages, 5 figures, 4 tables, for implementation see\n  https://github.com/sandialabs/GenTen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14686v1",
                "updated": "2025-10-16T13:53:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:53:47Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "title": "xLLM Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLLM Technical Report"
                },
                "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service."
                },
                "authors": [
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Xiaoyang Zhao"
                    },
                    {
                        "name": "Xiusheng Lu"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Jun Xiong"
                    },
                    {
                        "name": "Donghe Jin"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Jinrong Guo"
                    },
                    {
                        "name": "Yingxu Deng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xianzhe Dong"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zihan Tang"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Jinguang Liu"
                    },
                    {
                        "name": "Meng Kang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Yunlong Wang"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yichen Zhang"
                    },
                    {
                        "name": "Jinrun Yin"
                    },
                    {
                        "name": "Keyang Zheng"
                    },
                    {
                        "name": "Jiawei Yin"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Xiaobo Lin"
                    },
                    {
                        "name": "Liangyu Liu"
                    },
                    {
                        "name": "Liwei Lan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chunhua Peng"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Songcheng Ren"
                    },
                    {
                        "name": "Xuezhu Wang"
                    },
                    {
                        "name": "Yunheng Shen"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Ke Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhang"
                },
                "author": "Ke Zhang",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v5",
                "updated": "2025-10-16T13:25:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    25,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14622v1",
                "updated": "2025-10-16T12:32:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:32:51Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "title": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems"
                },
                "summary": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Hyein Woo"
                    },
                    {
                        "name": "Junhee Kim"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Kyungkuk Nam"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Hanyeoreum Bae"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14531v1",
                "updated": "2025-10-16T10:21:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T10:21:35Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "title": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation"
                },
                "summary": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona."
                },
                "authors": [
                    {
                        "name": "Sebastian Onder"
                    },
                    {
                        "name": "Philipp Gaggl"
                    },
                    {
                        "name": "Jrgen Burin"
                    },
                    {
                        "name": "Andreas Gsponer"
                    },
                    {
                        "name": "Matthias Knopf"
                    },
                    {
                        "name": "Simon Waid"
                    },
                    {
                        "name": "Neil Moffat"
                    },
                    {
                        "name": "Giulio Pellegrini"
                    },
                    {
                        "name": "Thomas Bergauer"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bergauer"
                },
                "author": "Thomas Bergauer",
                "arxiv_doi": "10.1016/j.nima.2025.170740",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nima.2025.170740",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.14531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16040v1",
                "updated": "2025-10-16T07:12:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T07:12:08Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing"
                },
                "summary": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions."
                },
                "authors": [
                    {
                        "name": "Tianhua Xia"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14126v1",
                "updated": "2025-10-15T21:49:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T21:49:38Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "title": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving"
                },
                "summary": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\""
                },
                "authors": [
                    {
                        "name": "Nikos Pagonas"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Kostis Kaffes"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Krishnamurthy"
                },
                "author": "Arvind Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13940v1",
                "updated": "2025-10-15T17:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention"
                },
                "summary": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ganggui Ding"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen",
                "arxiv_comment": "Code: https://github.com/EnVision-Research/MTI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13797v1",
                "updated": "2025-10-15T17:57:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons"
                },
                "summary": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques."
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kiant Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v3",
                "updated": "2025-10-15T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    3,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: More for Keys, Less for Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: More for Keys, Less for Values"
                },
                "summary": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13602v1",
                "updated": "2025-10-15T14:33:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "NOSA: Native and Offloadable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOSA: Native and Offloadable Sparse Attention"
                },
                "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2)."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13401v1",
                "updated": "2025-10-15T10:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs"
                },
                "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second)."
                },
                "authors": [
                    {
                        "name": "Jude Haris"
                    },
                    {
                        "name": "Jos Cano"
                    }
                ],
                "author_detail": {
                    "name": "Jos Cano"
                },
                "author": "Jos Cano",
                "arxiv_comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13334v1",
                "updated": "2025-10-15T09:18:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T09:18:58Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "title": "Taming the Fragility of KV Cache Eviction in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming the Fragility of KV Cache Eviction in LLM Inference"
                },
                "summary": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Haoyu Guo"
                    },
                    {
                        "name": "JunLin Lv"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    },
                    {
                        "name": "Xike Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xike Xie"
                },
                "author": "Xike Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13279v1",
                "updated": "2025-10-15T08:25:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T08:25:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time"
                },
                "summary": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets."
                },
                "authors": [
                    {
                        "name": "Fuma Omori"
                    },
                    {
                        "name": "Atsushi Yano"
                    },
                    {
                        "name": "Takuya Azumi"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Azumi"
                },
                "author": "Takuya Azumi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13223v1",
                "updated": "2025-10-15T07:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T07:20:14Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "title": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction."
                },
                "authors": [
                    {
                        "name": "Yiyuan He"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Jingfeng Wu"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Chong Ma"
                    },
                    {
                        "name": "Min Shen"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13084v1",
                "updated": "2025-10-15T01:55:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T01:55:32Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation"
                },
                "summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Yi Zuo"
                    },
                    {
                        "name": "Zitao Wang"
                    },
                    {
                        "name": "Lingling Li"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Licheng Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Licheng Jiao"
                },
                "author": "Licheng Jiao",
                "arxiv_comment": "32 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v3",
                "updated": "2025-10-15T01:55:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12889v1",
                "updated": "2025-10-14T18:04:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:04:00Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "title": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters"
                },
                "summary": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads."
                },
                "authors": [
                    {
                        "name": "Wei Da"
                    },
                    {
                        "name": "Evangelia Kalyvianaki"
                    }
                ],
                "author_detail": {
                    "name": "Evangelia Kalyvianaki"
                },
                "author": "Evangelia Kalyvianaki",
                "arxiv_comment": "single column,20 pages and 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v1",
                "updated": "2025-10-14T18:00:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/HankYe/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12705v1",
                "updated": "2025-10-14T16:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:39:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices"
                },
                "summary": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU."
                },
                "authors": [
                    {
                        "name": "Evelyne Ringoot"
                    },
                    {
                        "name": "Rabab Alomairy"
                    },
                    {
                        "name": "Alan Edelman"
                    }
                ],
                "author_detail": {
                    "name": "Alan Edelman"
                },
                "author": "Alan Edelman",
                "arxiv_comment": "13 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v3",
                "updated": "2025-10-14T16:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v5",
                "updated": "2025-10-14T15:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    42,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12642v1",
                "updated": "2025-10-14T15:34:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:34:35Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis"
                },
                "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system."
                },
                "authors": [
                    {
                        "name": "Meihui Zhang"
                    },
                    {
                        "name": "Liming Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Zhaojing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojing Luo"
                },
                "author": "Zhaojing Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12629v1",
                "updated": "2025-10-14T15:26:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:26:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds"
                },
                "summary": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications."
                },
                "authors": [
                    {
                        "name": "Gunwoo Kim"
                    },
                    {
                        "name": "Taejune Park"
                    },
                    {
                        "name": "Jinwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Kim"
                },
                "author": "Jinwoo Kim",
                "arxiv_comment": "20 pages, 14 figures, presented at the 4th International Workshop on\n  System Security Assurance (SecAssure 2025), co-located with ESORICS 2025, to\n  appear in Springer LNCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12280v1",
                "updated": "2025-10-14T08:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T08:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores"
                },
                "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM."
                },
                "authors": [
                    {
                        "name": "Yosuke Bando"
                    },
                    {
                        "name": "Akinobu Mita"
                    },
                    {
                        "name": "Kazuhiro Hiwada"
                    },
                    {
                        "name": "Shintaro Sano"
                    },
                    {
                        "name": "Tomoya Suzuki"
                    },
                    {
                        "name": "Yu Nakanishi"
                    },
                    {
                        "name": "Kazutaka Tomida"
                    },
                    {
                        "name": "Hirotsugu Kajihara"
                    },
                    {
                        "name": "Akiyuki Kaneko"
                    },
                    {
                        "name": "Daisuke Taki"
                    },
                    {
                        "name": "Yukimasa Miyamoto"
                    },
                    {
                        "name": "Tomokazu Yoshida"
                    },
                    {
                        "name": "Tatsuo Shiozawa"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuo Shiozawa"
                },
                "author": "Tatsuo Shiozawa",
                "arxiv_doi": "10.1145/3769759",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769759",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.12280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Manag. Data 3, 6 (SIGMOD), Article 294 (December 2025),\n  28 pages",
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10975v2",
                "updated": "2025-10-14T07:41:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    41,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T03:26:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    26,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model"
                },
                "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."
                },
                "authors": [
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Lingbo Liu"
                    },
                    {
                        "name": "Yongjie Bai"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhouxia Wang"
                    },
                    {
                        "name": "Rui SU"
                    },
                    {
                        "name": "Chunjie Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Xinyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Wu"
                },
                "author": "Xinyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11496v2",
                "updated": "2025-10-14T05:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    5,
                    5,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model"
                },
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer."
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu",
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12051v1",
                "updated": "2025-10-14T01:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T01:26:36Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APCE: Adaptive Progressive Context Expansion for Long Context Processing"
                },
                "summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks."
                },
                "authors": [
                    {
                        "name": "Baisub Lee"
                    },
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Mohanad Odema"
                    },
                    {
                        "name": "Jung Guack"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "arxiv_comment": "NeurIPS 2025 Workshop: ML For Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v2",
                "updated": "2025-10-13T22:41:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    22,
                    41,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11938v1",
                "updated": "2025-10-13T21:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T21:01:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters"
                },
                "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity."
                },
                "authors": [
                    {
                        "name": "Yanying Lin"
                    },
                    {
                        "name": "Shijie Peng"
                    },
                    {
                        "name": "Chengzhi Lu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_doi": "10.1145/3767295.3769316",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767295.3769316",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EuroSys 26",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v3",
                "updated": "2025-10-13T17:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    15,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v2",
                "updated": "2025-10-13T16:48:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    48,
                    37,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "arxiv_comment": "Corrected typo in arxiv abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11292v1",
                "updated": "2025-10-13T11:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences"
                },
                "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v2",
                "updated": "2025-10-13T11:21:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    21,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_doi": "10.1088/1367-2630/ae0ea7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1367-2630/ae0ea7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Added a figure, minor changes to text",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.21706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21706v1",
                "updated": "2025-10-24T17:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    59,
                    46,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:59:46Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    59,
                    46,
                    4,
                    297,
                    0
                ],
                "title": "Equivariance by Contrast: Identifiable Equivariant Embeddings from\n  Unlabeled Finite Group Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equivariance by Contrast: Identifiable Equivariant Embeddings from\n  Unlabeled Finite Group Actions"
                },
                "summary": "We propose Equivariance by Contrast (EbC) to learn equivariant embeddings\nfrom observation pairs $(\\mathbf{y}, g \\cdot \\mathbf{y})$, where $g$ is drawn\nfrom a finite group acting on the data. Our method jointly learns a latent\nspace and a group representation in which group actions correspond to\ninvertible linear maps -- without relying on group-specific inductive biases.\nWe validate our approach on the infinite dSprites dataset with structured\ntransformations defined by the finite group $G:= (R_m \\times \\mathbb{Z}_n\n\\times \\mathbb{Z}_n)$, combining discrete rotations and periodic translations.\nThe resulting embeddings exhibit high-fidelity equivariance, with group\noperations faithfully reproduced in latent space. On synthetic data, we further\nvalidate the approach on the non-abelian orthogonal group $O(n)$ and the\ngeneral linear group $GL(n)$. We also provide a theoretical proof for\nidentifiability. While broad evaluation across diverse group types on\nreal-world data remains future work, our results constitute the first\nsuccessful demonstration of general-purpose encoder-only equivariant learning\nfrom group action observations alone, including non-trivial non-abelian groups\nand a product group motivated by modeling affine equivariances in computer\nvision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Equivariance by Contrast (EbC) to learn equivariant embeddings\nfrom observation pairs $(\\mathbf{y}, g \\cdot \\mathbf{y})$, where $g$ is drawn\nfrom a finite group acting on the data. Our method jointly learns a latent\nspace and a group representation in which group actions correspond to\ninvertible linear maps -- without relying on group-specific inductive biases.\nWe validate our approach on the infinite dSprites dataset with structured\ntransformations defined by the finite group $G:= (R_m \\times \\mathbb{Z}_n\n\\times \\mathbb{Z}_n)$, combining discrete rotations and periodic translations.\nThe resulting embeddings exhibit high-fidelity equivariance, with group\noperations faithfully reproduced in latent space. On synthetic data, we further\nvalidate the approach on the non-abelian orthogonal group $O(n)$ and the\ngeneral linear group $GL(n)$. We also provide a theoretical proof for\nidentifiability. While broad evaluation across diverse group types on\nreal-world data remains future work, our results constitute the first\nsuccessful demonstration of general-purpose encoder-only equivariant learning\nfrom group action observations alone, including non-trivial non-abelian groups\nand a product group motivated by modeling affine equivariances in computer\nvision."
                },
                "authors": [
                    {
                        "name": "Tobias Schmidt"
                    },
                    {
                        "name": "Steffen Schneider"
                    },
                    {
                        "name": "Matthias Bethge"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Bethge"
                },
                "author": "Matthias Bethge",
                "arxiv_comment": "Accepted at NeurIPS 2025. The last two authors contributed equally.\n  Code is available at https://github.com/dynamical-inference/ebc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21703v1",
                "updated": "2025-10-24T17:59:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    59,
                    1,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:59:01Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    59,
                    1,
                    4,
                    297,
                    0
                ],
                "title": "An H Transit of HD 189733b to Assess Stellar Activity Across the\n  Transit Chord Close to JWST Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An H Transit of HD 189733b to Assess Stellar Activity Across the\n  Transit Chord Close to JWST Observations"
                },
                "summary": "Transmission spectroscopy allows us to detect molecules in planetary\natmospheres, but is subject to contamination from inhomogeneities on the\nstellar surface. Quantifying the extent of this contamination is essential for\naccurate measurements of atmospheric composition, as stellar activity can\nmanifest as false atmospheric signals in planetary transmission spectra. We\npresent a study of hot Jupiter HD 189733b, which has over 50 hours of JWST\nobservations scheduled or taken, to measure the activity level of the host star\nat the current epoch. We utilize high-resolution spectra of the H${\\alpha}$\nline from the MEGARA spectrograph on the 10-m GTC to examine the activity level\nof HD 189733 during a transit. We measure H${\\alpha}$ becoming shallower\nmid-transit by an H${\\alpha}$ index of ${\\delta}$ = 0.00156 ${\\pm}$ 0.00026,\nwhich suggests that HD 189733b crosses an active region as it transits. We\nposit this deviation is likely caused by a spot along the transit chord with an\napproximate radius of $R_{spot}$ = 3.47 ${\\pm}$ 0.30R${\\oplus}$ becoming\nocculted during transit. Including an approximation for unocculted spots, we\nestimate that this spot could result in transit depth variations of ${\\sim}$17\nppm at the 4.3 micron CO2 feature. Since this is comparable to JWST NIRCam\nGrism mode's noise floor of ${\\sim}$20 ppm, it could bias atmospheric studies\nby altering the inferred depths of the planet's features. Thus, we suggest\nground-based high-resolution monitoring of activity indicator species\nconcurrently taken with JWST data when feasible to disentangle stellar activity\nsignals from planetary atmospheric signals during transit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transmission spectroscopy allows us to detect molecules in planetary\natmospheres, but is subject to contamination from inhomogeneities on the\nstellar surface. Quantifying the extent of this contamination is essential for\naccurate measurements of atmospheric composition, as stellar activity can\nmanifest as false atmospheric signals in planetary transmission spectra. We\npresent a study of hot Jupiter HD 189733b, which has over 50 hours of JWST\nobservations scheduled or taken, to measure the activity level of the host star\nat the current epoch. We utilize high-resolution spectra of the H${\\alpha}$\nline from the MEGARA spectrograph on the 10-m GTC to examine the activity level\nof HD 189733 during a transit. We measure H${\\alpha}$ becoming shallower\nmid-transit by an H${\\alpha}$ index of ${\\delta}$ = 0.00156 ${\\pm}$ 0.00026,\nwhich suggests that HD 189733b crosses an active region as it transits. We\nposit this deviation is likely caused by a spot along the transit chord with an\napproximate radius of $R_{spot}$ = 3.47 ${\\pm}$ 0.30R${\\oplus}$ becoming\nocculted during transit. Including an approximation for unocculted spots, we\nestimate that this spot could result in transit depth variations of ${\\sim}$17\nppm at the 4.3 micron CO2 feature. Since this is comparable to JWST NIRCam\nGrism mode's noise floor of ${\\sim}$20 ppm, it could bias atmospheric studies\nby altering the inferred depths of the planet's features. Thus, we suggest\nground-based high-resolution monitoring of activity indicator species\nconcurrently taken with JWST data when feasible to disentangle stellar activity\nsignals from planetary atmospheric signals during transit."
                },
                "authors": [
                    {
                        "name": "Kingsley E. Ehrich"
                    },
                    {
                        "name": "Jason A. Dittmann"
                    },
                    {
                        "name": "Samuel P. Halverson"
                    },
                    {
                        "name": "Alejandro Camazn-Pinilla"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Camazn-Pinilla"
                },
                "author": "Alejandro Camazn-Pinilla",
                "arxiv_doi": "10.3847/1538-3881/ae169b",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-3881/ae169b",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.21703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures, accepted for publication in AJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13974v2",
                "updated": "2025-10-24T17:47:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    47,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-15T18:02:17Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    18,
                    2,
                    17,
                    2,
                    288,
                    0
                ],
                "title": "Predictions of the Nancy Grace Roman Space Telescope Galactic Exoplanet\n  Survey. IV. Lens Mass and Distance Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictions of the Nancy Grace Roman Space Telescope Galactic Exoplanet\n  Survey. IV. Lens Mass and Distance Measurements"
                },
                "summary": "As part of the Galactic Bulge Time Domain Survey (GBTDS), the Nancy Grace\nRoman Galactic Exoplanet Survey (RGES) will use microlensing to discover cold\nouter planets and free-floating planets unbound to stars. NASA has established\nseveral science requirements for the GBTDS to ensure RGES success. A key\nadvantage of RGES is Roman's high angular resolution, which will allow\ndetection of flux from many host stars. One requirement specifies that Roman\nmust measure the masses and distances of 40% of detected planet hosts with 20%\nprecision or better. To test this, we simulated microlensing events toward the\nGBTDS fields and used Fisher matrix analysis to estimate light curve parameter\nuncertainties. Combining these with Roman imaging observables (lens flux,\nrelative lens-source proper motion), we estimated the achievable precision of\nlens mass and distance measurements. Using pyLIMASS, a publicly available code\nfor estimating lens properties, we applied this analysis to 3,000 simulated\nevents. Assuming the Cassan et al. (2012) exoplanet mass function, we find that\n>40% of host stars meet the required 20% precision threshold, confirming that\nthe GBTDS can satisfy the mission requirement. We validated our approach by\ncomparing our inferred lens masses and distances to empirical measurements from\ndetailed image-constrained light curve modeling of historical microlensing\nevents with Hubble and Keck follow-up imaging. Our results agree within roughly\n1 sigma, demonstrating that both approaches yield consistent and reliable mass\nand distance estimates, and confirming the robustness of our simulations for\nRoman-era microlensing science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As part of the Galactic Bulge Time Domain Survey (GBTDS), the Nancy Grace\nRoman Galactic Exoplanet Survey (RGES) will use microlensing to discover cold\nouter planets and free-floating planets unbound to stars. NASA has established\nseveral science requirements for the GBTDS to ensure RGES success. A key\nadvantage of RGES is Roman's high angular resolution, which will allow\ndetection of flux from many host stars. One requirement specifies that Roman\nmust measure the masses and distances of 40% of detected planet hosts with 20%\nprecision or better. To test this, we simulated microlensing events toward the\nGBTDS fields and used Fisher matrix analysis to estimate light curve parameter\nuncertainties. Combining these with Roman imaging observables (lens flux,\nrelative lens-source proper motion), we estimated the achievable precision of\nlens mass and distance measurements. Using pyLIMASS, a publicly available code\nfor estimating lens properties, we applied this analysis to 3,000 simulated\nevents. Assuming the Cassan et al. (2012) exoplanet mass function, we find that\n>40% of host stars meet the required 20% precision threshold, confirming that\nthe GBTDS can satisfy the mission requirement. We validated our approach by\ncomparing our inferred lens masses and distances to empirical measurements from\ndetailed image-constrained light curve modeling of historical microlensing\nevents with Hubble and Keck follow-up imaging. Our results agree within roughly\n1 sigma, demonstrating that both approaches yield consistent and reliable mass\nand distance estimates, and confirming the robustness of our simulations for\nRoman-era microlensing science."
                },
                "authors": [
                    {
                        "name": "Sean K. Terry"
                    },
                    {
                        "name": "Etienne Bachelet"
                    },
                    {
                        "name": "Farzaneh Zohrabi"
                    },
                    {
                        "name": "Himanshu Verma"
                    },
                    {
                        "name": "Alison Crisp"
                    },
                    {
                        "name": "Macy Huston"
                    },
                    {
                        "name": "Carissma McGee"
                    },
                    {
                        "name": "Matthew Penny"
                    },
                    {
                        "name": "Natasha S. Abrams"
                    },
                    {
                        "name": "Michael D. Albrow"
                    },
                    {
                        "name": "Jay Anderson"
                    },
                    {
                        "name": "Fatemeh Bagheri"
                    },
                    {
                        "name": "Jean-Phillipe Beaulieu"
                    },
                    {
                        "name": "Andrea Bellini"
                    },
                    {
                        "name": "David P. Bennett"
                    },
                    {
                        "name": "Galen Bergsten"
                    },
                    {
                        "name": "T. Dex Bhadra"
                    },
                    {
                        "name": "Aparna Bhattacharya"
                    },
                    {
                        "name": "Ian A. Bond"
                    },
                    {
                        "name": "Valerio Bozza"
                    },
                    {
                        "name": "Christopher Brandon"
                    },
                    {
                        "name": "Sebastiano Calchi Novati"
                    },
                    {
                        "name": "Sean Carey"
                    },
                    {
                        "name": "Jessie Christiansen"
                    },
                    {
                        "name": "William DeRocco"
                    },
                    {
                        "name": "B. Scott Gaudi"
                    },
                    {
                        "name": "Jon Hulberg"
                    },
                    {
                        "name": "Stela Ishitani Silva"
                    },
                    {
                        "name": "Sinclaire E. Jones"
                    },
                    {
                        "name": "Eamonn Kerins"
                    },
                    {
                        "name": "Somayeh Khakpash"
                    },
                    {
                        "name": "Katarzyna Kruszynska"
                    },
                    {
                        "name": "Casey Lam"
                    },
                    {
                        "name": "Jessica R. Lu"
                    },
                    {
                        "name": "Amber Malpas"
                    },
                    {
                        "name": "Shota Miyazaki"
                    },
                    {
                        "name": "Przemek Mroz"
                    },
                    {
                        "name": "Arjun Murlidhar"
                    },
                    {
                        "name": "David Nataf"
                    },
                    {
                        "name": "Marz Newman"
                    },
                    {
                        "name": "Greg Olmschenk"
                    },
                    {
                        "name": "Rakek Poleski"
                    },
                    {
                        "name": "Clement Ranc"
                    },
                    {
                        "name": "Nicholas J. Rattenbury"
                    },
                    {
                        "name": "Krzysztof Rybicki"
                    },
                    {
                        "name": "Vito Saggese"
                    },
                    {
                        "name": "Jennifer Sobeck"
                    },
                    {
                        "name": "Keivan G. Stassun"
                    },
                    {
                        "name": "Alexander P. Stephan"
                    },
                    {
                        "name": "Rachel A. Street"
                    },
                    {
                        "name": "Takahiro Sumi"
                    },
                    {
                        "name": "Daisuke Suzuki"
                    },
                    {
                        "name": "Aikaterini Vandorou"
                    },
                    {
                        "name": "Meet Vyas"
                    },
                    {
                        "name": "Jennifer C. Yee"
                    },
                    {
                        "name": "Weicheng Zang"
                    },
                    {
                        "name": "Keming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Keming Zhang"
                },
                "author": "Keming Zhang",
                "arxiv_comment": "Updated. 29 pages, 8 figures, 6 tables, submitted to AJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23773v2",
                "updated": "2025-10-24T17:44:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    44,
                    52,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-31T17:57:20Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    57,
                    20,
                    3,
                    212,
                    0
                ],
                "title": "SimuRA: A World-Model-Driven Simulative Reasoning Architecture for\n  General Goal-Oriented Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimuRA: A World-Model-Driven Simulative Reasoning Architecture for\n  General Goal-Oriented Agents"
                },
                "summary": "AI agents built on foundation models hold enormous promise. Current practice,\nhowever, focuses on a one-task-one-agent approach, which not only falls short\nof scalability and generality, but also faces practical limitations from\nblack-box autoregressive reasoning, where decisions unfold token by token\nwithout explicit simulation or counterfactual evaluation of outcomes. Humans,\non the other hand, reason and plan by mentally simulating the consequences of\nactions within an internal model of the world -- a capability that supports\nflexible, goal-directed behavior across diverse contexts. Moving towards a more\ngeneral and powerful AI agent, we introduce SimuRA, a goal-oriented\narchitecture for generalized agentic reasoning. Based on a principled\nformulation of an optimal agent in any general environment, SimuRA addresses\nthe limitations of black-box autoregressive reasoning by incorporating the\nworld model for planning via simulation. Our prototype world model is\nimplemented using LLMs as a substrate, leveraging the natural language as a\ndiscrete, hierarchical representation grounded in concepts for planning, while\nremaining model-agnostic. On complex web-browsing tasks such as flight search,\nSimuRA improves the success rate from 0% to 32.2% compared to a representative\nopen-web agent baseline. Across tasks, world-model-based planning achieves up\nto 124% higher task completion rates than a matched black-box autoregressive\nbaseline, demonstrating the advantages of simulative reasoning. We release\nReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source\nresearch demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents built on foundation models hold enormous promise. Current practice,\nhowever, focuses on a one-task-one-agent approach, which not only falls short\nof scalability and generality, but also faces practical limitations from\nblack-box autoregressive reasoning, where decisions unfold token by token\nwithout explicit simulation or counterfactual evaluation of outcomes. Humans,\non the other hand, reason and plan by mentally simulating the consequences of\nactions within an internal model of the world -- a capability that supports\nflexible, goal-directed behavior across diverse contexts. Moving towards a more\ngeneral and powerful AI agent, we introduce SimuRA, a goal-oriented\narchitecture for generalized agentic reasoning. Based on a principled\nformulation of an optimal agent in any general environment, SimuRA addresses\nthe limitations of black-box autoregressive reasoning by incorporating the\nworld model for planning via simulation. Our prototype world model is\nimplemented using LLMs as a substrate, leveraging the natural language as a\ndiscrete, hierarchical representation grounded in concepts for planning, while\nremaining model-agnostic. On complex web-browsing tasks such as flight search,\nSimuRA improves the success rate from 0% to 32.2% compared to a representative\nopen-web agent baseline. Across tasks, world-model-based planning achieves up\nto 124% higher task completion rates than a matched black-box autoregressive\nbaseline, demonstrating the advantages of simulative reasoning. We release\nReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source\nresearch demo."
                },
                "authors": [
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Jinyu Hou"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "arxiv_comment": "This submission has been updated to adjust the scope and presentation\n  of the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21684v1",
                "updated": "2025-10-24T17:40:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    40,
                    12,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:40:12Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    40,
                    12,
                    4,
                    297,
                    0
                ],
                "title": "Toward provably private analytics and insights into GenAI use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward provably private analytics and insights into GenAI use"
                },
                "summary": "Large-scale systems that compute analytics over a fleet of devices must\nachieve high privacy and security standards while also meeting data quality,\nusability, and resource efficiency expectations. We present a next-generation\nfederated analytics system that uses Trusted Execution Environments (TEEs)\nbased on technologies like AMD SEV-SNP and Intel TDX to provide verifiable\nprivacy guarantees for all server-side processing. In our system, devices\nencrypt and upload data, tagging it with a limited set of allowable server-side\nprocessing steps. An open source, TEE-hosted key management service guarantees\nthat the data is accessible only to those steps, which are themselves protected\nby TEE confidentiality and integrity assurance guarantees. The system is\ndesigned for flexible workloads, including processing unstructured data with\nLLMs (for structured summarization) before aggregation into differentially\nprivate insights (with automatic parameter tuning). The transparency properties\nof our system allow any external party to verify that all raw and derived data\nis processed in TEEs, protecting it from inspection by the system operator, and\nthat differential privacy is applied to all released results. This system has\nbeen successfully deployed in production, providing helpful insights into\nreal-world GenAI experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale systems that compute analytics over a fleet of devices must\nachieve high privacy and security standards while also meeting data quality,\nusability, and resource efficiency expectations. We present a next-generation\nfederated analytics system that uses Trusted Execution Environments (TEEs)\nbased on technologies like AMD SEV-SNP and Intel TDX to provide verifiable\nprivacy guarantees for all server-side processing. In our system, devices\nencrypt and upload data, tagging it with a limited set of allowable server-side\nprocessing steps. An open source, TEE-hosted key management service guarantees\nthat the data is accessible only to those steps, which are themselves protected\nby TEE confidentiality and integrity assurance guarantees. The system is\ndesigned for flexible workloads, including processing unstructured data with\nLLMs (for structured summarization) before aggregation into differentially\nprivate insights (with automatic parameter tuning). The transparency properties\nof our system allow any external party to verify that all raw and derived data\nis processed in TEEs, protecting it from inspection by the system operator, and\nthat differential privacy is applied to all released results. This system has\nbeen successfully deployed in production, providing helpful insights into\nreal-world GenAI experiences."
                },
                "authors": [
                    {
                        "name": "Albert Cheu"
                    },
                    {
                        "name": "Artem Lagzdin"
                    },
                    {
                        "name": "Brett McLarnon"
                    },
                    {
                        "name": "Daniel Ramage"
                    },
                    {
                        "name": "Katharine Daly"
                    },
                    {
                        "name": "Marco Gruteser"
                    },
                    {
                        "name": "Peter Kairouz"
                    },
                    {
                        "name": "Rakshita Tandon"
                    },
                    {
                        "name": "Stanislav Chiknavaryan"
                    },
                    {
                        "name": "Timon Van Overveldt"
                    },
                    {
                        "name": "Zoe Gong"
                    }
                ],
                "author_detail": {
                    "name": "Zoe Gong"
                },
                "author": "Zoe Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23801v2",
                "updated": "2025-10-24T17:39:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    39,
                    1,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-28T10:55:23Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    10,
                    55,
                    23,
                    6,
                    271,
                    0
                ],
                "title": "High-Precision Climbing Robot Localization Using Planar Array\n  UWB/GPS/IMU/Barometer Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Precision Climbing Robot Localization Using Planar Array\n  UWB/GPS/IMU/Barometer Integration"
                },
                "summary": "To address the need for high-precision localization of climbing robots in\ncomplex high-altitude environments, this paper proposes a multi-sensor fusion\nsystem that overcomes the limitations of single-sensor approaches. Firstly, the\nlocalization scenarios and the problem model are analyzed. An integrated\narchitecture of Attention Mechanism-based Fusion Algorithm (AMFA) incorporating\nplanar array Ultra-Wideband (UWB), GPS, Inertial Measurement Unit (IMU), and\nbarometer is designed to handle challenges such as GPS occlusion and UWB\nNon-Line-of-Sight (NLOS) problem. Then, End-to-end neural network inference\nmodels for UWB and barometer are developed, along with a multimodal attention\nmechanism for adaptive data fusion. An Unscented Kalman Filter (UKF) is applied\nto refine the trajectory, improving accuracy and robustness. Finally,\nreal-world experiments show that the method achieves 0.48 m localization\naccuracy and lower MAX error of 1.50 m, outperforming baseline algorithms such\nas GPS/INS-EKF and demonstrating stronger robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the need for high-precision localization of climbing robots in\ncomplex high-altitude environments, this paper proposes a multi-sensor fusion\nsystem that overcomes the limitations of single-sensor approaches. Firstly, the\nlocalization scenarios and the problem model are analyzed. An integrated\narchitecture of Attention Mechanism-based Fusion Algorithm (AMFA) incorporating\nplanar array Ultra-Wideband (UWB), GPS, Inertial Measurement Unit (IMU), and\nbarometer is designed to handle challenges such as GPS occlusion and UWB\nNon-Line-of-Sight (NLOS) problem. Then, End-to-end neural network inference\nmodels for UWB and barometer are developed, along with a multimodal attention\nmechanism for adaptive data fusion. An Unscented Kalman Filter (UKF) is applied\nto refine the trajectory, improving accuracy and robustness. Finally,\nreal-world experiments show that the method achieves 0.48 m localization\naccuracy and lower MAX error of 1.50 m, outperforming baseline algorithms such\nas GPS/INS-EKF and demonstrating stronger robustness."
                },
                "authors": [
                    {
                        "name": "Shuning Zhang"
                    },
                    {
                        "name": "Zhanchen Zhu"
                    },
                    {
                        "name": "Xiangyu Chen"
                    },
                    {
                        "name": "Yunheng Wang"
                    },
                    {
                        "name": "Xu Jiang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13678v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13678v4",
                "updated": "2025-10-24T17:36:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    36,
                    52,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-16T16:32:51Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    32,
                    51,
                    0,
                    167,
                    0
                ],
                "title": "A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction"
                },
                "summary": "Human activity intensity prediction is crucial to many location-based\nservices. Despite tremendous progress in modeling dynamics of human activity,\nmost existing methods overlook physical constraints of spatial interaction,\nleading to uninterpretable spatial correlations and over-smoothing phenomenon.\nTo address these limitations, this work proposes a physics-informed deep\nlearning framework, namely Gravity-informed Spatiotemporal Transformer\n(Gravityformer) by integrating the universal law of gravitation to refine\ntransformer attention. Specifically, it (1) estimates two spatially explicit\nmass parameters based on spatiotemporal embedding feature, (2) models the\nspatial interaction in end-to-end neural network using proposed adaptive\ngravity model to learn the physical constraint, and (3) utilizes the learned\nspatial interaction to guide and mitigate the over-smoothing phenomenon in\ntransformer attention. Moreover, a parallel spatiotemporal graph convolution\ntransformer is proposed for achieving a balance between coupled spatial and\ntemporal learning. Systematic experiments on six real-world large-scale\nactivity datasets demonstrate the quantitative and qualitative superiority of\nour model over state-of-the-art benchmarks. Additionally, the learned gravity\nattention matrix can be not only disentangled and interpreted based on\ngeographical laws, but also improved the generalization in zero-shot\ncross-region inference. This work provides a novel insight into integrating\nphysical laws with deep learning for spatiotemporal prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human activity intensity prediction is crucial to many location-based\nservices. Despite tremendous progress in modeling dynamics of human activity,\nmost existing methods overlook physical constraints of spatial interaction,\nleading to uninterpretable spatial correlations and over-smoothing phenomenon.\nTo address these limitations, this work proposes a physics-informed deep\nlearning framework, namely Gravity-informed Spatiotemporal Transformer\n(Gravityformer) by integrating the universal law of gravitation to refine\ntransformer attention. Specifically, it (1) estimates two spatially explicit\nmass parameters based on spatiotemporal embedding feature, (2) models the\nspatial interaction in end-to-end neural network using proposed adaptive\ngravity model to learn the physical constraint, and (3) utilizes the learned\nspatial interaction to guide and mitigate the over-smoothing phenomenon in\ntransformer attention. Moreover, a parallel spatiotemporal graph convolution\ntransformer is proposed for achieving a balance between coupled spatial and\ntemporal learning. Systematic experiments on six real-world large-scale\nactivity datasets demonstrate the quantitative and qualitative superiority of\nour model over state-of-the-art benchmarks. Additionally, the learned gravity\nattention matrix can be not only disentangled and interpreted based on\ngeographical laws, but also improved the generalization in zero-shot\ncross-region inference. This work provides a novel insight into integrating\nphysical laws with deep learning for spatiotemporal prediction."
                },
                "authors": [
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Zhenghong Wang"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Chaogui Kang"
                    },
                    {
                        "name": "Sijie Ruan"
                    },
                    {
                        "name": "Di Zhu"
                    },
                    {
                        "name": "Chengling Tang"
                    },
                    {
                        "name": "Zhongfu Ma"
                    },
                    {
                        "name": "Weiyu Zhang"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_doi": "10.1109/TPAMI.2025.3625859",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3625859",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.13678v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13678v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE TPAMI 2025. 18 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21671v1",
                "updated": "2025-10-24T17:27:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    27,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:27:35Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    27,
                    35,
                    4,
                    297,
                    0
                ],
                "title": "A Data-Centric Approach to Multilingual E-Commerce Product Search: Case\n  Study on Query-Category and Query-Item Relevance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-Centric Approach to Multilingual E-Commerce Product Search: Case\n  Study on Query-Category and Query-Item Relevance"
                },
                "summary": "Multilingual e-commerce search suffers from severe data imbalance across\nlanguages, label noise, and limited supervision for low-resource\nlanguages--challenges that impede the cross-lingual generalization of relevance\nmodels despite the strong capabilities of large language models (LLMs). In this\nwork, we present a practical, architecture-agnostic, data-centric framework to\nenhance performance on two core tasks: Query-Category (QC) relevance (matching\nqueries to product categories) and Query-Item (QI) relevance (matching queries\nto product titles). Rather than altering the model, we redesign the training\ndata through three complementary strategies: (1) translation-based augmentation\nto synthesize examples for languages absent in training, (2) semantic negative\nsampling to generate hard negatives and mitigate class imbalance, and (3)\nself-validation filtering to detect and remove likely mislabeled instances.\nEvaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields\nsubstantial F1 score improvements over strong LLM baselines, achieving\ncompetitive results in the official competition. Our findings demonstrate that\nsystematic data engineering can be as impactful as--and often more deployable\nthan--complex model modifications, offering actionable guidance for building\nrobust multilingual search systems in the real-world e-commerce settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual e-commerce search suffers from severe data imbalance across\nlanguages, label noise, and limited supervision for low-resource\nlanguages--challenges that impede the cross-lingual generalization of relevance\nmodels despite the strong capabilities of large language models (LLMs). In this\nwork, we present a practical, architecture-agnostic, data-centric framework to\nenhance performance on two core tasks: Query-Category (QC) relevance (matching\nqueries to product categories) and Query-Item (QI) relevance (matching queries\nto product titles). Rather than altering the model, we redesign the training\ndata through three complementary strategies: (1) translation-based augmentation\nto synthesize examples for languages absent in training, (2) semantic negative\nsampling to generate hard negatives and mitigate class imbalance, and (3)\nself-validation filtering to detect and remove likely mislabeled instances.\nEvaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields\nsubstantial F1 score improvements over strong LLM baselines, achieving\ncompetitive results in the official competition. Our findings demonstrate that\nsystematic data engineering can be as impactful as--and often more deployable\nthan--complex model modifications, offering actionable guidance for building\nrobust multilingual search systems in the real-world e-commerce settings."
                },
                "authors": [
                    {
                        "name": "Yabo Yin"
                    },
                    {
                        "name": "Yang Xi"
                    },
                    {
                        "name": "Jialong Wang"
                    },
                    {
                        "name": "Shanqi Wang"
                    },
                    {
                        "name": "Jiateng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jiateng Hu"
                },
                "author": "Jiateng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05437v2",
                "updated": "2025-10-24T17:26:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    26,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-06T23:03:29Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    23,
                    3,
                    29,
                    0,
                    279,
                    0
                ],
                "title": "Operational Risks in Grid Integration of Large Data Center Loads:\n  Characteristics, Stability Assessments, and Sensitivity Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operational Risks in Grid Integration of Large Data Center Loads:\n  Characteristics, Stability Assessments, and Sensitivity Studies"
                },
                "summary": "This paper investigates the dynamic interactions between large-scale data\ncenters and the power grid, focusing on reliability challenges arising from\nsudden fluctuations in demand. With the rapid growth of AI-driven workloads,\nsuch fluctuations, along with fast ramp patterns, are expected to exacerbate\nstressed grid conditions and system instabilities. We consider a few\nopen-source AI data center consumption profiles from the MIT supercloud\ndatasets, along with generating a few experimental HPC job-distribution-based\ninference profiles. Subsequently, we develop analytical methodologies for\nreal-time assessment of grid stability, focusing on both transient and\nsmall-signal stability assessments. Energy-flow-like metrics for nonlinear\ntransient stability, formulated by computing localized data center bus\nkinetic-like flows and coupling interactions with neighboring buses over\nvarying time windows, help provide operators with real-time assessments of the\nregional grid stress in the data center hubs. On the other hand, small-signal\nstability metrics, constructed from analytical state matrices under variable\noperating conditions during a fast ramping period, enable snapshot-based\nassessments of data center load fluctuations and provide enhanced observability\ninto evolving grid conditions. By quantifying the stability impacts of large\ndata center clusters, studies conducted in the modified IEEE benchmark $68-$bus\nmodel support improved operator situational awareness to capture risks in\nreliable integration of large data center loads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the dynamic interactions between large-scale data\ncenters and the power grid, focusing on reliability challenges arising from\nsudden fluctuations in demand. With the rapid growth of AI-driven workloads,\nsuch fluctuations, along with fast ramp patterns, are expected to exacerbate\nstressed grid conditions and system instabilities. We consider a few\nopen-source AI data center consumption profiles from the MIT supercloud\ndatasets, along with generating a few experimental HPC job-distribution-based\ninference profiles. Subsequently, we develop analytical methodologies for\nreal-time assessment of grid stability, focusing on both transient and\nsmall-signal stability assessments. Energy-flow-like metrics for nonlinear\ntransient stability, formulated by computing localized data center bus\nkinetic-like flows and coupling interactions with neighboring buses over\nvarying time windows, help provide operators with real-time assessments of the\nregional grid stress in the data center hubs. On the other hand, small-signal\nstability metrics, constructed from analytical state matrices under variable\noperating conditions during a fast ramping period, enable snapshot-based\nassessments of data center load fluctuations and provide enhanced observability\ninto evolving grid conditions. By quantifying the stability impacts of large\ndata center clusters, studies conducted in the modified IEEE benchmark $68-$bus\nmodel support improved operator situational awareness to capture risks in\nreliable integration of large data center loads."
                },
                "authors": [
                    {
                        "name": "Kyung-Bin Kwon"
                    },
                    {
                        "name": "Sayak Mukherjee"
                    },
                    {
                        "name": "Veronica Adetola"
                    }
                ],
                "author_detail": {
                    "name": "Veronica Adetola"
                },
                "author": "Veronica Adetola",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21668v1",
                "updated": "2025-10-24T17:24:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    24,
                    24,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:24:24Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    24,
                    24,
                    4,
                    297,
                    0
                ],
                "title": "Privacy Guarantee for Nash Equilibrium Computation of Aggregative Games\n  Based on Pointwise Maximal Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Guarantee for Nash Equilibrium Computation of Aggregative Games\n  Based on Pointwise Maximal Leakage"
                },
                "summary": "Privacy preservation has served as a key metric in designing Nash equilibrium\n(NE) computation algorithms. Although differential privacy (DP) has been widely\nemployed for privacy guarantees, it does not exploit prior distributional\nknowledge of datasets and is ineffective in assessing information leakage for\ncorrelated datasets. To address these concerns, we establish a pointwise\nmaximal leakage (PML) framework when computing NE in aggregative games. By\nincorporating prior knowledge of players' cost function datasets, we obtain a\nprecise and computable upper bound of privacy leakage with PML guarantees. In\nthe entire view, we show PML refines DP by offering a tighter privacy\nguarantee, enabling flexibility in designing NE computation. Also, in the\nindividual view, we reveal that the lower bound of PML can exceed the upper\nbound of DP by constructing specific correlated datasets. The results emphasize\nthat PML is a more proper privacy measure than DP since the latter fails to\nadequately capture privacy leakage in correlated datasets. Moreover, we conduct\nexperiments with adversaries who attempt to infer players' private information\nto illustrate the effectiveness of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy preservation has served as a key metric in designing Nash equilibrium\n(NE) computation algorithms. Although differential privacy (DP) has been widely\nemployed for privacy guarantees, it does not exploit prior distributional\nknowledge of datasets and is ineffective in assessing information leakage for\ncorrelated datasets. To address these concerns, we establish a pointwise\nmaximal leakage (PML) framework when computing NE in aggregative games. By\nincorporating prior knowledge of players' cost function datasets, we obtain a\nprecise and computable upper bound of privacy leakage with PML guarantees. In\nthe entire view, we show PML refines DP by offering a tighter privacy\nguarantee, enabling flexibility in designing NE computation. Also, in the\nindividual view, we reveal that the lower bound of PML can exceed the upper\nbound of DP by constructing specific correlated datasets. The results emphasize\nthat PML is a more proper privacy measure than DP since the latter fails to\nadequately capture privacy leakage in correlated datasets. Moreover, we conduct\nexperiments with adversaries who attempt to infer players' private information\nto illustrate the effectiveness of our framework."
                },
                "authors": [
                    {
                        "name": "Zhaoyang Cheng"
                    },
                    {
                        "name": "Guanpu Chen"
                    },
                    {
                        "name": "Tobias J. Oechtering"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12945v2",
                "updated": "2025-10-24T17:23:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    23,
                    51,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-15T19:12:37Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    19,
                    12,
                    37,
                    6,
                    166,
                    0
                ],
                "title": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction"
                },
                "summary": "We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS)\nthat leverages comprehensive multi-view photometric error signals within a\nunified Metropolis-Hastings approach. Vanilla 3DGS heavily relies on\nheuristic-based density-control mechanisms (e.g., cloning, splitting, and\npruning), which can lead to redundant computations or premature removal of\nbeneficial Gaussians. Our framework overcomes these limitations by\nreformulating densification and pruning as a probabilistic sampling process,\ndynamically inserting and relocating Gaussians based on aggregated multi-view\nerrors and opacity scores. Guided by Bayesian acceptance tests derived from\nthese error-based importance scores, our method substantially reduces reliance\non heuristics, offers greater flexibility, and adaptively infers Gaussian\ndistributions without requiring predefined scene complexity. Experiments on\nbenchmark datasets, including Mip-NeRF360, Tanks and Temples and Deep Blending,\nshow that our approach reduces the number of Gaussians needed, achieving faster\nconvergence while matching or modestly surpassing the view-synthesis quality of\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS)\nthat leverages comprehensive multi-view photometric error signals within a\nunified Metropolis-Hastings approach. Vanilla 3DGS heavily relies on\nheuristic-based density-control mechanisms (e.g., cloning, splitting, and\npruning), which can lead to redundant computations or premature removal of\nbeneficial Gaussians. Our framework overcomes these limitations by\nreformulating densification and pruning as a probabilistic sampling process,\ndynamically inserting and relocating Gaussians based on aggregated multi-view\nerrors and opacity scores. Guided by Bayesian acceptance tests derived from\nthese error-based importance scores, our method substantially reduces reliance\non heuristics, offers greater flexibility, and adaptively infers Gaussian\ndistributions without requiring predefined scene complexity. Experiments on\nbenchmark datasets, including Mip-NeRF360, Tanks and Temples and Deep Blending,\nshow that our approach reduces the number of Gaussians needed, achieving faster\nconvergence while matching or modestly surpassing the view-synthesis quality of\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Hyunjin Kim"
                    },
                    {
                        "name": "Haebeom Jung"
                    },
                    {
                        "name": "Jaesik Park"
                    }
                ],
                "author_detail": {
                    "name": "Jaesik Park"
                },
                "author": "Jaesik Park",
                "arxiv_comment": "NeurIPS 2025. Project Page: https://hjhyunjinkim.github.io/MH-3DGS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06186v2",
                "updated": "2025-10-24T17:20:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    20,
                    26,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-07T17:45:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    45,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback"
                },
                "summary": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation"
                },
                "authors": [
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Fangxin Wang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Wooseong Yang"
                    },
                    {
                        "name": "Bowei He"
                    },
                    {
                        "name": "Xinni Zhang"
                    },
                    {
                        "name": "Dianzhi Yu"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Hoang H Nguyen"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jizhou Guo"
                    },
                    {
                        "name": "Wenzhe Fan"
                    },
                    {
                        "name": "Chin-Yuan Yeh"
                    },
                    {
                        "name": "Panpan Meng"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Jinhu Qi"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Zhengyao Gu"
                    },
                    {
                        "name": "Yuwei Han"
                    },
                    {
                        "name": "Langzhou He"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "Code and dataset are available at github.com/ChunyuMiao98/RECODE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23579v2",
                "updated": "2025-10-24T17:16:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    16,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-29T15:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    49,
                    27,
                    3,
                    149,
                    0
                ],
                "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model"
                },
                "summary": "Unlocking deep and interpretable biological reasoning from complex genomic\ndata remains a major AI challenge limiting scientific progress. While current\nDNA foundation models excel at representing sequences, they struggle with\nmulti-step reasoning and lack transparent, biologically meaningful\nexplanations. BioReason addresses this by tightly integrating a DNA foundation\nmodel with a large language model (LLM), enabling the LLM to directly interpret\nand reason over genomic information. Through supervised fine-tuning and\nreinforcement learning, BioReason learns to produce logical, biologically\ncoherent deductions. It achieves major performance gains, boosting KEGG-based\ndisease pathway prediction accuracy from 86% to 98% and improving variant\neffect prediction by an average of 15% over strong baselines. BioReason can\nreason over unseen biological entities and explain its decisions step by step,\noffering a transformative framework for interpretable, mechanistic AI in\nbiology. All data, code, and checkpoints are available at\nhttps://github.com/bowang-lab/BioReason",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking deep and interpretable biological reasoning from complex genomic\ndata remains a major AI challenge limiting scientific progress. While current\nDNA foundation models excel at representing sequences, they struggle with\nmulti-step reasoning and lack transparent, biologically meaningful\nexplanations. BioReason addresses this by tightly integrating a DNA foundation\nmodel with a large language model (LLM), enabling the LLM to directly interpret\nand reason over genomic information. Through supervised fine-tuning and\nreinforcement learning, BioReason learns to produce logical, biologically\ncoherent deductions. It achieves major performance gains, boosting KEGG-based\ndisease pathway prediction accuracy from 86% to 98% and improving variant\neffect prediction by an average of 15% over strong baselines. BioReason can\nreason over unseen biological entities and explain its decisions step by step,\noffering a transformative framework for interpretable, mechanistic AI in\nbiology. All data, code, and checkpoints are available at\nhttps://github.com/bowang-lab/BioReason"
                },
                "authors": [
                    {
                        "name": "Adibvafa Fallahpour"
                    },
                    {
                        "name": "Andrew Magnuson"
                    },
                    {
                        "name": "Purav Gupta"
                    },
                    {
                        "name": "Shihao Ma"
                    },
                    {
                        "name": "Jack Naimer"
                    },
                    {
                        "name": "Arnav Shah"
                    },
                    {
                        "name": "Haonan Duan"
                    },
                    {
                        "name": "Omar Ibrahim"
                    },
                    {
                        "name": "Hani Goodarzi"
                    },
                    {
                        "name": "Chris J. Maddison"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "arxiv_comment": "28 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21661v1",
                "updated": "2025-10-24T17:15:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    15,
                    24,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:15:24Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    15,
                    24,
                    4,
                    297,
                    0
                ],
                "title": "MECfda: An R Package for Bias Correction Due to Measurement Error in\n  Functional and Scalar Covariates in Scalar-on-Function Regression Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MECfda: An R Package for Bias Correction Due to Measurement Error in\n  Functional and Scalar Covariates in Scalar-on-Function Regression Models"
                },
                "summary": "Functional data analysis (FDA) deals with high-resolution data recorded over\na continuum, such as time, space or frequency. Device-based assessments of\nphysical activity or sleep are objective yet still prone to measurement error.\nWe present MECfda, an R package that (i) fits scalar-on-function, generalized\nscalar-on-function, and functional quantile regression models, and (ii)\nprovides bias-corrected estimation when functional covariates are measured with\nerror. By unifying these tools under a consistent syntax, MECfda enables robust\ninference for FDA applications that involve noisy functional data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional data analysis (FDA) deals with high-resolution data recorded over\na continuum, such as time, space or frequency. Device-based assessments of\nphysical activity or sleep are objective yet still prone to measurement error.\nWe present MECfda, an R package that (i) fits scalar-on-function, generalized\nscalar-on-function, and functional quantile regression models, and (ii)\nprovides bias-corrected estimation when functional covariates are measured with\nerror. By unifying these tools under a consistent syntax, MECfda enables robust\ninference for FDA applications that involve noisy functional data."
                },
                "authors": [
                    {
                        "name": "Heyang Ji"
                    },
                    {
                        "name": "Carmen Tekwe"
                    }
                ],
                "author_detail": {
                    "name": "Carmen Tekwe"
                },
                "author": "Carmen Tekwe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11194v3",
                "updated": "2025-10-24T17:15:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    15,
                    17,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-16T12:53:21Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    53,
                    21,
                    4,
                    136,
                    0
                ],
                "title": "Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive\n  Alignment"
                },
                "summary": "Predicting protein function from sequence is a central challenge in\ncomputational biology. While existing methods rely heavily on structured\nontologies or similarity-based techniques, they often lack the flexibility to\nexpress structure-free functional descriptions and novel biological functions.\nIn this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text\nmodel that generates free-form natural language descriptions of protein\nfunction directly from amino acid sequences. Our method combines a protein\nlanguage model as a sequence encoder (ESM-3B) and a decoder-only language model\n(LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A\nkey innovation is our Hybrid Sequence-level Contrastive Alignment Learning\n(H-SCALE), which improves cross-modal learning by matching mean- and std-pooled\nprotein embeddings with text representations via contrastive loss. After the\nalignment phase, we apply instruction-based fine-tuning using LoRA on the\ndecoder to teach the model how to generate accurate protein function\ndescriptions conditioned on the protein sequence. We train Prot2Text-V2 on\nabout 250K curated entries from SwissProt and evaluate it under low-homology\nconditions, where test sequences have low similarity with training samples.\nProt2Text-V2 consistently outperforms traditional and LLM-based baselines\nacross various metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting protein function from sequence is a central challenge in\ncomputational biology. While existing methods rely heavily on structured\nontologies or similarity-based techniques, they often lack the flexibility to\nexpress structure-free functional descriptions and novel biological functions.\nIn this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text\nmodel that generates free-form natural language descriptions of protein\nfunction directly from amino acid sequences. Our method combines a protein\nlanguage model as a sequence encoder (ESM-3B) and a decoder-only language model\n(LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A\nkey innovation is our Hybrid Sequence-level Contrastive Alignment Learning\n(H-SCALE), which improves cross-modal learning by matching mean- and std-pooled\nprotein embeddings with text representations via contrastive loss. After the\nalignment phase, we apply instruction-based fine-tuning using LoRA on the\ndecoder to teach the model how to generate accurate protein function\ndescriptions conditioned on the protein sequence. We train Prot2Text-V2 on\nabout 250K curated entries from SwissProt and evaluate it under low-homology\nconditions, where test sequences have low similarity with training samples.\nProt2Text-V2 consistently outperforms traditional and LLM-based baselines\nacross various metrics."
                },
                "authors": [
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Sarah Almeida Carneiro"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Lawrence P. Petalidis"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "arxiv_comment": "24 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21659v1",
                "updated": "2025-10-24T17:14:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    14,
                    12,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:14:12Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    14,
                    12,
                    4,
                    297,
                    0
                ],
                "title": "Smule Renaissance Small: Efficient General-Purpose Vocal Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smule Renaissance Small: Efficient General-Purpose Vocal Restoration"
                },
                "summary": "Vocal recordings on consumer devices commonly suffer from multiple concurrent\ndegradations: noise, reverberation, band-limiting, and clipping. We present\nSmule Renaissance Small (SRS), a compact single-stage model that performs\nend-to-end vocal restoration directly in the complex STFT domain. By\nincorporating phase-aware losses, SRS enables large analysis windows for\nimproved frequency resolution while achieving 10.5x real-time inference on\niPhone 12 CPU at 48 kHz. On the DNS 5 Challenge blind set, despite no speech\ntraining, SRS outperforms a strong GAN baseline and closely matches a\ncomputationally expensive flow-matching system. To enable evaluation under\nrealistic multi-degradation scenarios, we introduce the Extreme Degradation\nBench (EDB): 87 singing and speech recordings captured under severe acoustic\nconditions. On EDB, SRS surpasses all open-source baselines on singing and\nmatches commercial systems, while remaining competitive on speech despite no\nspeech-specific training. We release both SRS and EDB under the MIT License.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vocal recordings on consumer devices commonly suffer from multiple concurrent\ndegradations: noise, reverberation, band-limiting, and clipping. We present\nSmule Renaissance Small (SRS), a compact single-stage model that performs\nend-to-end vocal restoration directly in the complex STFT domain. By\nincorporating phase-aware losses, SRS enables large analysis windows for\nimproved frequency resolution while achieving 10.5x real-time inference on\niPhone 12 CPU at 48 kHz. On the DNS 5 Challenge blind set, despite no speech\ntraining, SRS outperforms a strong GAN baseline and closely matches a\ncomputationally expensive flow-matching system. To enable evaluation under\nrealistic multi-degradation scenarios, we introduce the Extreme Degradation\nBench (EDB): 87 singing and speech recordings captured under severe acoustic\nconditions. On EDB, SRS surpasses all open-source baselines on singing and\nmatches commercial systems, while remaining competitive on speech despite no\nspeech-specific training. We release both SRS and EDB under the MIT License."
                },
                "authors": [
                    {
                        "name": "Yongyi Zang"
                    },
                    {
                        "name": "Chris Manchester"
                    },
                    {
                        "name": "David Young"
                    },
                    {
                        "name": "Ivan Ivanov"
                    },
                    {
                        "name": "Jeffrey Lufkin"
                    },
                    {
                        "name": "Martin Vladimirov"
                    },
                    {
                        "name": "PJ Solomon"
                    },
                    {
                        "name": "Svetoslav Kepchelev"
                    },
                    {
                        "name": "Fei Yueh Chen"
                    },
                    {
                        "name": "Dongting Cai"
                    },
                    {
                        "name": "Teodor Naydenov"
                    },
                    {
                        "name": "Randal Leistikow"
                    }
                ],
                "author_detail": {
                    "name": "Randal Leistikow"
                },
                "author": "Randal Leistikow",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03325v2",
                "updated": "2025-10-24T16:55:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    55,
                    30,
                    4,
                    297,
                    0
                ],
                "published": "2025-02-05T16:22:33Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    33,
                    2,
                    36,
                    0
                ],
                "title": "Electronic Circuit Principles of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Circuit Principles of Large Language Models"
                },
                "summary": "Large language models (LLMs) such as DeepSeek-R1 have achieved remarkable\nperformance across diverse reasoning tasks. To uncover the principles that\ngovern their behaviour, we introduce the Electronic Circuit Principles (ECP),\nwhich maps inference-time learning (ITL) onto a semantic electromotive force\nand inference-time reasoning (ITR) onto a resistive network governed by Ohm's\nand Faraday's laws. This circuit-based modelling yields closed-form predictions\nof task performance and reveals how modular prompt components interact to shape\naccuracy. We validated ECP on 70,000 samples spanning 350 reasoning tasks and 9\nadvanced LLMs, observing a about 60% improvement in Pearson correlation\nrelative to the conventional inference-time scaling law. Moreover, ECP explains\nthe efficacy of 15 established prompting strategies and directs the development\nof new modular interventions that exceed the median score of the top 80% of\nparticipants in both the International Olympiad in Informatics and the\nInternational Mathematical Olympiad. By grounding LLM reasoning in\nelectronic-circuit principles, ECP provides a rigorous framework for predicting\nperformance and optimising modular components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as DeepSeek-R1 have achieved remarkable\nperformance across diverse reasoning tasks. To uncover the principles that\ngovern their behaviour, we introduce the Electronic Circuit Principles (ECP),\nwhich maps inference-time learning (ITL) onto a semantic electromotive force\nand inference-time reasoning (ITR) onto a resistive network governed by Ohm's\nand Faraday's laws. This circuit-based modelling yields closed-form predictions\nof task performance and reveals how modular prompt components interact to shape\naccuracy. We validated ECP on 70,000 samples spanning 350 reasoning tasks and 9\nadvanced LLMs, observing a about 60% improvement in Pearson correlation\nrelative to the conventional inference-time scaling law. Moreover, ECP explains\nthe efficacy of 15 established prompting strategies and directs the development\nof new modular interventions that exceed the median score of the top 80% of\nparticipants in both the International Olympiad in Informatics and the\nInternational Mathematical Olympiad. By grounding LLM reasoning in\nelectronic-circuit principles, ECP provides a rigorous framework for predicting\nperformance and optimising modular components."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "Manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20978v2",
                "updated": "2025-10-24T16:41:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    41,
                    44,
                    4,
                    297,
                    0
                ],
                "published": "2025-08-28T16:33:27Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    33,
                    27,
                    3,
                    240,
                    0
                ],
                "title": "Scaling Neuro-symbolic Problem Solving: Solver-Free Learning of\n  Constraints and Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Neuro-symbolic Problem Solving: Solver-Free Learning of\n  Constraints and Objectives"
                },
                "summary": "In the ongoing quest for hybridizing discrete reasoning with neural nets,\nthere is an increasing interest in neural architectures that can learn how to\nsolve discrete reasoning or optimization problems from natural inputs, a task\nthat Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a\nloss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints\nand the objective, thus delivering a complete model that can be scrutinized and\ncompleted with side constraints. By pushing the combinatorial solver out of the\ntraining loop, our architecture also offers scalable training while exact\ninference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve\nNP-hard reasoning problems from natural inputs. On three variants of the Sudoku\nbenchmark -- symbolic, visual, and many-solution --, our approach requires a\nfraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut\ntask, it optimizes the regret better than a Decision-Focused-Learning\nregret-dedicated loss. Finally, it efficiently learns the energy optimization\nformulation of the large real-world problem of designing proteins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the ongoing quest for hybridizing discrete reasoning with neural nets,\nthere is an increasing interest in neural architectures that can learn how to\nsolve discrete reasoning or optimization problems from natural inputs, a task\nthat Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a\nloss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints\nand the objective, thus delivering a complete model that can be scrutinized and\ncompleted with side constraints. By pushing the combinatorial solver out of the\ntraining loop, our architecture also offers scalable training while exact\ninference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve\nNP-hard reasoning problems from natural inputs. On three variants of the Sudoku\nbenchmark -- symbolic, visual, and many-solution --, our approach requires a\nfraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut\ntask, it optimizes the regret better than a Decision-Focused-Learning\nregret-dedicated loss. Finally, it efficiently learns the energy optimization\nformulation of the large real-world problem of designing proteins."
                },
                "authors": [
                    {
                        "name": "Marianne Defresne"
                    },
                    {
                        "name": "Romain Gambardella"
                    },
                    {
                        "name": "Sophie Barbe"
                    },
                    {
                        "name": "Thomas Schiex"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Schiex"
                },
                "author": "Thomas Schiex",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21632v1",
                "updated": "2025-10-24T16:37:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    37,
                    7,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:37:07Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    37,
                    7,
                    4,
                    297,
                    0
                ],
                "title": "Exploring the statistical properties of double radio relics in the\n  TNG-Cluster and TNG300 simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the statistical properties of double radio relics in the\n  TNG-Cluster and TNG300 simulations"
                },
                "summary": "Double radio relics, pairs of diffuse radio features located on opposite\nsides of merging galaxy clusters, are a rare subclass of radio relics that are\nbelieved to trace merger shocks and provide valuable constraints on plasma\nacceleration models and merger history. With the number of known double relics\ngrowing in recent and upcoming radio surveys, statistical analyses of their\nproperties are becoming feasible. In this study, we utilize the cosmological\nmagnetohydrodynamics zoom-in simulations TNG-Cluster, in combination with\nTNG300-1, to examine the statistical properties of double radio relics. The\nsimulated double relic pairs exhibit a wide range of luminosity ratios, broadly\nconsistent with the observations. We find that the two relics in a given double\nsystem often differ significantly in their shock properties and magnetic field\nstrengths. This diversity implies that the observed brightness asymmetry in the\npair cannot be explained by a single factor alone, but instead reflects an\ninterplay of multiple physical parameters. Nevertheless, double radio relics\ntend to align with the collision axis within $\\sim30^{\\circ}$ and their\nseparation ($d_{\\rm drr}$) correlates tightly with the time since collision\n(TSC) as ${\\rm TSC~[Gyr]} = 0.52 d_{\\rm drr}/R_{500\\rm c} - 0.24$, allowing it\nto be inferred with an accuracy of $\\sim0.2~\\rm Gyr$. With the statistical\nsamples of simulated radio relics, we predict that low-mass clusters will\nconstitute the dominant population of double radio relic systems detected with\nupcoming surveys such as SKA. These results demonstrate that double radio\nrelics can serve as robust probes of merger dynamics and plasma acceleration,\nand that simulations provide critical guidance for interpreting the large\nsamples expected from next-generation radio surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double radio relics, pairs of diffuse radio features located on opposite\nsides of merging galaxy clusters, are a rare subclass of radio relics that are\nbelieved to trace merger shocks and provide valuable constraints on plasma\nacceleration models and merger history. With the number of known double relics\ngrowing in recent and upcoming radio surveys, statistical analyses of their\nproperties are becoming feasible. In this study, we utilize the cosmological\nmagnetohydrodynamics zoom-in simulations TNG-Cluster, in combination with\nTNG300-1, to examine the statistical properties of double radio relics. The\nsimulated double relic pairs exhibit a wide range of luminosity ratios, broadly\nconsistent with the observations. We find that the two relics in a given double\nsystem often differ significantly in their shock properties and magnetic field\nstrengths. This diversity implies that the observed brightness asymmetry in the\npair cannot be explained by a single factor alone, but instead reflects an\ninterplay of multiple physical parameters. Nevertheless, double radio relics\ntend to align with the collision axis within $\\sim30^{\\circ}$ and their\nseparation ($d_{\\rm drr}$) correlates tightly with the time since collision\n(TSC) as ${\\rm TSC~[Gyr]} = 0.52 d_{\\rm drr}/R_{500\\rm c} - 0.24$, allowing it\nto be inferred with an accuracy of $\\sim0.2~\\rm Gyr$. With the statistical\nsamples of simulated radio relics, we predict that low-mass clusters will\nconstitute the dominant population of double radio relic systems detected with\nupcoming surveys such as SKA. These results demonstrate that double radio\nrelics can serve as robust probes of merger dynamics and plasma acceleration,\nand that simulations provide critical guidance for interpreting the large\nsamples expected from next-generation radio surveys."
                },
                "authors": [
                    {
                        "name": "Wonki Lee"
                    },
                    {
                        "name": "Annalisa Pillepich"
                    },
                    {
                        "name": "Dylan Nelson"
                    },
                    {
                        "name": "Myungkook James Jee"
                    },
                    {
                        "name": "Daisuke Nagai"
                    },
                    {
                        "name": "Kyle Finner"
                    },
                    {
                        "name": "John ZuHone"
                    }
                ],
                "author_detail": {
                    "name": "John ZuHone"
                },
                "author": "John ZuHone",
                "arxiv_comment": "22 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21631v1",
                "updated": "2025-10-24T16:36:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    36,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:36:34Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    36,
                    34,
                    4,
                    297,
                    0
                ],
                "title": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations"
                },
                "summary": "Knowledge distillation is a promising approach to transfer capabilities from\ncomplex teacher models to smaller, resource-efficient student models that can\nbe deployed easily, particularly in task-aware scenarios. However, existing\nmethods of task-aware distillation typically require substantial quantities of\ndata which may be unavailable or expensive to obtain in many practical\nscenarios. In this paper, we address this challenge by introducing a novel\nstrategy called Counterfactual-explanation-infused Distillation CoD for\nfew-shot task-aware knowledge distillation by systematically infusing\ncounterfactual explanations. Counterfactual explanations (CFEs) refer to inputs\nthat can flip the output prediction of the teacher model with minimum\nperturbation. Our strategy CoD leverages these CFEs to precisely map the\nteacher's decision boundary with significantly fewer samples. We provide\ntheoretical guarantees for motivating the role of CFEs in distillation, from\nboth statistical and geometric perspectives. We mathematically show that CFEs\ncan improve parameter estimation by providing more informative examples near\nthe teacher's decision boundary. We also derive geometric insights on how CFEs\neffectively act as knowledge probes, helping the students mimic the teacher's\ndecision boundaries more effectively than standard data. We perform experiments\nacross various datasets and LLMs to show that CoD outperforms standard\ndistillation approaches in few-shot regimes (as low as 8-512 samples). Notably,\nCoD only uses half of the original samples used by the baselines, paired with\ntheir corresponding CFEs and still improves performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation is a promising approach to transfer capabilities from\ncomplex teacher models to smaller, resource-efficient student models that can\nbe deployed easily, particularly in task-aware scenarios. However, existing\nmethods of task-aware distillation typically require substantial quantities of\ndata which may be unavailable or expensive to obtain in many practical\nscenarios. In this paper, we address this challenge by introducing a novel\nstrategy called Counterfactual-explanation-infused Distillation CoD for\nfew-shot task-aware knowledge distillation by systematically infusing\ncounterfactual explanations. Counterfactual explanations (CFEs) refer to inputs\nthat can flip the output prediction of the teacher model with minimum\nperturbation. Our strategy CoD leverages these CFEs to precisely map the\nteacher's decision boundary with significantly fewer samples. We provide\ntheoretical guarantees for motivating the role of CFEs in distillation, from\nboth statistical and geometric perspectives. We mathematically show that CFEs\ncan improve parameter estimation by providing more informative examples near\nthe teacher's decision boundary. We also derive geometric insights on how CFEs\neffectively act as knowledge probes, helping the students mimic the teacher's\ndecision boundaries more effectively than standard data. We perform experiments\nacross various datasets and LLMs to show that CoD outperforms standard\ndistillation approaches in few-shot regimes (as low as 8-512 samples). Notably,\nCoD only uses half of the original samples used by the baselines, paired with\ntheir corresponding CFEs and still improves performance."
                },
                "authors": [
                    {
                        "name": "Faisal Hamman"
                    },
                    {
                        "name": "Pasan Dissanayake"
                    },
                    {
                        "name": "Yanjun Fu"
                    },
                    {
                        "name": "Sanghamitra Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Sanghamitra Dutta"
                },
                "author": "Sanghamitra Dutta",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13144v3",
                "updated": "2025-10-24T16:27:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    27,
                    55,
                    4,
                    297,
                    0
                ],
                "published": "2025-04-17T17:53:39Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    53,
                    39,
                    3,
                    107,
                    0
                ],
                "title": "Bayesian model-data comparison incorporating theoretical uncertainties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian model-data comparison incorporating theoretical uncertainties"
                },
                "summary": "Accurate comparisons between theoretical models and experimental data are\ncritical for scientific progress. However, inferred physical model parameters\ncan vary significantly with the chosen physics model, highlighting the\nimportance of properly accounting for theoretical uncertainties. In this\nLetter, we present a Bayesian framework that explicitly quantifies these\nuncertainties by statistically modeling theory errors, guided by qualitative\nknowledge of a theory's varying reliability across the input domain. We\ndemonstrate the effectiveness of this approach using two systems: a simple ball\ndrop experiment and multi-stage heavy-ion simulations. In both cases\nincorporating model discrepancy leads to improved parameter estimates, with\nsystematic improvements observed as additional experimental observables are\nintegrated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate comparisons between theoretical models and experimental data are\ncritical for scientific progress. However, inferred physical model parameters\ncan vary significantly with the chosen physics model, highlighting the\nimportance of properly accounting for theoretical uncertainties. In this\nLetter, we present a Bayesian framework that explicitly quantifies these\nuncertainties by statistically modeling theory errors, guided by qualitative\nknowledge of a theory's varying reliability across the input domain. We\ndemonstrate the effectiveness of this approach using two systems: a simple ball\ndrop experiment and multi-stage heavy-ion simulations. In both cases\nincorporating model discrepancy leads to improved parameter estimates, with\nsystematic improvements observed as additional experimental observables are\nintegrated."
                },
                "authors": [
                    {
                        "name": "Sunil Jaiswal"
                    },
                    {
                        "name": "Chun Shen"
                    },
                    {
                        "name": "Richard J. Furnstahl"
                    },
                    {
                        "name": "Ulrich Heinz"
                    },
                    {
                        "name": "Matthew T. Pratola"
                    }
                ],
                "author_detail": {
                    "name": "Matthew T. Pratola"
                },
                "author": "Matthew T. Pratola",
                "arxiv_comment": "11 pages, 7 figures. Added Figure 2 and Ref. 24 (open-source code\n  link). Matches published version",
                "arxiv_journal_ref": "Phys. Lett. B 870 (2025) 139946",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21623v1",
                "updated": "2025-10-24T16:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    26,
                    36,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:26:36Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    26,
                    36,
                    4,
                    297,
                    0
                ],
                "title": "The Universal Landscape of Human Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Universal Landscape of Human Reasoning"
                },
                "summary": "Understanding how information is dynamically accumulated and transformed in\nhuman reasoning has long challenged cognitive psychology, philosophy, and\nartificial intelligence. Existing accounts, from classical logic to\nprobabilistic models, illuminate aspects of output or individual modelling, but\ndo not offer a unified, quantitative description of general human reasoning\ndynamics. To solve this, we introduce Information Flow Tracking (IF-Track),\nthat uses large language models (LLMs) as probabilistic encoder to quantify\ninformation entropy and gain at each reasoning step. Through fine-grained\nanalyses across diverse tasks, our method is the first successfully models the\nuniversal landscape of human reasoning behaviors within a single metric space.\nWe show that IF-Track captures essential reasoning features, identifies\nsystematic error patterns, and characterizes individual differences. Applied to\ndiscussion of advanced psychological theory, we first reconcile single- versus\ndual-process theories in IF-Track and discover the alignment of artificial and\nhuman cognition and how LLMs reshaping human reasoning process. This approach\nestablishes a quantitative bridge between theory and measurement, offering\nmechanistic insights into the architecture of reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how information is dynamically accumulated and transformed in\nhuman reasoning has long challenged cognitive psychology, philosophy, and\nartificial intelligence. Existing accounts, from classical logic to\nprobabilistic models, illuminate aspects of output or individual modelling, but\ndo not offer a unified, quantitative description of general human reasoning\ndynamics. To solve this, we introduce Information Flow Tracking (IF-Track),\nthat uses large language models (LLMs) as probabilistic encoder to quantify\ninformation entropy and gain at each reasoning step. Through fine-grained\nanalyses across diverse tasks, our method is the first successfully models the\nuniversal landscape of human reasoning behaviors within a single metric space.\nWe show that IF-Track captures essential reasoning features, identifies\nsystematic error patterns, and characterizes individual differences. Applied to\ndiscussion of advanced psychological theory, we first reconcile single- versus\ndual-process theories in IF-Track and discover the alignment of artificial and\nhuman cognition and how LLMs reshaping human reasoning process. This approach\nestablishes a quantitative bridge between theory and measurement, offering\nmechanistic insights into the architecture of reasoning."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Yihao Liang"
                    },
                    {
                        "name": "Shangxu Ren"
                    },
                    {
                        "name": "Chengyu Luan"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Hanjing Li"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Zheng Yan"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Yantao Du"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21043v3",
                "updated": "2025-10-24T16:26:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    26,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-25T11:48:37Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    11,
                    48,
                    37,
                    3,
                    268,
                    0
                ],
                "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Creativity: A New Frontier in Generalization Abilities"
                },
                "summary": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence."
                },
                "authors": [
                    {
                        "name": "Samuel Schapiro"
                    },
                    {
                        "name": "Sumuk Shashidhar"
                    },
                    {
                        "name": "Alexi Gladstone"
                    },
                    {
                        "name": "Jonah Black"
                    },
                    {
                        "name": "Royce Moon"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "arxiv_comment": "Preprint. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14198v2",
                "updated": "2025-10-24T16:26:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    26,
                    0,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-14T15:51:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    51,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "Retention analysis of edited knowledge after fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retention analysis of edited knowledge after fine-tuning"
                },
                "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that knowledge retention can be significantly improved by either\naugmenting edit knowledge with paraphrases or by freezing layers associated\nwith edited content in fine-tuning stage, offering insight for developing more\nrobust editing algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that knowledge retention can be significantly improved by either\naugmenting edit knowledge with paraphrases or by freezing layers associated\nwith edited content in fine-tuning stage, offering insight for developing more\nrobust editing algorithms."
                },
                "authors": [
                    {
                        "name": "Fufang Wen"
                    },
                    {
                        "name": "Shichang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shichang Zhang"
                },
                "author": "Shichang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21618v1",
                "updated": "2025-10-24T16:24:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    24,
                    1,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:24:01Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    24,
                    1,
                    4,
                    297,
                    0
                ],
                "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepAgent: A General Reasoning Agent with Scalable Toolsets"
                },
                "summary": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent."
                },
                "authors": [
                    {
                        "name": "Xiaoxi Li"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Jiarui Jin"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Jiajie Jin"
                    },
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06485v2",
                "updated": "2025-10-24T16:19:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    19,
                    27,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-09T02:06:13Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    6,
                    13,
                    2,
                    190,
                    0
                ],
                "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning"
                },
                "summary": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and fine-tuning remain\nsignificant challenges. These methods often rely on large-scale supervised\nfine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT)\nannotations, making them costly and hard to scale. To address this, we present\nVideo-RTS, a new approach to improve video reasoning capability with\ndrastically improved data efficiency by combining data-efficient RL with a\nvideo-adaptive test-time scaling (TTS) strategy. Building on observations about\nthe data scaling, we skip the resource-intensive SFT step and employ efficient\npure-RL training with output-based rewards, requiring no additional annotations\nor extensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by 2.4% in accuracy using only 3.6%\ntraining samples. Specifically, Video-RTS achieves a 4.2% improvement on\nVideo-Holmes, a recent and challenging video reasoning benchmark. Notably, our\npure RL training and adaptive video TTS offer complementary strengths, enabling\nVideo-RTS's strong reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and fine-tuning remain\nsignificant challenges. These methods often rely on large-scale supervised\nfine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT)\nannotations, making them costly and hard to scale. To address this, we present\nVideo-RTS, a new approach to improve video reasoning capability with\ndrastically improved data efficiency by combining data-efficient RL with a\nvideo-adaptive test-time scaling (TTS) strategy. Building on observations about\nthe data scaling, we skip the resource-intensive SFT step and employ efficient\npure-RL training with output-based rewards, requiring no additional annotations\nor extensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by 2.4% in accuracy using only 3.6%\ntraining samples. Specifically, Video-RTS achieves a 4.2% improvement on\nVideo-Holmes, a recent and challenging video reasoning benchmark. Notably, our\npure RL training and adaptive video TTS offer complementary strengths, enabling\nVideo-RTS's strong reasoning performance."
                },
                "authors": [
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "EMNLP 2025. The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21608v1",
                "updated": "2025-10-24T16:14:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    14,
                    31,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:14:31Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    14,
                    31,
                    4,
                    297,
                    0
                ],
                "title": "Generalised Flow Maps for Few-Step Generative Modelling on Riemannian\n  Manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalised Flow Maps for Few-Step Generative Modelling on Riemannian\n  Manifolds"
                },
                "summary": "Geometric data and purpose-built generative models on them have become\nubiquitous in high-impact deep learning application domains, ranging from\nprotein backbone generation and computational chemistry to geospatial data.\nCurrent geometric generative models remain computationally expensive at\ninference -- requiring many steps of complex numerical simulation -- as they\nare derived from dynamical measure transport frameworks such as diffusion and\nflow-matching on Riemannian manifolds. In this paper, we propose Generalised\nFlow Maps (GFM), a new class of few-step generative models that generalises the\nFlow Map framework in Euclidean spaces to arbitrary Riemannian manifolds. We\ninstantiate GFMs with three self-distillation-based training methods:\nGeneralised Lagrangian Flow Maps, Generalised Eulerian Flow Maps, and\nGeneralised Progressive Flow Maps. We theoretically show that GFMs, under\nspecific design decisions, unify and elevate existing Euclidean few-step\ngenerative models, such as consistency models, shortcut models, and meanflows,\nto the Riemannian setting. We benchmark GFMs against other geometric generative\nmodels on a suite of geometric datasets, including geospatial data, RNA torsion\nangles, and hyperbolic manifolds, and achieve state-of-the-art sample quality\nfor single- and few-step evaluations, and superior or competitive\nlog-likelihoods using the implicit probability flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric data and purpose-built generative models on them have become\nubiquitous in high-impact deep learning application domains, ranging from\nprotein backbone generation and computational chemistry to geospatial data.\nCurrent geometric generative models remain computationally expensive at\ninference -- requiring many steps of complex numerical simulation -- as they\nare derived from dynamical measure transport frameworks such as diffusion and\nflow-matching on Riemannian manifolds. In this paper, we propose Generalised\nFlow Maps (GFM), a new class of few-step generative models that generalises the\nFlow Map framework in Euclidean spaces to arbitrary Riemannian manifolds. We\ninstantiate GFMs with three self-distillation-based training methods:\nGeneralised Lagrangian Flow Maps, Generalised Eulerian Flow Maps, and\nGeneralised Progressive Flow Maps. We theoretically show that GFMs, under\nspecific design decisions, unify and elevate existing Euclidean few-step\ngenerative models, such as consistency models, shortcut models, and meanflows,\nto the Riemannian setting. We benchmark GFMs against other geometric generative\nmodels on a suite of geometric datasets, including geospatial data, RNA torsion\nangles, and hyperbolic manifolds, and achieve state-of-the-art sample quality\nfor single- and few-step evaluations, and superior or competitive\nlog-likelihoods using the implicit probability flow."
                },
                "authors": [
                    {
                        "name": "Oscar Davis"
                    },
                    {
                        "name": "Michael S. Albergo"
                    },
                    {
                        "name": "Nicholas M. Boffi"
                    },
                    {
                        "name": "Michael M. Bronstein"
                    },
                    {
                        "name": "Avishek Joey Bose"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Joey Bose"
                },
                "author": "Avishek Joey Bose",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21604v1",
                "updated": "2025-10-24T16:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    8,
                    33,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:08:33Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    8,
                    33,
                    4,
                    297,
                    0
                ],
                "title": "RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction\n  with Large Language Models"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated outstanding\nreasoning capabilities on mathematical and coding tasks. However, their\napplication to financial tasks-especially the most fundamental task of stock\nmovement prediction-remains underexplored. We study a three-class\nclassification problem (up, hold, down) and, by analyzing existing reasoning\nresponses, observe that: (1) LLMs follow analysts' opinions rather than exhibit\na systematic, independent analytical logic (CoTs). (2) LLMs list summaries from\ndifferent sources without weighing adversarial evidence, yet such\ncounterevidence is crucial for reliable prediction. It shows that the model\ndoes not make good use of its reasoning ability to complete the task. To\naddress this, we propose Reflective Evidence Tuning (RETuning), a cold-start\nmethod prior to reinforcement learning, to enhance prediction ability. While\ngenerating CoT, RETuning encourages dynamically constructing an analytical\nframework from diverse information sources, organizing and scoring evidence for\nprice up or down based on that framework-rather than on contextual\nviewpoints-and finally reflecting to derive the prediction. This approach\nmaximally aligns the model with its learned analytical framework, ensuring\nindependent logical reasoning and reducing undue influence from context. We\nalso build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,\nwith long contexts (32K tokens) and over 200K samples. In addition to price and\nnews, it incorporates analysts' opinions, quantitative reports, fundamental\ndata, macroeconomic indicators, and similar stocks. Experiments show that\nRETuning successfully unlocks the model's reasoning ability in the financial\ndomain. Inference-time scaling still works even after 6 months or on\nout-of-distribution stocks, since the models gain valuable insights about stock\nmovement prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated outstanding\nreasoning capabilities on mathematical and coding tasks. However, their\napplication to financial tasks-especially the most fundamental task of stock\nmovement prediction-remains underexplored. We study a three-class\nclassification problem (up, hold, down) and, by analyzing existing reasoning\nresponses, observe that: (1) LLMs follow analysts' opinions rather than exhibit\na systematic, independent analytical logic (CoTs). (2) LLMs list summaries from\ndifferent sources without weighing adversarial evidence, yet such\ncounterevidence is crucial for reliable prediction. It shows that the model\ndoes not make good use of its reasoning ability to complete the task. To\naddress this, we propose Reflective Evidence Tuning (RETuning), a cold-start\nmethod prior to reinforcement learning, to enhance prediction ability. While\ngenerating CoT, RETuning encourages dynamically constructing an analytical\nframework from diverse information sources, organizing and scoring evidence for\nprice up or down based on that framework-rather than on contextual\nviewpoints-and finally reflecting to derive the prediction. This approach\nmaximally aligns the model with its learned analytical framework, ensuring\nindependent logical reasoning and reducing undue influence from context. We\nalso build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,\nwith long contexts (32K tokens) and over 200K samples. In addition to price and\nnews, it incorporates analysts' opinions, quantitative reports, fundamental\ndata, macroeconomic indicators, and similar stocks. Experiments show that\nRETuning successfully unlocks the model's reasoning ability in the financial\ndomain. Inference-time scaling still works even after 6 months or on\nout-of-distribution stocks, since the models gain valuable insights about stock\nmovement prediction."
                },
                "authors": [
                    {
                        "name": "Xueyuan Lin"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Ye Ma"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Rongjunchen Zhang"
                    },
                    {
                        "name": "Yang Ni"
                    },
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Jian Guo"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21603v1",
                "updated": "2025-10-24T16:07:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    7,
                    54,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:07:54Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    7,
                    54,
                    4,
                    297,
                    0
                ],
                "title": "Doc-Researcher: A Unified System for Multimodal Document Parsing and\n  Deep Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doc-Researcher: A Unified System for Multimodal Document Parsing and\n  Deep Research"
                },
                "summary": "Deep Research systems have revolutionized how LLMs solve complex questions\nthrough iterative reasoning and evidence gathering. However, current systems\nremain fundamentally constrained to textual web data, overlooking the vast\nknowledge embedded in multimodal documents Processing such documents demands\nsophisticated parsing to preserve visual semantics (figures, tables, charts,\nand equations), intelligent chunking to maintain structural coherence, and\nadaptive retrieval across modalities, which are capabilities absent in existing\nsystems. In response, we present Doc-Researcher, a unified system that bridges\nthis gap through three integrated components: (i) deep multimodal parsing that\npreserves layout structure and visual semantics while creating multi-granular\nrepresentations from chunk to document level, (ii) systematic retrieval\narchitecture supporting text-only, vision-only, and hybrid paradigms with\ndynamic granularity selection, and (iii) iterative multi-agent workflows that\ndecompose complex queries, progressively accumulate evidence, and synthesize\ncomprehensive answers across documents and modalities. To enable rigorous\nevaluation, we introduce M4DocBench, the first benchmark for Multi-modal,\nMulti-hop, Multi-document, and Multi-turn deep research. Featuring 158\nexpert-annotated questions with complete evidence chains across 304 documents,\nM4DocBench tests capabilities that existing benchmarks cannot assess.\nExperiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter\nthan state-of-the-art baselines, validating that effective document research\nrequires not just better retrieval, but fundamentally deep parsing that\npreserve multimodal integrity and support iterative research. Our work\nestablishes a new paradigm for conducting deep research on multimodal document\ncollections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Research systems have revolutionized how LLMs solve complex questions\nthrough iterative reasoning and evidence gathering. However, current systems\nremain fundamentally constrained to textual web data, overlooking the vast\nknowledge embedded in multimodal documents Processing such documents demands\nsophisticated parsing to preserve visual semantics (figures, tables, charts,\nand equations), intelligent chunking to maintain structural coherence, and\nadaptive retrieval across modalities, which are capabilities absent in existing\nsystems. In response, we present Doc-Researcher, a unified system that bridges\nthis gap through three integrated components: (i) deep multimodal parsing that\npreserves layout structure and visual semantics while creating multi-granular\nrepresentations from chunk to document level, (ii) systematic retrieval\narchitecture supporting text-only, vision-only, and hybrid paradigms with\ndynamic granularity selection, and (iii) iterative multi-agent workflows that\ndecompose complex queries, progressively accumulate evidence, and synthesize\ncomprehensive answers across documents and modalities. To enable rigorous\nevaluation, we introduce M4DocBench, the first benchmark for Multi-modal,\nMulti-hop, Multi-document, and Multi-turn deep research. Featuring 158\nexpert-annotated questions with complete evidence chains across 304 documents,\nM4DocBench tests capabilities that existing benchmarks cannot assess.\nExperiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter\nthan state-of-the-art baselines, validating that effective document research\nrequires not just better retrieval, but fundamentally deep parsing that\npreserve multimodal integrity and support iterative research. Our work\nestablishes a new paradigm for conducting deep research on multimodal document\ncollections."
                },
                "authors": [
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Shurui Huang"
                    },
                    {
                        "name": "Fangda Ye"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Qu Yang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07612v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07612v3",
                "updated": "2025-10-24T16:07:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    7,
                    28,
                    4,
                    297,
                    0
                ],
                "published": "2024-07-10T12:50:44Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    50,
                    44,
                    2,
                    192,
                    0
                ],
                "title": "Teaching Transformers Causal Reasoning through Axiomatic Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Transformers Causal Reasoning through Axiomatic Training"
                },
                "summary": "For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since active interventions are costly, we study to what\nextent a system can learn causal reasoning from symbolic demonstrations of\ncausal axioms. Specifically, we present an axiomatic training method where the\nsystem learns from multiple demonstrations of a causal axiom (or rule), rather\nthan incorporating the axiom as an inductive bias or inferring it from data\nvalues. A key question is whether the system would learn to generalize from the\naxiom demonstrations to more complex scenarios. Our results, based on applying\naxiomatic training to learn the transitivity axiom and d-separation rule,\nindicate that such generalization is possible. To avoid data contamination\nissues, we start with a 67 million parameter transformer model and train it\nfrom scratch. On both tasks, we find that a model trained on linear causal\nchains (along with some noisy variations) can generalize well to complex\ngraphs, including longer causal chains, causal chains with reversed order, and\ngraphs with branching.To handle diverse text inputs, the same method is\nextended to finetune language models. Finetuning Llama-3-8B-Instruct model on\nour axiomatic data leads to significant gains on causal benchmarks such as\nCorr2Cause and CLEAR, in some cases providing state-of-the-art performance\nsurpassing GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since active interventions are costly, we study to what\nextent a system can learn causal reasoning from symbolic demonstrations of\ncausal axioms. Specifically, we present an axiomatic training method where the\nsystem learns from multiple demonstrations of a causal axiom (or rule), rather\nthan incorporating the axiom as an inductive bias or inferring it from data\nvalues. A key question is whether the system would learn to generalize from the\naxiom demonstrations to more complex scenarios. Our results, based on applying\naxiomatic training to learn the transitivity axiom and d-separation rule,\nindicate that such generalization is possible. To avoid data contamination\nissues, we start with a 67 million parameter transformer model and train it\nfrom scratch. On both tasks, we find that a model trained on linear causal\nchains (along with some noisy variations) can generalize well to complex\ngraphs, including longer causal chains, causal chains with reversed order, and\ngraphs with branching.To handle diverse text inputs, the same method is\nextended to finetune language models. Finetuning Llama-3-8B-Instruct model on\nour axiomatic data leads to significant gains on causal benchmarks such as\nCorr2Cause and CLEAR, in some cases providing state-of-the-art performance\nsurpassing GPT-4."
                },
                "authors": [
                    {
                        "name": "Aniket Vashishtha"
                    },
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Atharva Pandey"
                    },
                    {
                        "name": "Abbavaram Gowtham Reddy"
                    },
                    {
                        "name": "Kabir Ahuja"
                    },
                    {
                        "name": "Vineeth N Balasubramanian"
                    },
                    {
                        "name": "Amit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sharma"
                },
                "author": "Amit Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07612v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07612v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21598v1",
                "updated": "2025-10-24T16:02:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    2,
                    18,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:02:18Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    2,
                    18,
                    4,
                    297,
                    0
                ],
                "title": "Fisher meets Feynman: score-based variational inference with a product\n  of experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fisher meets Feynman: score-based variational inference with a product\n  of experts"
                },
                "summary": "We introduce a highly expressive yet distinctly tractable family for\nblack-box variational inference (BBVI). Each member of this family is a\nweighted product of experts (PoE), and each weighted expert in the product is\nproportional to a multivariate $t$-distribution. These products of experts can\nmodel distributions with skew, heavy tails, and multiple modes, but to use them\nfor BBVI, we must be able to sample from their densities. We show how to do\nthis by reformulating these products of experts as latent variable models with\nauxiliary Dirichlet random variables. These Dirichlet variables emerge from a\nFeynman identity, originally developed for loop integrals in quantum field\ntheory, that expresses the product of multiple fractions (or in our case,\n$t$-distributions) as an integral over the simplex. We leverage this simplicial\nlatent space to draw weighted samples from these products of experts -- samples\nwhich BBVI then uses to find the PoE that best approximates a target density.\nGiven a collection of experts, we derive an iterative procedure to optimize the\nexponents that determine their geometric weighting in the PoE. At each\niteration, this procedure minimizes a regularized Fisher divergence to match\nthe scores of the variational and target densities at a batch of samples drawn\nfrom the current approximation. This minimization reduces to a convex quadratic\nprogram, and we prove under general conditions that these updates converge\nexponentially fast to a near-optimal weighting of experts. We conclude by\nevaluating this approach on a variety of synthetic and real-world target\ndistributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a highly expressive yet distinctly tractable family for\nblack-box variational inference (BBVI). Each member of this family is a\nweighted product of experts (PoE), and each weighted expert in the product is\nproportional to a multivariate $t$-distribution. These products of experts can\nmodel distributions with skew, heavy tails, and multiple modes, but to use them\nfor BBVI, we must be able to sample from their densities. We show how to do\nthis by reformulating these products of experts as latent variable models with\nauxiliary Dirichlet random variables. These Dirichlet variables emerge from a\nFeynman identity, originally developed for loop integrals in quantum field\ntheory, that expresses the product of multiple fractions (or in our case,\n$t$-distributions) as an integral over the simplex. We leverage this simplicial\nlatent space to draw weighted samples from these products of experts -- samples\nwhich BBVI then uses to find the PoE that best approximates a target density.\nGiven a collection of experts, we derive an iterative procedure to optimize the\nexponents that determine their geometric weighting in the PoE. At each\niteration, this procedure minimizes a regularized Fisher divergence to match\nthe scores of the variational and target densities at a batch of samples drawn\nfrom the current approximation. This minimization reduces to a convex quadratic\nprogram, and we prove under general conditions that these updates converge\nexponentially fast to a near-optimal weighting of experts. We conclude by\nevaluating this approach on a variety of synthetic and real-world target\ndistributions."
                },
                "authors": [
                    {
                        "name": "Diana Cai"
                    },
                    {
                        "name": "Robert M. Gower"
                    },
                    {
                        "name": "David M. Blei"
                    },
                    {
                        "name": "Lawrence K. Saul"
                    }
                ],
                "author_detail": {
                    "name": "Lawrence K. Saul"
                },
                "author": "Lawrence K. Saul",
                "arxiv_comment": "27 pages, 11 figures. To appear in Advances in Neural Processing\n  Information Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21587v1",
                "updated": "2025-10-24T15:54:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    54,
                    14,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:54:14Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    54,
                    14,
                    4,
                    297,
                    0
                ],
                "title": "Resilient Radio Access Networks: AI and the Unknown Unknowns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resilient Radio Access Networks: AI and the Unknown Unknowns"
                },
                "summary": "5G networks offer exceptional reliability and availability, ensuring\nconsistent performance and user satisfaction. Yet they might still fail when\nconfronted with the unexpected. A resilient system is able to adapt to\nreal-world complexity, including operating conditions completely unanticipated\nduring system design. This makes resilience a vital attribute for communication\nsystems that must sustain service in scenarios where models are absent or too\nintricate to provide statistical guarantees. Such considerations indicate that\nartifical intelligence (AI) will play a major role in delivering resilience. In\nthis paper, we examine the challenges of designing AIs for resilient radio\naccess networks, especially with respect to unanticipated and rare disruptions.\nOur theoretical results indicate strong limitations of current statistical\nlearning methods for resilience and suggest connections to online learning and\ncausal inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5G networks offer exceptional reliability and availability, ensuring\nconsistent performance and user satisfaction. Yet they might still fail when\nconfronted with the unexpected. A resilient system is able to adapt to\nreal-world complexity, including operating conditions completely unanticipated\nduring system design. This makes resilience a vital attribute for communication\nsystems that must sustain service in scenarios where models are absent or too\nintricate to provide statistical guarantees. Such considerations indicate that\nartifical intelligence (AI) will play a major role in delivering resilience. In\nthis paper, we examine the challenges of designing AIs for resilient radio\naccess networks, especially with respect to unanticipated and rare disruptions.\nOur theoretical results indicate strong limitations of current statistical\nlearning methods for resilience and suggest connections to online learning and\ncausal inference."
                },
                "authors": [
                    {
                        "name": "Bho Matthiesen"
                    },
                    {
                        "name": "Armin Dekorsy"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "arxiv_comment": "Accepted for presentation at 2025 IEEE Globecom Workshop on\n  Resilience in Next-Generation Wireless Communication Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23794v2",
                "updated": "2025-10-24T15:52:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    52,
                    23,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-26T12:25:37Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    25,
                    37,
                    0,
                    146,
                    0
                ],
                "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via\n  Reinforcement Learning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG."
                },
                "authors": [
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Qi Luo"
                    },
                    {
                        "name": "Xiaonan Li"
                    },
                    {
                        "name": "Bufan Li"
                    },
                    {
                        "name": "Qinyuan Cheng"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Yining Zheng"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13192v2",
                "updated": "2025-10-24T15:51:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    51,
                    36,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-19T14:49:10Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    49,
                    10,
                    0,
                    139,
                    0
                ],
                "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term\n  Statistics"
                },
                "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters (0.1%)\nand orders of magnitude faster inference times. DynaMix outperforms TS\nfoundation models in terms of long-term statistics, and often also short-term\nforecasts, even on real-world time series, like traffic or weather data,\ntypically used for training and evaluating TS models, but not at all part of\nDynaMix' training corpus. We illustrate some of the failure modes of TS models\nfor DSR problems, and conclude that models built on DS principles may bear a\nhuge potential also for advancing the TS prediction field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters (0.1%)\nand orders of magnitude faster inference times. DynaMix outperforms TS\nfoundation models in terms of long-term statistics, and often also short-term\nforecasts, even on real-world time series, like traffic or weather data,\ntypically used for training and evaluating TS models, but not at all part of\nDynaMix' training corpus. We illustrate some of the failure modes of TS models\nfor DSR problems, and conclude that models built on DS principles may bear a\nhuge potential also for advancing the TS prediction field."
                },
                "authors": [
                    {
                        "name": "Christoph Jrgen Hemmer"
                    },
                    {
                        "name": "Daniel Durstewitz"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Durstewitz"
                },
                "author": "Daniel Durstewitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01930v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01930v3",
                "updated": "2025-10-24T15:47:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    47,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-02-04T02:03:19Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    3,
                    19,
                    1,
                    35,
                    0
                ],
                "title": "Robust LLM Alignment via Distributionally Robust Direct Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Alignment via Distributionally Robust Direct Preference\n  Optimization"
                },
                "summary": "A major challenge in aligning large language models (LLMs) with human\npreferences is the issue of distribution shift. LLM alignment algorithms rely\non static preference datasets, assuming that they accurately represent\nreal-world user preferences. However, user preferences vary significantly\nacross geographical regions, demographics, linguistic patterns, and evolving\ncultural trends. This preference distribution shift leads to catastrophic\nalignment failures in many real-world applications. We address this problem\nusing the principled framework of distributionally robust optimization, and\ndevelop two novel distributionally robust direct preference optimization (DPO)\nalgorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We\ncharacterize the sample complexity of learning the optimal policy parameters\nfor WDPO and KLDPO. Moreover, we propose scalable gradient descent-style\nlearning algorithms by developing suitable approximations for the challenging\nminimax loss functions of WDPO and KLDPO. Our empirical experiments using\nbenchmark data sets and LLMs demonstrate the superior performance of WDPO and\nKLDPO in substantially improving the alignment when there is a preference\ndistribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge in aligning large language models (LLMs) with human\npreferences is the issue of distribution shift. LLM alignment algorithms rely\non static preference datasets, assuming that they accurately represent\nreal-world user preferences. However, user preferences vary significantly\nacross geographical regions, demographics, linguistic patterns, and evolving\ncultural trends. This preference distribution shift leads to catastrophic\nalignment failures in many real-world applications. We address this problem\nusing the principled framework of distributionally robust optimization, and\ndevelop two novel distributionally robust direct preference optimization (DPO)\nalgorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We\ncharacterize the sample complexity of learning the optimal policy parameters\nfor WDPO and KLDPO. Moreover, we propose scalable gradient descent-style\nlearning algorithms by developing suitable approximations for the challenging\nminimax loss functions of WDPO and KLDPO. Our empirical experiments using\nbenchmark data sets and LLMs demonstrate the superior performance of WDPO and\nKLDPO in substantially improving the alignment when there is a preference\ndistribution shift."
                },
                "authors": [
                    {
                        "name": "Zaiyan Xu"
                    },
                    {
                        "name": "Sushil Vemuri"
                    },
                    {
                        "name": "Kishan Panaganti"
                    },
                    {
                        "name": "Dileep Kalathil"
                    },
                    {
                        "name": "Rahul Jain"
                    },
                    {
                        "name": "Deepak Ramachandran"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Ramachandran"
                },
                "author": "Deepak Ramachandran",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01930v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01930v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21575v1",
                "updated": "2025-10-24T15:43:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    43,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:43:42Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    43,
                    42,
                    4,
                    297,
                    0
                ],
                "title": "From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics\n  Understanding Benchmarks for Slovene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics\n  Understanding Benchmarks for Slovene"
                },
                "summary": "Large language models are demonstrating increasing capabilities, excelling at\nbenchmarks once considered very difficult. As their capabilities grow, there is\na need for more challenging evaluations that go beyond surface-level linguistic\ncompetence. Namely, language competence involves not only syntax and semantics\nbut also pragmatics, i.e., understanding situational meaning as shaped by\ncontext as well as linguistic and cultural norms. To contribute to this line of\nresearch, we introduce SloPragEval and SloPragMega, the first pragmatics\nunderstanding benchmarks for Slovene that contain altogether 405\nmultiple-choice questions. We discuss the difficulties of translation, describe\nthe campaign to establish a human baseline, and report pilot evaluations with\nLLMs. Our results indicate that current models have greatly improved in\nunderstanding nuanced language but may still fail to infer implied speaker\nmeaning in non-literal utterances, especially those that are culture-specific.\nWe also observe a significant gap between proprietary and open-source models.\nFinally, we argue that benchmarks targeting nuanced language understanding and\nknowledge of the target culture must be designed with care, preferably\nconstructed from native data, and validated with human responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are demonstrating increasing capabilities, excelling at\nbenchmarks once considered very difficult. As their capabilities grow, there is\na need for more challenging evaluations that go beyond surface-level linguistic\ncompetence. Namely, language competence involves not only syntax and semantics\nbut also pragmatics, i.e., understanding situational meaning as shaped by\ncontext as well as linguistic and cultural norms. To contribute to this line of\nresearch, we introduce SloPragEval and SloPragMega, the first pragmatics\nunderstanding benchmarks for Slovene that contain altogether 405\nmultiple-choice questions. We discuss the difficulties of translation, describe\nthe campaign to establish a human baseline, and report pilot evaluations with\nLLMs. Our results indicate that current models have greatly improved in\nunderstanding nuanced language but may still fail to infer implied speaker\nmeaning in non-literal utterances, especially those that are culture-specific.\nWe also observe a significant gap between proprietary and open-source models.\nFinally, we argue that benchmarks targeting nuanced language understanding and\nknowledge of the target culture must be designed with care, preferably\nconstructed from native data, and validated with human responses."
                },
                "authors": [
                    {
                        "name": "Mojca Brglez"
                    },
                    {
                        "name": "pela Vintar"
                    }
                ],
                "author_detail": {
                    "name": "pela Vintar"
                },
                "author": "pela Vintar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20171v2",
                "updated": "2025-10-24T15:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    39,
                    29,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T03:32:04Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    3,
                    32,
                    4,
                    3,
                    296,
                    0
                ],
                "title": "Collective Communication for 100k+ GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collective Communication for 100k+ GPUs"
                },
                "summary": "The increasing scale of large language models (LLMs) necessitates highly\nefficient collective communication frameworks, particularly as training\nworkloads extend to hundreds of thousands of GPUs. Traditional communication\nmethods face significant throughput and latency limitations at this scale,\nhindering both the development and deployment of state-of-the-art models. This\npaper presents the NCCLX collective communication framework, developed at Meta,\nengineered to optimize performance across the full LLM lifecycle, from the\nsynchronous demands of large-scale training to the low-latency requirements of\ninference. The framework is designed to support complex workloads on clusters\nexceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency\ndata exchange. Empirical evaluation on the Llama4 model demonstrates\nsubstantial improvements in communication efficiency. This research contributes\na robust solution for enabling the next generation of LLMs to operate at\nunprecedented scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale of large language models (LLMs) necessitates highly\nefficient collective communication frameworks, particularly as training\nworkloads extend to hundreds of thousands of GPUs. Traditional communication\nmethods face significant throughput and latency limitations at this scale,\nhindering both the development and deployment of state-of-the-art models. This\npaper presents the NCCLX collective communication framework, developed at Meta,\nengineered to optimize performance across the full LLM lifecycle, from the\nsynchronous demands of large-scale training to the low-latency requirements of\ninference. The framework is designed to support complex workloads on clusters\nexceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency\ndata exchange. Empirical evaluation on the Llama4 model demonstrates\nsubstantial improvements in communication efficiency. This research contributes\na robust solution for enabling the next generation of LLMs to operate at\nunprecedented scales."
                },
                "authors": [
                    {
                        "name": "Min Si"
                    },
                    {
                        "name": "Pavan Balaji"
                    },
                    {
                        "name": "Yongzhou Chen"
                    },
                    {
                        "name": "Ching-Hsiang Chu"
                    },
                    {
                        "name": "Adi Gangidi"
                    },
                    {
                        "name": "Saif Hasan"
                    },
                    {
                        "name": "Subodh Iyengar"
                    },
                    {
                        "name": "Dan Johnson"
                    },
                    {
                        "name": "Bingzhe Liu"
                    },
                    {
                        "name": "Regina Ren"
                    },
                    {
                        "name": "Ashmitha Jeevaraj Shetty"
                    },
                    {
                        "name": "Greg Steinbrecher"
                    },
                    {
                        "name": "Yulun Wang"
                    },
                    {
                        "name": "Bruce Wu"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Mingran Yang"
                    },
                    {
                        "name": "Kenny Yu"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Cen Zhao"
                    },
                    {
                        "name": "Wes Bland"
                    },
                    {
                        "name": "Denis Boyda"
                    },
                    {
                        "name": "Suman Gumudavelli"
                    },
                    {
                        "name": "Prashanth Kannan"
                    },
                    {
                        "name": "Cristian Lumezanu"
                    },
                    {
                        "name": "Rui Miao"
                    },
                    {
                        "name": "Zhe Qu"
                    },
                    {
                        "name": "Venkat Ramesh"
                    },
                    {
                        "name": "Maxim Samoylov"
                    },
                    {
                        "name": "Jan Seidel"
                    },
                    {
                        "name": "Srikanth Sundaresan"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Qiye Tan"
                    },
                    {
                        "name": "Shuqiang Zhang"
                    },
                    {
                        "name": "Yimeng Zhao"
                    },
                    {
                        "name": "Shengbao Zheng"
                    },
                    {
                        "name": "Art Zhu"
                    },
                    {
                        "name": "Hongyi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Hongyi Zeng"
                },
                "author": "Hongyi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17853v2",
                "updated": "2025-10-24T15:36:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    36,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-15T00:32:26Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    0,
                    32,
                    26,
                    2,
                    288,
                    0
                ],
                "title": "CiteGuard: Faithful Citation Attribution for LLMs via\n  Retrieval-Augmented Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CiteGuard: Faithful Citation Attribution for LLMs via\n  Retrieval-Augmented Validation"
                },
                "summary": "Large Language Models (LLMs) have emerged as promising assistants for\nscientific writing. However, there have been concerns regarding the quality and\nreliability of the generated text, one of which is the citation accuracy and\nfaithfulness. While most recent work relies on methods such as LLM-as-a-Judge,\nthe reliability of LLM-as-a-Judge alone is also in doubt. In this work, we\nreframe citation evaluation as a problem of citation attribution alignment,\nwhich is assessing whether LLM-generated citations match those a human author\nwould include for the same text. We propose CiteGuard, a retrieval-aware agent\nframework designed to provide more faithful grounding for citation validation.\nCiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4%\naccuracy on the CiteME benchmark, on par with human-level performance (69.7%).\nIt also enables the identification of alternative but valid citations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as promising assistants for\nscientific writing. However, there have been concerns regarding the quality and\nreliability of the generated text, one of which is the citation accuracy and\nfaithfulness. While most recent work relies on methods such as LLM-as-a-Judge,\nthe reliability of LLM-as-a-Judge alone is also in doubt. In this work, we\nreframe citation evaluation as a problem of citation attribution alignment,\nwhich is assessing whether LLM-generated citations match those a human author\nwould include for the same text. We propose CiteGuard, a retrieval-aware agent\nframework designed to provide more faithful grounding for citation validation.\nCiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4%\naccuracy on the CiteME benchmark, on par with human-level performance (69.7%).\nIt also enables the identification of alternative but valid citations."
                },
                "authors": [
                    {
                        "name": "Yee Man Choi"
                    },
                    {
                        "name": "Xuehang Guo"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Qingyun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Wang"
                },
                "author": "Qingyun Wang",
                "arxiv_comment": "https://kathcym.github.io/CiteGuard_Page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20176v2",
                "updated": "2025-10-24T15:36:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    36,
                    31,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T03:51:17Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    3,
                    51,
                    17,
                    3,
                    296,
                    0
                ],
                "title": "Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table\n  Understanding"
                },
                "summary": "Understanding and reasoning over tables is a critical capability for many\nreal-world applications. Large language models (LLMs) have shown promise on\nthis task, but current approaches remain limited. Fine-tuning based methods\nstrengthen language reasoning; yet they are prone to arithmetic errors and\nhallucination. In contrast, tool-based methods enable precise table\nmanipulation but rely on rigid schemas and lack semantic understanding. These\ncomplementary drawbacks highlight the need for approaches that integrate robust\nreasoning with reliable table processing. In this work, we propose\nMixture-of-Minds, a multi-agent framework that decomposes table reasoning into\nthree specialized roles: planning, coding, and answering. This design enables\neach agent to focus on a specific aspect of the task while leveraging code\nexecution for precise table manipulation. Building on this workflow, we\nintroduce a self-improvement training framework that employs Monte Carlo Tree\nSearch (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents\nwith reinforcement learning (RL). Extensive experiments show that\nMixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and\nsurpassing OpenAI-o4-mini-high. These results demonstrate the promise of\ncombining structured multi-agent workflows with RL to advance table\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and reasoning over tables is a critical capability for many\nreal-world applications. Large language models (LLMs) have shown promise on\nthis task, but current approaches remain limited. Fine-tuning based methods\nstrengthen language reasoning; yet they are prone to arithmetic errors and\nhallucination. In contrast, tool-based methods enable precise table\nmanipulation but rely on rigid schemas and lack semantic understanding. These\ncomplementary drawbacks highlight the need for approaches that integrate robust\nreasoning with reliable table processing. In this work, we propose\nMixture-of-Minds, a multi-agent framework that decomposes table reasoning into\nthree specialized roles: planning, coding, and answering. This design enables\neach agent to focus on a specific aspect of the task while leveraging code\nexecution for precise table manipulation. Building on this workflow, we\nintroduce a self-improvement training framework that employs Monte Carlo Tree\nSearch (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents\nwith reinforcement learning (RL). Extensive experiments show that\nMixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and\nsurpassing OpenAI-o4-mini-high. These results demonstrate the promise of\ncombining structured multi-agent workflows with RL to advance table\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Mingrui Zhang"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Mingyi Wang"
                    },
                    {
                        "name": "Qiao Liu"
                    },
                    {
                        "name": "Qifei Wang"
                    },
                    {
                        "name": "Jiayi Liu"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Serena Li"
                    },
                    {
                        "name": "Weiwei Li"
                    },
                    {
                        "name": "Mingze Gao"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Xiangjun Fan"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Lizhu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lizhu Zhang"
                },
                "author": "Lizhu Zhang",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21561v1",
                "updated": "2025-10-24T15:20:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    20,
                    40,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:20:40Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    20,
                    40,
                    4,
                    297,
                    0
                ],
                "title": "Are the LLMs Capable of Maintaining at Least the Language Genus?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are the LLMs Capable of Maintaining at Least the Language Genus?"
                },
                "summary": "Large Language Models (LLMs) display notable variation in multilingual\nbehavior, yet the role of genealogical language structure in shaping this\nvariation remains underexplored. In this paper, we investigate whether LLMs\nexhibit sensitivity to linguistic genera by extending prior analyses on the\nMultiQ dataset. We first check if models prefer to switch to genealogically\nrelated languages when prompt language fidelity is not maintained. Next, we\ninvestigate whether knowledge consistency is better preserved within than\nacross genera. We show that genus-level effects are present but strongly\nconditioned by training resource availability. We further observe distinct\nmultilingual strategies across LLMs families. Our findings suggest that LLMs\nencode aspects of genus-level structure, but training data imbalances remain\nthe primary factor shaping their multilingual performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) display notable variation in multilingual\nbehavior, yet the role of genealogical language structure in shaping this\nvariation remains underexplored. In this paper, we investigate whether LLMs\nexhibit sensitivity to linguistic genera by extending prior analyses on the\nMultiQ dataset. We first check if models prefer to switch to genealogically\nrelated languages when prompt language fidelity is not maintained. Next, we\ninvestigate whether knowledge consistency is better preserved within than\nacross genera. We show that genus-level effects are present but strongly\nconditioned by training resource availability. We further observe distinct\nmultilingual strategies across LLMs families. Our findings suggest that LLMs\nencode aspects of genus-level structure, but training data imbalances remain\nthe primary factor shaping their multilingual performance."
                },
                "authors": [
                    {
                        "name": "Sandra Mitrovi"
                    },
                    {
                        "name": "David Kletz"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    },
                    {
                        "name": "Fabio Rinaldi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Rinaldi"
                },
                "author": "Fabio Rinaldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14550v2",
                "updated": "2025-10-24T15:16:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    16,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-01-24T14:53:42Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    53,
                    42,
                    4,
                    24,
                    0
                ],
                "title": "Bean: A Language for Backward Error Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bean: A Language for Backward Error Analysis"
                },
                "summary": "Backward error analysis offers a method for assessing the quality of\nnumerical programs in the presence of floating-point rounding errors. However,\ntechniques from the numerical analysis literature for quantifying backward\nerror require substantial human effort, and there are currently no tools or\nautomated methods for statically deriving sound backward error bounds. To\naddress this gap, we propose Bean, a typed first-order programming language\ndesigned to express quantitative bounds on backward error. Bean's type system\ncombines a graded coeffect system with strict linearity to soundly track the\nflow of backward error through programs. We prove the soundness of our system\nusing a novel categorical semantics, where every Bean program denotes a triple\nof related transformations that together satisfy a backward error guarantee.\n  To illustrate Bean's potential as a practical tool for automated backward\nerror analysis, we implement a variety of standard algorithms from numerical\nlinear algebra in Bean, establishing fine-grained backward error bounds via\ntyping in a compositional style. We also develop a prototype implementation of\nBean that infers backward error bounds automatically. Our evaluation shows that\nthese inferred bounds match worst-case theoretical relative backward error\nbounds from the literature, underscoring Bean's utility in validating a key\nproperty of numerical programs: numerical stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward error analysis offers a method for assessing the quality of\nnumerical programs in the presence of floating-point rounding errors. However,\ntechniques from the numerical analysis literature for quantifying backward\nerror require substantial human effort, and there are currently no tools or\nautomated methods for statically deriving sound backward error bounds. To\naddress this gap, we propose Bean, a typed first-order programming language\ndesigned to express quantitative bounds on backward error. Bean's type system\ncombines a graded coeffect system with strict linearity to soundly track the\nflow of backward error through programs. We prove the soundness of our system\nusing a novel categorical semantics, where every Bean program denotes a triple\nof related transformations that together satisfy a backward error guarantee.\n  To illustrate Bean's potential as a practical tool for automated backward\nerror analysis, we implement a variety of standard algorithms from numerical\nlinear algebra in Bean, establishing fine-grained backward error bounds via\ntyping in a compositional style. We also develop a prototype implementation of\nBean that infers backward error bounds automatically. Our evaluation shows that\nthese inferred bounds match worst-case theoretical relative backward error\nbounds from the literature, underscoring Bean's utility in validating a key\nproperty of numerical programs: numerical stability."
                },
                "authors": [
                    {
                        "name": "Ariel E. Kellison"
                    },
                    {
                        "name": "Laura Zielinski"
                    },
                    {
                        "name": "David Bindel"
                    },
                    {
                        "name": "Justin Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Justin Hsu"
                },
                "author": "Justin Hsu",
                "arxiv_doi": "10.1145/3729324",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729324",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Michael Hicks (Ed.). 2025. Proc. ACM Program. Lang. 9, PLDI (June\n  2025)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21557v1",
                "updated": "2025-10-24T15:14:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    14,
                    14,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:14:14Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    14,
                    14,
                    4,
                    297,
                    0
                ],
                "title": "Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware\n  Meta-Verification and Trustworthy Reasoning with Structured Facts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware\n  Meta-Verification and Trustworthy Reasoning with Structured Facts"
                },
                "summary": "Long-horizon reasoning in LLM-based agents often fails not from generative\nweakness but from insufficient verification of intermediate reasoning. Co-Sight\naddresses this challenge by turning reasoning into a falsifiable and auditable\nprocess through two complementary mechanisms: Conflict-Aware Meta-Verification\n(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV\nreformulates verification as conflict identification and targeted\nfalsification, allocating computation only to disagreement hotspots among\nexpert agents rather than to full reasoning chains. This bounds verification\ncost to the number of inconsistencies and improves efficiency and reliability.\nTRSF continuously organizes, validates, and synchronizes evidence across agents\nthrough a structured facts module. By maintaining verified, traceable, and\nauditable knowledge, it ensures that all reasoning is grounded in consistent,\nsource-verified information and supports transparent verification throughout\nthe reasoning process. Together, TRSF and CAMV form a closed verification loop,\nwhere TRSF supplies structured facts and CAMV selectively falsifies or\nreinforces them, yielding transparent and trustworthy reasoning. Empirically,\nCo-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last\nExam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies\nconfirm that the synergy between structured factual grounding and\nconflict-aware verification drives these improvements. Co-Sight thus offers a\nscalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code\nis available at\nhttps://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon reasoning in LLM-based agents often fails not from generative\nweakness but from insufficient verification of intermediate reasoning. Co-Sight\naddresses this challenge by turning reasoning into a falsifiable and auditable\nprocess through two complementary mechanisms: Conflict-Aware Meta-Verification\n(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV\nreformulates verification as conflict identification and targeted\nfalsification, allocating computation only to disagreement hotspots among\nexpert agents rather than to full reasoning chains. This bounds verification\ncost to the number of inconsistencies and improves efficiency and reliability.\nTRSF continuously organizes, validates, and synchronizes evidence across agents\nthrough a structured facts module. By maintaining verified, traceable, and\nauditable knowledge, it ensures that all reasoning is grounded in consistent,\nsource-verified information and supports transparent verification throughout\nthe reasoning process. Together, TRSF and CAMV form a closed verification loop,\nwhere TRSF supplies structured facts and CAMV selectively falsifies or\nreinforces them, yielding transparent and trustworthy reasoning. Empirically,\nCo-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last\nExam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies\nconfirm that the synergy between structured factual grounding and\nconflict-aware verification drives these improvements. Co-Sight thus offers a\nscalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code\nis available at\nhttps://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks."
                },
                "authors": [
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Ji Lu"
                    },
                    {
                        "name": "Shiqing Jiang"
                    },
                    {
                        "name": "Chenxiang Zhu"
                    },
                    {
                        "name": "Li Xie"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Haoran Chen"
                    },
                    {
                        "name": "Yurui Zhu"
                    },
                    {
                        "name": "Yongsheng Du"
                    },
                    {
                        "name": "Yanqin Gao"
                    },
                    {
                        "name": "Lingjun Huang"
                    },
                    {
                        "name": "Baoli Wang"
                    },
                    {
                        "name": "Fang Tan"
                    },
                    {
                        "name": "Peng Zou"
                    }
                ],
                "author_detail": {
                    "name": "Peng Zou"
                },
                "author": "Peng Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08221v2",
                "updated": "2025-10-24T15:10:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    10,
                    11,
                    4,
                    297,
                    0
                ],
                "published": "2025-08-11T17:39:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    39,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning"
                },
                "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO."
                },
                "authors": [
                    {
                        "name": "Zihe Liu"
                    },
                    {
                        "name": "Jiashun Liu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ling Pan"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Shengyi Huang"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "26 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21551v1",
                "updated": "2025-10-24T15:09:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    9,
                    9,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:09:09Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    9,
                    9,
                    4,
                    297,
                    0
                ],
                "title": "Interpretable Multimodal Zero-Shot ECG Diagnosis via Structured Clinical\n  Knowledge Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Multimodal Zero-Shot ECG Diagnosis via Structured Clinical\n  Knowledge Alignment"
                },
                "summary": "Electrocardiogram (ECG) interpretation is essential for cardiovascular\ndisease diagnosis, but current automated systems often struggle with\ntransparency and generalization to unseen conditions. To address this, we\nintroduce ZETA, a zero-shot multimodal framework designed for interpretable ECG\ndiagnosis aligned with clinical workflows. ZETA uniquely compares ECG signals\nagainst structured positive and negative clinical observations, which are\ncurated through an LLM-assisted, expert-validated process, thereby mimicking\ndifferential diagnosis. Our approach leverages a pre-trained multimodal model\nto align ECG and text embeddings without disease-specific fine-tuning.\nEmpirical evaluations demonstrate ZETA's competitive zero-shot classification\nperformance and, importantly, provide qualitative and quantitative evidence of\nenhanced interpretability, grounding predictions in specific, clinically\nrelevant positive and negative diagnostic features. ZETA underscores the\npotential of aligning ECG analysis with structured clinical knowledge for\nbuilding more transparent, generalizable, and trustworthy AI diagnostic\nsystems. We will release the curated observation dataset and code to facilitate\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram (ECG) interpretation is essential for cardiovascular\ndisease diagnosis, but current automated systems often struggle with\ntransparency and generalization to unseen conditions. To address this, we\nintroduce ZETA, a zero-shot multimodal framework designed for interpretable ECG\ndiagnosis aligned with clinical workflows. ZETA uniquely compares ECG signals\nagainst structured positive and negative clinical observations, which are\ncurated through an LLM-assisted, expert-validated process, thereby mimicking\ndifferential diagnosis. Our approach leverages a pre-trained multimodal model\nto align ECG and text embeddings without disease-specific fine-tuning.\nEmpirical evaluations demonstrate ZETA's competitive zero-shot classification\nperformance and, importantly, provide qualitative and quantitative evidence of\nenhanced interpretability, grounding predictions in specific, clinically\nrelevant positive and negative diagnostic features. ZETA underscores the\npotential of aligning ECG analysis with structured clinical knowledge for\nbuilding more transparent, generalizable, and trustworthy AI diagnostic\nsystems. We will release the curated observation dataset and code to facilitate\nfuture research."
                },
                "authors": [
                    {
                        "name": "Jialu Tang"
                    },
                    {
                        "name": "Hung Manh Pham"
                    },
                    {
                        "name": "Ignace De Lathauwer"
                    },
                    {
                        "name": "Henk S. Schipper"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Dong Ma"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21539v1",
                "updated": "2025-10-24T15:02:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    2,
                    43,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:02:43Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    2,
                    43,
                    4,
                    297,
                    0
                ],
                "title": "Direct test for critical slowing down before Dansgaard-Oeschger events\n  via the volcanic climate response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct test for critical slowing down before Dansgaard-Oeschger events\n  via the volcanic climate response"
                },
                "summary": "It is tested whether past abrupt climate changes support the validity of\nstatistical early-warning signals (EWS) as predictor of future climate tipping\npoints. EWS are expected increases in amplitude and correlation of fluctuations\ndriven by noise. This is a symptom of critical slowing down (CSD), where a\nsystem's recovery from an external perturbation becomes slower as a tipping\npoint (represented by a bifurcation) is approached. EWS are a simple, indirect\nmeasure of CSD, but subject to assumptions on the noise process and measurement\nstationarity that are hard to verify. In this work the existence of CSD before\nthe Dansgaard-Oeschger (DO) events of the last glacial period is directly\ntested by inferring the climate's recovery from large volcanic eruptions. By\naveraging over hundreds of eruptions, a well-defined, stationary perturbation\nis constructed and the average climate response is measured by eight ice core\nproxies. As the abrupt DO warming transitions are approached, the climate\nresponse to eruptions remains the same, indicating no CSD. For the abrupt DO\ncooling transitions, however, some key proxies show evidence of larger climate\nresponse and slower recovery as the transitions are approached. By comparison,\nalmost all proxies show statistical EWS before cooling and warming transitions,\nbut with only weak confidence for the warming transitions. There is thus\nqualitative agreement of CSD and EWS, in that the evidence for bifurcation\nprecursors is larger for the cooling transitions. However, the discrepancy that\nmany proxies show EWS but no direct CSD (and vice versa) highlights that\nstatistical EWS in individual observables need to be interpreted with care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is tested whether past abrupt climate changes support the validity of\nstatistical early-warning signals (EWS) as predictor of future climate tipping\npoints. EWS are expected increases in amplitude and correlation of fluctuations\ndriven by noise. This is a symptom of critical slowing down (CSD), where a\nsystem's recovery from an external perturbation becomes slower as a tipping\npoint (represented by a bifurcation) is approached. EWS are a simple, indirect\nmeasure of CSD, but subject to assumptions on the noise process and measurement\nstationarity that are hard to verify. In this work the existence of CSD before\nthe Dansgaard-Oeschger (DO) events of the last glacial period is directly\ntested by inferring the climate's recovery from large volcanic eruptions. By\naveraging over hundreds of eruptions, a well-defined, stationary perturbation\nis constructed and the average climate response is measured by eight ice core\nproxies. As the abrupt DO warming transitions are approached, the climate\nresponse to eruptions remains the same, indicating no CSD. For the abrupt DO\ncooling transitions, however, some key proxies show evidence of larger climate\nresponse and slower recovery as the transitions are approached. By comparison,\nalmost all proxies show statistical EWS before cooling and warming transitions,\nbut with only weak confidence for the warming transitions. There is thus\nqualitative agreement of CSD and EWS, in that the evidence for bifurcation\nprecursors is larger for the cooling transitions. However, the discrepancy that\nmany proxies show EWS but no direct CSD (and vice versa) highlights that\nstatistical EWS in individual observables need to be interpreted with care."
                },
                "authors": [
                    {
                        "name": "Johannes Lohmann"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Lohmann"
                },
                "author": "Johannes Lohmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21538v1",
                "updated": "2025-10-24T15:02:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    2,
                    1,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:02:01Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    2,
                    1,
                    4,
                    297,
                    0
                ],
                "title": "InterpDetect: Interpretable Signals for Detecting Hallucinations in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterpDetect: Interpretable Signals for Detecting Hallucinations in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) integrates external knowledge to\nmitigate hallucinations, yet models often generate outputs inconsistent with\nretrieved content. Accurate hallucination detection requires disentangling the\ncontributions of external context and parametric knowledge, which prior methods\ntypically conflate. We investigate the mechanisms underlying RAG hallucinations\nand find they arise when later-layer FFN modules disproportionately inject\nparametric knowledge into the residual stream. To address this, we explore a\nmechanistic detection approach based on external context scores and parametric\nknowledge scores. Using Qwen3-0.6b, we compute these scores across layers and\nattention heads and train regression-based classifiers to predict\nhallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,\nGPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,\nclassifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,\ndemonstrating the potential of proxy-model evaluation. Our results highlight\nmechanistic signals as efficient, generalizable predictors for hallucination\ndetection in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) integrates external knowledge to\nmitigate hallucinations, yet models often generate outputs inconsistent with\nretrieved content. Accurate hallucination detection requires disentangling the\ncontributions of external context and parametric knowledge, which prior methods\ntypically conflate. We investigate the mechanisms underlying RAG hallucinations\nand find they arise when later-layer FFN modules disproportionately inject\nparametric knowledge into the residual stream. To address this, we explore a\nmechanistic detection approach based on external context scores and parametric\nknowledge scores. Using Qwen3-0.6b, we compute these scores across layers and\nattention heads and train regression-based classifiers to predict\nhallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,\nGPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,\nclassifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,\ndemonstrating the potential of proxy-model evaluation. Our results highlight\nmechanistic signals as efficient, generalizable predictors for hallucination\ndetection in RAG systems."
                },
                "authors": [
                    {
                        "name": "Likun Tan"
                    },
                    {
                        "name": "Kuan-Wei Huang"
                    },
                    {
                        "name": "Joy Shi"
                    },
                    {
                        "name": "Kevin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Wu"
                },
                "author": "Kevin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21536v1",
                "updated": "2025-10-24T15:01:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    1,
                    18,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:01:18Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    1,
                    18,
                    4,
                    297,
                    0
                ],
                "title": "AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive\n  Refinement for Drivable-Area Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive\n  Refinement for Drivable-Area Segmentation"
                },
                "summary": "Free space ground segmentation is essential to navigate robots and autonomous\nvehicles, recognize drivable zones, and traverse efficiently. Fine-grained\nfeatures remain challenging for existing segmentation models, particularly for\nrobots in indoor and structured environments. These difficulties arise from\nineffective multi-scale processing, suboptimal boundary refinement, and limited\nfeature representation. In order to overcome these limitations, we propose\nAttention-Guided Upsampling with Residual Boundary-Assistive Refinement\n(AURASeg), a ground-plane semantic segmentation model that maintains high\nsegmentation accuracy while improving border precision. Our method uses\nCSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for\naccurate edge delineation and an Attention Progressive Upsampling Decoder\n(APUD) for strong feature integration. We also incorporate a lightweight Atrous\nSpatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context\nextraction without compromising real-time performance. The proposed model beats\nbenchmark segmentation architectures in mIoU and F1 metrics when tested on the\nGround Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor\ndataset. Our approach achieves an improvement in mean Intersection-over-Union\n(mIoU) of +1.26% and segmentation precision of +1.65% compared to\nstate-of-the-art models. These results show that our technique is feasible for\nautonomous perception in both indoor and outdoor environments, enabling precise\nborder refinement with minimal effect on inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free space ground segmentation is essential to navigate robots and autonomous\nvehicles, recognize drivable zones, and traverse efficiently. Fine-grained\nfeatures remain challenging for existing segmentation models, particularly for\nrobots in indoor and structured environments. These difficulties arise from\nineffective multi-scale processing, suboptimal boundary refinement, and limited\nfeature representation. In order to overcome these limitations, we propose\nAttention-Guided Upsampling with Residual Boundary-Assistive Refinement\n(AURASeg), a ground-plane semantic segmentation model that maintains high\nsegmentation accuracy while improving border precision. Our method uses\nCSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for\naccurate edge delineation and an Attention Progressive Upsampling Decoder\n(APUD) for strong feature integration. We also incorporate a lightweight Atrous\nSpatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context\nextraction without compromising real-time performance. The proposed model beats\nbenchmark segmentation architectures in mIoU and F1 metrics when tested on the\nGround Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor\ndataset. Our approach achieves an improvement in mean Intersection-over-Union\n(mIoU) of +1.26% and segmentation precision of +1.65% compared to\nstate-of-the-art models. These results show that our technique is feasible for\nautonomous perception in both indoor and outdoor environments, enabling precise\nborder refinement with minimal effect on inference speed."
                },
                "authors": [
                    {
                        "name": "Narendhiran Vijayakumar"
                    },
                    {
                        "name": "Sridevi. M"
                    }
                ],
                "author_detail": {
                    "name": "Sridevi. M"
                },
                "author": "Sridevi. M",
                "arxiv_comment": "10 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21533v1",
                "updated": "2025-10-24T15:00:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    0,
                    5,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:00:05Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    0,
                    5,
                    4,
                    297,
                    0
                ],
                "title": "Hardware-Efficient Accurate 4-bit Multiplier for Xilinx 7 Series FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Accurate 4-bit Multiplier for Xilinx 7 Series FPGAs"
                },
                "summary": "As IoT and edge inference proliferate,there is a growing need to\nsimultaneously optimize area and delay in lookup-table (LUT)-based multipliers\nthat implement large numbers of low-bitwidth operations in parallel. This paper\nproposes a hardwareefficientaccurate 4-bit multiplier design for AMD Xilinx\n7-series FPGAs using only 11 LUTs and two CARRY4 blocks. By reorganizing the\nlogic functions mapped to the LUTs, the proposed method reduces the LUT count\nby one compared with the prior 12-LUT design while also shortening the critical\npath. Evaluation confirms that the circuit attains minimal resource usage and a\ncritical-path delay of 2.750 ns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As IoT and edge inference proliferate,there is a growing need to\nsimultaneously optimize area and delay in lookup-table (LUT)-based multipliers\nthat implement large numbers of low-bitwidth operations in parallel. This paper\nproposes a hardwareefficientaccurate 4-bit multiplier design for AMD Xilinx\n7-series FPGAs using only 11 LUTs and two CARRY4 blocks. By reorganizing the\nlogic functions mapped to the LUTs, the proposed method reduces the LUT count\nby one compared with the prior 12-LUT design while also shortening the critical\npath. Evaluation confirms that the circuit attains minimal resource usage and a\ncritical-path delay of 2.750 ns."
                },
                "authors": [
                    {
                        "name": "Misaki Kida"
                    },
                    {
                        "name": "Shimpei Sato"
                    }
                ],
                "author_detail": {
                    "name": "Shimpei Sato"
                },
                "author": "Shimpei Sato",
                "arxiv_comment": "5 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20075v2",
                "updated": "2025-10-24T14:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    59,
                    45,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T23:16:50Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    16,
                    50,
                    2,
                    295,
                    0
                ],
                "title": "LLMs can hide text in other text of the same length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can hide text in other text of the same length"
                },
                "summary": "A meaningful text can be hidden inside another, completely different yet\nstill coherent and plausible, text of the same length. For example, a tweet\ncontaining a harsh political critique could be embedded in a tweet that\ncelebrates the same political leader, or an ordinary product review could\nconceal a secret manuscript. This uncanny state of affairs is now possible\nthanks to Large Language Models, and in this paper we present a simple and\nefficient protocol to achieve it. We show that even modest 8-billion-parameter\nopen-source LLMs are sufficient to obtain high-quality results, and a message\nas long as this abstract can be encoded and decoded locally on a laptop in\nseconds. The existence of such a protocol demonstrates a radical decoupling of\ntext from authorial intent, further eroding trust in written communication,\nalready shaken by the rise of LLM chatbots. We illustrate this with a concrete\nscenario: a company could covertly deploy an unfiltered LLM by encoding its\nanswers within the compliant responses of a safe model. This possibility raises\nurgent questions for AI safety and challenges our understanding of what it\nmeans for a Large Language Model to know something.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A meaningful text can be hidden inside another, completely different yet\nstill coherent and plausible, text of the same length. For example, a tweet\ncontaining a harsh political critique could be embedded in a tweet that\ncelebrates the same political leader, or an ordinary product review could\nconceal a secret manuscript. This uncanny state of affairs is now possible\nthanks to Large Language Models, and in this paper we present a simple and\nefficient protocol to achieve it. We show that even modest 8-billion-parameter\nopen-source LLMs are sufficient to obtain high-quality results, and a message\nas long as this abstract can be encoded and decoded locally on a laptop in\nseconds. The existence of such a protocol demonstrates a radical decoupling of\ntext from authorial intent, further eroding trust in written communication,\nalready shaken by the rise of LLM chatbots. We illustrate this with a concrete\nscenario: a company could covertly deploy an unfiltered LLM by encoding its\nanswers within the compliant responses of a safe model. This possibility raises\nurgent questions for AI safety and challenges our understanding of what it\nmeans for a Large Language Model to know something."
                },
                "authors": [
                    {
                        "name": "Antonio Norelli"
                    },
                    {
                        "name": "Michael Bronstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bronstein"
                },
                "author": "Michael Bronstein",
                "arxiv_comment": "21 pages, main paper 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15188v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15188v3",
                "updated": "2025-10-24T14:56:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    56,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-18T17:48:21Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    48,
                    21,
                    3,
                    261,
                    0
                ],
                "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and\n  Rejective Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and\n  Rejective Fine-tuning"
                },
                "summary": "Autoregressive (AR) language models generate text one token at a time, which\nlimits their inference speed. Diffusion-based language models offer a promising\nalternative, as they can decode multiple tokens in parallel. However, we\nidentify a key bottleneck in current diffusion LMs: the long decoding-window\nproblem, where tokens generated far from the input context often become\nirrelevant or repetitive. Previous solutions like semi-autoregressive address\nthis issue by splitting windows into blocks (sacrificing bidirectionality), but\nwe find that this also leads to time-interval expansion problem, sacrificing\nthe speed. Therefore, semi-AR eliminates the main advantages of diffusion\nmodels. To overcome this, we propose Convolutional decoding (Conv), a\nnormalization-based method that narrows the decoding window without hard\nsegmentation, leading to better fluency and flexibility. Additionally, we\nintroduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme\nthat better aligns tokens at positions far from context. Our methods achieve\nstate-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval)\namong diffusion LM baselines, with significantly lower step size than previous\nworks, demonstrating both speed and quality improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) language models generate text one token at a time, which\nlimits their inference speed. Diffusion-based language models offer a promising\nalternative, as they can decode multiple tokens in parallel. However, we\nidentify a key bottleneck in current diffusion LMs: the long decoding-window\nproblem, where tokens generated far from the input context often become\nirrelevant or repetitive. Previous solutions like semi-autoregressive address\nthis issue by splitting windows into blocks (sacrificing bidirectionality), but\nwe find that this also leads to time-interval expansion problem, sacrificing\nthe speed. Therefore, semi-AR eliminates the main advantages of diffusion\nmodels. To overcome this, we propose Convolutional decoding (Conv), a\nnormalization-based method that narrows the decoding window without hard\nsegmentation, leading to better fluency and flexibility. Additionally, we\nintroduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme\nthat better aligns tokens at positions far from context. Our methods achieve\nstate-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval)\namong diffusion LM baselines, with significantly lower step size than previous\nworks, demonstrating both speed and quality improvements."
                },
                "authors": [
                    {
                        "name": "Yeongbin Seo"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "NeurIPS 2025 spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15188v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15188v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19225v2",
                "updated": "2025-10-24T14:49:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    49,
                    45,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:19:37Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    19,
                    37,
                    2,
                    295,
                    0
                ],
                "title": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient\n  Reinforcement Learning on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient\n  Reinforcement Learning on LLMs"
                },
                "summary": "Reinforcement learning (RL) has become essential for unlocking advanced\nreasoning capabilities in large language models (LLMs). RL workflows involve\ninterleaving rollout and training stages with fundamentally different resource\nrequirements. Rollout typically dominates overall execution time, yet scales\nefficiently through multiple independent instances. In contrast, training\nrequires tightly-coupled GPUs with full-mesh communication. Existing RL\nframeworks fall into two categories: co-located and disaggregated\narchitectures. Co-located ones fail to address this resource tension by forcing\nboth stages to share the same GPUs. Disaggregated architectures, without\nmodifications of well-established RL algorithms, suffer from resource\nunder-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances\non public clouds and spare capacity in production clusters, present significant\ncost-saving opportunities for accelerating RL workflows, if efficiently\nharvested for rollout.\n  In this paper, we present RLBoost, a systematic solution for cost-efficient\nRL training that harvests preemptible GPU resources. Our key insight is that\nrollout's stateless and embarrassingly parallel nature aligns perfectly with\npreemptible and often fragmented resources. To efficiently utilize these\nresources despite frequent and unpredictable availability changes, RLBoost\nadopts a hybrid architecture with three key techniques: (1) adaptive rollout\noffload to dynamically adjust workloads on the reserved (on-demand) cluster,\n(2) pull-based weight transfer that quickly provisions newly available\ninstances, and (3) token-level response collection and migration for efficient\npreemption handling and continuous load balancing. Extensive experiments show\nRLBoost increases training throughput by 1.51x-1.97x while improving cost\nefficiency by 28%-49% compared to using only on-demand GPU resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become essential for unlocking advanced\nreasoning capabilities in large language models (LLMs). RL workflows involve\ninterleaving rollout and training stages with fundamentally different resource\nrequirements. Rollout typically dominates overall execution time, yet scales\nefficiently through multiple independent instances. In contrast, training\nrequires tightly-coupled GPUs with full-mesh communication. Existing RL\nframeworks fall into two categories: co-located and disaggregated\narchitectures. Co-located ones fail to address this resource tension by forcing\nboth stages to share the same GPUs. Disaggregated architectures, without\nmodifications of well-established RL algorithms, suffer from resource\nunder-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances\non public clouds and spare capacity in production clusters, present significant\ncost-saving opportunities for accelerating RL workflows, if efficiently\nharvested for rollout.\n  In this paper, we present RLBoost, a systematic solution for cost-efficient\nRL training that harvests preemptible GPU resources. Our key insight is that\nrollout's stateless and embarrassingly parallel nature aligns perfectly with\npreemptible and often fragmented resources. To efficiently utilize these\nresources despite frequent and unpredictable availability changes, RLBoost\nadopts a hybrid architecture with three key techniques: (1) adaptive rollout\noffload to dynamically adjust workloads on the reserved (on-demand) cluster,\n(2) pull-based weight transfer that quickly provisions newly available\ninstances, and (3) token-level response collection and migration for efficient\npreemption handling and continuous load balancing. Extensive experiments show\nRLBoost increases training throughput by 1.51x-1.97x while improving cost\nefficiency by 28%-49% compared to using only on-demand GPU resources."
                },
                "authors": [
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Juncheng Gu"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Z. Morley Mao"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21524v1",
                "updated": "2025-10-24T14:48:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    48,
                    10,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T14:48:10Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    48,
                    10,
                    4,
                    297,
                    0
                ],
                "title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law"
                },
                "summary": "Large language models (LLMs) are increasingly deployed as agents in various\ncontexts by providing tools at their disposal. However, LLM agents can exhibit\nunpredictable behaviors, including taking undesirable and/or unsafe actions. In\norder to measure the latent propensity of LLM agents for taking illegal actions\nunder an EU legislative context, we introduce EU-Agent-Bench, a verifiable\nhuman-curated benchmark that evaluates an agent's alignment with EU legal norms\nin situations where benign user inputs could lead to unlawful actions. Our\nbenchmark spans scenarios across several categories, including data protection,\nbias/discrimination, and scientific integrity, with each user request allowing\nfor both compliant and non-compliant execution of the requested actions.\nComparing the model's function calls against a rubric exhaustively supported by\ncitations of the relevant legislature, we evaluate the legal compliance of\nfrontier LLMs, and furthermore investigate the compliance effect of providing\nthe relevant legislative excerpts in the agent's system prompt along with\nexplicit instructions to comply. We release a public preview set for the\nresearch community, while holding out a private test set to prevent data\ncontamination in evaluating upcoming models. We encourage future work extending\nagentic safety benchmarks to different legal jurisdictions and to multi-turn\nand multilingual interactions. We release our code on\n\\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed as agents in various\ncontexts by providing tools at their disposal. However, LLM agents can exhibit\nunpredictable behaviors, including taking undesirable and/or unsafe actions. In\norder to measure the latent propensity of LLM agents for taking illegal actions\nunder an EU legislative context, we introduce EU-Agent-Bench, a verifiable\nhuman-curated benchmark that evaluates an agent's alignment with EU legal norms\nin situations where benign user inputs could lead to unlawful actions. Our\nbenchmark spans scenarios across several categories, including data protection,\nbias/discrimination, and scientific integrity, with each user request allowing\nfor both compliant and non-compliant execution of the requested actions.\nComparing the model's function calls against a rubric exhaustively supported by\ncitations of the relevant legislature, we evaluate the legal compliance of\nfrontier LLMs, and furthermore investigate the compliance effect of providing\nthe relevant legislative excerpts in the agent's system prompt along with\nexplicit instructions to comply. We release a public preview set for the\nresearch community, while holding out a private test set to prevent data\ncontamination in evaluating upcoming models. We encourage future work extending\nagentic safety benchmarks to different legal jurisdictions and to multi-turn\nand multilingual interactions. We release our code on\n\\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}."
                },
                "authors": [
                    {
                        "name": "Ilija Lichkovski"
                    },
                    {
                        "name": "Alexander Mller"
                    },
                    {
                        "name": "Mariam Ibrahim"
                    },
                    {
                        "name": "Tiwai Mhundwa"
                    }
                ],
                "author_detail": {
                    "name": "Tiwai Mhundwa"
                },
                "author": "Tiwai Mhundwa",
                "arxiv_comment": "Accepted at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09702v3",
                "updated": "2025-10-24T14:48:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    48,
                    8,
                    4,
                    297,
                    0
                ],
                "published": "2025-04-13T19:35:43Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    19,
                    35,
                    43,
                    6,
                    103,
                    0
                ],
                "title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research\n  Challenges?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLRC-Bench: Can Language Agents Solve Machine Learning Research\n  Challenges?"
                },
                "summary": "We introduce MLRC-Bench, a benchmark designed to quantify how effectively\nlanguage agents can tackle challenging Machine Learning (ML) Research\nCompetitions, with a focus on open research problems that demand novel\nmethodologies. Unlike prior work, e.g., AI Scientist, which evaluates the\nend-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the\nkey steps of proposing and implementing novel research methods and evaluates\nthem with rigorous protocol and objective metrics. Our curated suite of 7\ncompetition tasks reveals significant challenges for LLM agents. Even the\nbest-performing tested agent (gemini-exp-1206 under MLAB) closes only 9.3% of\nthe gap between baseline and top human participant scores. Furthermore, our\nanalysis reveals a misalignment between the LLM-judged innovation and actual\nperformance on cutting-edge ML research problems. MLRC-Bench is a dynamic\nbenchmark, designed to grow with new ML competitions and encourage rigorous,\nobjective evaluations of AI research capabilities. Our leaderboard and code are\navailable at: https://huggingface.co/spaces/launch/MLRC_Bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MLRC-Bench, a benchmark designed to quantify how effectively\nlanguage agents can tackle challenging Machine Learning (ML) Research\nCompetitions, with a focus on open research problems that demand novel\nmethodologies. Unlike prior work, e.g., AI Scientist, which evaluates the\nend-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the\nkey steps of proposing and implementing novel research methods and evaluates\nthem with rigorous protocol and objective metrics. Our curated suite of 7\ncompetition tasks reveals significant challenges for LLM agents. Even the\nbest-performing tested agent (gemini-exp-1206 under MLAB) closes only 9.3% of\nthe gap between baseline and top human participant scores. Furthermore, our\nanalysis reveals a misalignment between the LLM-judged innovation and actual\nperformance on cutting-edge ML research problems. MLRC-Bench is a dynamic\nbenchmark, designed to grow with new ML competitions and encourage rigorous,\nobjective evaluations of AI research capabilities. Our leaderboard and code are\navailable at: https://huggingface.co/spaces/launch/MLRC_Bench"
                },
                "authors": [
                    {
                        "name": "Yunxiang Zhang"
                    },
                    {
                        "name": "Muhammad Khalifa"
                    },
                    {
                        "name": "Shitanshu Bhushan"
                    },
                    {
                        "name": "Grant D Murphy"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Jaekyeom Kim"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Honglak Lee"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "arxiv_comment": "NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04103v2",
                "updated": "2025-10-24T14:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    45,
                    40,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-05T17:12:33Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    17,
                    12,
                    33,
                    5,
                    186,
                    0
                ],
                "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Train Your LLM Web Agent: A Statistical Diagnosis"
                },
                "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models."
                },
                "authors": [
                    {
                        "name": "Dheeraj Vattikonda"
                    },
                    {
                        "name": "Santhoshi Ravichandran"
                    },
                    {
                        "name": "Emiliano Penaloza"
                    },
                    {
                        "name": "Hadi Nekoei"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Thibault Le Sellier de Chezelles"
                    },
                    {
                        "name": "Nicolas Gontier"
                    },
                    {
                        "name": "Miguel Muoz-Mrmol"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Stefania Raimondo"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Laurent Charlin"
                    },
                    {
                        "name": "Alexandre Pich"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Massimo Caccia"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Caccia"
                },
                "author": "Massimo Caccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21521v1",
                "updated": "2025-10-24T14:43:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    43,
                    9,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T14:43:09Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    43,
                    9,
                    4,
                    297,
                    0
                ],
                "title": "Synergy between CSST and third-generation gravitational-wave detectors:\n  Inferring cosmological parameters using cross-correlation of dark sirens and\n  galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergy between CSST and third-generation gravitational-wave detectors:\n  Inferring cosmological parameters using cross-correlation of dark sirens and\n  galaxies"
                },
                "summary": "Gravitational-wave (GW) events are generally believed to originate in\ngalaxies and can thus serve, like galaxies, as tracers of the universe's\nlarge-scale structure. In GW observations, waveform analysis provides direct\nmeasurements of luminosity distances; however, the redshifts of GW sources\ncannot be determined due to the mass-redshift degeneracy. By cross-correlating\nGW events with galaxies, one can establish a correspondence between luminosity\ndistance and redshift shells, enabling cosmological inference. In this work, we\nexplore the scientific potential of cross-correlating GW sources detected by\nthird-generation (3G) ground-based GW detectors with the photometric redshift\nsurvey of the China Space Station Survey Telescope (CSST). We find that the\nconstraint precisions of the Hubble constant and the matter density parameter\ncan reach $1.04\\%$ and $2.04\\%$, respectively. The GW clustering bias\nparameters $A_{\\rm GW}$ and $\\gamma$ can be constrained to $1.52\\%$ and\n$4.67\\%$, respectively. These results highlight the significant potential of\nthe synergy between CSST and 3G ground-based GW detectors in constraining\ncosmological models and probing GW source formation channels using\ncross-correlation of dark sirens and galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational-wave (GW) events are generally believed to originate in\ngalaxies and can thus serve, like galaxies, as tracers of the universe's\nlarge-scale structure. In GW observations, waveform analysis provides direct\nmeasurements of luminosity distances; however, the redshifts of GW sources\ncannot be determined due to the mass-redshift degeneracy. By cross-correlating\nGW events with galaxies, one can establish a correspondence between luminosity\ndistance and redshift shells, enabling cosmological inference. In this work, we\nexplore the scientific potential of cross-correlating GW sources detected by\nthird-generation (3G) ground-based GW detectors with the photometric redshift\nsurvey of the China Space Station Survey Telescope (CSST). We find that the\nconstraint precisions of the Hubble constant and the matter density parameter\ncan reach $1.04\\%$ and $2.04\\%$, respectively. The GW clustering bias\nparameters $A_{\\rm GW}$ and $\\gamma$ can be constrained to $1.52\\%$ and\n$4.67\\%$, respectively. These results highlight the significant potential of\nthe synergy between CSST and 3G ground-based GW detectors in constraining\ncosmological models and probing GW source formation channels using\ncross-correlation of dark sirens and galaxies."
                },
                "authors": [
                    {
                        "name": "Ya-Nan Du"
                    },
                    {
                        "name": "Ji-Yu Song"
                    },
                    {
                        "name": "Yichao Li"
                    },
                    {
                        "name": "Shang-Jie Jin"
                    },
                    {
                        "name": "Ling-Feng Wang"
                    },
                    {
                        "name": "Jing-Fei Zhang"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21513v1",
                "updated": "2025-10-24T14:39:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    39,
                    23,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T14:39:23Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    39,
                    23,
                    4,
                    297,
                    0
                ],
                "title": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair"
                },
                "summary": "Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems.\n  To address this gap, we empirically compare ten individual LLMs from five\nfamilies, and three ensembles of these LLMs across three software engineering\nbenchmarks covering code generation and program repair. We assess the\ncomplementarity between models and the performance gap between the best\nindividual model and the ensembles. Next, we evaluate various selection\nheuristics to identify correct solutions from an ensemble's candidate pool.\n  We find that the theoretical upperbound for an ensemble's performance can be\n83% above the best single model. Our results show that consensus-based\nstrategies for selecting solutions fall into a \"popularity trap,\" amplifying\ncommon but incorrect outputs. In contrast, a diversity-based strategy realizes\nup to 95% of this theoretical potential, and proves effective even in small\ntwo-model ensembles, enabling a cost-efficient way to enhance performance by\nleveraging multiple LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems.\n  To address this gap, we empirically compare ten individual LLMs from five\nfamilies, and three ensembles of these LLMs across three software engineering\nbenchmarks covering code generation and program repair. We assess the\ncomplementarity between models and the performance gap between the best\nindividual model and the ensembles. Next, we evaluate various selection\nheuristics to identify correct solutions from an ensemble's candidate pool.\n  We find that the theoretical upperbound for an ensemble's performance can be\n83% above the best single model. Our results show that consensus-based\nstrategies for selecting solutions fall into a \"popularity trap,\" amplifying\ncommon but incorrect outputs. In contrast, a diversity-based strategy realizes\nup to 95% of this theoretical potential, and proves effective even in small\ntwo-model ensembles, enabling a cost-efficient way to enhance performance by\nleveraging multiple LLMs."
                },
                "authors": [
                    {
                        "name": "Fernando Vallecillos Ruiz"
                    },
                    {
                        "name": "Max Hort"
                    },
                    {
                        "name": "Leon Moonen"
                    }
                ],
                "author_detail": {
                    "name": "Leon Moonen"
                },
                "author": "Leon Moonen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21508v1",
                "updated": "2025-10-24T14:36:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    36,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T14:36:35Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    36,
                    35,
                    4,
                    297,
                    0
                ],
                "title": "Actionable Cybersecurity Notifications for Smart Homes: A User Study on\n  the Role of Length and Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Actionable Cybersecurity Notifications for Smart Homes: A User Study on\n  the Role of Length and Complexity"
                },
                "summary": "The proliferation of smart home devices has increased convenience but also\nintroduced cybersecurity risks for everyday users, as many devices lack robust\nsecurity features. Intrusion Detection Systems are a prominent approach to\ndetecting cybersecurity threats. However, their alerts often use technical\nterms and require users to interpret them correctly, which is challenging for a\ntypical smart home user. Large Language Models can bridge this gap by\ntranslating IDS alerts into actionable security notifications. However, it has\nnot yet been clear what an actionable cybersecurity notification should look\nlike. In this paper, we conduct an experimental online user study with 130\nparticipants to examine how the length and complexity of LLM-generated\nnotifications affect user likability, understandability, and motivation to act.\nOur results show that intermediate-complexity notifications are the most\neffective across all user groups, regardless of their technological\nproficiency. Across the board, users rated beginner-level messages as more\neffective when they were longer, while expert-level messages were rated\nmarginally more effective when they were shorter. These findings provide\ninsights for designing security notifications that are both actionable and\nbroadly accessible to smart home users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of smart home devices has increased convenience but also\nintroduced cybersecurity risks for everyday users, as many devices lack robust\nsecurity features. Intrusion Detection Systems are a prominent approach to\ndetecting cybersecurity threats. However, their alerts often use technical\nterms and require users to interpret them correctly, which is challenging for a\ntypical smart home user. Large Language Models can bridge this gap by\ntranslating IDS alerts into actionable security notifications. However, it has\nnot yet been clear what an actionable cybersecurity notification should look\nlike. In this paper, we conduct an experimental online user study with 130\nparticipants to examine how the length and complexity of LLM-generated\nnotifications affect user likability, understandability, and motivation to act.\nOur results show that intermediate-complexity notifications are the most\neffective across all user groups, regardless of their technological\nproficiency. Across the board, users rated beginner-level messages as more\neffective when they were longer, while expert-level messages were rated\nmarginally more effective when they were shorter. These findings provide\ninsights for designing security notifications that are both actionable and\nbroadly accessible to smart home users."
                },
                "authors": [
                    {
                        "name": "Victor Jttner"
                    },
                    {
                        "name": "Charlotte S. Lffler"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "arxiv_doi": "10.1007/978-3-032-07989-3_19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-07989-3_19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.21508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version of the article has been accepted for publication, but is\n  not the Version of Record and does not reflect post-acceptance improvements,\n  or any corrections. The Version of Record is available online at:\n  https://doi.org/10.1007/978-3-032-07989-3_19",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17196v2",
                "updated": "2025-10-24T14:34:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    34,
                    28,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-22T18:05:16Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    18,
                    5,
                    16,
                    3,
                    142,
                    0
                ],
                "title": "Shape it Up! Restoring LLM Safety during Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shape it Up! Restoring LLM Safety during Finetuning"
                },
                "summary": "Finetuning large language models (LLMs) enables user-specific customization\nbut introduces critical safety risks: even a few harmful examples can\ncompromise safety alignment. A common mitigation strategy is to update the\nmodel more strongly on examples deemed safe, while downweighting or excluding\nthose flagged as unsafe. However, because safety context can shift within a\nsingle example, updating the model equally on both harmful and harmless parts\nof a response is suboptimal-a coarse treatment we term static safety shaping.\nIn contrast, we propose dynamic safety shaping (DSS), a framework that uses\nfine-grained safety signals to reinforce learning from safe segments of a\nresponse while suppressing unsafe content. To enable such fine-grained control\nduring finetuning, we introduce a key insight: guardrail models, traditionally\nused for filtering, can be repurposed to evaluate partial responses, tracking\nhow safety risk evolves throughout the response, segment by segment. This leads\nto the Safety Trajectory Assessment of Response (STAR), a token-level signal\nthat enables shaping to operate dynamically over the training sequence.\nBuilding on this, we present STAR-DSS, guided by STAR scores, that robustly\nmitigates finetuning risks and delivers substantial safety improvements across\ndiverse threats, datasets, and model families-all without compromising\ncapability on intended tasks. We encourage future safety research to build on\ndynamic shaping principles for stronger mitigation against evolving finetuning\nrisks. Our code is publicly available at https://github.com/poloclub/star-dss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning large language models (LLMs) enables user-specific customization\nbut introduces critical safety risks: even a few harmful examples can\ncompromise safety alignment. A common mitigation strategy is to update the\nmodel more strongly on examples deemed safe, while downweighting or excluding\nthose flagged as unsafe. However, because safety context can shift within a\nsingle example, updating the model equally on both harmful and harmless parts\nof a response is suboptimal-a coarse treatment we term static safety shaping.\nIn contrast, we propose dynamic safety shaping (DSS), a framework that uses\nfine-grained safety signals to reinforce learning from safe segments of a\nresponse while suppressing unsafe content. To enable such fine-grained control\nduring finetuning, we introduce a key insight: guardrail models, traditionally\nused for filtering, can be repurposed to evaluate partial responses, tracking\nhow safety risk evolves throughout the response, segment by segment. This leads\nto the Safety Trajectory Assessment of Response (STAR), a token-level signal\nthat enables shaping to operate dynamically over the training sequence.\nBuilding on this, we present STAR-DSS, guided by STAR scores, that robustly\nmitigates finetuning risks and delivers substantial safety improvements across\ndiverse threats, datasets, and model families-all without compromising\ncapability on intended tasks. We encourage future safety research to build on\ndynamic shaping principles for stronger mitigation against evolving finetuning\nrisks. Our code is publicly available at https://github.com/poloclub/star-dss."
                },
                "authors": [
                    {
                        "name": "ShengYun Peng"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Jianfeng Chi"
                    },
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Duen Horng Chau"
                    }
                ],
                "author_detail": {
                    "name": "Duen Horng Chau"
                },
                "author": "Duen Horng Chau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15144v2",
                "updated": "2025-10-24T14:23:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    23,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-16T21:03:54Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    3,
                    54,
                    3,
                    289,
                    0
                ],
                "title": "HugAgent: Evaluating LLMs in Simulating Individual-Level Human Reasoning\n  on Open-Ended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HugAgent: Evaluating LLMs in Simulating Individual-Level Human Reasoning\n  on Open-Ended Tasks"
                },
                "summary": "Simulating human reasoning in open-ended tasks has been a long-standing\naspiration in AI and cognitive science. While large language models now\napproximate human responses at scale, they remain tuned to population-level\nconsensus, often erasing the individuality of reasoning styles and belief\ntrajectories. To advance the vision of more human-like reasoning in machines,\nwe introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for\naverage-to-individual reasoning adaptation. The task is to predict how a\nspecific person would reason and update their beliefs in novel scenarios, given\npartial evidence of their past views. HugAgent adopts a dual-track design: a\nsynthetic track for scale and systematic stress tests, and a human track for\necologically valid, \"out-loud\" reasoning data. This design enables scalable,\nreproducible evaluation of intra-agent fidelity: whether models can capture not\njust what people believe, but how their reasoning evolves. Experiments with\nstate-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent\nas the first extensible benchmark for aligning machine reasoning with the\nindividuality of human thought. Our benchmark and chatbot are open-sourced as\nHugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking\n(https://anonymous.4open.science/r/trace-your-thinking).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating human reasoning in open-ended tasks has been a long-standing\naspiration in AI and cognitive science. While large language models now\napproximate human responses at scale, they remain tuned to population-level\nconsensus, often erasing the individuality of reasoning styles and belief\ntrajectories. To advance the vision of more human-like reasoning in machines,\nwe introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for\naverage-to-individual reasoning adaptation. The task is to predict how a\nspecific person would reason and update their beliefs in novel scenarios, given\npartial evidence of their past views. HugAgent adopts a dual-track design: a\nsynthetic track for scale and systematic stress tests, and a human track for\necologically valid, \"out-loud\" reasoning data. This design enables scalable,\nreproducible evaluation of intra-agent fidelity: whether models can capture not\njust what people believe, but how their reasoning evolves. Experiments with\nstate-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent\nas the first extensible benchmark for aligning machine reasoning with the\nindividuality of human thought. Our benchmark and chatbot are open-sourced as\nHugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking\n(https://anonymous.4open.science/r/trace-your-thinking)."
                },
                "authors": [
                    {
                        "name": "Chance Jiajie Li"
                    },
                    {
                        "name": "Zhenze Mo"
                    },
                    {
                        "name": "Yuhan Tang"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Kaiya Ivy Zhao"
                    },
                    {
                        "name": "Yulu Gan"
                    },
                    {
                        "name": "Jie Fan"
                    },
                    {
                        "name": "Jiangbo Yu"
                    },
                    {
                        "name": "Hang Jiang"
                    },
                    {
                        "name": "Paul Pu Liang"
                    },
                    {
                        "name": "Jinhua Zhao"
                    },
                    {
                        "name": "Luis Alberto Alonso Pastor"
                    },
                    {
                        "name": "Kent Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kent Larson"
                },
                "author": "Kent Larson",
                "arxiv_comment": "To appear in NeurIPS 2025 Workshop on Bridging Language, Agent, and\n  World Models (LAW)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11887v3",
                "updated": "2025-10-24T14:06:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    6,
                    15,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-13T15:36:22Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    22,
                    4,
                    164,
                    0
                ],
                "title": "Cascaded Language Models for Cost-effective Human-AI Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascaded Language Models for Cost-effective Human-AI Decision-Making"
                },
                "summary": "A challenge in human-AI decision-making is to balance three factors: the\ncorrectness of predictions, the cost of knowledge and reasoning complexity, and\nthe confidence about whether to abstain from automated answers or escalate to\nhuman experts. In this work, we present a cascaded LLM decision framework that\nadaptively delegates tasks across multiple tiers of expertise -- a base model\nfor initial candidate answers, a more capable and knowledgeable (but costlier)\nlarge model, and a human expert for when the model cascade abstains. Our method\nproceeds in two stages. First, a deferral policy determines whether to accept\nthe base model's answer or regenerate it with the large model based on the\nconfidence score. Second, an abstention policy decides whether the cascade\nmodel response is sufficiently certain or requires human intervention.\nMoreover, to overcome static policies and accommodate changing task difficulty,\nwe incorporate an online learning mechanism which uses human feedback. We\ndemonstrate this approach to general question-answering (ARC-Easy,\nARC-Challenge, and MMLU) and medical question-answering (MedQA and MedMCQA).\nOur results demonstrate that our cascaded strategy outperforms single-model\nbaselines in most cases, achieving higher accuracy while reducing costs and\nproviding a principled approach to handling abstentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A challenge in human-AI decision-making is to balance three factors: the\ncorrectness of predictions, the cost of knowledge and reasoning complexity, and\nthe confidence about whether to abstain from automated answers or escalate to\nhuman experts. In this work, we present a cascaded LLM decision framework that\nadaptively delegates tasks across multiple tiers of expertise -- a base model\nfor initial candidate answers, a more capable and knowledgeable (but costlier)\nlarge model, and a human expert for when the model cascade abstains. Our method\nproceeds in two stages. First, a deferral policy determines whether to accept\nthe base model's answer or regenerate it with the large model based on the\nconfidence score. Second, an abstention policy decides whether the cascade\nmodel response is sufficiently certain or requires human intervention.\nMoreover, to overcome static policies and accommodate changing task difficulty,\nwe incorporate an online learning mechanism which uses human feedback. We\ndemonstrate this approach to general question-answering (ARC-Easy,\nARC-Challenge, and MMLU) and medical question-answering (MedQA and MedMCQA).\nOur results demonstrate that our cascaded strategy outperforms single-model\nbaselines in most cases, achieving higher accuracy while reducing costs and\nproviding a principled approach to handling abstentions."
                },
                "authors": [
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20905v2",
                "updated": "2025-10-24T14:05:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    5,
                    13,
                    4,
                    297,
                    0
                ],
                "published": "2024-05-31T15:16:48Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    15,
                    16,
                    48,
                    4,
                    152,
                    0
                ],
                "title": "VENI, VINDy, VICI: a generative reduced-order modeling framework with\n  uncertainty quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VENI, VINDy, VICI: a generative reduced-order modeling framework with\n  uncertainty quantification"
                },
                "summary": "The simulation of many complex phenomena in engineering and science requires\nsolving expensive, high-dimensional systems of partial differential equations\n(PDEs). To circumvent this, reduced-order models (ROMs) have been developed to\nspeed up computations. However, when governing equations are unknown or\npartially known, typically ROMs lack interpretability and reliability of the\npredicted solutions.\n  In this work we present a data-driven, non-intrusive framework for building\nROMs where the latent variables and dynamics are identified in an interpretable\nmanner and uncertainty is quantified. Starting from a limited amount of\nhigh-dimensional, noisy data the proposed framework constructs an efficient ROM\nby leveraging variational autoencoders for dimensionality reduction along with\na newly introduced, variational version of sparse identification of nonlinear\ndynamics (SINDy), which we refer to as Variational Identification of Nonlinear\nDynamics (VINDy).\n  In detail, the method consists of Variational Encoding of Noisy Inputs (VENI)\nto identify the distribution of reduced coordinates. Simultaneously, we learn\nthe distribution of the coefficients of a pre-determined set of candidate\nfunctions by VINDy. Once trained offline, the identified model can be queried\nfor new parameter instances and new initial conditions to compute the\ncorresponding full-time solutions. The probabilistic setup enables uncertainty\nquantification as the online testing consists of Variational Inference\nnaturally providing Certainty Intervals (VICI). In this work we showcase the\neffectiveness of the newly proposed VINDy method in identifying interpretable\nand accurate dynamical system for the Roessler system with different noise\nintensities and sources. Then the performance of the overall method - named\nVENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics\nand fluid dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The simulation of many complex phenomena in engineering and science requires\nsolving expensive, high-dimensional systems of partial differential equations\n(PDEs). To circumvent this, reduced-order models (ROMs) have been developed to\nspeed up computations. However, when governing equations are unknown or\npartially known, typically ROMs lack interpretability and reliability of the\npredicted solutions.\n  In this work we present a data-driven, non-intrusive framework for building\nROMs where the latent variables and dynamics are identified in an interpretable\nmanner and uncertainty is quantified. Starting from a limited amount of\nhigh-dimensional, noisy data the proposed framework constructs an efficient ROM\nby leveraging variational autoencoders for dimensionality reduction along with\na newly introduced, variational version of sparse identification of nonlinear\ndynamics (SINDy), which we refer to as Variational Identification of Nonlinear\nDynamics (VINDy).\n  In detail, the method consists of Variational Encoding of Noisy Inputs (VENI)\nto identify the distribution of reduced coordinates. Simultaneously, we learn\nthe distribution of the coefficients of a pre-determined set of candidate\nfunctions by VINDy. Once trained offline, the identified model can be queried\nfor new parameter instances and new initial conditions to compute the\ncorresponding full-time solutions. The probabilistic setup enables uncertainty\nquantification as the online testing consists of Variational Inference\nnaturally providing Certainty Intervals (VICI). In this work we showcase the\neffectiveness of the newly proposed VINDy method in identifying interpretable\nand accurate dynamical system for the Roessler system with different noise\nintensities and sources. Then the performance of the overall method - named\nVENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics\nand fluid dynamics."
                },
                "authors": [
                    {
                        "name": "Paolo Conti"
                    },
                    {
                        "name": "Jonas Kneifl"
                    },
                    {
                        "name": "Andrea Manzoni"
                    },
                    {
                        "name": "Attilio Frangi"
                    },
                    {
                        "name": "Jrg Fehr"
                    },
                    {
                        "name": "Steven L. Brunton"
                    },
                    {
                        "name": "J. Nathan Kutz"
                    }
                ],
                "author_detail": {
                    "name": "J. Nathan Kutz"
                },
                "author": "J. Nathan Kutz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21473v1",
                "updated": "2025-10-24T13:57:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    57,
                    59,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:57:59Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    57,
                    59,
                    4,
                    297,
                    0
                ],
                "title": "MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward\n  Optimization"
                },
                "summary": "Recent advances in diffusion language models (DLMs) have presented a\npromising alternative to traditional autoregressive large language models\n(LLMs). However, DLMs still lag behind LLMs in reasoning performance,\nespecially as the number of denoising steps decreases. Our analysis reveals\nthat this shortcoming arises primarily from the independent generation of\nmasked tokens across denoising steps, which fails to capture the token\ncorrelation. In this paper, we define two types of token correlation:\nintra-sequence correlation and inter-sequence correlation, and demonstrate that\nenhancing these correlations improves reasoning performance. To this end, we\npropose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to\nconsider the token correlation during the denoising process. More specifically,\nour MRO approach leverages test-time scaling, reject sampling, and\nreinforcement learning to directly optimize the token correlation with multiple\nelaborate rewards. Additionally, we introduce group step and importance\nsampling strategies to mitigate reward variance and enhance sampling\nefficiency. Through extensive experiments, we demonstrate that MRO not only\nimproves reasoning performance but also achieves significant sampling speedups\nwhile maintaining high performance on reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion language models (DLMs) have presented a\npromising alternative to traditional autoregressive large language models\n(LLMs). However, DLMs still lag behind LLMs in reasoning performance,\nespecially as the number of denoising steps decreases. Our analysis reveals\nthat this shortcoming arises primarily from the independent generation of\nmasked tokens across denoising steps, which fails to capture the token\ncorrelation. In this paper, we define two types of token correlation:\nintra-sequence correlation and inter-sequence correlation, and demonstrate that\nenhancing these correlations improves reasoning performance. To this end, we\npropose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to\nconsider the token correlation during the denoising process. More specifically,\nour MRO approach leverages test-time scaling, reject sampling, and\nreinforcement learning to directly optimize the token correlation with multiple\nelaborate rewards. Additionally, we introduce group step and importance\nsampling strategies to mitigate reward variance and enhance sampling\nefficiency. Through extensive experiments, we demonstrate that MRO not only\nimproves reasoning performance but also achieves significant sampling speedups\nwhile maintaining high performance on reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Yang Gan"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Chi Hu"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Kai Song"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Chunliang Zhang"
                    },
                    {
                        "name": "Tongran Liu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Tong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xiao"
                },
                "author": "Tong Xiao",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01268v2",
                "updated": "2025-10-24T13:46:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    46,
                    9,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-29T10:04:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    10,
                    4,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical\n  Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical\n  Guarantees"
                },
                "summary": "We study the problem of determining whether a piece of text has been authored\nby a human or by a large language model (LLM). Existing state of the art\nlogits-based detectors make use of statistics derived from the log-probability\nof the observed text evaluated using the distribution function of a given\nsource LLM. However, relying solely on log probabilities can be sub-optimal. In\nresponse, we introduce AdaDetectGPT -- a novel classifier that adaptively\nlearns a witness function from training data to enhance the performance of\nlogits-based detectors. We provide statistical guarantees on its true positive\nrate, false positive rate, true negative rate and false negative rate.\nExtensive numerical studies show AdaDetectGPT nearly uniformly improves the\nstate-of-the-art method in various combination of datasets and LLMs, and the\nimprovement can reach up to 37\\%. A python implementation of our method is\navailable at https://github.com/Mamba413/AdaDetectGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of determining whether a piece of text has been authored\nby a human or by a large language model (LLM). Existing state of the art\nlogits-based detectors make use of statistics derived from the log-probability\nof the observed text evaluated using the distribution function of a given\nsource LLM. However, relying solely on log probabilities can be sub-optimal. In\nresponse, we introduce AdaDetectGPT -- a novel classifier that adaptively\nlearns a witness function from training data to enhance the performance of\nlogits-based detectors. We provide statistical guarantees on its true positive\nrate, false positive rate, true negative rate and false negative rate.\nExtensive numerical studies show AdaDetectGPT nearly uniformly improves the\nstate-of-the-art method in various combination of datasets and LLMs, and the\nimprovement can reach up to 37\\%. A python implementation of our method is\navailable at https://github.com/Mamba413/AdaDetectGPT."
                },
                "authors": [
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Jin Zhu"
                    },
                    {
                        "name": "Pingfan Su"
                    },
                    {
                        "name": "Kai Ye"
                    },
                    {
                        "name": "Ying Yang"
                    },
                    {
                        "name": "Shakeel A O B Gavioli-Akilagun"
                    },
                    {
                        "name": "Chengchun Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chengchun Shi"
                },
                "author": "Chengchun Shi",
                "arxiv_comment": "Accepted by NeurIPS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21460v1",
                "updated": "2025-10-24T13:43:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    43,
                    29,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:43:29Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    43,
                    29,
                    4,
                    297,
                    0
                ],
                "title": "Risk Management for Mitigating Benchmark Failure Modes: BenchRisk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Mitigating Benchmark Failure Modes: BenchRisk"
                },
                "summary": "Large language model (LLM) benchmarks inform LLM use decisions (e.g., \"is\nthis LLM safe to deploy for my use case and context?\"). However, benchmarks may\nbe rendered unreliable by various failure modes that impact benchmark bias,\nvariance, coverage, or people's capacity to understand benchmark evidence.\nUsing the National Institute of Standards and Technology's risk management\nprocess as a foundation, this research iteratively analyzed 26 popular\nbenchmarks, identifying 57 potential failure modes and 196 corresponding\nmitigation strategies. The mitigations reduce failure likelihood and/or\nseverity, providing a frame for evaluating \"benchmark risk,\" which is scored to\nprovide a metaevaluation benchmark: BenchRisk. Higher scores indicate that\nbenchmark users are less likely to reach an incorrect or unsupported conclusion\nabout an LLM. All 26 scored benchmarks present significant risk within one or\nmore of the five scored dimensions (comprehensiveness, intelligibility,\nconsistency, correctness, and longevity), which points to important open\nresearch directions for the field of LLM benchmarking. The BenchRisk workflow\nallows for comparison between benchmarks; as an open-source tool, it also\nfacilitates the identification and sharing of risks and their mitigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) benchmarks inform LLM use decisions (e.g., \"is\nthis LLM safe to deploy for my use case and context?\"). However, benchmarks may\nbe rendered unreliable by various failure modes that impact benchmark bias,\nvariance, coverage, or people's capacity to understand benchmark evidence.\nUsing the National Institute of Standards and Technology's risk management\nprocess as a foundation, this research iteratively analyzed 26 popular\nbenchmarks, identifying 57 potential failure modes and 196 corresponding\nmitigation strategies. The mitigations reduce failure likelihood and/or\nseverity, providing a frame for evaluating \"benchmark risk,\" which is scored to\nprovide a metaevaluation benchmark: BenchRisk. Higher scores indicate that\nbenchmark users are less likely to reach an incorrect or unsupported conclusion\nabout an LLM. All 26 scored benchmarks present significant risk within one or\nmore of the five scored dimensions (comprehensiveness, intelligibility,\nconsistency, correctness, and longevity), which points to important open\nresearch directions for the field of LLM benchmarking. The BenchRisk workflow\nallows for comparison between benchmarks; as an open-source tool, it also\nfacilitates the identification and sharing of risks and their mitigations."
                },
                "authors": [
                    {
                        "name": "Sean McGregor"
                    },
                    {
                        "name": "Victor Lu"
                    },
                    {
                        "name": "Vassil Tashev"
                    },
                    {
                        "name": "Armstrong Foundjem"
                    },
                    {
                        "name": "Aishwarya Ramasethu"
                    },
                    {
                        "name": "Sadegh AlMahdi Kazemi Zarkouei"
                    },
                    {
                        "name": "Chris Knotz"
                    },
                    {
                        "name": "Kongtao Chen"
                    },
                    {
                        "name": "Alicia Parrish"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Heather Frase"
                    }
                ],
                "author_detail": {
                    "name": "Heather Frase"
                },
                "author": "Heather Frase",
                "arxiv_comment": "19 pages, 7 figures, to be published in the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21459v1",
                "updated": "2025-10-24T13:41:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    41,
                    52,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:41:52Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    41,
                    52,
                    4,
                    297,
                    0
                ],
                "title": "SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM\n  Honeypots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM\n  Honeypots"
                },
                "summary": "Honeypots are decoy systems used for gathering valuable threat intelligence\nor diverting attackers away from production systems. Maximising attacker\nengagement is essential to their utility. However research has highlighted that\ncontext-awareness, such as the ability to respond to new attack types, systems\nand attacker agents, is necessary to increase engagement. Large Language Models\n(LLMs) have been shown as one approach to increase context awareness but suffer\nfrom several challenges including accuracy and timeliness of response time,\nhigh operational costs and data-protection issues due to cloud deployment. We\npropose the System-Based Attention Shell Honeypot (SBASH) framework which\nmanages data-protection issues through the use of lightweight local LLMs. We\ninvestigate the use of Retrieval Augmented Generation (RAG) supported LLMs and\nnon-RAG LLMs for Linux shell commands and evaluate them using several different\nmetrics such as response time differences, realism from human testers, and\nsimilarity to a real system calculated with Levenshtein distance, SBert, and\nBertScore. We show that RAG improves accuracy for untuned models while models\nthat have been tuned via a system prompt that tells the LLM to respond like a\nLinux system achieve without RAG a similar accuracy as untuned with RAG, while\nhaving a slightly lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honeypots are decoy systems used for gathering valuable threat intelligence\nor diverting attackers away from production systems. Maximising attacker\nengagement is essential to their utility. However research has highlighted that\ncontext-awareness, such as the ability to respond to new attack types, systems\nand attacker agents, is necessary to increase engagement. Large Language Models\n(LLMs) have been shown as one approach to increase context awareness but suffer\nfrom several challenges including accuracy and timeliness of response time,\nhigh operational costs and data-protection issues due to cloud deployment. We\npropose the System-Based Attention Shell Honeypot (SBASH) framework which\nmanages data-protection issues through the use of lightweight local LLMs. We\ninvestigate the use of Retrieval Augmented Generation (RAG) supported LLMs and\nnon-RAG LLMs for Linux shell commands and evaluate them using several different\nmetrics such as response time differences, realism from human testers, and\nsimilarity to a real system calculated with Levenshtein distance, SBert, and\nBertScore. We show that RAG improves accuracy for untuned models while models\nthat have been tuned via a system prompt that tells the LLM to respond like a\nLinux system achieve without RAG a similar accuracy as untuned with RAG, while\nhaving a slightly lower latency."
                },
                "authors": [
                    {
                        "name": "Adetayo Adebimpe"
                    },
                    {
                        "name": "Helmut Neukirchen"
                    },
                    {
                        "name": "Thomas Welsh"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Welsh"
                },
                "author": "Thomas Welsh",
                "arxiv_comment": "to be published in: The 3rd International Conference on Foundation\n  and Large Language Models (FLLM2025), IEEE, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; D.4.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14620v2",
                "updated": "2025-10-24T13:28:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    52,
                    4,
                    297,
                    0
                ],
                "published": "2025-04-20T13:58:20Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    13,
                    58,
                    20,
                    6,
                    110,
                    0
                ],
                "title": "A Hierarchical Framework for Measuring Scientific Paper Innovation via\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hierarchical Framework for Measuring Scientific Paper Innovation via\n  Large Language Models"
                },
                "summary": "Measuring scientific paper innovation is both important and challenging.\nExisting content-based methods often overlook the full-paper context, fail to\ncapture the full scope of innovation, and lack generalization. We propose\nHSPIM, a hierarchical and training-free framework based on large language\nmodels (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess\ninnovation. We segment the text by section titles and use zero-shot LLM\nprompting to implement section classification, question-answering (QA)\naugmentation, and weighted innovation scoring. The generated QA pair focuses on\nsection-level innovation and serves as additional context to improve the LLM\nscoring. For each chunk, the LLM outputs a novelty score and a confidence\nscore. We use confidence scores as weights to aggregate novelty scores into a\npaper-level innovation score. To further improve performance, we propose a\ntwo-layer question structure consisting of common and section-specific\nquestions, and apply a genetic algorithm to optimize the question-prompt\ncombinations. Furthermore, under the fine-grained structure of innovation, we\nextend HSPIM to an HSPIM$^+$ that generates novelty, contribution, and\nfeasibility scores with respective confidence scores. Comprehensive experiments\non scientific conference paper datasets show that HSPIM outperforms baseline\nmethods in effectiveness, generalization, and interpretability. Demo code is\navailable at https://github.com/Jasaxion/HSPIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring scientific paper innovation is both important and challenging.\nExisting content-based methods often overlook the full-paper context, fail to\ncapture the full scope of innovation, and lack generalization. We propose\nHSPIM, a hierarchical and training-free framework based on large language\nmodels (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess\ninnovation. We segment the text by section titles and use zero-shot LLM\nprompting to implement section classification, question-answering (QA)\naugmentation, and weighted innovation scoring. The generated QA pair focuses on\nsection-level innovation and serves as additional context to improve the LLM\nscoring. For each chunk, the LLM outputs a novelty score and a confidence\nscore. We use confidence scores as weights to aggregate novelty scores into a\npaper-level innovation score. To further improve performance, we propose a\ntwo-layer question structure consisting of common and section-specific\nquestions, and apply a genetic algorithm to optimize the question-prompt\ncombinations. Furthermore, under the fine-grained structure of innovation, we\nextend HSPIM to an HSPIM$^+$ that generates novelty, contribution, and\nfeasibility scores with respective confidence scores. Comprehensive experiments\non scientific conference paper datasets show that HSPIM outperforms baseline\nmethods in effectiveness, generalization, and interpretability. Demo code is\navailable at https://github.com/Jasaxion/HSPIM."
                },
                "authors": [
                    {
                        "name": "Hongming Tan"
                    },
                    {
                        "name": "Shaoxiong Zhan"
                    },
                    {
                        "name": "Fengwei Jia"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Wai Kin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Wai Kin Chan"
                },
                "author": "Wai Kin Chan",
                "arxiv_doi": "10.1016/j.ins.2025.122787",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ins.2025.122787",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21451v1",
                "updated": "2025-10-24T13:28:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    41,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:28:41Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    41,
                    4,
                    297,
                    0
                ],
                "title": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model\n  Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model\n  Components"
                },
                "summary": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "An Guo"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21449v1",
                "updated": "2025-10-24T13:28:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    29,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:28:29Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    29,
                    4,
                    297,
                    0
                ],
                "title": "MoniTor: Exploiting Large Language Models with Instruction for Online\n  Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoniTor: Exploiting Large Language Models with Instruction for Online\n  Video Anomaly Detection"
                },
                "summary": "Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors\nwithin videos. Recently, offline VAD has garnered substantial research\nattention, which has been invigorated by the progress in large language models\n(LLMs) and vision-language models (VLMs), offering the potential for a more\nnuanced understanding of anomalies. However, online VAD has seldom received\nattention due to real-time constraints and computational intensity. In this\npaper, we introduce a novel Memory-based online scoring queue scheme for\nTraining-free VAD (MoniTor), to address the inherent complexities in online\nVAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the\ncapabilities of pre-trained large-scale models. To capture temporal\ndependencies more effectively, we incorporate a novel prediction mechanism\ninspired by Long Short-Term Memory (LSTM) networks. This ensures the model can\neffectively model past states and leverage previous predictions to identify\nanomalous behaviors. Thereby, it better understands the current frame.\nMoreover, we design a scoring queue and an anomaly prior to dynamically store\nrecent scores and cover all anomalies in the monitoring scenario, providing\nguidance for LLMs to distinguish between normal and abnormal behaviors over\ntime. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and\nXD-Violence) containing various surveillance and real-world scenarios. The\nresults demonstrate that MoniTor outperforms state-of-the-art methods and is\ncompetitive with weakly supervised methods without training. Code is available\nat https://github.com/YsTvT/MoniTor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors\nwithin videos. Recently, offline VAD has garnered substantial research\nattention, which has been invigorated by the progress in large language models\n(LLMs) and vision-language models (VLMs), offering the potential for a more\nnuanced understanding of anomalies. However, online VAD has seldom received\nattention due to real-time constraints and computational intensity. In this\npaper, we introduce a novel Memory-based online scoring queue scheme for\nTraining-free VAD (MoniTor), to address the inherent complexities in online\nVAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the\ncapabilities of pre-trained large-scale models. To capture temporal\ndependencies more effectively, we incorporate a novel prediction mechanism\ninspired by Long Short-Term Memory (LSTM) networks. This ensures the model can\neffectively model past states and leverage previous predictions to identify\nanomalous behaviors. Thereby, it better understands the current frame.\nMoreover, we design a scoring queue and an anomaly prior to dynamically store\nrecent scores and cover all anomalies in the monitoring scenario, providing\nguidance for LLMs to distinguish between normal and abnormal behaviors over\ntime. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and\nXD-Violence) containing various surveillance and real-world scenarios. The\nresults demonstrate that MoniTor outperforms state-of-the-art methods and is\ncompetitive with weakly supervised methods without training. Code is available\nat https://github.com/YsTvT/MoniTor."
                },
                "authors": [
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Yue Feng"
                    },
                    {
                        "name": "Yingshi Liu"
                    },
                    {
                        "name": "Jingrou Zhang"
                    },
                    {
                        "name": "Jie Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jie Qin"
                },
                "author": "Jie Qin",
                "arxiv_comment": "Accepted to NeurIPS 2025. The first two authors hold equal\n  contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14828v2",
                "updated": "2025-10-24T13:25:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    25,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-02-20T18:45:01Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    1,
                    3,
                    51,
                    0
                ],
                "title": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs"
                },
                "summary": "LLM developers have imposed technical interventions to prevent fine-tuning\nmisuse attacks, attacks where adversaries evade safeguards by fine-tuning the\nmodel using a public API. Previous work has established several successful\nattacks against specific fine-tuning API defences. In this work, we show that\ndefences of fine-tuning APIs that seek to detect individual harmful training or\ninference samples ('pointwise' detection) are fundamentally limited in their\nability to prevent fine-tuning attacks. We construct 'pointwise-undetectable'\nattacks that repurpose entropy in benign model outputs (e.g. semantic or\nsyntactic variations) to covertly transmit dangerous knowledge. Our attacks are\ncomposed solely of unsuspicious benign samples that can be collected from the\nmodel before fine-tuning, meaning training and inference samples are all\nindividually benign and low-perplexity. We test our attacks against the OpenAI\nfine-tuning API, finding they succeed in eliciting answers to harmful\nmultiple-choice questions, and that they evade an enhanced monitoring system we\ndesign that successfully detects other fine-tuning attacks. We encourage the\ncommunity to develop defences that tackle the fundamental limitations we\nuncover in pointwise fine-tuning API defences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM developers have imposed technical interventions to prevent fine-tuning\nmisuse attacks, attacks where adversaries evade safeguards by fine-tuning the\nmodel using a public API. Previous work has established several successful\nattacks against specific fine-tuning API defences. In this work, we show that\ndefences of fine-tuning APIs that seek to detect individual harmful training or\ninference samples ('pointwise' detection) are fundamentally limited in their\nability to prevent fine-tuning attacks. We construct 'pointwise-undetectable'\nattacks that repurpose entropy in benign model outputs (e.g. semantic or\nsyntactic variations) to covertly transmit dangerous knowledge. Our attacks are\ncomposed solely of unsuspicious benign samples that can be collected from the\nmodel before fine-tuning, meaning training and inference samples are all\nindividually benign and low-perplexity. We test our attacks against the OpenAI\nfine-tuning API, finding they succeed in eliciting answers to harmful\nmultiple-choice questions, and that they evade an enhanced monitoring system we\ndesign that successfully detects other fine-tuning attacks. We encourage the\ncommunity to develop defences that tackle the fundamental limitations we\nuncover in pointwise fine-tuning API defences."
                },
                "authors": [
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Yarin Gal"
                    }
                ],
                "author_detail": {
                    "name": "Yarin Gal"
                },
                "author": "Yarin Gal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21447v1",
                "updated": "2025-10-24T13:25:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    25,
                    39,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:25:39Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    25,
                    39,
                    4,
                    297,
                    0
                ],
                "title": "PhysWorld: From Real Videos to World Models of Deformable Objects via\n  Physics-Aware Demonstration Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysWorld: From Real Videos to World Models of Deformable Objects via\n  Physics-Aware Demonstration Synthesis"
                },
                "summary": "Interactive world models that simulate object dynamics are crucial for\nrobotics, VR, and AR. However, it remains a significant challenge to learn\nphysics-consistent dynamics models from limited real-world video data,\nespecially for deformable objects with spatially-varying physical properties.\nTo overcome the challenge of data scarcity, we propose PhysWorld, a novel\nframework that utilizes a simulator to synthesize physically plausible and\ndiverse demonstrations to learn efficient world models. Specifically, we first\nconstruct a physics-consistent digital twin within MPM simulator via\nconstitutive model selection and global-to-local optimization of physical\nproperties. Subsequently, we apply part-aware perturbations to the physical\nproperties and generate various motion patterns for the digital twin,\nsynthesizing extensive and diverse demonstrations. Finally, using these\ndemonstrations, we train a lightweight GNN-based world model that is embedded\nwith physical properties. The real video can be used to further refine the\nphysical properties. PhysWorld achieves accurate and fast future predictions\nfor various deformable objects, and also generalizes well to novel\ninteractions. Experiments show that PhysWorld has competitive performance while\nenabling inference speeds 47 times faster than the recent state-of-the-art\nmethod, i.e., PhysTwin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive world models that simulate object dynamics are crucial for\nrobotics, VR, and AR. However, it remains a significant challenge to learn\nphysics-consistent dynamics models from limited real-world video data,\nespecially for deformable objects with spatially-varying physical properties.\nTo overcome the challenge of data scarcity, we propose PhysWorld, a novel\nframework that utilizes a simulator to synthesize physically plausible and\ndiverse demonstrations to learn efficient world models. Specifically, we first\nconstruct a physics-consistent digital twin within MPM simulator via\nconstitutive model selection and global-to-local optimization of physical\nproperties. Subsequently, we apply part-aware perturbations to the physical\nproperties and generate various motion patterns for the digital twin,\nsynthesizing extensive and diverse demonstrations. Finally, using these\ndemonstrations, we train a lightweight GNN-based world model that is embedded\nwith physical properties. The real video can be used to further refine the\nphysical properties. PhysWorld achieves accurate and fast future predictions\nfor various deformable objects, and also generalizes well to novel\ninteractions. Experiments show that PhysWorld has competitive performance while\nenabling inference speeds 47 times faster than the recent state-of-the-art\nmethod, i.e., PhysTwin."
                },
                "authors": [
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Zhilu Zhang"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21443v1",
                "updated": "2025-10-24T13:20:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    20,
                    30,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:20:30Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    20,
                    30,
                    4,
                    297,
                    0
                ],
                "title": "Does Model Size Matter? A Comparison of Small and Large Language Models\n  for Requirements Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Model Size Matter? A Comparison of Small and Large Language Models\n  for Requirements Classification"
                },
                "summary": "[Context and motivation] Large language models (LLMs) show notable results in\nnatural language processing (NLP) tasks for requirements engineering (RE).\nHowever, their use is compromised by high computational cost, data sharing\nrisks, and dependence on external services. In contrast, small language models\n(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]\nIt remains unclear how well SLMs perform compared to LLMs in RE tasks in terms\nof accuracy. [Results] Our preliminary study compares eight models, including\nthree LLMs and five SLMs, on requirements classification tasks using the\nPROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although\nLLMs achieve an average F1 score of 2% higher than SLMs, this difference is not\nstatistically significant. SLMs almost reach LLMs performance across all\ndatasets and even outperform them in recall on the PROMISE Reclass dataset,\ndespite being up to 300 times smaller. We also found that dataset\ncharacteristics play a more significant role in performance than model size.\n[Contribution] Our study contributes with evidence that SLMs are a valid\nalternative to LLMs for requirements classification, offering advantages in\nprivacy, cost, and local deployability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context and motivation] Large language models (LLMs) show notable results in\nnatural language processing (NLP) tasks for requirements engineering (RE).\nHowever, their use is compromised by high computational cost, data sharing\nrisks, and dependence on external services. In contrast, small language models\n(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]\nIt remains unclear how well SLMs perform compared to LLMs in RE tasks in terms\nof accuracy. [Results] Our preliminary study compares eight models, including\nthree LLMs and five SLMs, on requirements classification tasks using the\nPROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although\nLLMs achieve an average F1 score of 2% higher than SLMs, this difference is not\nstatistically significant. SLMs almost reach LLMs performance across all\ndatasets and even outperform them in recall on the PROMISE Reclass dataset,\ndespite being up to 300 times smaller. We also found that dataset\ncharacteristics play a more significant role in performance than model size.\n[Contribution] Our study contributes with evidence that SLMs are a valid\nalternative to LLMs for requirements classification, offering advantages in\nprivacy, cost, and local deployability."
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Zadenoori"
                    },
                    {
                        "name": "Vincenzo De Martino"
                    },
                    {
                        "name": "Jacek Dabrowski"
                    },
                    {
                        "name": "Xavier Franch"
                    },
                    {
                        "name": "Alessio Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Ferrari"
                },
                "author": "Alessio Ferrari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21441v1",
                "updated": "2025-10-24T13:17:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    17,
                    56,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:17:56Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    17,
                    56,
                    4,
                    297,
                    0
                ],
                "title": "OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary\n  Radiance Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary\n  Radiance Fields"
                },
                "summary": "Modeling the inherent hierarchical structure of 3D objects and 3D scenes is\nhighly desirable, as it enables a more holistic understanding of environments\nfor autonomous agents. Accomplishing this with implicit representations, such\nas Neural Radiance Fields, remains an unexplored challenge. Existing methods\nthat explicitly model hierarchical structures often face significant\nlimitations: they either require multiple rendering passes to capture\nembeddings at different levels of granularity, significantly increasing\ninference time, or rely on predefined, closed-set discrete hierarchies that\ngeneralize poorly to the diverse and nuanced structures encountered by agents\nin the real world. To address these challenges, we propose OpenHype, a novel\napproach that represents scene hierarchies using a continuous hyperbolic latent\nspace. By leveraging the properties of hyperbolic geometry, OpenHype naturally\nencodes multi-scale relationships and enables smooth traversal of hierarchies\nthrough geodesic paths in latent space. Our method outperforms state-of-the-art\napproaches on standard benchmarks, demonstrating superior efficiency and\nadaptability in 3D scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling the inherent hierarchical structure of 3D objects and 3D scenes is\nhighly desirable, as it enables a more holistic understanding of environments\nfor autonomous agents. Accomplishing this with implicit representations, such\nas Neural Radiance Fields, remains an unexplored challenge. Existing methods\nthat explicitly model hierarchical structures often face significant\nlimitations: they either require multiple rendering passes to capture\nembeddings at different levels of granularity, significantly increasing\ninference time, or rely on predefined, closed-set discrete hierarchies that\ngeneralize poorly to the diverse and nuanced structures encountered by agents\nin the real world. To address these challenges, we propose OpenHype, a novel\napproach that represents scene hierarchies using a continuous hyperbolic latent\nspace. By leveraging the properties of hyperbolic geometry, OpenHype naturally\nencodes multi-scale relationships and enables smooth traversal of hierarchies\nthrough geodesic paths in latent space. Our method outperforms state-of-the-art\napproaches on standard benchmarks, demonstrating superior efficiency and\nadaptability in 3D scene understanding."
                },
                "authors": [
                    {
                        "name": "Lisa Weijler"
                    },
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Fabio Poiesi"
                    },
                    {
                        "name": "Timo Ropinski"
                    },
                    {
                        "name": "Pedro Hermosilla"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Hermosilla"
                },
                "author": "Pedro Hermosilla",
                "arxiv_journal_ref": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21440v1",
                "updated": "2025-10-24T13:17:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    17,
                    0,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:17:00Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    17,
                    0,
                    4,
                    297,
                    0
                ],
                "title": "Redefining Retrieval Evaluation in the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Retrieval Evaluation in the Era of LLMs"
                },
                "summary": "Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,\nassume that human users sequentially examine documents with diminishing\nattention to lower ranks. This assumption breaks down in Retrieval Augmented\nGeneration (RAG) systems, where search results are consumed by Large Language\nModels (LLMs), which, unlike humans, process all retrieved documents as a whole\nrather than sequentially. Additionally, traditional IR metrics do not account\nfor related but irrelevant documents that actively degrade generation quality,\nrather than merely being ignored. Due to these two major misalignments, namely\nhuman vs. machine position discount and human relevance vs. machine utility,\nclassical IR metrics do not accurately predict RAG performance. We introduce a\nutility-based annotation schema that quantifies both the positive contribution\nof relevant passages and the negative impact of distracting ones. Building on\nthis foundation, we propose UDCG (Utility and Distraction-aware Cumulative\nGain), a metric using an LLM-oriented positional discount to directly optimize\nthe correlation with the end-to-end answer accuracy. Experiments on five\ndatasets and six LLMs demonstrate that UDCG improves correlation by up to 36%\ncompared to traditional metrics. Our work provides a critical step toward\naligning IR evaluation with LLM consumers and enables more reliable assessment\nof RAG components",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,\nassume that human users sequentially examine documents with diminishing\nattention to lower ranks. This assumption breaks down in Retrieval Augmented\nGeneration (RAG) systems, where search results are consumed by Large Language\nModels (LLMs), which, unlike humans, process all retrieved documents as a whole\nrather than sequentially. Additionally, traditional IR metrics do not account\nfor related but irrelevant documents that actively degrade generation quality,\nrather than merely being ignored. Due to these two major misalignments, namely\nhuman vs. machine position discount and human relevance vs. machine utility,\nclassical IR metrics do not accurately predict RAG performance. We introduce a\nutility-based annotation schema that quantifies both the positive contribution\nof relevant passages and the negative impact of distracting ones. Building on\nthis foundation, we propose UDCG (Utility and Distraction-aware Cumulative\nGain), a metric using an LLM-oriented positional discount to directly optimize\nthe correlation with the end-to-end answer accuracy. Experiments on five\ndatasets and six LLMs demonstrate that UDCG improves correlation by up to 36%\ncompared to traditional metrics. Our work provides a critical step toward\naligning IR evaluation with LLM consumers and enables more reliable assessment\nof RAG components"
                },
                "authors": [
                    {
                        "name": "Giovanni Trappolini"
                    },
                    {
                        "name": "Florin Cuconasu"
                    },
                    {
                        "name": "Simone Filice"
                    },
                    {
                        "name": "Yoelle Maarek"
                    },
                    {
                        "name": "Fabrizio Silvestri"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Silvestri"
                },
                "author": "Fabrizio Silvestri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21436v1",
                "updated": "2025-10-24T13:14:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    14,
                    53,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:14:53Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    14,
                    53,
                    4,
                    297,
                    0
                ],
                "title": "AutoOpt: A Dataset and a Unified Framework for Automating Optimization\n  Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoOpt: A Dataset and a Unified Framework for Automating Optimization\n  Problem Solving"
                },
                "summary": "This study presents AutoOpt-11k, a unique image dataset of over 11,000\nhandwritten and printed mathematical optimization models corresponding to\nsingle-objective, multi-objective, multi-level, and stochastic optimization\nproblems exhibiting various types of complexities such as non-linearity,\nnon-convexity, non-differentiability, discontinuity, and high-dimensionality.\nThe labels consist of the LaTeX representation for all the images and modeling\nlanguage representation for a subset of images. The dataset is created by 25\nexperts following ethical data creation guidelines and verified in two-phases\nto avoid errors. Further, we develop AutoOpt framework, a machine learning\nbased automated approach for solving optimization problems, where the user just\nneeds to provide an image of the formulation and AutoOpt solves it efficiently\nwithout any further human intervention. AutoOpt framework consists of three\nModules: (i) M1 (Image_to_Text)- a deep learning model performs the\nMathematical Expression Recognition (MER) task to generate the LaTeX code\ncorresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-\na small-scale fine-tuned LLM generates the PYOMO script (optimization modeling\nlanguage) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization\nbased Decomposition (BOBD) method solves the optimization formulation described\nin the PYOMO script. We use AutoOpt-11k dataset for training and testing of\ndeep learning models employed in AutoOpt. The deep learning model for MER task\n(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method\n(M3), which is a hybrid approach, yields better results on complex test\nproblems compared to common approaches, like interior-point algorithm and\ngenetic algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents AutoOpt-11k, a unique image dataset of over 11,000\nhandwritten and printed mathematical optimization models corresponding to\nsingle-objective, multi-objective, multi-level, and stochastic optimization\nproblems exhibiting various types of complexities such as non-linearity,\nnon-convexity, non-differentiability, discontinuity, and high-dimensionality.\nThe labels consist of the LaTeX representation for all the images and modeling\nlanguage representation for a subset of images. The dataset is created by 25\nexperts following ethical data creation guidelines and verified in two-phases\nto avoid errors. Further, we develop AutoOpt framework, a machine learning\nbased automated approach for solving optimization problems, where the user just\nneeds to provide an image of the formulation and AutoOpt solves it efficiently\nwithout any further human intervention. AutoOpt framework consists of three\nModules: (i) M1 (Image_to_Text)- a deep learning model performs the\nMathematical Expression Recognition (MER) task to generate the LaTeX code\ncorresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-\na small-scale fine-tuned LLM generates the PYOMO script (optimization modeling\nlanguage) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization\nbased Decomposition (BOBD) method solves the optimization formulation described\nin the PYOMO script. We use AutoOpt-11k dataset for training and testing of\ndeep learning models employed in AutoOpt. The deep learning model for MER task\n(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method\n(M3), which is a hybrid approach, yields better results on complex test\nproblems compared to common approaches, like interior-point algorithm and\ngenetic algorithm."
                },
                "authors": [
                    {
                        "name": "Ankur Sinha"
                    },
                    {
                        "name": "Shobhit Arora"
                    },
                    {
                        "name": "Dhaval Pujara"
                    }
                ],
                "author_detail": {
                    "name": "Dhaval Pujara"
                },
                "author": "Dhaval Pujara",
                "arxiv_comment": "NeurIPS 2025, 28 pages, 11 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15555v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15555v3",
                "updated": "2025-10-24T13:11:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    11,
                    27,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-17T11:41:35Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    11,
                    41,
                    35,
                    4,
                    290,
                    0
                ],
                "title": "Doubly Robust Estimation of Causal Effects in Strategic Equilibrium\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly Robust Estimation of Causal Effects in Strategic Equilibrium\n  Systems"
                },
                "summary": "We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework\nthat integrates strategic equilibrium modeling with doubly robust estimation\nfor causal inference in strategic environments. SDR addresses endogenous\ntreatment assignment arising from strategic agent behavior, maintaining double\nrobustness while incorporating strategic considerations. Theoretical analysis\nconfirms SDR's consistency and asymptotic normality under strategic\nunconfoundedness. Empirical evaluations demonstrate SDR's superior performance\nover baseline methods, achieving 7.6\\%-29.3\\% bias reduction across varying\nstrategic strengths and maintaining robust scalability with agent populations.\nThe framework provides a principled approach for reliable causal inference when\nagents respond strategically to interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework\nthat integrates strategic equilibrium modeling with doubly robust estimation\nfor causal inference in strategic environments. SDR addresses endogenous\ntreatment assignment arising from strategic agent behavior, maintaining double\nrobustness while incorporating strategic considerations. Theoretical analysis\nconfirms SDR's consistency and asymptotic normality under strategic\nunconfoundedness. Empirical evaluations demonstrate SDR's superior performance\nover baseline methods, achieving 7.6\\%-29.3\\% bias reduction across varying\nstrategic strengths and maintaining robust scalability with agent populations.\nThe framework provides a principled approach for reliable causal inference when\nagents respond strategically to interventions."
                },
                "authors": [
                    {
                        "name": "Sibo Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Sibo Xiao"
                },
                "author": "Sibo Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15555v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15555v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21425v1",
                "updated": "2025-10-24T13:05:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    5,
                    50,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:05:50Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    5,
                    50,
                    4,
                    297,
                    0
                ],
                "title": "Advancing Symbolic Integration in Large Language Models: Beyond\n  Conventional Neurosymbolic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Symbolic Integration in Large Language Models: Beyond\n  Conventional Neurosymbolic AI"
                },
                "summary": "LLMs have demonstrated highly effective learning, human-like response\ngeneration,and decision-making capabilities in high-risk sectors. However,\nthese models remain black boxes because they struggle to ensure transparency in\nresponses. The literature has explored numerous approaches to address\ntransparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI\napproaches were primarily developed for conventional neural networks and are\nnot well-suited to the unique features of LLMs. Consequently, there is a\nlimited systematic understanding of how symbolic AI can be effectively\nintegrated into LLMs. This paper aims to address this gap by first reviewing\nestablished NeSy AI methods and then proposing a novel taxonomy of symbolic\nintegration in LLMs, along with a roadmap to merge symbolic techniques with\nLLMs. The roadmap introduces a new categorisation framework across four\ndimensions by organising existing literature within these categories. These\ninclude symbolic integration across various stages of LLM, coupling mechanisms,\narchitectural paradigms, as well as algorithmic and application-level\nperspectives. The paper thoroughly identifies current benchmarks, cutting-edge\nadvancements, and critical gaps within the field to propose a roadmap for\nfuture research. By highlighting the latest developments and notable gaps in\nthe literature, it offers practical insights for implementing frameworks for\nsymbolic integration into LLMs to enhance transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated highly effective learning, human-like response\ngeneration,and decision-making capabilities in high-risk sectors. However,\nthese models remain black boxes because they struggle to ensure transparency in\nresponses. The literature has explored numerous approaches to address\ntransparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI\napproaches were primarily developed for conventional neural networks and are\nnot well-suited to the unique features of LLMs. Consequently, there is a\nlimited systematic understanding of how symbolic AI can be effectively\nintegrated into LLMs. This paper aims to address this gap by first reviewing\nestablished NeSy AI methods and then proposing a novel taxonomy of symbolic\nintegration in LLMs, along with a roadmap to merge symbolic techniques with\nLLMs. The roadmap introduces a new categorisation framework across four\ndimensions by organising existing literature within these categories. These\ninclude symbolic integration across various stages of LLM, coupling mechanisms,\narchitectural paradigms, as well as algorithmic and application-level\nperspectives. The paper thoroughly identifies current benchmarks, cutting-edge\nadvancements, and critical gaps within the field to propose a roadmap for\nfuture research. By highlighting the latest developments and notable gaps in\nthe literature, it offers practical insights for implementing frameworks for\nsymbolic integration into LLMs to enhance transparency."
                },
                "authors": [
                    {
                        "name": "Maneeha Rani"
                    },
                    {
                        "name": "Bhupesh Kumar Mishra"
                    },
                    {
                        "name": "Dhavalkumar Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Dhavalkumar Thakker"
                },
                "author": "Dhavalkumar Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19245v2",
                "updated": "2025-10-24T12:56:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    56,
                    1,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-25T17:49:37Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    17,
                    49,
                    37,
                    6,
                    145,
                    0
                ],
                "title": "To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and\n  Looped Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and\n  Looped Transformers"
                },
                "summary": "Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically\nimprove performance on reasoning tasks and to theoretically enhance\nexpressivity by recursively increasing the number of computational steps.\nHowever, their comparative capabilities are still not well understood. In this\npaper, we provide a formal analysis of their respective strengths and\nlimitations. We show that Looped Transformers can efficiently simulate parallel\ncomputations for deterministic tasks, which we formalize as evaluation over\ndirected acyclic graphs. In contrast, CoT with stochastic decoding excels at\napproximate inference for compositional structures, namely self-reducible\nproblems. These separations suggest the tasks for which depth-driven recursion\nis more suitable, thereby offering practical cues for choosing between\nreasoning paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically\nimprove performance on reasoning tasks and to theoretically enhance\nexpressivity by recursively increasing the number of computational steps.\nHowever, their comparative capabilities are still not well understood. In this\npaper, we provide a formal analysis of their respective strengths and\nlimitations. We show that Looped Transformers can efficiently simulate parallel\ncomputations for deterministic tasks, which we formalize as evaluation over\ndirected acyclic graphs. In contrast, CoT with stochastic decoding excels at\napproximate inference for compositional structures, namely self-reducible\nproblems. These separations suggest the tasks for which depth-driven recursion\nis more suitable, thereby offering practical cues for choosing between\nreasoning paradigms."
                },
                "authors": [
                    {
                        "name": "Kevin Xu"
                    },
                    {
                        "name": "Issei Sato"
                    }
                ],
                "author_detail": {
                    "name": "Issei Sato"
                },
                "author": "Issei Sato",
                "arxiv_comment": "For the latest version, see arXiv:2509.25239",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03516v2",
                "updated": "2025-10-24T12:53:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    53,
                    50,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-03T21:02:34Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    21,
                    2,
                    34,
                    4,
                    276,
                    0
                ],
                "title": "COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC\n  Techniques"
                },
                "summary": "Convolutional Neural Networks (CNNs) are highly effective for computer vision\nand pattern recognition tasks; however, their computational intensity and\nreliance on hardware such as FPGAs pose challenges for deployment on low-power\nedge devices. In this work, we present COMET, a framework of CNN designs that\nemploy efficient hardware offset-binary coding (OBC) techniques to enable\nco-optimization of performance and resource utilization. The approach\nformulates CNN inference with OBC representations of inputs (Scheme A) and\nweights (Scheme B) separately, enabling exploitation of bit-width asymmetry.\nThe shift-accumulate operation is modified by incorporating the offset term\nwith the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we\nintroduce four novel look-up table (LUT) techniques -- parallel, shared, split,\nand hybrid -- and analyze them to identify the most efficient options. Building\non this foundation, we develop an OBC-based general matrix multiplication core\nusing the im2col transformation, enabling efficient acceleration of a\nfixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the\nproposed co-optimization approach significantly reduces resource utilization\ncompared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) are highly effective for computer vision\nand pattern recognition tasks; however, their computational intensity and\nreliance on hardware such as FPGAs pose challenges for deployment on low-power\nedge devices. In this work, we present COMET, a framework of CNN designs that\nemploy efficient hardware offset-binary coding (OBC) techniques to enable\nco-optimization of performance and resource utilization. The approach\nformulates CNN inference with OBC representations of inputs (Scheme A) and\nweights (Scheme B) separately, enabling exploitation of bit-width asymmetry.\nThe shift-accumulate operation is modified by incorporating the offset term\nwith the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we\nintroduce four novel look-up table (LUT) techniques -- parallel, shared, split,\nand hybrid -- and analyze them to identify the most efficient options. Building\non this foundation, we develop an OBC-based general matrix multiplication core\nusing the im2col transformation, enabling efficient acceleration of a\nfixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the\nproposed co-optimization approach significantly reduces resource utilization\ncompared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on\naccuracy."
                },
                "authors": [
                    {
                        "name": "Boyang Chen"
                    },
                    {
                        "name": "Mohd Tasleem Khan"
                    },
                    {
                        "name": "George Goussetis"
                    },
                    {
                        "name": "Mathini Sellathurai"
                    },
                    {
                        "name": "Yuan Ding"
                    },
                    {
                        "name": "Joo F. C. Mota"
                    },
                    {
                        "name": "Jongeun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongeun Lee"
                },
                "author": "Jongeun Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01084v2",
                "updated": "2025-10-24T12:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-01T17:03:02Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    17,
                    3,
                    2,
                    6,
                    152,
                    0
                ],
                "title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression"
                },
                "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized on general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a novel\nmethod for achieving context-adaptive tokenization in LLMs at inference time.\nLeveraging an online data compression algorithm (Lempel-Ziv-Welch), zip2zip\ndynamically expands its active vocabulary at inference time by continuously\nreplacing fragmented token sequences with more compact hypertokens, which it\ncan immediately output during generation. In doing so, the model refines its\ninternal tokenization scheme to match the token distribution of the current\ncontext, reducing redundancy and improving representational efficiency. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\ncompression that incrementally merges co-occurring tokens into reusable\nhypertokens on the fly; (2) a dynamic embedding (and unembedding) layer that\ncomputes embeddings for newly formed hypertokens at runtime; and (3) a variant\nof autoregressive language modeling that pretrains the model to handle\nhypertokenized, compressed text sequences as inputs and outputs. We show that\nan existing LLM can be uptrained for zip2zip in 10 GPU-hours via\nparameter-efficient finetuning. The resulting LLM performs test-time\nadaptation, learning to use hypertokens in unseen contexts and reducing input\nand output tokens by 15-40%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized on general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a novel\nmethod for achieving context-adaptive tokenization in LLMs at inference time.\nLeveraging an online data compression algorithm (Lempel-Ziv-Welch), zip2zip\ndynamically expands its active vocabulary at inference time by continuously\nreplacing fragmented token sequences with more compact hypertokens, which it\ncan immediately output during generation. In doing so, the model refines its\ninternal tokenization scheme to match the token distribution of the current\ncontext, reducing redundancy and improving representational efficiency. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\ncompression that incrementally merges co-occurring tokens into reusable\nhypertokens on the fly; (2) a dynamic embedding (and unembedding) layer that\ncomputes embeddings for newly formed hypertokens at runtime; and (3) a variant\nof autoregressive language modeling that pretrains the model to handle\nhypertokenized, compressed text sequences as inputs and outputs. We show that\nan existing LLM can be uptrained for zip2zip in 10 GPU-hours via\nparameter-efficient finetuning. The resulting LLM performs test-time\nadaptation, learning to use hypertokens in unseen contexts and reducing input\nand output tokens by 15-40%."
                },
                "authors": [
                    {
                        "name": "Saibo Geng"
                    },
                    {
                        "name": "Nathan Ranchin"
                    },
                    {
                        "name": "Yunzhen yao"
                    },
                    {
                        "name": "Maxime Peyrard"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Michael Gastpar"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21408v1",
                "updated": "2025-10-24T12:52:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    52,
                    11,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:52:11Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    52,
                    11,
                    4,
                    297,
                    0
                ],
                "title": "Large Language Models as Model Organisms for Human Associative Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Model Organisms for Human Associative Learning"
                },
                "summary": "Associative learning--forming links between co-occurring items--is\nfundamental to human cognition, reshaping internal representations in complex\nways. Testing hypotheses on how representational changes occur in biological\nsystems is challenging, but large language models (LLMs) offer a scalable\nalternative. Building on LLMs' in-context learning, we adapt a cognitive\nneuroscience associative learning paradigm and investigate how representations\nevolve across six models. Our initial findings reveal a non-monotonic pattern\nconsistent with the Non-Monotonic Plasticity Hypothesis, with moderately\nsimilar items differentiating after learning. Leveraging the controllability of\nLLMs, we further show that this differentiation is modulated by the overlap of\nassociated items with the broader vocabulary--a factor we term vocabulary\ninterference, capturing how new associations compete with prior knowledge. We\nfind that higher vocabulary interference amplifies differentiation, suggesting\nthat representational change is influenced by both item similarity and global\ncompetition. Our findings position LLMs not only as powerful tools for studying\nrepresentational dynamics in human-like learning systems, but also as\naccessible and general computational models for generating new hypotheses about\nthe principles underlying memory reorganization in the brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associative learning--forming links between co-occurring items--is\nfundamental to human cognition, reshaping internal representations in complex\nways. Testing hypotheses on how representational changes occur in biological\nsystems is challenging, but large language models (LLMs) offer a scalable\nalternative. Building on LLMs' in-context learning, we adapt a cognitive\nneuroscience associative learning paradigm and investigate how representations\nevolve across six models. Our initial findings reveal a non-monotonic pattern\nconsistent with the Non-Monotonic Plasticity Hypothesis, with moderately\nsimilar items differentiating after learning. Leveraging the controllability of\nLLMs, we further show that this differentiation is modulated by the overlap of\nassociated items with the broader vocabulary--a factor we term vocabulary\ninterference, capturing how new associations compete with prior knowledge. We\nfind that higher vocabulary interference amplifies differentiation, suggesting\nthat representational change is influenced by both item similarity and global\ncompetition. Our findings position LLMs not only as powerful tools for studying\nrepresentational dynamics in human-like learning systems, but also as\naccessible and general computational models for generating new hypotheses about\nthe principles underlying memory reorganization in the brain."
                },
                "authors": [
                    {
                        "name": "Camila Kolling"
                    },
                    {
                        "name": "Vy Ai Vo"
                    },
                    {
                        "name": "Mariya Toneva"
                    }
                ],
                "author_detail": {
                    "name": "Mariya Toneva"
                },
                "author": "Mariya Toneva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21407v1",
                "updated": "2025-10-24T12:50:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    50,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:50:35Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    50,
                    35,
                    4,
                    297,
                    0
                ],
                "title": "REvolution: An Evolutionary Framework for RTL Generation driven by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REvolution: An Evolutionary Framework for RTL Generation driven by Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are used for Register-Transfer Level (RTL) code\ngeneration, but they face two main challenges: functional correctness and\nPower, Performance, and Area (PPA) optimization. Iterative, feedback-based\nmethods partially address these, but they are limited to local search,\nhindering the discovery of a global optimum. This paper introduces REvolution,\na framework that combines Evolutionary Computation (EC) with LLMs for automatic\nRTL generation and optimization. REvolution evolves a population of candidates\nin parallel, each defined by a design strategy, RTL implementation, and\nevaluation feedback. The framework includes a dual-population algorithm that\ndivides candidates into Fail and Success groups for bug fixing and PPA\noptimization, respectively. An adaptive mechanism further improves search\nefficiency by dynamically adjusting the selection probability of each prompt\nstrategy according to its success rate. Experiments on the VerilogEval and\nRTLLM benchmarks show that REvolution increased the initial pass rate of\nvarious LLMs by up to 24.0 percentage points. The DeepSeek-V3 model achieved a\nfinal pass rate of 95.5\\%, comparable to state-of-the-art results, without the\nneed for separate training or domain-specific tools. Additionally, the\ngenerated RTL designs showed significant PPA improvements over reference\ndesigns. This work introduces a new RTL design approach by combining LLMs'\ngenerative capabilities with EC's broad search power, overcoming the\nlocal-search limitations of previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are used for Register-Transfer Level (RTL) code\ngeneration, but they face two main challenges: functional correctness and\nPower, Performance, and Area (PPA) optimization. Iterative, feedback-based\nmethods partially address these, but they are limited to local search,\nhindering the discovery of a global optimum. This paper introduces REvolution,\na framework that combines Evolutionary Computation (EC) with LLMs for automatic\nRTL generation and optimization. REvolution evolves a population of candidates\nin parallel, each defined by a design strategy, RTL implementation, and\nevaluation feedback. The framework includes a dual-population algorithm that\ndivides candidates into Fail and Success groups for bug fixing and PPA\noptimization, respectively. An adaptive mechanism further improves search\nefficiency by dynamically adjusting the selection probability of each prompt\nstrategy according to its success rate. Experiments on the VerilogEval and\nRTLLM benchmarks show that REvolution increased the initial pass rate of\nvarious LLMs by up to 24.0 percentage points. The DeepSeek-V3 model achieved a\nfinal pass rate of 95.5\\%, comparable to state-of-the-art results, without the\nneed for separate training or domain-specific tools. Additionally, the\ngenerated RTL designs showed significant PPA improvements over reference\ndesigns. This work introduces a new RTL design approach by combining LLMs'\ngenerative capabilities with EC's broad search power, overcoming the\nlocal-search limitations of previous methods."
                },
                "authors": [
                    {
                        "name": "Kyungjun Min"
                    },
                    {
                        "name": "Kyumin Cho"
                    },
                    {
                        "name": "Junhwan Jang"
                    },
                    {
                        "name": "Seokhyeong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Seokhyeong Kang"
                },
                "author": "Seokhyeong Kang",
                "arxiv_comment": "Accepted for publication at the 2026 Asia and South Pacific Design\n  Automation Conference (ASP-DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19092v2",
                "updated": "2025-10-24T12:45:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    45,
                    47,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-25T11:03:45Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    11,
                    3,
                    45,
                    6,
                    145,
                    0
                ],
                "title": "Reinforced Latent Reasoning for LLM-based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Latent Reasoning for LLM-based Recommendation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities in complex problem-solving tasks, sparking growing interest in\ntheir application to preference reasoning in recommendation systems. Existing\nmethods typically rely on fine-tuning with explicit chain-of-thought (CoT)\ndata. However, these methods face significant practical limitations due to (1)\nthe difficulty of obtaining high-quality CoT data in recommendation and (2) the\nhigh inference latency caused by generating CoT reasoning. In this work, we\nexplore an alternative approach that shifts from explicit CoT reasoning to\ncompact, information-dense latent reasoning. This approach eliminates the need\nfor explicit CoT generation and improves inference efficiency, as few latent\ntokens can effectively capture the entire reasoning process. Building on this\nidea, we propose \\textit{\\underline{R}einforced \\underline{Latent}\n\\underline{R}easoning for \\underline{R}ecommendation} (LatentR$^3$), a novel\nend-to-end training framework that leverages reinforcement learning (RL) to\noptimize latent reasoning without relying on any CoT data. LatentR$^3$ adopts a\ntwo-stage training strategy: first, supervised fine-tuning to initialize the\nlatent reasoning module, followed by pure RL training to encourage exploration\nthrough a rule-based reward design. Our RL implementation is based on a\nmodified GRPO algorithm, which reduces computational overhead during training\nand introduces continuous reward signals for more efficient learning. Extensive\nexperiments demonstrate that LatentR$^3$ enables effective latent reasoning\nwithout any direct supervision of the reasoning process, significantly\nimproving performance when integrated with different LLM-based recommendation\nmethods. Our codes are available at https://github.com/xuwenxinedu/R3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities in complex problem-solving tasks, sparking growing interest in\ntheir application to preference reasoning in recommendation systems. Existing\nmethods typically rely on fine-tuning with explicit chain-of-thought (CoT)\ndata. However, these methods face significant practical limitations due to (1)\nthe difficulty of obtaining high-quality CoT data in recommendation and (2) the\nhigh inference latency caused by generating CoT reasoning. In this work, we\nexplore an alternative approach that shifts from explicit CoT reasoning to\ncompact, information-dense latent reasoning. This approach eliminates the need\nfor explicit CoT generation and improves inference efficiency, as few latent\ntokens can effectively capture the entire reasoning process. Building on this\nidea, we propose \\textit{\\underline{R}einforced \\underline{Latent}\n\\underline{R}easoning for \\underline{R}ecommendation} (LatentR$^3$), a novel\nend-to-end training framework that leverages reinforcement learning (RL) to\noptimize latent reasoning without relying on any CoT data. LatentR$^3$ adopts a\ntwo-stage training strategy: first, supervised fine-tuning to initialize the\nlatent reasoning module, followed by pure RL training to encourage exploration\nthrough a rule-based reward design. Our RL implementation is based on a\nmodified GRPO algorithm, which reduces computational overhead during training\nand introduces continuous reward signals for more efficient learning. Extensive\nexperiments demonstrate that LatentR$^3$ enables effective latent reasoning\nwithout any direct supervision of the reasoning process, significantly\nimproving performance when integrated with different LLM-based recommendation\nmethods. Our codes are available at https://github.com/xuwenxinedu/R3."
                },
                "authors": [
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Wenxin Xu"
                    },
                    {
                        "name": "Xiaoyan Zhao"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21401v1",
                "updated": "2025-10-24T12:44:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    44,
                    8,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:44:08Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    44,
                    8,
                    4,
                    297,
                    0
                ],
                "title": "FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract\n  Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract\n  Security"
                },
                "summary": "Smart contract vulnerabilities cost billions of dollars annually, yet\nexisting automated analysis tools fail to generate deployable defenses. We\npresent FLAMES, a novel automated approach that synthesizes executable runtime\nguards as Solidity \"require\" statements to harden smart contracts against\nexploits. Unlike prior work that relies on vulnerability labels, symbolic\nanalysis, or natural language specifications, FLAMES employs domain-adapted\nlarge language models trained through fill-in-the-middle supervised fine-tuning\non real-world invariants extracted from 514,506 verified contracts. Our\nextensive evaluation across three dimensions demonstrates FLAMES's\neffectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for\nsynthesized invariant (2) Semantic Quality: on a curated test set of 5,000\nchallenging invariants, FLAMES produces exact or semantically equivalent\nmatches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES\nprevents 22 out of 108 real exploits (20.4%) while preserving contract\nfunctionality, and (4) FLAMES successfully blocks the real-world APEMAGA\nincident by synthesizing a pre-condition that mitigates the attack. FLAMES\nestablishes that domain-adapted LLMs can automatically generate\nproduction-ready security defenses for smart contracts without requiring\nvulnerability detection, formal specifications, or human intervention. We\nrelease our code, model weights, datasets, and evaluation infrastructure to\nenable reproducible research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract vulnerabilities cost billions of dollars annually, yet\nexisting automated analysis tools fail to generate deployable defenses. We\npresent FLAMES, a novel automated approach that synthesizes executable runtime\nguards as Solidity \"require\" statements to harden smart contracts against\nexploits. Unlike prior work that relies on vulnerability labels, symbolic\nanalysis, or natural language specifications, FLAMES employs domain-adapted\nlarge language models trained through fill-in-the-middle supervised fine-tuning\non real-world invariants extracted from 514,506 verified contracts. Our\nextensive evaluation across three dimensions demonstrates FLAMES's\neffectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for\nsynthesized invariant (2) Semantic Quality: on a curated test set of 5,000\nchallenging invariants, FLAMES produces exact or semantically equivalent\nmatches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES\nprevents 22 out of 108 real exploits (20.4%) while preserving contract\nfunctionality, and (4) FLAMES successfully blocks the real-world APEMAGA\nincident by synthesizing a pre-condition that mitigates the attack. FLAMES\nestablishes that domain-adapted LLMs can automatically generate\nproduction-ready security defenses for smart contracts without requiring\nvulnerability detection, formal specifications, or human intervention. We\nrelease our code, model weights, datasets, and evaluation infrastructure to\nenable reproducible research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Mojtaba Eshghie"
                    },
                    {
                        "name": "Gabriele Morello"
                    },
                    {
                        "name": "Matteo Lauretano"
                    },
                    {
                        "name": "Alexandre Bartel"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21398v1",
                "updated": "2025-10-24T12:39:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    39,
                    15,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:39:15Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    39,
                    15,
                    4,
                    297,
                    0
                ],
                "title": "Boosting Accuracy and Efficiency of Budget Forcing in LLMs via\n  Reinforcement Learning for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Accuracy and Efficiency of Budget Forcing in LLMs via\n  Reinforcement Learning for Mathematical Reasoning"
                },
                "summary": "Test-time scaling methods have seen a rapid increase in popularity for its\ncomputational efficiency and parameter-independent training to improve\nreasoning performance on Large Language Models. One such method is called\nbudget forcing, a decoding intervention strategy which allocates extra compute\nbudget for thinking and elicits the inherent self-correcting behavior of the\nmodel. However, this relies on supervised fine-tuning (SFT) on long-context\nreasoning traces which causes performance degradation on smaller models due to\nverbose responses. For this reason, we offer a framework integrating\nreinforcement learning (RL) to improve token efficiency and boost the\nperformance of a 1.5B model for mathematical reasoning. We demonstrate this\nusing only 1.5K training samples and found that our SFT+RL model performed\nbetter on the GSM8K dataset with varying compute budgets. Our main findings\nshowed an overall higher accuracy while significantly reducing its token usage\nby over 40% compared to the SFT model, revealing how RL can recover the losses\ndue to long-context training and altogether improving performance in\nmathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling methods have seen a rapid increase in popularity for its\ncomputational efficiency and parameter-independent training to improve\nreasoning performance on Large Language Models. One such method is called\nbudget forcing, a decoding intervention strategy which allocates extra compute\nbudget for thinking and elicits the inherent self-correcting behavior of the\nmodel. However, this relies on supervised fine-tuning (SFT) on long-context\nreasoning traces which causes performance degradation on smaller models due to\nverbose responses. For this reason, we offer a framework integrating\nreinforcement learning (RL) to improve token efficiency and boost the\nperformance of a 1.5B model for mathematical reasoning. We demonstrate this\nusing only 1.5K training samples and found that our SFT+RL model performed\nbetter on the GSM8K dataset with varying compute budgets. Our main findings\nshowed an overall higher accuracy while significantly reducing its token usage\nby over 40% compared to the SFT model, revealing how RL can recover the losses\ndue to long-context training and altogether improving performance in\nmathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Ravindra Aribowo Tarunokusumo"
                    },
                    {
                        "name": "Rafael Fernandes Cunha"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Fernandes Cunha"
                },
                "author": "Rafael Fernandes Cunha",
                "arxiv_comment": "Submitted to the European Conference on Artificial Intelligence\n  (ECAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06958v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06958v3",
                "updated": "2025-10-24T12:32:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    32,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-08T00:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    0,
                    59,
                    2,
                    6,
                    159,
                    0
                ],
                "title": "Simulating Society Requires Simulating Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Society Requires Simulating Thought"
                },
                "summary": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior; it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior, primarily through prompting and\nsupervised fine-tuning. Yet current simulations remain grounded in a\nbehaviorist \"demographics in, behavior out\" paradigm, focusing on surface-level\nplausibility. As a result, they often lack internal coherence, causal\nreasoning, and belief traceability, making them unreliable for modeling how\npeople reason, deliberate, and respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought, not just\nlanguage, for social simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior; it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior, primarily through prompting and\nsupervised fine-tuning. Yet current simulations remain grounded in a\nbehaviorist \"demographics in, behavior out\" paradigm, focusing on surface-level\nplausibility. As a result, they often lack internal coherence, causal\nreasoning, and belief traceability, making them unreliable for modeling how\npeople reason, deliberate, and respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought, not just\nlanguage, for social simulations."
                },
                "authors": [
                    {
                        "name": "Chance Jiajie Li"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Zhenze Mo"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Yuhan Tang"
                    },
                    {
                        "name": "Kaiya Ivy Zhao"
                    },
                    {
                        "name": "Yulu Gan"
                    },
                    {
                        "name": "Jie Fan"
                    },
                    {
                        "name": "Jiangbo Yu"
                    },
                    {
                        "name": "Jinhua Zhao"
                    },
                    {
                        "name": "Paul Liang"
                    },
                    {
                        "name": "Luis Alonso"
                    },
                    {
                        "name": "Kent Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kent Larson"
                },
                "author": "Kent Larson",
                "arxiv_comment": "NeurIPS 2025 (Position Paper Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06958v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06958v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18631v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18631v4",
                "updated": "2025-10-24T12:32:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    32,
                    0,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-23T13:36:24Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    36,
                    24,
                    0,
                    174,
                    0
                ],
                "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDit: Reward Dithering for Improved LLM Policy Optimization"
                },
                "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Jiarui Yu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "arxiv_comment": "34 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18631v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18631v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15795v2",
                "updated": "2025-10-24T12:30:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    30,
                    19,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-21T17:48:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    48,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "Reverse Engineering Human Preferences with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Engineering Human Preferences with Reinforcement Learning"
                },
                "summary": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks."
                },
                "authors": [
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Tan Yi-Chern"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "arxiv_comment": "NeurIPS 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06072v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06072v3",
                "updated": "2025-10-24T12:20:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    20,
                    29,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-06T13:26:16Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    13,
                    26,
                    16,
                    4,
                    157,
                    0
                ],
                "title": "BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for\n  Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for\n  Imitation Learning"
                },
                "summary": "We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel\naction tokenizer that encodes action sequences into compact discrete or\ncontinuous tokens using B-splines. In contrast to existing action tokenizers\nbased on vector quantization or byte pair encoding, BEAST requires no separate\ntokenizer training and consistently produces tokens of uniform length, enabling\nfast action sequence generation via parallel decoding. Leveraging our B-spline\nformulation, BEAST inherently ensures generating smooth trajectories without\ndiscontinuities between adjacent segments. We extensively evaluate BEAST by\nintegrating it with three distinct model architectures: a Variational\nAutoencoder (VAE) with continuous tokens, a decoder-only Transformer with\ndiscrete tokens, and Florence-2, a pretrained Vision-Language Model with an\nencoder-decoder architecture, demonstrating BEAST's compatibility and\nscalability with large pretrained models. We evaluate BEAST across three\nestablished benchmarks consisting of 166 simulated tasks and on three distinct\nrobot settings with a total of 8 real-world tasks. Experimental results\ndemonstrate that BEAST (i) significantly reduces both training and inference\ncomputational costs, and (ii) consistently generates smooth, high-frequency\ncontrol signals suitable for continuous control tasks while (iii) reliably\nachieves competitive task success rates compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel\naction tokenizer that encodes action sequences into compact discrete or\ncontinuous tokens using B-splines. In contrast to existing action tokenizers\nbased on vector quantization or byte pair encoding, BEAST requires no separate\ntokenizer training and consistently produces tokens of uniform length, enabling\nfast action sequence generation via parallel decoding. Leveraging our B-spline\nformulation, BEAST inherently ensures generating smooth trajectories without\ndiscontinuities between adjacent segments. We extensively evaluate BEAST by\nintegrating it with three distinct model architectures: a Variational\nAutoencoder (VAE) with continuous tokens, a decoder-only Transformer with\ndiscrete tokens, and Florence-2, a pretrained Vision-Language Model with an\nencoder-decoder architecture, demonstrating BEAST's compatibility and\nscalability with large pretrained models. We evaluate BEAST across three\nestablished benchmarks consisting of 166 simulated tasks and on three distinct\nrobot settings with a total of 8 real-world tasks. Experimental results\ndemonstrate that BEAST (i) significantly reduces both training and inference\ncomputational costs, and (ii) consistently generates smooth, high-frequency\ncontrol signals suitable for continuous control tasks while (iii) reliably\nachieves competitive task success rates compared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Weiran Liao"
                    },
                    {
                        "name": "Xi Huang"
                    },
                    {
                        "name": "Yucheng Tang"
                    },
                    {
                        "name": "Fabian Otto"
                    },
                    {
                        "name": "Xiaogang Jia"
                    },
                    {
                        "name": "Xinkai Jiang"
                    },
                    {
                        "name": "Simon Hilber"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "mer Erdin Yamurlu"
                    },
                    {
                        "name": "Nils Blank"
                    },
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06072v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06072v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21388v1",
                "updated": "2025-10-24T12:19:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    19,
                    19,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:19:19Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    19,
                    19,
                    4,
                    297,
                    0
                ],
                "title": "Compressing Quaternion Convolutional Neural Networks for Audio\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Quaternion Convolutional Neural Networks for Audio\n  Classification"
                },
                "summary": "Conventional Convolutional Neural Networks (CNNs) in the real domain have\nbeen widely used for audio classification. However, their convolution\noperations process multi-channel inputs independently, limiting the ability to\ncapture correlations among channels. This can lead to suboptimal feature\nlearning, particularly for complex audio patterns such as multi-channel\nspectrogram representations. Quaternion Convolutional Neural Networks (QCNNs)\naddress this limitation by employing quaternion algebra to jointly capture\ninter-channel dependencies, enabling more compact models with fewer learnable\nparameters while better exploiting the multi-dimensional nature of audio\nsignals. However, QCNNs exhibit higher computational complexity due to the\noverhead of quaternion operations, resulting in increased inference latency and\nreduced efficiency compared to conventional CNNs, posing challenges for\ndeployment on resource-constrained platforms. To address this challenge, this\nstudy explores knowledge distillation (KD) and pruning, to reduce the\ncomputational complexity of QCNNs while maintaining performance. Our\nexperiments on audio classification reveal that pruning QCNNs achieves similar\nor superior performance compared to KD while requiring less computational\neffort. Compared to conventional CNNs and Transformer-based architectures,\npruned QCNNs achieve competitive performance with a reduced learnable parameter\ncount and computational complexity. On the AudioSet dataset, pruned QCNNs\nreduce computational cost by 50\\% and parameter count by 80\\%, while\nmaintaining performance comparable to the conventional CNNs. Furthermore,\npruned QCNNs generalize well across multiple audio classification benchmarks,\nincluding GTZAN for music genre recognition, ESC-50 for environmental sound\nclassification and RAVDESS for speech emotion recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional Convolutional Neural Networks (CNNs) in the real domain have\nbeen widely used for audio classification. However, their convolution\noperations process multi-channel inputs independently, limiting the ability to\ncapture correlations among channels. This can lead to suboptimal feature\nlearning, particularly for complex audio patterns such as multi-channel\nspectrogram representations. Quaternion Convolutional Neural Networks (QCNNs)\naddress this limitation by employing quaternion algebra to jointly capture\ninter-channel dependencies, enabling more compact models with fewer learnable\nparameters while better exploiting the multi-dimensional nature of audio\nsignals. However, QCNNs exhibit higher computational complexity due to the\noverhead of quaternion operations, resulting in increased inference latency and\nreduced efficiency compared to conventional CNNs, posing challenges for\ndeployment on resource-constrained platforms. To address this challenge, this\nstudy explores knowledge distillation (KD) and pruning, to reduce the\ncomputational complexity of QCNNs while maintaining performance. Our\nexperiments on audio classification reveal that pruning QCNNs achieves similar\nor superior performance compared to KD while requiring less computational\neffort. Compared to conventional CNNs and Transformer-based architectures,\npruned QCNNs achieve competitive performance with a reduced learnable parameter\ncount and computational complexity. On the AudioSet dataset, pruned QCNNs\nreduce computational cost by 50\\% and parameter count by 80\\%, while\nmaintaining performance comparable to the conventional CNNs. Furthermore,\npruned QCNNs generalize well across multiple audio classification benchmarks,\nincluding GTZAN for music genre recognition, ESC-50 for environmental sound\nclassification and RAVDESS for speech emotion recognition."
                },
                "authors": [
                    {
                        "name": "Arshdeep Singh"
                    },
                    {
                        "name": "Vinayak Abrol"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    }
                ],
                "author_detail": {
                    "name": "Mark D. Plumbley"
                },
                "author": "Mark D. Plumbley",
                "arxiv_comment": "Under review in IEEE TASLPRO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12171v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12171v3",
                "updated": "2025-10-24T12:16:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    16,
                    12,
                    4,
                    297,
                    0
                ],
                "published": "2025-02-13T10:33:58Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    33,
                    58,
                    3,
                    44,
                    0
                ],
                "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoRA: Gradient-driven Adaptive Low Rank Adaptation"
                },
                "summary": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework--GoRA (Gradient-driven Adaptive Low Rank Adaptation)--that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches--which often focus on either rank\nselection or initialization in isolation--but also unifies both aspects within\na single framework, enabling more effective and efficient adaptation. Extensive\nexperiments across various architectures and modalities show that GoRA\nconsistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings. Code is available at: https://github.com/hhnqqq/MyTransformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework--GoRA (Gradient-driven Adaptive Low Rank Adaptation)--that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches--which often focus on either rank\nselection or initialization in isolation--but also unifies both aspects within\na single framework, enabling more effective and efficient adaptation. Extensive\nexperiments across various architectures and modalities show that GoRA\nconsistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings. Code is available at: https://github.com/hhnqqq/MyTransformers."
                },
                "authors": [
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Yuchen Ren"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Luyang Zhou"
                    },
                    {
                        "name": "Shucun Ju"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12171v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12171v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21386v1",
                "updated": "2025-10-24T12:13:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    13,
                    55,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:13:55Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    13,
                    55,
                    4,
                    297,
                    0
                ],
                "title": "Low-Complexity MIMO Channel Estimation with Latent Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity MIMO Channel Estimation with Latent Diffusion Models"
                },
                "summary": "Deep generative models offer a powerful alternative to conventional channel\nestimation by learning the complex prior distribution of wireless channels.\nCapitalizing on this potential, this paper proposes a novel channel estimation\nalgorithm based on latent diffusion models (LDMs), termed posterior sampling\nwith latent diffusion for channel estimation (PSLD-CE). The core of our\napproach is a lightweight LDM architecture specifically designed for channel\nestimation, which serves as a powerful generative prior to capture the\nintricate channel distribution. Furthermore, we enhance the diffusion posterior\nsampling process by introducing an effective approximation for the likelihood\nterm and a tailored self-consistency constraint on the variational autoencoder\nlatent space. Extensive experimental results demonstrate that PSLD-CE\nconsistently outperforms a wide range of existing methods. Notably, these\nsignificant performance gains are achieved while maintaining low computational\ncomplexity and fast inference speed, establishing our method as a highly\npromising and practical solution for next-generation wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep generative models offer a powerful alternative to conventional channel\nestimation by learning the complex prior distribution of wireless channels.\nCapitalizing on this potential, this paper proposes a novel channel estimation\nalgorithm based on latent diffusion models (LDMs), termed posterior sampling\nwith latent diffusion for channel estimation (PSLD-CE). The core of our\napproach is a lightweight LDM architecture specifically designed for channel\nestimation, which serves as a powerful generative prior to capture the\nintricate channel distribution. Furthermore, we enhance the diffusion posterior\nsampling process by introducing an effective approximation for the likelihood\nterm and a tailored self-consistency constraint on the variational autoencoder\nlatent space. Extensive experimental results demonstrate that PSLD-CE\nconsistently outperforms a wide range of existing methods. Notably, these\nsignificant performance gains are achieved while maintaining low computational\ncomplexity and fast inference speed, establishing our method as a highly\npromising and practical solution for next-generation wireless systems."
                },
                "authors": [
                    {
                        "name": "Xiaotian Fan"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13665v2",
                "updated": "2025-10-24T12:06:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    6,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-15T15:25:20Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    25,
                    20,
                    2,
                    288,
                    0
                ],
                "title": "Axial Neural Networks for Dimension-Free Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Axial Neural Networks for Dimension-Free Foundation Models"
                },
                "summary": "The advent of foundation models in AI has significantly advanced\ngeneral-purpose learning, enabling remarkable capabilities in zero-shot\ninference and in-context learning. However, training such models on physics\ndata, including solutions to partial differential equations (PDEs), poses a\nunique challenge due to varying dimensionalities across different systems.\nTraditional approaches either fix a maximum dimension or employ separate\nencoders for different dimensionalities, resulting in inefficiencies. To\naddress this, we propose a dimension-agnostic neural network architecture, the\nAxial Neural Network (XNN), inspired by parameter-sharing structures such as\nDeep Sets and Graph Neural Networks. XNN generalizes across varying tensor\ndimensions while maintaining computational efficiency. We convert existing PDE\nfoundation models into axial neural networks and evaluate their performance\nacross three training scenarios: training from scratch, pretraining on multiple\nPDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform\ncompetitively with original models and exhibit superior generalization to\nunseen dimensions, highlighting the importance of multidimensional pretraining\nfor foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models in AI has significantly advanced\ngeneral-purpose learning, enabling remarkable capabilities in zero-shot\ninference and in-context learning. However, training such models on physics\ndata, including solutions to partial differential equations (PDEs), poses a\nunique challenge due to varying dimensionalities across different systems.\nTraditional approaches either fix a maximum dimension or employ separate\nencoders for different dimensionalities, resulting in inefficiencies. To\naddress this, we propose a dimension-agnostic neural network architecture, the\nAxial Neural Network (XNN), inspired by parameter-sharing structures such as\nDeep Sets and Graph Neural Networks. XNN generalizes across varying tensor\ndimensions while maintaining computational efficiency. We convert existing PDE\nfoundation models into axial neural networks and evaluate their performance\nacross three training scenarios: training from scratch, pretraining on multiple\nPDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform\ncompetitively with original models and exhibit superior generalization to\nunseen dimensions, highlighting the importance of multidimensional pretraining\nfor foundation models."
                },
                "authors": [
                    {
                        "name": "Hyunsu Kim"
                    },
                    {
                        "name": "Jonggeon Park"
                    },
                    {
                        "name": "Joan Bruna"
                    },
                    {
                        "name": "Hongseok Yang"
                    },
                    {
                        "name": "Juho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Juho Lee"
                },
                "author": "Juho Lee",
                "arxiv_journal_ref": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18988v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18988v3",
                "updated": "2025-10-24T12:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    5,
                    14,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-21T18:10:45Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    18,
                    10,
                    45,
                    1,
                    294,
                    0
                ],
                "title": "Timely Clinical Diagnosis through Active Test Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Clinical Diagnosis through Active Test Selection"
                },
                "summary": "There is growing interest in using machine learning (ML) to support clinical\ndiagnosis, but most approaches rely on static, fully observed datasets and fail\nto reflect the sequential, resource-aware reasoning clinicians use in practice.\nDiagnosis remains complex and error prone, especially in high-pressure or\nresource-limited settings, underscoring the need for frameworks that help\nclinicians make timely and cost-effective decisions. We propose ACTMED\n(Adaptive Clinical Test selection via Model-based Experimental Design), a\ndiagnostic framework that integrates Bayesian Experimental Design (BED) with\nlarge language models (LLMs) to better emulate real-world diagnostic reasoning.\nAt each step, ACTMED selects the test expected to yield the greatest reduction\nin diagnostic uncertainty for a given patient. LLMs act as flexible simulators,\ngenerating plausible patient state distributions and supporting belief updates\nwithout requiring structured, task-specific training data. Clinicians can\nremain in the loop; reviewing test suggestions, interpreting intermediate\noutputs, and applying clinical judgment throughout. We evaluate ACTMED on\nreal-world datasets and show it can optimize test selection to improve\ndiagnostic accuracy, interpretability, and resource use. This represents a step\ntoward transparent, adaptive, and clinician-aligned diagnostic systems that\ngeneralize across settings with reduced reliance on domain-specific data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest in using machine learning (ML) to support clinical\ndiagnosis, but most approaches rely on static, fully observed datasets and fail\nto reflect the sequential, resource-aware reasoning clinicians use in practice.\nDiagnosis remains complex and error prone, especially in high-pressure or\nresource-limited settings, underscoring the need for frameworks that help\nclinicians make timely and cost-effective decisions. We propose ACTMED\n(Adaptive Clinical Test selection via Model-based Experimental Design), a\ndiagnostic framework that integrates Bayesian Experimental Design (BED) with\nlarge language models (LLMs) to better emulate real-world diagnostic reasoning.\nAt each step, ACTMED selects the test expected to yield the greatest reduction\nin diagnostic uncertainty for a given patient. LLMs act as flexible simulators,\ngenerating plausible patient state distributions and supporting belief updates\nwithout requiring structured, task-specific training data. Clinicians can\nremain in the loop; reviewing test suggestions, interpreting intermediate\noutputs, and applying clinical judgment throughout. We evaluate ACTMED on\nreal-world datasets and show it can optimize test selection to improve\ndiagnostic accuracy, interpretability, and resource use. This represents a step\ntoward transparent, adaptive, and clinician-aligned diagnostic systems that\ngeneralize across settings with reduced reliance on domain-specific data."
                },
                "authors": [
                    {
                        "name": "Silas Ruhrberg Estvez"
                    },
                    {
                        "name": "Nicols Astorga"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "None",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18988v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18988v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21378v1",
                "updated": "2025-10-24T12:00:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    0,
                    47,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:00:47Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    0,
                    47,
                    4,
                    297,
                    0
                ],
                "title": "Optimized Power Control for Multi-User Integrated Sensing and Edge AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized Power Control for Multi-User Integrated Sensing and Edge AI"
                },
                "summary": "This work investigates an integrated sensing and edge artificial intelligence\n(ISEA) system, where multiple devices first transmit probing signals for target\nsensing and then offload locally extracted features to the access point (AP)\nvia analog over-the-air computation (AirComp) for collaborative inference. To\ncharacterize the relationship between AirComp error and inference performance,\ntwo proxies are established: the \\emph{computation-optimal} proxy that\nminimizes the aggregation distortion, and the \\emph{decision-optimal} proxy\nthat maximizes the inter-class separability, respectively. Optimal transceiver\ndesigns in terms of closed-form power allocation are derived for both\ntime-division multiplexing (TDM) and frequency-division multiplexing (FDM)\nsettings, revealing threshold-based and dual-decomposition structures,\nrespectively. Experimental results validate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates an integrated sensing and edge artificial intelligence\n(ISEA) system, where multiple devices first transmit probing signals for target\nsensing and then offload locally extracted features to the access point (AP)\nvia analog over-the-air computation (AirComp) for collaborative inference. To\ncharacterize the relationship between AirComp error and inference performance,\ntwo proxies are established: the \\emph{computation-optimal} proxy that\nminimizes the aggregation distortion, and the \\emph{decision-optimal} proxy\nthat maximizes the inter-class separability, respectively. Optimal transceiver\ndesigns in terms of closed-form power allocation are derived for both\ntime-division multiplexing (TDM) and frequency-division multiplexing (FDM)\nsettings, revealing threshold-based and dual-decomposition structures,\nrespectively. Experimental results validate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Biao Dong"
                    },
                    {
                        "name": "Bin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cao"
                },
                "author": "Bin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12854v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12854v3",
                "updated": "2025-10-24T11:56:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    56,
                    39,
                    4,
                    297,
                    0
                ],
                "published": "2024-10-10T22:22:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    22,
                    22,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees"
                },
                "summary": "In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets. Our code is publicly available at\nhttps://github.com/MrBlankness/TPO.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets. Our code is publicly available at\nhttps://github.com/MrBlankness/TPO.git."
                },
                "authors": [
                    {
                        "name": "Weibin Liao"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12854v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12854v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08730v2",
                "updated": "2025-10-24T11:50:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    50,
                    54,
                    4,
                    297,
                    0
                ],
                "published": "2025-08-12T08:21:58Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    21,
                    58,
                    1,
                    224,
                    0
                ],
                "title": "Magical: Medical Lay Language Generation via Semantic Invariance and\n  Layperson-tailored Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magical: Medical Lay Language Generation via Semantic Invariance and\n  Layperson-tailored Adaptation"
                },
                "summary": "Medical Lay Language Generation (MLLG) plays a vital role in improving the\naccessibility of complex scientific content for broader audiences. Recent\nliterature to MLLG commonly employ parameter-efficient fine-tuning methods such\nas Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using\npaired expert-lay language datasets. However, LoRA struggles with the\nchallenges posed by multi-source heterogeneous MLLG datasets. Specifically,\nthrough a series of exploratory experiments, we reveal that standard LoRA fail\nto meet the requirement for semantic fidelity and diverse lay-style generation\nin MLLG task. To address these limitations, we propose Magical, an asymmetric\nLoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical\nemploys a shared matrix $A$ for abstractive summarization, along with multiple\nisolated matrices $B$ for diverse lay-style generation. To preserve semantic\nfidelity during the lay language generation process, Magical introduces a\nSemantic Invariance Constraint to mitigate semantic subspace shifts on matrix\n$A$. Furthermore, to better adapt to diverse lay-style generation, Magical\nincorporates the Recommendation-guided Switch, an externally interface to\nprompt the LLM to switch between different matrices $B$. Experimental results\non three real-world lay language generation datasets demonstrate that Magical\nconsistently outperforms prompt-based methods, vanilla LoRA, and its recent\nvariants, while also reducing trainable parameters by 31.66%. Our code is\npublicly available at https://github.com/tianlwang/Magical.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Lay Language Generation (MLLG) plays a vital role in improving the\naccessibility of complex scientific content for broader audiences. Recent\nliterature to MLLG commonly employ parameter-efficient fine-tuning methods such\nas Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using\npaired expert-lay language datasets. However, LoRA struggles with the\nchallenges posed by multi-source heterogeneous MLLG datasets. Specifically,\nthrough a series of exploratory experiments, we reveal that standard LoRA fail\nto meet the requirement for semantic fidelity and diverse lay-style generation\nin MLLG task. To address these limitations, we propose Magical, an asymmetric\nLoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical\nemploys a shared matrix $A$ for abstractive summarization, along with multiple\nisolated matrices $B$ for diverse lay-style generation. To preserve semantic\nfidelity during the lay language generation process, Magical introduces a\nSemantic Invariance Constraint to mitigate semantic subspace shifts on matrix\n$A$. Furthermore, to better adapt to diverse lay-style generation, Magical\nincorporates the Recommendation-guided Switch, an externally interface to\nprompt the LLM to switch between different matrices $B$. Experimental results\non three real-world lay language generation datasets demonstrate that Magical\nconsistently outperforms prompt-based methods, vanilla LoRA, and its recent\nvariants, while also reducing trainable parameters by 31.66%. Our code is\npublicly available at https://github.com/tianlwang/Magical.git."
                },
                "authors": [
                    {
                        "name": "Weibin Liao"
                    },
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Liantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Liantao Ma"
                },
                "author": "Liantao Ma",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11128v2",
                "updated": "2025-10-24T11:47:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    47,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-10T17:04:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    4,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "Theory-Grounded Evaluation of Human-Like Fallacy Patterns in LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-Grounded Evaluation of Human-Like Fallacy Patterns in LLM\n  Reasoning"
                },
                "summary": "We study logical reasoning in language models by asking whether their errors\nfollow established human fallacy patterns. Using the Erotetic Theory of\nReasoning (ETR) and its open-source implementation, PyETR, we programmatically\ngenerate 383 formally specified reasoning problems and evaluate 38 models. For\neach response, we judge logical correctness and, when incorrect, whether it\nmatches an ETR-predicted fallacy. Two results stand out: (i) as a capability\nproxy (Chatbot Arena Elo) increases, a larger share of a model's incorrect\nanswers are ETR-predicted fallacies $(\\rho=0.360, p=0.0265)$, while overall\ncorrectness on this dataset shows no correlation with capability; (ii)\nreversing premise order significantly reduces fallacy production for many\nmodels, mirroring human order effects. Methodologically, PyETR provides an\nopen-source pipeline for unbounded, synthetic, contamination-resistant\nreasoning tests linked to a cognitive theory, enabling analyses that focus on\nerror composition rather than error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study logical reasoning in language models by asking whether their errors\nfollow established human fallacy patterns. Using the Erotetic Theory of\nReasoning (ETR) and its open-source implementation, PyETR, we programmatically\ngenerate 383 formally specified reasoning problems and evaluate 38 models. For\neach response, we judge logical correctness and, when incorrect, whether it\nmatches an ETR-predicted fallacy. Two results stand out: (i) as a capability\nproxy (Chatbot Arena Elo) increases, a larger share of a model's incorrect\nanswers are ETR-predicted fallacies $(\\rho=0.360, p=0.0265)$, while overall\ncorrectness on this dataset shows no correlation with capability; (ii)\nreversing premise order significantly reduces fallacy production for many\nmodels, mirroring human order effects. Methodologically, PyETR provides an\nopen-source pipeline for unbounded, synthetic, contamination-resistant\nreasoning tests linked to a cognitive theory, enabling analyses that focus on\nerror composition rather than error rate."
                },
                "authors": [
                    {
                        "name": "Andrew Keenan Richardson"
                    },
                    {
                        "name": "Ryan Othniel Kearns"
                    },
                    {
                        "name": "Sean Moss"
                    },
                    {
                        "name": "Vincent Wang-Mascianica"
                    },
                    {
                        "name": "Philipp Koralus"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Koralus"
                },
                "author": "Philipp Koralus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02422v2",
                "updated": "2025-10-24T11:43:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    43,
                    40,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-02T16:40:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    40,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "Dynamic Target Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Target Attack"
                },
                "summary": "Existing gradient-based jailbreak attacks typically optimize an adversarial\nsuffix to induce a fixed affirmative response. However, this fixed target\nusually resides in an extremely low-density region of a safety-aligned LLM's\noutput distribution conditioned on diverse harmful inputs. Due to the\nsubstantial discrepancy between the target and the original output, existing\nattacks require numerous iterations to optimize the adversarial prompt, which\nmight still fail to induce the low-probability target response from the target\nLLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking\nframework relying on the target LLM's own responses as targets to optimize the\nadversarial prompts. In each optimization round, DTA iteratively samples\nmultiple candidate responses directly from the output distribution conditioned\non the current prompt, and selects the most harmful response as a temporary\ntarget for prompt optimization. In contrast to existing attacks, DTA\nsignificantly reduces the discrepancy between the target and the output\ndistribution, substantially easing the optimization process to search for an\neffective adversarial prompt.\n  Extensive experiments demonstrate the superior effectiveness and efficiency\nof DTA: under the white-box setting, DTA only needs 200 optimization iterations\nto achieve an average attack success rate (ASR) of over 87\\% on recent\nsafety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\\%. The\ntime cost of DTA is 2-26 times less than existing baselines. Under the\nblack-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target\nsampling and achieves an ASR of 85\\% against the black-box target model\nLlama-3-70B-Instruct, exceeding its counterparts by over 25\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing gradient-based jailbreak attacks typically optimize an adversarial\nsuffix to induce a fixed affirmative response. However, this fixed target\nusually resides in an extremely low-density region of a safety-aligned LLM's\noutput distribution conditioned on diverse harmful inputs. Due to the\nsubstantial discrepancy between the target and the original output, existing\nattacks require numerous iterations to optimize the adversarial prompt, which\nmight still fail to induce the low-probability target response from the target\nLLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking\nframework relying on the target LLM's own responses as targets to optimize the\nadversarial prompts. In each optimization round, DTA iteratively samples\nmultiple candidate responses directly from the output distribution conditioned\non the current prompt, and selects the most harmful response as a temporary\ntarget for prompt optimization. In contrast to existing attacks, DTA\nsignificantly reduces the discrepancy between the target and the output\ndistribution, substantially easing the optimization process to search for an\neffective adversarial prompt.\n  Extensive experiments demonstrate the superior effectiveness and efficiency\nof DTA: under the white-box setting, DTA only needs 200 optimization iterations\nto achieve an average attack success rate (ASR) of over 87\\% on recent\nsafety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\\%. The\ntime cost of DTA is 2-26 times less than existing baselines. Under the\nblack-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target\nsampling and achieves an ASR of 85\\% against the black-box target model\nLlama-3-70B-Instruct, exceeding its counterparts by over 25\\%."
                },
                "authors": [
                    {
                        "name": "Kedong Xiu"
                    },
                    {
                        "name": "Churui Zeng"
                    },
                    {
                        "name": "Tianhang Zheng"
                    },
                    {
                        "name": "Xinzhe Huang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Puning Zhao"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06041v2",
                "updated": "2025-10-24T11:37:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    37,
                    52,
                    4,
                    297,
                    0
                ],
                "published": "2025-08-08T05:57:04Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    5,
                    57,
                    4,
                    4,
                    220,
                    0
                ],
                "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision\n  Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision\n  Assignment"
                },
                "summary": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding steps. Building on this insight,\nwe introduce DP-LLM, a novel mechanism that dynamically assigns precision to\neach layer based on input values. Experimental results across multiple models\nand benchmarks demonstrate that DP-LLM achieves a superior performance-latency\ntrade-off, outperforming prior approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding steps. Building on this insight,\nwe introduce DP-LLM, a novel mechanism that dynamically assigns precision to\neach layer based on input values. Experimental results across multiple models\nand benchmarks demonstrate that DP-LLM achieves a superior performance-latency\ntrade-off, outperforming prior approaches."
                },
                "authors": [
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Seong Hoon Seo"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Yeonhong Park"
                    }
                ],
                "author_detail": {
                    "name": "Yeonhong Park"
                },
                "author": "Yeonhong Park",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.23726v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23726v2",
                "updated": "2025-10-24T17:47:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    47,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-30T10:58:49Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    10,
                    58,
                    49,
                    0,
                    181,
                    0
                ],
                "title": "System-Embedded Diffusion Bridge Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System-Embedded Diffusion Bridge Models"
                },
                "summary": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications."
                },
                "authors": [
                    {
                        "name": "Bartlomiej Sobieski"
                    },
                    {
                        "name": "Matthew Tivnan"
                    },
                    {
                        "name": "Yuang Wang"
                    },
                    {
                        "name": "Siyeop Yoon"
                    },
                    {
                        "name": "Pengfei Jin"
                    },
                    {
                        "name": "Dufan Wu"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Przemyslaw Biecek"
                    }
                ],
                "author_detail": {
                    "name": "Przemyslaw Biecek"
                },
                "author": "Przemyslaw Biecek",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23726v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23726v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21689v1",
                "updated": "2025-10-24T17:46:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    46,
                    24,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:46:24Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    46,
                    24,
                    4,
                    297,
                    0
                ],
                "title": "On Thin Ice: Towards Explainable Conservation Monitoring via Attribution\n  and Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Thin Ice: Towards Explainable Conservation Monitoring via Attribution\n  and Perturbations"
                },
                "summary": "Computer vision can accelerate ecological research and conservation\nmonitoring, yet adoption in ecology lags in part because of a lack of trust in\nblack-box neural-network-based models. We seek to address this challenge by\napplying post-hoc explanations to provide evidence for predictions and document\nlimitations that are important to field deployment. Using aerial imagery from\nGlacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor\nseals) and generate explanations via gradient-based class activation mapping\n(HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME),\nand perturbation-based explanations. We assess explanations along three axes\nrelevant to field use: (i) localization fidelity: whether high-attribution\nregions coincide with the animal rather than background context; (ii)\nfaithfulness: whether deletion/insertion tests produce changes in detector\nconfidence; and (iii) diagnostic utility: whether explanations reveal\nsystematic failure modes. Explanations concentrate on seal torsos and contours\nrather than surrounding ice/rock, and removal of the seals reduces detection\nconfidence, providing model-evidence for true positives. The analysis also\nuncovers recurrent error sources, including confusion between seals and black\nice and rocks. We translate these findings into actionable next steps for model\ndevelopment, including more targeted data curation and augmentation. By pairing\nobject detection with post-hoc explainability, we can move beyond \"black-box\"\npredictions toward auditable, decision-supporting tools for conservation\nmonitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer vision can accelerate ecological research and conservation\nmonitoring, yet adoption in ecology lags in part because of a lack of trust in\nblack-box neural-network-based models. We seek to address this challenge by\napplying post-hoc explanations to provide evidence for predictions and document\nlimitations that are important to field deployment. Using aerial imagery from\nGlacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor\nseals) and generate explanations via gradient-based class activation mapping\n(HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME),\nand perturbation-based explanations. We assess explanations along three axes\nrelevant to field use: (i) localization fidelity: whether high-attribution\nregions coincide with the animal rather than background context; (ii)\nfaithfulness: whether deletion/insertion tests produce changes in detector\nconfidence; and (iii) diagnostic utility: whether explanations reveal\nsystematic failure modes. Explanations concentrate on seal torsos and contours\nrather than surrounding ice/rock, and removal of the seals reduces detection\nconfidence, providing model-evidence for true positives. The analysis also\nuncovers recurrent error sources, including confusion between seals and black\nice and rocks. We translate these findings into actionable next steps for model\ndevelopment, including more targeted data curation and augmentation. By pairing\nobject detection with post-hoc explainability, we can move beyond \"black-box\"\npredictions toward auditable, decision-supporting tools for conservation\nmonitoring."
                },
                "authors": [
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Gnel Aghakishiyeva"
                    },
                    {
                        "name": "Saagar Arya"
                    },
                    {
                        "name": "Julian Dale"
                    },
                    {
                        "name": "James David Poling"
                    },
                    {
                        "name": "Holly R. Houliston"
                    },
                    {
                        "name": "Jamie N. Womble"
                    },
                    {
                        "name": "Gregory D. Larsen"
                    },
                    {
                        "name": "David W. Johnston"
                    },
                    {
                        "name": "Brinnae Bent"
                    }
                ],
                "author_detail": {
                    "name": "Brinnae Bent"
                },
                "author": "Brinnae Bent",
                "arxiv_comment": "NeurIPS Imageomics Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23773v2",
                "updated": "2025-10-24T17:44:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    44,
                    52,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-31T17:57:20Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    17,
                    57,
                    20,
                    3,
                    212,
                    0
                ],
                "title": "SimuRA: A World-Model-Driven Simulative Reasoning Architecture for\n  General Goal-Oriented Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimuRA: A World-Model-Driven Simulative Reasoning Architecture for\n  General Goal-Oriented Agents"
                },
                "summary": "AI agents built on foundation models hold enormous promise. Current practice,\nhowever, focuses on a one-task-one-agent approach, which not only falls short\nof scalability and generality, but also faces practical limitations from\nblack-box autoregressive reasoning, where decisions unfold token by token\nwithout explicit simulation or counterfactual evaluation of outcomes. Humans,\non the other hand, reason and plan by mentally simulating the consequences of\nactions within an internal model of the world -- a capability that supports\nflexible, goal-directed behavior across diverse contexts. Moving towards a more\ngeneral and powerful AI agent, we introduce SimuRA, a goal-oriented\narchitecture for generalized agentic reasoning. Based on a principled\nformulation of an optimal agent in any general environment, SimuRA addresses\nthe limitations of black-box autoregressive reasoning by incorporating the\nworld model for planning via simulation. Our prototype world model is\nimplemented using LLMs as a substrate, leveraging the natural language as a\ndiscrete, hierarchical representation grounded in concepts for planning, while\nremaining model-agnostic. On complex web-browsing tasks such as flight search,\nSimuRA improves the success rate from 0% to 32.2% compared to a representative\nopen-web agent baseline. Across tasks, world-model-based planning achieves up\nto 124% higher task completion rates than a matched black-box autoregressive\nbaseline, demonstrating the advantages of simulative reasoning. We release\nReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source\nresearch demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents built on foundation models hold enormous promise. Current practice,\nhowever, focuses on a one-task-one-agent approach, which not only falls short\nof scalability and generality, but also faces practical limitations from\nblack-box autoregressive reasoning, where decisions unfold token by token\nwithout explicit simulation or counterfactual evaluation of outcomes. Humans,\non the other hand, reason and plan by mentally simulating the consequences of\nactions within an internal model of the world -- a capability that supports\nflexible, goal-directed behavior across diverse contexts. Moving towards a more\ngeneral and powerful AI agent, we introduce SimuRA, a goal-oriented\narchitecture for generalized agentic reasoning. Based on a principled\nformulation of an optimal agent in any general environment, SimuRA addresses\nthe limitations of black-box autoregressive reasoning by incorporating the\nworld model for planning via simulation. Our prototype world model is\nimplemented using LLMs as a substrate, leveraging the natural language as a\ndiscrete, hierarchical representation grounded in concepts for planning, while\nremaining model-agnostic. On complex web-browsing tasks such as flight search,\nSimuRA improves the success rate from 0% to 32.2% compared to a representative\nopen-web agent baseline. Across tasks, world-model-based planning achieves up\nto 124% higher task completion rates than a matched black-box autoregressive\nbaseline, demonstrating the advantages of simulative reasoning. We release\nReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source\nresearch demo."
                },
                "authors": [
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Jinyu Hou"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "arxiv_comment": "This submission has been updated to adjust the scope and presentation\n  of the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05822v2",
                "updated": "2025-10-24T17:44:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    44,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-04-08T09:05:33Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    5,
                    33,
                    1,
                    98,
                    0
                ],
                "title": "Federated Unlearning Made Practical: Seamless Integration via Negated\n  Pseudo-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Unlearning Made Practical: Seamless Integration via Negated\n  Pseudo-Gradients"
                },
                "summary": "The right to be forgotten is a fundamental principle of privacy-preserving\nregulations and extends to Machine Learning (ML) paradigms such as Federated\nLearning (FL). While FL enhances privacy by enabling collaborative model\ntraining without sharing private data, trained models still retain the\ninfluence of training data. Federated Unlearning (FU) methods recently proposed\noften rely on impractical assumptions for real-world FL deployments, such as\nstoring client update histories or requiring access to a publicly available\ndataset. To address these constraints, this paper introduces a novel method\nthat leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).\nOur approach only uses standard client model updates, which are employed during\nregular FL rounds, and interprets them as pseudo-gradients. When a client needs\nto be forgotten, we apply the negation of their pseudo-gradients, appropriately\nscaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly\nintegrates with FL workflows, incurs no additional computational and\ncommunication overhead beyond standard FL rounds, and supports concurrent\nunlearning requests. We extensively evaluated the proposed method on two\nwell-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and\na real-world medical imaging dataset for segmentation (ProstateMRI), using\nthree different neural architectures: two residual networks and a vision\ntransformer. The experimental results across various settings demonstrate that\nPUF achieves state-of-the-art forgetting effectiveness and recovery time,\nwithout relying on any additional assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The right to be forgotten is a fundamental principle of privacy-preserving\nregulations and extends to Machine Learning (ML) paradigms such as Federated\nLearning (FL). While FL enhances privacy by enabling collaborative model\ntraining without sharing private data, trained models still retain the\ninfluence of training data. Federated Unlearning (FU) methods recently proposed\noften rely on impractical assumptions for real-world FL deployments, such as\nstoring client update histories or requiring access to a publicly available\ndataset. To address these constraints, this paper introduces a novel method\nthat leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).\nOur approach only uses standard client model updates, which are employed during\nregular FL rounds, and interprets them as pseudo-gradients. When a client needs\nto be forgotten, we apply the negation of their pseudo-gradients, appropriately\nscaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly\nintegrates with FL workflows, incurs no additional computational and\ncommunication overhead beyond standard FL rounds, and supports concurrent\nunlearning requests. We extensively evaluated the proposed method on two\nwell-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and\na real-world medical imaging dataset for segmentation (ProstateMRI), using\nthree different neural architectures: two residual networks and a vision\ntransformer. The experimental results across various settings demonstrate that\nPUF achieves state-of-the-art forgetting effectiveness and recovery time,\nwithout relying on any additional assumptions."
                },
                "authors": [
                    {
                        "name": "Alessio Mora"
                    },
                    {
                        "name": "Carlo Mazzocca"
                    },
                    {
                        "name": "Rebecca Montanari"
                    },
                    {
                        "name": "Paolo Bellavista"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Bellavista"
                },
                "author": "Paolo Bellavista",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21684v1",
                "updated": "2025-10-24T17:40:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    40,
                    12,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:40:12Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    40,
                    12,
                    4,
                    297,
                    0
                ],
                "title": "Toward provably private analytics and insights into GenAI use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward provably private analytics and insights into GenAI use"
                },
                "summary": "Large-scale systems that compute analytics over a fleet of devices must\nachieve high privacy and security standards while also meeting data quality,\nusability, and resource efficiency expectations. We present a next-generation\nfederated analytics system that uses Trusted Execution Environments (TEEs)\nbased on technologies like AMD SEV-SNP and Intel TDX to provide verifiable\nprivacy guarantees for all server-side processing. In our system, devices\nencrypt and upload data, tagging it with a limited set of allowable server-side\nprocessing steps. An open source, TEE-hosted key management service guarantees\nthat the data is accessible only to those steps, which are themselves protected\nby TEE confidentiality and integrity assurance guarantees. The system is\ndesigned for flexible workloads, including processing unstructured data with\nLLMs (for structured summarization) before aggregation into differentially\nprivate insights (with automatic parameter tuning). The transparency properties\nof our system allow any external party to verify that all raw and derived data\nis processed in TEEs, protecting it from inspection by the system operator, and\nthat differential privacy is applied to all released results. This system has\nbeen successfully deployed in production, providing helpful insights into\nreal-world GenAI experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale systems that compute analytics over a fleet of devices must\nachieve high privacy and security standards while also meeting data quality,\nusability, and resource efficiency expectations. We present a next-generation\nfederated analytics system that uses Trusted Execution Environments (TEEs)\nbased on technologies like AMD SEV-SNP and Intel TDX to provide verifiable\nprivacy guarantees for all server-side processing. In our system, devices\nencrypt and upload data, tagging it with a limited set of allowable server-side\nprocessing steps. An open source, TEE-hosted key management service guarantees\nthat the data is accessible only to those steps, which are themselves protected\nby TEE confidentiality and integrity assurance guarantees. The system is\ndesigned for flexible workloads, including processing unstructured data with\nLLMs (for structured summarization) before aggregation into differentially\nprivate insights (with automatic parameter tuning). The transparency properties\nof our system allow any external party to verify that all raw and derived data\nis processed in TEEs, protecting it from inspection by the system operator, and\nthat differential privacy is applied to all released results. This system has\nbeen successfully deployed in production, providing helpful insights into\nreal-world GenAI experiences."
                },
                "authors": [
                    {
                        "name": "Albert Cheu"
                    },
                    {
                        "name": "Artem Lagzdin"
                    },
                    {
                        "name": "Brett McLarnon"
                    },
                    {
                        "name": "Daniel Ramage"
                    },
                    {
                        "name": "Katharine Daly"
                    },
                    {
                        "name": "Marco Gruteser"
                    },
                    {
                        "name": "Peter Kairouz"
                    },
                    {
                        "name": "Rakshita Tandon"
                    },
                    {
                        "name": "Stanislav Chiknavaryan"
                    },
                    {
                        "name": "Timon Van Overveldt"
                    },
                    {
                        "name": "Zoe Gong"
                    }
                ],
                "author_detail": {
                    "name": "Zoe Gong"
                },
                "author": "Zoe Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06615v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06615v3",
                "updated": "2025-10-24T17:38:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    38,
                    29,
                    4,
                    297,
                    0
                ],
                "published": "2025-08-08T18:05:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    5,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "Iris RESTful Server and IrisTileSource: An Iris implementation for\n  existing OpenSeaDragon viewers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris RESTful Server and IrisTileSource: An Iris implementation for\n  existing OpenSeaDragon viewers"
                },
                "summary": "The Iris File Extension (IFE) is a low overhead performance-oriented whole\nslide image (WSI) file format designed to improve the image rendering\nexperience for pathologists and simplify image management for system\nadministrators. However, static hypertext transfer protocol (HTTP) file servers\ncannot natively stream subregions of high-resolution image files, such as the\nIFE. The majority of contemporary WSI viewer systems are designed as\nbrowser-based web applications and leverage OpenSeaDragon as the tile-based\nrendering framework. These systems convert WSI files to Deep Zoom Images (DZI)\nfor compatibility with simple static HTTP file servers. To address this\nlimitation, we have developed the Iris RESTful Server, a low-overhead HTTP\nserver with a RESTful API that is natively compatible with the DICOMweb WADO-RS\nAPI. Written in C++ with Boost Beast HTTP and Asio networking libraries atop\nthe public IFE libraries, the server offers both security and high performance.\nTesting shows that a single Raspberry Pi equivalent system can handle a peak of\n5,061 req/s (average 3,883 req/s) with a median latency of 21 ms on a private\n(i.e. hospital) network. We also developed and merged a new OpenSeaDragon\nTileSource, compatible with the Iris RESTful API, into the next OpenSeaDragon\nrelease, enabling simple and immediate drop-in replacement of DZI images within\nWSI viewer stacks. Designed as a secure cross-origin resource sharing\nmicroservice, this architecture includes detailed deployment instructions for\nnew or existing WSI workflows, and the public\nexamples.restful.irisdigitalpathology.org subdomain is provided as a\ndevelopment tool to accelerate WSI web viewer development. All relevant Iris\nsoftware is available under the open-source MIT software license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Iris File Extension (IFE) is a low overhead performance-oriented whole\nslide image (WSI) file format designed to improve the image rendering\nexperience for pathologists and simplify image management for system\nadministrators. However, static hypertext transfer protocol (HTTP) file servers\ncannot natively stream subregions of high-resolution image files, such as the\nIFE. The majority of contemporary WSI viewer systems are designed as\nbrowser-based web applications and leverage OpenSeaDragon as the tile-based\nrendering framework. These systems convert WSI files to Deep Zoom Images (DZI)\nfor compatibility with simple static HTTP file servers. To address this\nlimitation, we have developed the Iris RESTful Server, a low-overhead HTTP\nserver with a RESTful API that is natively compatible with the DICOMweb WADO-RS\nAPI. Written in C++ with Boost Beast HTTP and Asio networking libraries atop\nthe public IFE libraries, the server offers both security and high performance.\nTesting shows that a single Raspberry Pi equivalent system can handle a peak of\n5,061 req/s (average 3,883 req/s) with a median latency of 21 ms on a private\n(i.e. hospital) network. We also developed and merged a new OpenSeaDragon\nTileSource, compatible with the Iris RESTful API, into the next OpenSeaDragon\nrelease, enabling simple and immediate drop-in replacement of DZI images within\nWSI viewer stacks. Designed as a secure cross-origin resource sharing\nmicroservice, this architecture includes detailed deployment instructions for\nnew or existing WSI workflows, and the public\nexamples.restful.irisdigitalpathology.org subdomain is provided as a\ndevelopment tool to accelerate WSI web viewer development. All relevant Iris\nsoftware is available under the open-source MIT software license."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater MD"
                    },
                    {
                        "name": "Navin Kathawa"
                    },
                    {
                        "name": "Mustafa Yousif MD"
                    },
                    {
                        "name": "Ulysses Balis MD"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis MD"
                },
                "author": "Ulysses Balis MD",
                "arxiv_comment": "10 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06615v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06615v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21671v1",
                "updated": "2025-10-24T17:27:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    27,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:27:35Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    27,
                    35,
                    4,
                    297,
                    0
                ],
                "title": "A Data-Centric Approach to Multilingual E-Commerce Product Search: Case\n  Study on Query-Category and Query-Item Relevance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-Centric Approach to Multilingual E-Commerce Product Search: Case\n  Study on Query-Category and Query-Item Relevance"
                },
                "summary": "Multilingual e-commerce search suffers from severe data imbalance across\nlanguages, label noise, and limited supervision for low-resource\nlanguages--challenges that impede the cross-lingual generalization of relevance\nmodels despite the strong capabilities of large language models (LLMs). In this\nwork, we present a practical, architecture-agnostic, data-centric framework to\nenhance performance on two core tasks: Query-Category (QC) relevance (matching\nqueries to product categories) and Query-Item (QI) relevance (matching queries\nto product titles). Rather than altering the model, we redesign the training\ndata through three complementary strategies: (1) translation-based augmentation\nto synthesize examples for languages absent in training, (2) semantic negative\nsampling to generate hard negatives and mitigate class imbalance, and (3)\nself-validation filtering to detect and remove likely mislabeled instances.\nEvaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields\nsubstantial F1 score improvements over strong LLM baselines, achieving\ncompetitive results in the official competition. Our findings demonstrate that\nsystematic data engineering can be as impactful as--and often more deployable\nthan--complex model modifications, offering actionable guidance for building\nrobust multilingual search systems in the real-world e-commerce settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual e-commerce search suffers from severe data imbalance across\nlanguages, label noise, and limited supervision for low-resource\nlanguages--challenges that impede the cross-lingual generalization of relevance\nmodels despite the strong capabilities of large language models (LLMs). In this\nwork, we present a practical, architecture-agnostic, data-centric framework to\nenhance performance on two core tasks: Query-Category (QC) relevance (matching\nqueries to product categories) and Query-Item (QI) relevance (matching queries\nto product titles). Rather than altering the model, we redesign the training\ndata through three complementary strategies: (1) translation-based augmentation\nto synthesize examples for languages absent in training, (2) semantic negative\nsampling to generate hard negatives and mitigate class imbalance, and (3)\nself-validation filtering to detect and remove likely mislabeled instances.\nEvaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields\nsubstantial F1 score improvements over strong LLM baselines, achieving\ncompetitive results in the official competition. Our findings demonstrate that\nsystematic data engineering can be as impactful as--and often more deployable\nthan--complex model modifications, offering actionable guidance for building\nrobust multilingual search systems in the real-world e-commerce settings."
                },
                "authors": [
                    {
                        "name": "Yabo Yin"
                    },
                    {
                        "name": "Yang Xi"
                    },
                    {
                        "name": "Jialong Wang"
                    },
                    {
                        "name": "Shanqi Wang"
                    },
                    {
                        "name": "Jiateng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jiateng Hu"
                },
                "author": "Jiateng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06186v2",
                "updated": "2025-10-24T17:20:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    20,
                    26,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-07T17:45:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    45,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback"
                },
                "summary": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation"
                },
                "authors": [
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Fangxin Wang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Wooseong Yang"
                    },
                    {
                        "name": "Bowei He"
                    },
                    {
                        "name": "Xinni Zhang"
                    },
                    {
                        "name": "Dianzhi Yu"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Hoang H Nguyen"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jizhou Guo"
                    },
                    {
                        "name": "Wenzhe Fan"
                    },
                    {
                        "name": "Chin-Yuan Yeh"
                    },
                    {
                        "name": "Panpan Meng"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Jinhu Qi"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Zhengyao Gu"
                    },
                    {
                        "name": "Yuwei Han"
                    },
                    {
                        "name": "Langzhou He"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "Code and dataset are available at github.com/ChunyuMiao98/RECODE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23579v2",
                "updated": "2025-10-24T17:16:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    16,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-29T15:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    49,
                    27,
                    3,
                    149,
                    0
                ],
                "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioReason: Incentivizing Multimodal Biological Reasoning within a\n  DNA-LLM Model"
                },
                "summary": "Unlocking deep and interpretable biological reasoning from complex genomic\ndata remains a major AI challenge limiting scientific progress. While current\nDNA foundation models excel at representing sequences, they struggle with\nmulti-step reasoning and lack transparent, biologically meaningful\nexplanations. BioReason addresses this by tightly integrating a DNA foundation\nmodel with a large language model (LLM), enabling the LLM to directly interpret\nand reason over genomic information. Through supervised fine-tuning and\nreinforcement learning, BioReason learns to produce logical, biologically\ncoherent deductions. It achieves major performance gains, boosting KEGG-based\ndisease pathway prediction accuracy from 86% to 98% and improving variant\neffect prediction by an average of 15% over strong baselines. BioReason can\nreason over unseen biological entities and explain its decisions step by step,\noffering a transformative framework for interpretable, mechanistic AI in\nbiology. All data, code, and checkpoints are available at\nhttps://github.com/bowang-lab/BioReason",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking deep and interpretable biological reasoning from complex genomic\ndata remains a major AI challenge limiting scientific progress. While current\nDNA foundation models excel at representing sequences, they struggle with\nmulti-step reasoning and lack transparent, biologically meaningful\nexplanations. BioReason addresses this by tightly integrating a DNA foundation\nmodel with a large language model (LLM), enabling the LLM to directly interpret\nand reason over genomic information. Through supervised fine-tuning and\nreinforcement learning, BioReason learns to produce logical, biologically\ncoherent deductions. It achieves major performance gains, boosting KEGG-based\ndisease pathway prediction accuracy from 86% to 98% and improving variant\neffect prediction by an average of 15% over strong baselines. BioReason can\nreason over unseen biological entities and explain its decisions step by step,\noffering a transformative framework for interpretable, mechanistic AI in\nbiology. All data, code, and checkpoints are available at\nhttps://github.com/bowang-lab/BioReason"
                },
                "authors": [
                    {
                        "name": "Adibvafa Fallahpour"
                    },
                    {
                        "name": "Andrew Magnuson"
                    },
                    {
                        "name": "Purav Gupta"
                    },
                    {
                        "name": "Shihao Ma"
                    },
                    {
                        "name": "Jack Naimer"
                    },
                    {
                        "name": "Arnav Shah"
                    },
                    {
                        "name": "Haonan Duan"
                    },
                    {
                        "name": "Omar Ibrahim"
                    },
                    {
                        "name": "Hani Goodarzi"
                    },
                    {
                        "name": "Chris J. Maddison"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "arxiv_comment": "28 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11194v3",
                "updated": "2025-10-24T17:15:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    15,
                    17,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-16T12:53:21Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    53,
                    21,
                    4,
                    136,
                    0
                ],
                "title": "Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive\n  Alignment"
                },
                "summary": "Predicting protein function from sequence is a central challenge in\ncomputational biology. While existing methods rely heavily on structured\nontologies or similarity-based techniques, they often lack the flexibility to\nexpress structure-free functional descriptions and novel biological functions.\nIn this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text\nmodel that generates free-form natural language descriptions of protein\nfunction directly from amino acid sequences. Our method combines a protein\nlanguage model as a sequence encoder (ESM-3B) and a decoder-only language model\n(LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A\nkey innovation is our Hybrid Sequence-level Contrastive Alignment Learning\n(H-SCALE), which improves cross-modal learning by matching mean- and std-pooled\nprotein embeddings with text representations via contrastive loss. After the\nalignment phase, we apply instruction-based fine-tuning using LoRA on the\ndecoder to teach the model how to generate accurate protein function\ndescriptions conditioned on the protein sequence. We train Prot2Text-V2 on\nabout 250K curated entries from SwissProt and evaluate it under low-homology\nconditions, where test sequences have low similarity with training samples.\nProt2Text-V2 consistently outperforms traditional and LLM-based baselines\nacross various metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting protein function from sequence is a central challenge in\ncomputational biology. While existing methods rely heavily on structured\nontologies or similarity-based techniques, they often lack the flexibility to\nexpress structure-free functional descriptions and novel biological functions.\nIn this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text\nmodel that generates free-form natural language descriptions of protein\nfunction directly from amino acid sequences. Our method combines a protein\nlanguage model as a sequence encoder (ESM-3B) and a decoder-only language model\n(LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A\nkey innovation is our Hybrid Sequence-level Contrastive Alignment Learning\n(H-SCALE), which improves cross-modal learning by matching mean- and std-pooled\nprotein embeddings with text representations via contrastive loss. After the\nalignment phase, we apply instruction-based fine-tuning using LoRA on the\ndecoder to teach the model how to generate accurate protein function\ndescriptions conditioned on the protein sequence. We train Prot2Text-V2 on\nabout 250K curated entries from SwissProt and evaluate it under low-homology\nconditions, where test sequences have low similarity with training samples.\nProt2Text-V2 consistently outperforms traditional and LLM-based baselines\nacross various metrics."
                },
                "authors": [
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Sarah Almeida Carneiro"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Lawrence P. Petalidis"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "arxiv_comment": "24 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11128v2",
                "updated": "2025-10-24T17:14:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    14,
                    46,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-13T08:19:56Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    19,
                    56,
                    0,
                    286,
                    0
                ],
                "title": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level\n  Cross-Modal Knowledge Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level\n  Cross-Modal Knowledge Transfer"
                },
                "summary": "Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead."
                },
                "authors": [
                    {
                        "name": "Qiyi Tong"
                    },
                    {
                        "name": "Olivia Nocentini"
                    },
                    {
                        "name": "Marta Lagomarsino"
                    },
                    {
                        "name": "Kuanqi Cai"
                    },
                    {
                        "name": "Marta Lorenzini"
                    },
                    {
                        "name": "Arash Ajoudani"
                    }
                ],
                "author_detail": {
                    "name": "Arash Ajoudani"
                },
                "author": "Arash Ajoudani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17470v2",
                "updated": "2025-10-24T17:04:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    4,
                    17,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T08:05:44Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    8,
                    5,
                    44,
                    0,
                    265,
                    0
                ],
                "title": "Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for\n  Entity Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for\n  Entity Resolution"
                },
                "summary": "Entity resolution plays a significant role in enterprise systems where data\nintegrity must be rigorously maintained. Traditional methods often struggle\nwith handling noisy data or semantic understanding, while modern methods suffer\nfrom computational costs or the excessive need for parallel computation. In\nthis study, we introduce a scalable hybrid framework, which is designed to\naddress several important problems, including scalability, noise robustness,\nand reliable results. We utilized a pre-trained language model to encode each\nstructured data into corresponding semantic embedding vectors. Subsequently,\nafter retrieving a semantically relevant subset of candidates, we apply a\nsyntactic verification stage using fuzzy string matching techniques to refine\nclassification on the unlabeled data. This approach was applied to a real-world\nentity resolution task, which exposed a linkage between a central user\nmanagement database and numerous shared hosting server records. Compared to\nother methods, this approach exhibits an outstanding performance in terms of\nboth processing time and robustness, making it a reliable solution for a\nserver-side product. Crucially, this efficiency does not compromise results, as\nthe system maintains a high retrieval recall of approximately 0.97. The\nscalability of the framework makes it deployable on standard CPU-based\ninfrastructure, offering a practical and effective solution for\nenterprise-level data integrity auditing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity resolution plays a significant role in enterprise systems where data\nintegrity must be rigorously maintained. Traditional methods often struggle\nwith handling noisy data or semantic understanding, while modern methods suffer\nfrom computational costs or the excessive need for parallel computation. In\nthis study, we introduce a scalable hybrid framework, which is designed to\naddress several important problems, including scalability, noise robustness,\nand reliable results. We utilized a pre-trained language model to encode each\nstructured data into corresponding semantic embedding vectors. Subsequently,\nafter retrieving a semantically relevant subset of candidates, we apply a\nsyntactic verification stage using fuzzy string matching techniques to refine\nclassification on the unlabeled data. This approach was applied to a real-world\nentity resolution task, which exposed a linkage between a central user\nmanagement database and numerous shared hosting server records. Compared to\nother methods, this approach exhibits an outstanding performance in terms of\nboth processing time and robustness, making it a reliable solution for a\nserver-side product. Crucially, this efficiency does not compromise results, as\nthe system maintains a high retrieval recall of approximately 0.97. The\nscalability of the framework makes it deployable on standard CPU-based\ninfrastructure, offering a practical and effective solution for\nenterprise-level data integrity auditing."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Sharifi"
                    },
                    {
                        "name": "Danial Ahmadzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Danial Ahmadzadeh"
                },
                "author": "Danial Ahmadzadeh",
                "arxiv_comment": "Accepted at ICCKE 2025 Conference. 6 tables, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03325v2",
                "updated": "2025-10-24T16:55:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    55,
                    30,
                    4,
                    297,
                    0
                ],
                "published": "2025-02-05T16:22:33Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    33,
                    2,
                    36,
                    0
                ],
                "title": "Electronic Circuit Principles of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Circuit Principles of Large Language Models"
                },
                "summary": "Large language models (LLMs) such as DeepSeek-R1 have achieved remarkable\nperformance across diverse reasoning tasks. To uncover the principles that\ngovern their behaviour, we introduce the Electronic Circuit Principles (ECP),\nwhich maps inference-time learning (ITL) onto a semantic electromotive force\nand inference-time reasoning (ITR) onto a resistive network governed by Ohm's\nand Faraday's laws. This circuit-based modelling yields closed-form predictions\nof task performance and reveals how modular prompt components interact to shape\naccuracy. We validated ECP on 70,000 samples spanning 350 reasoning tasks and 9\nadvanced LLMs, observing a about 60% improvement in Pearson correlation\nrelative to the conventional inference-time scaling law. Moreover, ECP explains\nthe efficacy of 15 established prompting strategies and directs the development\nof new modular interventions that exceed the median score of the top 80% of\nparticipants in both the International Olympiad in Informatics and the\nInternational Mathematical Olympiad. By grounding LLM reasoning in\nelectronic-circuit principles, ECP provides a rigorous framework for predicting\nperformance and optimising modular components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as DeepSeek-R1 have achieved remarkable\nperformance across diverse reasoning tasks. To uncover the principles that\ngovern their behaviour, we introduce the Electronic Circuit Principles (ECP),\nwhich maps inference-time learning (ITL) onto a semantic electromotive force\nand inference-time reasoning (ITR) onto a resistive network governed by Ohm's\nand Faraday's laws. This circuit-based modelling yields closed-form predictions\nof task performance and reveals how modular prompt components interact to shape\naccuracy. We validated ECP on 70,000 samples spanning 350 reasoning tasks and 9\nadvanced LLMs, observing a about 60% improvement in Pearson correlation\nrelative to the conventional inference-time scaling law. Moreover, ECP explains\nthe efficacy of 15 established prompting strategies and directs the development\nof new modular interventions that exceed the median score of the top 80% of\nparticipants in both the International Olympiad in Informatics and the\nInternational Mathematical Olympiad. By grounding LLM reasoning in\nelectronic-circuit principles, ECP provides a rigorous framework for predicting\nperformance and optimising modular components."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "Manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21631v1",
                "updated": "2025-10-24T16:36:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    36,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:36:34Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    36,
                    34,
                    4,
                    297,
                    0
                ],
                "title": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations"
                },
                "summary": "Knowledge distillation is a promising approach to transfer capabilities from\ncomplex teacher models to smaller, resource-efficient student models that can\nbe deployed easily, particularly in task-aware scenarios. However, existing\nmethods of task-aware distillation typically require substantial quantities of\ndata which may be unavailable or expensive to obtain in many practical\nscenarios. In this paper, we address this challenge by introducing a novel\nstrategy called Counterfactual-explanation-infused Distillation CoD for\nfew-shot task-aware knowledge distillation by systematically infusing\ncounterfactual explanations. Counterfactual explanations (CFEs) refer to inputs\nthat can flip the output prediction of the teacher model with minimum\nperturbation. Our strategy CoD leverages these CFEs to precisely map the\nteacher's decision boundary with significantly fewer samples. We provide\ntheoretical guarantees for motivating the role of CFEs in distillation, from\nboth statistical and geometric perspectives. We mathematically show that CFEs\ncan improve parameter estimation by providing more informative examples near\nthe teacher's decision boundary. We also derive geometric insights on how CFEs\neffectively act as knowledge probes, helping the students mimic the teacher's\ndecision boundaries more effectively than standard data. We perform experiments\nacross various datasets and LLMs to show that CoD outperforms standard\ndistillation approaches in few-shot regimes (as low as 8-512 samples). Notably,\nCoD only uses half of the original samples used by the baselines, paired with\ntheir corresponding CFEs and still improves performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation is a promising approach to transfer capabilities from\ncomplex teacher models to smaller, resource-efficient student models that can\nbe deployed easily, particularly in task-aware scenarios. However, existing\nmethods of task-aware distillation typically require substantial quantities of\ndata which may be unavailable or expensive to obtain in many practical\nscenarios. In this paper, we address this challenge by introducing a novel\nstrategy called Counterfactual-explanation-infused Distillation CoD for\nfew-shot task-aware knowledge distillation by systematically infusing\ncounterfactual explanations. Counterfactual explanations (CFEs) refer to inputs\nthat can flip the output prediction of the teacher model with minimum\nperturbation. Our strategy CoD leverages these CFEs to precisely map the\nteacher's decision boundary with significantly fewer samples. We provide\ntheoretical guarantees for motivating the role of CFEs in distillation, from\nboth statistical and geometric perspectives. We mathematically show that CFEs\ncan improve parameter estimation by providing more informative examples near\nthe teacher's decision boundary. We also derive geometric insights on how CFEs\neffectively act as knowledge probes, helping the students mimic the teacher's\ndecision boundaries more effectively than standard data. We perform experiments\nacross various datasets and LLMs to show that CoD outperforms standard\ndistillation approaches in few-shot regimes (as low as 8-512 samples). Notably,\nCoD only uses half of the original samples used by the baselines, paired with\ntheir corresponding CFEs and still improves performance."
                },
                "authors": [
                    {
                        "name": "Faisal Hamman"
                    },
                    {
                        "name": "Pasan Dissanayake"
                    },
                    {
                        "name": "Yanjun Fu"
                    },
                    {
                        "name": "Sanghamitra Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Sanghamitra Dutta"
                },
                "author": "Sanghamitra Dutta",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21623v1",
                "updated": "2025-10-24T16:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    26,
                    36,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:26:36Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    26,
                    36,
                    4,
                    297,
                    0
                ],
                "title": "The Universal Landscape of Human Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Universal Landscape of Human Reasoning"
                },
                "summary": "Understanding how information is dynamically accumulated and transformed in\nhuman reasoning has long challenged cognitive psychology, philosophy, and\nartificial intelligence. Existing accounts, from classical logic to\nprobabilistic models, illuminate aspects of output or individual modelling, but\ndo not offer a unified, quantitative description of general human reasoning\ndynamics. To solve this, we introduce Information Flow Tracking (IF-Track),\nthat uses large language models (LLMs) as probabilistic encoder to quantify\ninformation entropy and gain at each reasoning step. Through fine-grained\nanalyses across diverse tasks, our method is the first successfully models the\nuniversal landscape of human reasoning behaviors within a single metric space.\nWe show that IF-Track captures essential reasoning features, identifies\nsystematic error patterns, and characterizes individual differences. Applied to\ndiscussion of advanced psychological theory, we first reconcile single- versus\ndual-process theories in IF-Track and discover the alignment of artificial and\nhuman cognition and how LLMs reshaping human reasoning process. This approach\nestablishes a quantitative bridge between theory and measurement, offering\nmechanistic insights into the architecture of reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how information is dynamically accumulated and transformed in\nhuman reasoning has long challenged cognitive psychology, philosophy, and\nartificial intelligence. Existing accounts, from classical logic to\nprobabilistic models, illuminate aspects of output or individual modelling, but\ndo not offer a unified, quantitative description of general human reasoning\ndynamics. To solve this, we introduce Information Flow Tracking (IF-Track),\nthat uses large language models (LLMs) as probabilistic encoder to quantify\ninformation entropy and gain at each reasoning step. Through fine-grained\nanalyses across diverse tasks, our method is the first successfully models the\nuniversal landscape of human reasoning behaviors within a single metric space.\nWe show that IF-Track captures essential reasoning features, identifies\nsystematic error patterns, and characterizes individual differences. Applied to\ndiscussion of advanced psychological theory, we first reconcile single- versus\ndual-process theories in IF-Track and discover the alignment of artificial and\nhuman cognition and how LLMs reshaping human reasoning process. This approach\nestablishes a quantitative bridge between theory and measurement, offering\nmechanistic insights into the architecture of reasoning."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Yihao Liang"
                    },
                    {
                        "name": "Shangxu Ren"
                    },
                    {
                        "name": "Chengyu Luan"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Hanjing Li"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Zheng Yan"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Yantao Du"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21043v3",
                "updated": "2025-10-24T16:26:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    26,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-25T11:48:37Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    11,
                    48,
                    37,
                    3,
                    268,
                    0
                ],
                "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Creativity: A New Frontier in Generalization Abilities"
                },
                "summary": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence."
                },
                "authors": [
                    {
                        "name": "Samuel Schapiro"
                    },
                    {
                        "name": "Sumuk Shashidhar"
                    },
                    {
                        "name": "Alexi Gladstone"
                    },
                    {
                        "name": "Jonah Black"
                    },
                    {
                        "name": "Royce Moon"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "arxiv_comment": "Preprint. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14198v2",
                "updated": "2025-10-24T16:26:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    26,
                    0,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-14T15:51:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    51,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "Retention analysis of edited knowledge after fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retention analysis of edited knowledge after fine-tuning"
                },
                "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that knowledge retention can be significantly improved by either\naugmenting edit knowledge with paraphrases or by freezing layers associated\nwith edited content in fine-tuning stage, offering insight for developing more\nrobust editing algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that knowledge retention can be significantly improved by either\naugmenting edit knowledge with paraphrases or by freezing layers associated\nwith edited content in fine-tuning stage, offering insight for developing more\nrobust editing algorithms."
                },
                "authors": [
                    {
                        "name": "Fufang Wen"
                    },
                    {
                        "name": "Shichang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shichang Zhang"
                },
                "author": "Shichang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21618v1",
                "updated": "2025-10-24T16:24:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    24,
                    1,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:24:01Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    24,
                    1,
                    4,
                    297,
                    0
                ],
                "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepAgent: A General Reasoning Agent with Scalable Toolsets"
                },
                "summary": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent."
                },
                "authors": [
                    {
                        "name": "Xiaoxi Li"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Jiarui Jin"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Jiajie Jin"
                    },
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06485v2",
                "updated": "2025-10-24T16:19:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    19,
                    27,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-09T02:06:13Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    6,
                    13,
                    2,
                    190,
                    0
                ],
                "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning"
                },
                "summary": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and fine-tuning remain\nsignificant challenges. These methods often rely on large-scale supervised\nfine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT)\nannotations, making them costly and hard to scale. To address this, we present\nVideo-RTS, a new approach to improve video reasoning capability with\ndrastically improved data efficiency by combining data-efficient RL with a\nvideo-adaptive test-time scaling (TTS) strategy. Building on observations about\nthe data scaling, we skip the resource-intensive SFT step and employ efficient\npure-RL training with output-based rewards, requiring no additional annotations\nor extensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by 2.4% in accuracy using only 3.6%\ntraining samples. Specifically, Video-RTS achieves a 4.2% improvement on\nVideo-Holmes, a recent and challenging video reasoning benchmark. Notably, our\npure RL training and adaptive video TTS offer complementary strengths, enabling\nVideo-RTS's strong reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and fine-tuning remain\nsignificant challenges. These methods often rely on large-scale supervised\nfine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT)\nannotations, making them costly and hard to scale. To address this, we present\nVideo-RTS, a new approach to improve video reasoning capability with\ndrastically improved data efficiency by combining data-efficient RL with a\nvideo-adaptive test-time scaling (TTS) strategy. Building on observations about\nthe data scaling, we skip the resource-intensive SFT step and employ efficient\npure-RL training with output-based rewards, requiring no additional annotations\nor extensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by 2.4% in accuracy using only 3.6%\ntraining samples. Specifically, Video-RTS achieves a 4.2% improvement on\nVideo-Holmes, a recent and challenging video reasoning benchmark. Notably, our\npure RL training and adaptive video TTS offer complementary strengths, enabling\nVideo-RTS's strong reasoning performance."
                },
                "authors": [
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "EMNLP 2025. The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21604v1",
                "updated": "2025-10-24T16:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    8,
                    33,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:08:33Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    8,
                    33,
                    4,
                    297,
                    0
                ],
                "title": "RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction\n  with Large Language Models"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated outstanding\nreasoning capabilities on mathematical and coding tasks. However, their\napplication to financial tasks-especially the most fundamental task of stock\nmovement prediction-remains underexplored. We study a three-class\nclassification problem (up, hold, down) and, by analyzing existing reasoning\nresponses, observe that: (1) LLMs follow analysts' opinions rather than exhibit\na systematic, independent analytical logic (CoTs). (2) LLMs list summaries from\ndifferent sources without weighing adversarial evidence, yet such\ncounterevidence is crucial for reliable prediction. It shows that the model\ndoes not make good use of its reasoning ability to complete the task. To\naddress this, we propose Reflective Evidence Tuning (RETuning), a cold-start\nmethod prior to reinforcement learning, to enhance prediction ability. While\ngenerating CoT, RETuning encourages dynamically constructing an analytical\nframework from diverse information sources, organizing and scoring evidence for\nprice up or down based on that framework-rather than on contextual\nviewpoints-and finally reflecting to derive the prediction. This approach\nmaximally aligns the model with its learned analytical framework, ensuring\nindependent logical reasoning and reducing undue influence from context. We\nalso build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,\nwith long contexts (32K tokens) and over 200K samples. In addition to price and\nnews, it incorporates analysts' opinions, quantitative reports, fundamental\ndata, macroeconomic indicators, and similar stocks. Experiments show that\nRETuning successfully unlocks the model's reasoning ability in the financial\ndomain. Inference-time scaling still works even after 6 months or on\nout-of-distribution stocks, since the models gain valuable insights about stock\nmovement prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated outstanding\nreasoning capabilities on mathematical and coding tasks. However, their\napplication to financial tasks-especially the most fundamental task of stock\nmovement prediction-remains underexplored. We study a three-class\nclassification problem (up, hold, down) and, by analyzing existing reasoning\nresponses, observe that: (1) LLMs follow analysts' opinions rather than exhibit\na systematic, independent analytical logic (CoTs). (2) LLMs list summaries from\ndifferent sources without weighing adversarial evidence, yet such\ncounterevidence is crucial for reliable prediction. It shows that the model\ndoes not make good use of its reasoning ability to complete the task. To\naddress this, we propose Reflective Evidence Tuning (RETuning), a cold-start\nmethod prior to reinforcement learning, to enhance prediction ability. While\ngenerating CoT, RETuning encourages dynamically constructing an analytical\nframework from diverse information sources, organizing and scoring evidence for\nprice up or down based on that framework-rather than on contextual\nviewpoints-and finally reflecting to derive the prediction. This approach\nmaximally aligns the model with its learned analytical framework, ensuring\nindependent logical reasoning and reducing undue influence from context. We\nalso build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,\nwith long contexts (32K tokens) and over 200K samples. In addition to price and\nnews, it incorporates analysts' opinions, quantitative reports, fundamental\ndata, macroeconomic indicators, and similar stocks. Experiments show that\nRETuning successfully unlocks the model's reasoning ability in the financial\ndomain. Inference-time scaling still works even after 6 months or on\nout-of-distribution stocks, since the models gain valuable insights about stock\nmovement prediction."
                },
                "authors": [
                    {
                        "name": "Xueyuan Lin"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Ye Ma"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Rongjunchen Zhang"
                    },
                    {
                        "name": "Yang Ni"
                    },
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Jian Guo"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21603v1",
                "updated": "2025-10-24T16:07:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    7,
                    54,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T16:07:54Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    7,
                    54,
                    4,
                    297,
                    0
                ],
                "title": "Doc-Researcher: A Unified System for Multimodal Document Parsing and\n  Deep Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doc-Researcher: A Unified System for Multimodal Document Parsing and\n  Deep Research"
                },
                "summary": "Deep Research systems have revolutionized how LLMs solve complex questions\nthrough iterative reasoning and evidence gathering. However, current systems\nremain fundamentally constrained to textual web data, overlooking the vast\nknowledge embedded in multimodal documents Processing such documents demands\nsophisticated parsing to preserve visual semantics (figures, tables, charts,\nand equations), intelligent chunking to maintain structural coherence, and\nadaptive retrieval across modalities, which are capabilities absent in existing\nsystems. In response, we present Doc-Researcher, a unified system that bridges\nthis gap through three integrated components: (i) deep multimodal parsing that\npreserves layout structure and visual semantics while creating multi-granular\nrepresentations from chunk to document level, (ii) systematic retrieval\narchitecture supporting text-only, vision-only, and hybrid paradigms with\ndynamic granularity selection, and (iii) iterative multi-agent workflows that\ndecompose complex queries, progressively accumulate evidence, and synthesize\ncomprehensive answers across documents and modalities. To enable rigorous\nevaluation, we introduce M4DocBench, the first benchmark for Multi-modal,\nMulti-hop, Multi-document, and Multi-turn deep research. Featuring 158\nexpert-annotated questions with complete evidence chains across 304 documents,\nM4DocBench tests capabilities that existing benchmarks cannot assess.\nExperiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter\nthan state-of-the-art baselines, validating that effective document research\nrequires not just better retrieval, but fundamentally deep parsing that\npreserve multimodal integrity and support iterative research. Our work\nestablishes a new paradigm for conducting deep research on multimodal document\ncollections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Research systems have revolutionized how LLMs solve complex questions\nthrough iterative reasoning and evidence gathering. However, current systems\nremain fundamentally constrained to textual web data, overlooking the vast\nknowledge embedded in multimodal documents Processing such documents demands\nsophisticated parsing to preserve visual semantics (figures, tables, charts,\nand equations), intelligent chunking to maintain structural coherence, and\nadaptive retrieval across modalities, which are capabilities absent in existing\nsystems. In response, we present Doc-Researcher, a unified system that bridges\nthis gap through three integrated components: (i) deep multimodal parsing that\npreserves layout structure and visual semantics while creating multi-granular\nrepresentations from chunk to document level, (ii) systematic retrieval\narchitecture supporting text-only, vision-only, and hybrid paradigms with\ndynamic granularity selection, and (iii) iterative multi-agent workflows that\ndecompose complex queries, progressively accumulate evidence, and synthesize\ncomprehensive answers across documents and modalities. To enable rigorous\nevaluation, we introduce M4DocBench, the first benchmark for Multi-modal,\nMulti-hop, Multi-document, and Multi-turn deep research. Featuring 158\nexpert-annotated questions with complete evidence chains across 304 documents,\nM4DocBench tests capabilities that existing benchmarks cannot assess.\nExperiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter\nthan state-of-the-art baselines, validating that effective document research\nrequires not just better retrieval, but fundamentally deep parsing that\npreserve multimodal integrity and support iterative research. Our work\nestablishes a new paradigm for conducting deep research on multimodal document\ncollections."
                },
                "authors": [
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Shurui Huang"
                    },
                    {
                        "name": "Fangda Ye"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Qu Yang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21586v1",
                "updated": "2025-10-24T15:54:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    54,
                    5,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:54:05Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    54,
                    5,
                    4,
                    297,
                    0
                ],
                "title": "MATrack: Efficient Multiscale Adaptive Tracker for Real-Time Nighttime\n  UAV Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATrack: Efficient Multiscale Adaptive Tracker for Real-Time Nighttime\n  UAV Operations"
                },
                "summary": "Nighttime UAV tracking faces significant challenges in real-world robotics\noperations. Low-light conditions not only limit visual perception capabilities,\nbut cluttered backgrounds and frequent viewpoint changes also cause existing\ntrackers to drift or fail during deployment. To address these difficulties,\nresearchers have proposed solutions based on low-light enhancement and domain\nadaptation. However, these methods still have notable shortcomings in actual\nUAV systems: low-light enhancement often introduces visual artifacts, domain\nadaptation methods are computationally expensive and existing lightweight\ndesigns struggle to fully leverage dynamic object information. Based on an\nin-depth analysis of these key issues, we propose MATrack-a multiscale adaptive\nsystem designed specifically for nighttime UAV tracking. MATrack tackles the\nmain technical challenges of nighttime tracking through the collaborative work\nof three core modules: Multiscale Hierarchy Blende (MHB) enhances feature\nconsistency between static and dynamic templates. Adaptive Key Token Gate\naccurately identifies object information within complex backgrounds. Nighttime\nTemplate Calibrator (NTC) ensures stable tracking performance over long\nsequences. Extensive experiments show that MATrack achieves a significant\nperformance improvement. On the UAVDark135 benchmark, its precision, normalized\nprecision and AUC surpass state-of-the-art (SOTA) methods by 5.9%, 5.4% and\n4.2% respectively, while maintaining a real-time processing speed of 81 FPS.\nFurther tests on a real-world UAV platform validate the system's reliability,\ndemonstrating that MATrack can provide stable and effective nighttime UAV\ntracking support for critical robotics applications such as nighttime search\nand rescue and border patrol.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nighttime UAV tracking faces significant challenges in real-world robotics\noperations. Low-light conditions not only limit visual perception capabilities,\nbut cluttered backgrounds and frequent viewpoint changes also cause existing\ntrackers to drift or fail during deployment. To address these difficulties,\nresearchers have proposed solutions based on low-light enhancement and domain\nadaptation. However, these methods still have notable shortcomings in actual\nUAV systems: low-light enhancement often introduces visual artifacts, domain\nadaptation methods are computationally expensive and existing lightweight\ndesigns struggle to fully leverage dynamic object information. Based on an\nin-depth analysis of these key issues, we propose MATrack-a multiscale adaptive\nsystem designed specifically for nighttime UAV tracking. MATrack tackles the\nmain technical challenges of nighttime tracking through the collaborative work\nof three core modules: Multiscale Hierarchy Blende (MHB) enhances feature\nconsistency between static and dynamic templates. Adaptive Key Token Gate\naccurately identifies object information within complex backgrounds. Nighttime\nTemplate Calibrator (NTC) ensures stable tracking performance over long\nsequences. Extensive experiments show that MATrack achieves a significant\nperformance improvement. On the UAVDark135 benchmark, its precision, normalized\nprecision and AUC surpass state-of-the-art (SOTA) methods by 5.9%, 5.4% and\n4.2% respectively, while maintaining a real-time processing speed of 81 FPS.\nFurther tests on a real-world UAV platform validate the system's reliability,\ndemonstrating that MATrack can provide stable and effective nighttime UAV\ntracking support for critical robotics applications such as nighttime search\nand rescue and border patrol."
                },
                "authors": [
                    {
                        "name": "Xuzhao Li"
                    },
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Shiyu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Hu"
                },
                "author": "Shiyu Hu",
                "arxiv_comment": "Preprint, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23794v2",
                "updated": "2025-10-24T15:52:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    52,
                    23,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-26T12:25:37Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    25,
                    37,
                    0,
                    146,
                    0
                ],
                "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via\n  Reinforcement Learning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG."
                },
                "authors": [
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Qi Luo"
                    },
                    {
                        "name": "Xiaonan Li"
                    },
                    {
                        "name": "Bufan Li"
                    },
                    {
                        "name": "Qinyuan Cheng"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Yining Zheng"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13192v2",
                "updated": "2025-10-24T15:51:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    51,
                    36,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-19T14:49:10Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    49,
                    10,
                    0,
                    139,
                    0
                ],
                "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term\n  Statistics"
                },
                "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters (0.1%)\nand orders of magnitude faster inference times. DynaMix outperforms TS\nfoundation models in terms of long-term statistics, and often also short-term\nforecasts, even on real-world time series, like traffic or weather data,\ntypically used for training and evaluating TS models, but not at all part of\nDynaMix' training corpus. We illustrate some of the failure modes of TS models\nfor DSR problems, and conclude that models built on DS principles may bear a\nhuge potential also for advancing the TS prediction field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters (0.1%)\nand orders of magnitude faster inference times. DynaMix outperforms TS\nfoundation models in terms of long-term statistics, and often also short-term\nforecasts, even on real-world time series, like traffic or weather data,\ntypically used for training and evaluating TS models, but not at all part of\nDynaMix' training corpus. We illustrate some of the failure modes of TS models\nfor DSR problems, and conclude that models built on DS principles may bear a\nhuge potential also for advancing the TS prediction field."
                },
                "authors": [
                    {
                        "name": "Christoph Jrgen Hemmer"
                    },
                    {
                        "name": "Daniel Durstewitz"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Durstewitz"
                },
                "author": "Daniel Durstewitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21580v1",
                "updated": "2025-10-24T15:49:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    49,
                    51,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:49:51Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    49,
                    51,
                    4,
                    297,
                    0
                ],
                "title": "Source-Coded Online Algorithm for Multicast Subgraph Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-Coded Online Algorithm for Multicast Subgraph Construction"
                },
                "summary": "Multicast remains a fundamental mechanism for scalable content distribution,\nyet existing approaches face critical limitations. Traditional multicast trees\nsuffer from path redundancy and inefficient utilization of network resources,\nwhile network coding, although capacity-achieving, incurs significant\ncomputational overhead and deployment challenges. In this paper, we introduce a\nsource-coded multicast framework that exploits maximum-flow decomposition to\nconstruct multiple disjoint or partially overlapping paths from the source to\nall receivers. Our scheme incorporates a novel path redirection mechanism: when\nmultiple overlaps occur between receiver flows, downstream paths are realigned\nat the first intersection, ensuring loop-free delivery while maximizing overall\nthroughput. We develop algorithms for path construction, overlap detection, and\niterative refinement of multicast subgraphs, and analyze their computational\ncomplexity. Through extensive evaluation on synthetic and real network\ntopologies, we demonstrate that the proposed method consistently approaches the\nthroughput of network coding with substantially lower encoding and decoding\ncomplexity, while significantly outperforming multicast tree constructions in\nterms of fairness, robustness to link failures, and delivery efficiency. These\nresults position source-coded multicast as a practical and scalable solution\nfor next-generation networks requiring high-throughput and adaptive group\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multicast remains a fundamental mechanism for scalable content distribution,\nyet existing approaches face critical limitations. Traditional multicast trees\nsuffer from path redundancy and inefficient utilization of network resources,\nwhile network coding, although capacity-achieving, incurs significant\ncomputational overhead and deployment challenges. In this paper, we introduce a\nsource-coded multicast framework that exploits maximum-flow decomposition to\nconstruct multiple disjoint or partially overlapping paths from the source to\nall receivers. Our scheme incorporates a novel path redirection mechanism: when\nmultiple overlaps occur between receiver flows, downstream paths are realigned\nat the first intersection, ensuring loop-free delivery while maximizing overall\nthroughput. We develop algorithms for path construction, overlap detection, and\niterative refinement of multicast subgraphs, and analyze their computational\ncomplexity. Through extensive evaluation on synthetic and real network\ntopologies, we demonstrate that the proposed method consistently approaches the\nthroughput of network coding with substantially lower encoding and decoding\ncomplexity, while significantly outperforming multicast tree constructions in\nterms of fairness, robustness to link failures, and delivery efficiency. These\nresults position source-coded multicast as a practical and scalable solution\nfor next-generation networks requiring high-throughput and adaptive group\ncommunication."
                },
                "authors": [
                    {
                        "name": "Tomas Lestayo Martinez"
                    },
                    {
                        "name": "Manuel Fernandez Veiega Veiga"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Fernandez Veiega Veiga"
                },
                "author": "Manuel Fernandez Veiega Veiga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01930v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01930v3",
                "updated": "2025-10-24T15:47:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    47,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-02-04T02:03:19Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    3,
                    19,
                    1,
                    35,
                    0
                ],
                "title": "Robust LLM Alignment via Distributionally Robust Direct Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Alignment via Distributionally Robust Direct Preference\n  Optimization"
                },
                "summary": "A major challenge in aligning large language models (LLMs) with human\npreferences is the issue of distribution shift. LLM alignment algorithms rely\non static preference datasets, assuming that they accurately represent\nreal-world user preferences. However, user preferences vary significantly\nacross geographical regions, demographics, linguistic patterns, and evolving\ncultural trends. This preference distribution shift leads to catastrophic\nalignment failures in many real-world applications. We address this problem\nusing the principled framework of distributionally robust optimization, and\ndevelop two novel distributionally robust direct preference optimization (DPO)\nalgorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We\ncharacterize the sample complexity of learning the optimal policy parameters\nfor WDPO and KLDPO. Moreover, we propose scalable gradient descent-style\nlearning algorithms by developing suitable approximations for the challenging\nminimax loss functions of WDPO and KLDPO. Our empirical experiments using\nbenchmark data sets and LLMs demonstrate the superior performance of WDPO and\nKLDPO in substantially improving the alignment when there is a preference\ndistribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge in aligning large language models (LLMs) with human\npreferences is the issue of distribution shift. LLM alignment algorithms rely\non static preference datasets, assuming that they accurately represent\nreal-world user preferences. However, user preferences vary significantly\nacross geographical regions, demographics, linguistic patterns, and evolving\ncultural trends. This preference distribution shift leads to catastrophic\nalignment failures in many real-world applications. We address this problem\nusing the principled framework of distributionally robust optimization, and\ndevelop two novel distributionally robust direct preference optimization (DPO)\nalgorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We\ncharacterize the sample complexity of learning the optimal policy parameters\nfor WDPO and KLDPO. Moreover, we propose scalable gradient descent-style\nlearning algorithms by developing suitable approximations for the challenging\nminimax loss functions of WDPO and KLDPO. Our empirical experiments using\nbenchmark data sets and LLMs demonstrate the superior performance of WDPO and\nKLDPO in substantially improving the alignment when there is a preference\ndistribution shift."
                },
                "authors": [
                    {
                        "name": "Zaiyan Xu"
                    },
                    {
                        "name": "Sushil Vemuri"
                    },
                    {
                        "name": "Kishan Panaganti"
                    },
                    {
                        "name": "Dileep Kalathil"
                    },
                    {
                        "name": "Rahul Jain"
                    },
                    {
                        "name": "Deepak Ramachandran"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Ramachandran"
                },
                "author": "Deepak Ramachandran",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01930v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01930v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21575v1",
                "updated": "2025-10-24T15:43:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    43,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:43:42Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    43,
                    42,
                    4,
                    297,
                    0
                ],
                "title": "From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics\n  Understanding Benchmarks for Slovene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics\n  Understanding Benchmarks for Slovene"
                },
                "summary": "Large language models are demonstrating increasing capabilities, excelling at\nbenchmarks once considered very difficult. As their capabilities grow, there is\na need for more challenging evaluations that go beyond surface-level linguistic\ncompetence. Namely, language competence involves not only syntax and semantics\nbut also pragmatics, i.e., understanding situational meaning as shaped by\ncontext as well as linguistic and cultural norms. To contribute to this line of\nresearch, we introduce SloPragEval and SloPragMega, the first pragmatics\nunderstanding benchmarks for Slovene that contain altogether 405\nmultiple-choice questions. We discuss the difficulties of translation, describe\nthe campaign to establish a human baseline, and report pilot evaluations with\nLLMs. Our results indicate that current models have greatly improved in\nunderstanding nuanced language but may still fail to infer implied speaker\nmeaning in non-literal utterances, especially those that are culture-specific.\nWe also observe a significant gap between proprietary and open-source models.\nFinally, we argue that benchmarks targeting nuanced language understanding and\nknowledge of the target culture must be designed with care, preferably\nconstructed from native data, and validated with human responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are demonstrating increasing capabilities, excelling at\nbenchmarks once considered very difficult. As their capabilities grow, there is\na need for more challenging evaluations that go beyond surface-level linguistic\ncompetence. Namely, language competence involves not only syntax and semantics\nbut also pragmatics, i.e., understanding situational meaning as shaped by\ncontext as well as linguistic and cultural norms. To contribute to this line of\nresearch, we introduce SloPragEval and SloPragMega, the first pragmatics\nunderstanding benchmarks for Slovene that contain altogether 405\nmultiple-choice questions. We discuss the difficulties of translation, describe\nthe campaign to establish a human baseline, and report pilot evaluations with\nLLMs. Our results indicate that current models have greatly improved in\nunderstanding nuanced language but may still fail to infer implied speaker\nmeaning in non-literal utterances, especially those that are culture-specific.\nWe also observe a significant gap between proprietary and open-source models.\nFinally, we argue that benchmarks targeting nuanced language understanding and\nknowledge of the target culture must be designed with care, preferably\nconstructed from native data, and validated with human responses."
                },
                "authors": [
                    {
                        "name": "Mojca Brglez"
                    },
                    {
                        "name": "pela Vintar"
                    }
                ],
                "author_detail": {
                    "name": "pela Vintar"
                },
                "author": "pela Vintar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20171v2",
                "updated": "2025-10-24T15:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    39,
                    29,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T03:32:04Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    3,
                    32,
                    4,
                    3,
                    296,
                    0
                ],
                "title": "Collective Communication for 100k+ GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collective Communication for 100k+ GPUs"
                },
                "summary": "The increasing scale of large language models (LLMs) necessitates highly\nefficient collective communication frameworks, particularly as training\nworkloads extend to hundreds of thousands of GPUs. Traditional communication\nmethods face significant throughput and latency limitations at this scale,\nhindering both the development and deployment of state-of-the-art models. This\npaper presents the NCCLX collective communication framework, developed at Meta,\nengineered to optimize performance across the full LLM lifecycle, from the\nsynchronous demands of large-scale training to the low-latency requirements of\ninference. The framework is designed to support complex workloads on clusters\nexceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency\ndata exchange. Empirical evaluation on the Llama4 model demonstrates\nsubstantial improvements in communication efficiency. This research contributes\na robust solution for enabling the next generation of LLMs to operate at\nunprecedented scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale of large language models (LLMs) necessitates highly\nefficient collective communication frameworks, particularly as training\nworkloads extend to hundreds of thousands of GPUs. Traditional communication\nmethods face significant throughput and latency limitations at this scale,\nhindering both the development and deployment of state-of-the-art models. This\npaper presents the NCCLX collective communication framework, developed at Meta,\nengineered to optimize performance across the full LLM lifecycle, from the\nsynchronous demands of large-scale training to the low-latency requirements of\ninference. The framework is designed to support complex workloads on clusters\nexceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency\ndata exchange. Empirical evaluation on the Llama4 model demonstrates\nsubstantial improvements in communication efficiency. This research contributes\na robust solution for enabling the next generation of LLMs to operate at\nunprecedented scales."
                },
                "authors": [
                    {
                        "name": "Min Si"
                    },
                    {
                        "name": "Pavan Balaji"
                    },
                    {
                        "name": "Yongzhou Chen"
                    },
                    {
                        "name": "Ching-Hsiang Chu"
                    },
                    {
                        "name": "Adi Gangidi"
                    },
                    {
                        "name": "Saif Hasan"
                    },
                    {
                        "name": "Subodh Iyengar"
                    },
                    {
                        "name": "Dan Johnson"
                    },
                    {
                        "name": "Bingzhe Liu"
                    },
                    {
                        "name": "Regina Ren"
                    },
                    {
                        "name": "Ashmitha Jeevaraj Shetty"
                    },
                    {
                        "name": "Greg Steinbrecher"
                    },
                    {
                        "name": "Yulun Wang"
                    },
                    {
                        "name": "Bruce Wu"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Mingran Yang"
                    },
                    {
                        "name": "Kenny Yu"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Cen Zhao"
                    },
                    {
                        "name": "Wes Bland"
                    },
                    {
                        "name": "Denis Boyda"
                    },
                    {
                        "name": "Suman Gumudavelli"
                    },
                    {
                        "name": "Prashanth Kannan"
                    },
                    {
                        "name": "Cristian Lumezanu"
                    },
                    {
                        "name": "Rui Miao"
                    },
                    {
                        "name": "Zhe Qu"
                    },
                    {
                        "name": "Venkat Ramesh"
                    },
                    {
                        "name": "Maxim Samoylov"
                    },
                    {
                        "name": "Jan Seidel"
                    },
                    {
                        "name": "Srikanth Sundaresan"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Qiye Tan"
                    },
                    {
                        "name": "Shuqiang Zhang"
                    },
                    {
                        "name": "Yimeng Zhao"
                    },
                    {
                        "name": "Shengbao Zheng"
                    },
                    {
                        "name": "Art Zhu"
                    },
                    {
                        "name": "Hongyi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Hongyi Zeng"
                },
                "author": "Hongyi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17853v2",
                "updated": "2025-10-24T15:36:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    36,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-15T00:32:26Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    0,
                    32,
                    26,
                    2,
                    288,
                    0
                ],
                "title": "CiteGuard: Faithful Citation Attribution for LLMs via\n  Retrieval-Augmented Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CiteGuard: Faithful Citation Attribution for LLMs via\n  Retrieval-Augmented Validation"
                },
                "summary": "Large Language Models (LLMs) have emerged as promising assistants for\nscientific writing. However, there have been concerns regarding the quality and\nreliability of the generated text, one of which is the citation accuracy and\nfaithfulness. While most recent work relies on methods such as LLM-as-a-Judge,\nthe reliability of LLM-as-a-Judge alone is also in doubt. In this work, we\nreframe citation evaluation as a problem of citation attribution alignment,\nwhich is assessing whether LLM-generated citations match those a human author\nwould include for the same text. We propose CiteGuard, a retrieval-aware agent\nframework designed to provide more faithful grounding for citation validation.\nCiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4%\naccuracy on the CiteME benchmark, on par with human-level performance (69.7%).\nIt also enables the identification of alternative but valid citations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as promising assistants for\nscientific writing. However, there have been concerns regarding the quality and\nreliability of the generated text, one of which is the citation accuracy and\nfaithfulness. While most recent work relies on methods such as LLM-as-a-Judge,\nthe reliability of LLM-as-a-Judge alone is also in doubt. In this work, we\nreframe citation evaluation as a problem of citation attribution alignment,\nwhich is assessing whether LLM-generated citations match those a human author\nwould include for the same text. We propose CiteGuard, a retrieval-aware agent\nframework designed to provide more faithful grounding for citation validation.\nCiteGuard improves the prior baseline by 12.3%, and achieves up to 65.4%\naccuracy on the CiteME benchmark, on par with human-level performance (69.7%).\nIt also enables the identification of alternative but valid citations."
                },
                "authors": [
                    {
                        "name": "Yee Man Choi"
                    },
                    {
                        "name": "Xuehang Guo"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Qingyun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Wang"
                },
                "author": "Qingyun Wang",
                "arxiv_comment": "https://kathcym.github.io/CiteGuard_Page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20176v2",
                "updated": "2025-10-24T15:36:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    36,
                    31,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T03:51:17Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    3,
                    51,
                    17,
                    3,
                    296,
                    0
                ],
                "title": "Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table\n  Understanding"
                },
                "summary": "Understanding and reasoning over tables is a critical capability for many\nreal-world applications. Large language models (LLMs) have shown promise on\nthis task, but current approaches remain limited. Fine-tuning based methods\nstrengthen language reasoning; yet they are prone to arithmetic errors and\nhallucination. In contrast, tool-based methods enable precise table\nmanipulation but rely on rigid schemas and lack semantic understanding. These\ncomplementary drawbacks highlight the need for approaches that integrate robust\nreasoning with reliable table processing. In this work, we propose\nMixture-of-Minds, a multi-agent framework that decomposes table reasoning into\nthree specialized roles: planning, coding, and answering. This design enables\neach agent to focus on a specific aspect of the task while leveraging code\nexecution for precise table manipulation. Building on this workflow, we\nintroduce a self-improvement training framework that employs Monte Carlo Tree\nSearch (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents\nwith reinforcement learning (RL). Extensive experiments show that\nMixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and\nsurpassing OpenAI-o4-mini-high. These results demonstrate the promise of\ncombining structured multi-agent workflows with RL to advance table\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and reasoning over tables is a critical capability for many\nreal-world applications. Large language models (LLMs) have shown promise on\nthis task, but current approaches remain limited. Fine-tuning based methods\nstrengthen language reasoning; yet they are prone to arithmetic errors and\nhallucination. In contrast, tool-based methods enable precise table\nmanipulation but rely on rigid schemas and lack semantic understanding. These\ncomplementary drawbacks highlight the need for approaches that integrate robust\nreasoning with reliable table processing. In this work, we propose\nMixture-of-Minds, a multi-agent framework that decomposes table reasoning into\nthree specialized roles: planning, coding, and answering. This design enables\neach agent to focus on a specific aspect of the task while leveraging code\nexecution for precise table manipulation. Building on this workflow, we\nintroduce a self-improvement training framework that employs Monte Carlo Tree\nSearch (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents\nwith reinforcement learning (RL). Extensive experiments show that\nMixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and\nsurpassing OpenAI-o4-mini-high. These results demonstrate the promise of\ncombining structured multi-agent workflows with RL to advance table\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Mingrui Zhang"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Mingyi Wang"
                    },
                    {
                        "name": "Qiao Liu"
                    },
                    {
                        "name": "Qifei Wang"
                    },
                    {
                        "name": "Jiayi Liu"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Serena Li"
                    },
                    {
                        "name": "Weiwei Li"
                    },
                    {
                        "name": "Mingze Gao"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Xiangjun Fan"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Lizhu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lizhu Zhang"
                },
                "author": "Lizhu Zhang",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21565v1",
                "updated": "2025-10-24T15:25:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    25,
                    25,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:25:25Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    25,
                    25,
                    4,
                    297,
                    0
                ],
                "title": "Chip-scale modulation-free laser stabilization using vacuum-gap\n  micro-Fabry-Prot cavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chip-scale modulation-free laser stabilization using vacuum-gap\n  micro-Fabry-Prot cavity"
                },
                "summary": "Narrow-linewidth lasers are vital for a broad range of scientific and\ntechnological applications, including atomic clocks and precision sensing.\nAchieving high frequency stability is often as critical as ensuring\nscalability, portability, and cost-effectiveness in the development of low\nnoise laser systems. Conventional electro-optic stabilization techniques, such\nas Pound-Drever-Hall locking to ultra-high-finesse resonators held in a vacuum\nchamber, provide excellent performance but remain challenging to scale. Here,\nwe propose and experimentally demonstrate a cavity-coupled interferometric\nlaser stabilization technique implemented on a silicon photonic chip and\nintegrated with a compact, scalable micro-Fabry-P\\'erot cavity. The vacuum-gap\noptical cavity operates in air, achieving a quality factor of approximately\n$2.0\\times 10^9$ and a fractional frequency instability of $1.45\\times\n10^{-12}$ at one-second averaging time. Integration of the proposed technique\nwith the compact cavity yields more than 38-fold reduction in the laser's\nintegrated linewidth and nearly three orders of magnitude suppression of\nfrequency noise at 10 Hz offset frequency. The hybrid-integration of the\nproposed photonic chip with the micro-Fabry-P\\'erot cavity establishes a\nscalable and portable route toward chip-integrated ultra-stable lasers, paving\nthe way for precision optical systems deployable beyond laboratory\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrow-linewidth lasers are vital for a broad range of scientific and\ntechnological applications, including atomic clocks and precision sensing.\nAchieving high frequency stability is often as critical as ensuring\nscalability, portability, and cost-effectiveness in the development of low\nnoise laser systems. Conventional electro-optic stabilization techniques, such\nas Pound-Drever-Hall locking to ultra-high-finesse resonators held in a vacuum\nchamber, provide excellent performance but remain challenging to scale. Here,\nwe propose and experimentally demonstrate a cavity-coupled interferometric\nlaser stabilization technique implemented on a silicon photonic chip and\nintegrated with a compact, scalable micro-Fabry-P\\'erot cavity. The vacuum-gap\noptical cavity operates in air, achieving a quality factor of approximately\n$2.0\\times 10^9$ and a fractional frequency instability of $1.45\\times\n10^{-12}$ at one-second averaging time. Integration of the proposed technique\nwith the compact cavity yields more than 38-fold reduction in the laser's\nintegrated linewidth and nearly three orders of magnitude suppression of\nfrequency noise at 10 Hz offset frequency. The hybrid-integration of the\nproposed photonic chip with the micro-Fabry-P\\'erot cavity establishes a\nscalable and portable route toward chip-integrated ultra-stable lasers, paving\nthe way for precision optical systems deployable beyond laboratory\nenvironments."
                },
                "authors": [
                    {
                        "name": "Mohamad Hossein Idjadi"
                    },
                    {
                        "name": "Haotian Cheng"
                    },
                    {
                        "name": "Farshid Ashtiani"
                    },
                    {
                        "name": "Benjia Li"
                    },
                    {
                        "name": "Kwangwoong Kim"
                    },
                    {
                        "name": "Naijun Jin"
                    },
                    {
                        "name": "Franklyn Quinlan"
                    },
                    {
                        "name": "Peter T. Rakich"
                    }
                ],
                "author_detail": {
                    "name": "Peter T. Rakich"
                },
                "author": "Peter T. Rakich",
                "arxiv_comment": "16 pages and 4 figures (main manuscript), 6 pages and 3 figures\n  (supplementary document)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16753v2",
                "updated": "2025-10-24T15:20:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    20,
                    55,
                    4,
                    297,
                    0
                ],
                "published": "2025-08-22T19:13:21Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    19,
                    13,
                    21,
                    4,
                    234,
                    0
                ],
                "title": "GAICo: A Deployed and Extensible Framework for Evaluating Diverse and\n  Multimodal Generative AI Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAICo: A Deployed and Extensible Framework for Evaluating Diverse and\n  Multimodal Generative AI Outputs"
                },
                "summary": "The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes\ndomains necessitates robust and reproducible evaluation methods. However,\npractitioners often resort to ad-hoc, non-standardized scripts, as common\nmetrics are often unsuitable for specialized, structured outputs (e.g.,\nautomated plans, time-series) or holistic comparison across modalities (e.g.,\ntext, audio, and image). This fragmentation hinders comparability and slows AI\nsystem development. To address this challenge, we present GAICo (Generative AI\nComparator): a deployed, open-source Python library that streamlines and\nstandardizes GenAI output comparison. GAICo provides a unified, extensible\nframework supporting a comprehensive suite of reference-based metrics for\nunstructured text, specialized structured data formats, and multimedia (images,\naudio). Its architecture features a high-level API for rapid, end-to-end\nanalysis, from multi-model comparison to visualization and reporting, alongside\ndirect metric access for granular control. We demonstrate GAICo's utility\nthrough a detailed case study evaluating and debugging complex, multi-modal AI\nTravel Assistant pipelines. GAICo empowers AI researchers and developers to\nefficiently assess system performance, make evaluation reproducible, improve\ndevelopment velocity, and ultimately build more trustworthy AI systems,\naligning with the goal of moving faster and safer in AI deployment. Since its\nrelease on PyPI in Jun 2025, the tool has been downloaded over 13K times,\nacross versions, by Aug 2025, demonstrating growing community interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes\ndomains necessitates robust and reproducible evaluation methods. However,\npractitioners often resort to ad-hoc, non-standardized scripts, as common\nmetrics are often unsuitable for specialized, structured outputs (e.g.,\nautomated plans, time-series) or holistic comparison across modalities (e.g.,\ntext, audio, and image). This fragmentation hinders comparability and slows AI\nsystem development. To address this challenge, we present GAICo (Generative AI\nComparator): a deployed, open-source Python library that streamlines and\nstandardizes GenAI output comparison. GAICo provides a unified, extensible\nframework supporting a comprehensive suite of reference-based metrics for\nunstructured text, specialized structured data formats, and multimedia (images,\naudio). Its architecture features a high-level API for rapid, end-to-end\nanalysis, from multi-model comparison to visualization and reporting, alongside\ndirect metric access for granular control. We demonstrate GAICo's utility\nthrough a detailed case study evaluating and debugging complex, multi-modal AI\nTravel Assistant pipelines. GAICo empowers AI researchers and developers to\nefficiently assess system performance, make evaluation reproducible, improve\ndevelopment velocity, and ultimately build more trustworthy AI systems,\naligning with the goal of moving faster and safer in AI deployment. Since its\nrelease on PyPI in Jun 2025, the tool has been downloaded over 13K times,\nacross versions, by Aug 2025, demonstrating growing community interest."
                },
                "authors": [
                    {
                        "name": "Nitin Gupta"
                    },
                    {
                        "name": "Pallav Koppisetti"
                    },
                    {
                        "name": "Kausik Lakkaraju"
                    },
                    {
                        "name": "Biplav Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Biplav Srivastava"
                },
                "author": "Biplav Srivastava",
                "arxiv_comment": "11 pages, 7 figures, accepted at IAAI/AAAI 2026; updated with\n  figures, captions, and acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21561v1",
                "updated": "2025-10-24T15:20:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    20,
                    40,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:20:40Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    20,
                    40,
                    4,
                    297,
                    0
                ],
                "title": "Are the LLMs Capable of Maintaining at Least the Language Genus?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are the LLMs Capable of Maintaining at Least the Language Genus?"
                },
                "summary": "Large Language Models (LLMs) display notable variation in multilingual\nbehavior, yet the role of genealogical language structure in shaping this\nvariation remains underexplored. In this paper, we investigate whether LLMs\nexhibit sensitivity to linguistic genera by extending prior analyses on the\nMultiQ dataset. We first check if models prefer to switch to genealogically\nrelated languages when prompt language fidelity is not maintained. Next, we\ninvestigate whether knowledge consistency is better preserved within than\nacross genera. We show that genus-level effects are present but strongly\nconditioned by training resource availability. We further observe distinct\nmultilingual strategies across LLMs families. Our findings suggest that LLMs\nencode aspects of genus-level structure, but training data imbalances remain\nthe primary factor shaping their multilingual performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) display notable variation in multilingual\nbehavior, yet the role of genealogical language structure in shaping this\nvariation remains underexplored. In this paper, we investigate whether LLMs\nexhibit sensitivity to linguistic genera by extending prior analyses on the\nMultiQ dataset. We first check if models prefer to switch to genealogically\nrelated languages when prompt language fidelity is not maintained. Next, we\ninvestigate whether knowledge consistency is better preserved within than\nacross genera. We show that genus-level effects are present but strongly\nconditioned by training resource availability. We further observe distinct\nmultilingual strategies across LLMs families. Our findings suggest that LLMs\nencode aspects of genus-level structure, but training data imbalances remain\nthe primary factor shaping their multilingual performance."
                },
                "authors": [
                    {
                        "name": "Sandra Mitrovi"
                    },
                    {
                        "name": "David Kletz"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    },
                    {
                        "name": "Fabio Rinaldi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Rinaldi"
                },
                "author": "Fabio Rinaldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21557v1",
                "updated": "2025-10-24T15:14:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    14,
                    14,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:14:14Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    14,
                    14,
                    4,
                    297,
                    0
                ],
                "title": "Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware\n  Meta-Verification and Trustworthy Reasoning with Structured Facts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware\n  Meta-Verification and Trustworthy Reasoning with Structured Facts"
                },
                "summary": "Long-horizon reasoning in LLM-based agents often fails not from generative\nweakness but from insufficient verification of intermediate reasoning. Co-Sight\naddresses this challenge by turning reasoning into a falsifiable and auditable\nprocess through two complementary mechanisms: Conflict-Aware Meta-Verification\n(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV\nreformulates verification as conflict identification and targeted\nfalsification, allocating computation only to disagreement hotspots among\nexpert agents rather than to full reasoning chains. This bounds verification\ncost to the number of inconsistencies and improves efficiency and reliability.\nTRSF continuously organizes, validates, and synchronizes evidence across agents\nthrough a structured facts module. By maintaining verified, traceable, and\nauditable knowledge, it ensures that all reasoning is grounded in consistent,\nsource-verified information and supports transparent verification throughout\nthe reasoning process. Together, TRSF and CAMV form a closed verification loop,\nwhere TRSF supplies structured facts and CAMV selectively falsifies or\nreinforces them, yielding transparent and trustworthy reasoning. Empirically,\nCo-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last\nExam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies\nconfirm that the synergy between structured factual grounding and\nconflict-aware verification drives these improvements. Co-Sight thus offers a\nscalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code\nis available at\nhttps://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon reasoning in LLM-based agents often fails not from generative\nweakness but from insufficient verification of intermediate reasoning. Co-Sight\naddresses this challenge by turning reasoning into a falsifiable and auditable\nprocess through two complementary mechanisms: Conflict-Aware Meta-Verification\n(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV\nreformulates verification as conflict identification and targeted\nfalsification, allocating computation only to disagreement hotspots among\nexpert agents rather than to full reasoning chains. This bounds verification\ncost to the number of inconsistencies and improves efficiency and reliability.\nTRSF continuously organizes, validates, and synchronizes evidence across agents\nthrough a structured facts module. By maintaining verified, traceable, and\nauditable knowledge, it ensures that all reasoning is grounded in consistent,\nsource-verified information and supports transparent verification throughout\nthe reasoning process. Together, TRSF and CAMV form a closed verification loop,\nwhere TRSF supplies structured facts and CAMV selectively falsifies or\nreinforces them, yielding transparent and trustworthy reasoning. Empirically,\nCo-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last\nExam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies\nconfirm that the synergy between structured factual grounding and\nconflict-aware verification drives these improvements. Co-Sight thus offers a\nscalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code\nis available at\nhttps://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks."
                },
                "authors": [
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Ji Lu"
                    },
                    {
                        "name": "Shiqing Jiang"
                    },
                    {
                        "name": "Chenxiang Zhu"
                    },
                    {
                        "name": "Li Xie"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Haoran Chen"
                    },
                    {
                        "name": "Yurui Zhu"
                    },
                    {
                        "name": "Yongsheng Du"
                    },
                    {
                        "name": "Yanqin Gao"
                    },
                    {
                        "name": "Lingjun Huang"
                    },
                    {
                        "name": "Baoli Wang"
                    },
                    {
                        "name": "Fang Tan"
                    },
                    {
                        "name": "Peng Zou"
                    }
                ],
                "author_detail": {
                    "name": "Peng Zou"
                },
                "author": "Peng Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08221v2",
                "updated": "2025-10-24T15:10:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    10,
                    11,
                    4,
                    297,
                    0
                ],
                "published": "2025-08-11T17:39:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    39,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning"
                },
                "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO."
                },
                "authors": [
                    {
                        "name": "Zihe Liu"
                    },
                    {
                        "name": "Jiashun Liu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ling Pan"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Shengyi Huang"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "26 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21551v1",
                "updated": "2025-10-24T15:09:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    9,
                    9,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:09:09Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    9,
                    9,
                    4,
                    297,
                    0
                ],
                "title": "Interpretable Multimodal Zero-Shot ECG Diagnosis via Structured Clinical\n  Knowledge Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Multimodal Zero-Shot ECG Diagnosis via Structured Clinical\n  Knowledge Alignment"
                },
                "summary": "Electrocardiogram (ECG) interpretation is essential for cardiovascular\ndisease diagnosis, but current automated systems often struggle with\ntransparency and generalization to unseen conditions. To address this, we\nintroduce ZETA, a zero-shot multimodal framework designed for interpretable ECG\ndiagnosis aligned with clinical workflows. ZETA uniquely compares ECG signals\nagainst structured positive and negative clinical observations, which are\ncurated through an LLM-assisted, expert-validated process, thereby mimicking\ndifferential diagnosis. Our approach leverages a pre-trained multimodal model\nto align ECG and text embeddings without disease-specific fine-tuning.\nEmpirical evaluations demonstrate ZETA's competitive zero-shot classification\nperformance and, importantly, provide qualitative and quantitative evidence of\nenhanced interpretability, grounding predictions in specific, clinically\nrelevant positive and negative diagnostic features. ZETA underscores the\npotential of aligning ECG analysis with structured clinical knowledge for\nbuilding more transparent, generalizable, and trustworthy AI diagnostic\nsystems. We will release the curated observation dataset and code to facilitate\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram (ECG) interpretation is essential for cardiovascular\ndisease diagnosis, but current automated systems often struggle with\ntransparency and generalization to unseen conditions. To address this, we\nintroduce ZETA, a zero-shot multimodal framework designed for interpretable ECG\ndiagnosis aligned with clinical workflows. ZETA uniquely compares ECG signals\nagainst structured positive and negative clinical observations, which are\ncurated through an LLM-assisted, expert-validated process, thereby mimicking\ndifferential diagnosis. Our approach leverages a pre-trained multimodal model\nto align ECG and text embeddings without disease-specific fine-tuning.\nEmpirical evaluations demonstrate ZETA's competitive zero-shot classification\nperformance and, importantly, provide qualitative and quantitative evidence of\nenhanced interpretability, grounding predictions in specific, clinically\nrelevant positive and negative diagnostic features. ZETA underscores the\npotential of aligning ECG analysis with structured clinical knowledge for\nbuilding more transparent, generalizable, and trustworthy AI diagnostic\nsystems. We will release the curated observation dataset and code to facilitate\nfuture research."
                },
                "authors": [
                    {
                        "name": "Jialu Tang"
                    },
                    {
                        "name": "Hung Manh Pham"
                    },
                    {
                        "name": "Ignace De Lathauwer"
                    },
                    {
                        "name": "Henk S. Schipper"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Dong Ma"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21538v1",
                "updated": "2025-10-24T15:02:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    2,
                    1,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:02:01Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    2,
                    1,
                    4,
                    297,
                    0
                ],
                "title": "InterpDetect: Interpretable Signals for Detecting Hallucinations in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterpDetect: Interpretable Signals for Detecting Hallucinations in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) integrates external knowledge to\nmitigate hallucinations, yet models often generate outputs inconsistent with\nretrieved content. Accurate hallucination detection requires disentangling the\ncontributions of external context and parametric knowledge, which prior methods\ntypically conflate. We investigate the mechanisms underlying RAG hallucinations\nand find they arise when later-layer FFN modules disproportionately inject\nparametric knowledge into the residual stream. To address this, we explore a\nmechanistic detection approach based on external context scores and parametric\nknowledge scores. Using Qwen3-0.6b, we compute these scores across layers and\nattention heads and train regression-based classifiers to predict\nhallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,\nGPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,\nclassifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,\ndemonstrating the potential of proxy-model evaluation. Our results highlight\nmechanistic signals as efficient, generalizable predictors for hallucination\ndetection in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) integrates external knowledge to\nmitigate hallucinations, yet models often generate outputs inconsistent with\nretrieved content. Accurate hallucination detection requires disentangling the\ncontributions of external context and parametric knowledge, which prior methods\ntypically conflate. We investigate the mechanisms underlying RAG hallucinations\nand find they arise when later-layer FFN modules disproportionately inject\nparametric knowledge into the residual stream. To address this, we explore a\nmechanistic detection approach based on external context scores and parametric\nknowledge scores. Using Qwen3-0.6b, we compute these scores across layers and\nattention heads and train regression-based classifiers to predict\nhallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,\nGPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,\nclassifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,\ndemonstrating the potential of proxy-model evaluation. Our results highlight\nmechanistic signals as efficient, generalizable predictors for hallucination\ndetection in RAG systems."
                },
                "authors": [
                    {
                        "name": "Likun Tan"
                    },
                    {
                        "name": "Kuan-Wei Huang"
                    },
                    {
                        "name": "Joy Shi"
                    },
                    {
                        "name": "Kevin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Wu"
                },
                "author": "Kevin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21535v1",
                "updated": "2025-10-24T15:01:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    1,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T15:01:06Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    15,
                    1,
                    6,
                    4,
                    297,
                    0
                ],
                "title": "Human and AI Trust: Trust Attitude Measurement Instrument",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and AI Trust: Trust Attitude Measurement Instrument"
                },
                "summary": "With the current progress of Artificial Intelligence (AI) technology and its\nincreasingly broader applications, trust is seen as a required criterion for AI\nusage, acceptance, and deployment. A robust measurement instrument is essential\nto correctly evaluate trust from a human-centered perspective. This paper\ndescribes the development and validation process of a trust measure instrument,\nwhich follows psychometric principles, and consists of a 16-items trust scale.\nThe instrument was built explicitly for research in human-AI interaction to\nmeasure trust attitudes towards AI systems from layperson (non-expert)\nperspective. The use-case we used to develop the scale was in the context of AI\nmedical support systems (specifically cancer/health prediction). The scale\ndevelopment (Measurement Item Development) and validation (Measurement Item\nEvaluation) involved six research stages: item development, item evaluation,\nsurvey administration, test of dimensionality, test of reliability, and test of\nvalidity. The results of the six-stages evaluation show that the proposed trust\nmeasurement instrument is empirically reliable and valid for systematically\nmeasuring and comparing non-experts' trust in AI Medical Support Systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the current progress of Artificial Intelligence (AI) technology and its\nincreasingly broader applications, trust is seen as a required criterion for AI\nusage, acceptance, and deployment. A robust measurement instrument is essential\nto correctly evaluate trust from a human-centered perspective. This paper\ndescribes the development and validation process of a trust measure instrument,\nwhich follows psychometric principles, and consists of a 16-items trust scale.\nThe instrument was built explicitly for research in human-AI interaction to\nmeasure trust attitudes towards AI systems from layperson (non-expert)\nperspective. The use-case we used to develop the scale was in the context of AI\nmedical support systems (specifically cancer/health prediction). The scale\ndevelopment (Measurement Item Development) and validation (Measurement Item\nEvaluation) involved six research stages: item development, item evaluation,\nsurvey administration, test of dimensionality, test of reliability, and test of\nvalidity. The results of the six-stages evaluation show that the proposed trust\nmeasurement instrument is empirically reliable and valid for systematically\nmeasuring and comparing non-experts' trust in AI Medical Support Systems."
                },
                "authors": [
                    {
                        "name": "Retno Larasati"
                    }
                ],
                "author_detail": {
                    "name": "Retno Larasati"
                },
                "author": "Retno Larasati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20075v2",
                "updated": "2025-10-24T14:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    59,
                    45,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T23:16:50Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    16,
                    50,
                    2,
                    295,
                    0
                ],
                "title": "LLMs can hide text in other text of the same length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can hide text in other text of the same length"
                },
                "summary": "A meaningful text can be hidden inside another, completely different yet\nstill coherent and plausible, text of the same length. For example, a tweet\ncontaining a harsh political critique could be embedded in a tweet that\ncelebrates the same political leader, or an ordinary product review could\nconceal a secret manuscript. This uncanny state of affairs is now possible\nthanks to Large Language Models, and in this paper we present a simple and\nefficient protocol to achieve it. We show that even modest 8-billion-parameter\nopen-source LLMs are sufficient to obtain high-quality results, and a message\nas long as this abstract can be encoded and decoded locally on a laptop in\nseconds. The existence of such a protocol demonstrates a radical decoupling of\ntext from authorial intent, further eroding trust in written communication,\nalready shaken by the rise of LLM chatbots. We illustrate this with a concrete\nscenario: a company could covertly deploy an unfiltered LLM by encoding its\nanswers within the compliant responses of a safe model. This possibility raises\nurgent questions for AI safety and challenges our understanding of what it\nmeans for a Large Language Model to know something.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A meaningful text can be hidden inside another, completely different yet\nstill coherent and plausible, text of the same length. For example, a tweet\ncontaining a harsh political critique could be embedded in a tweet that\ncelebrates the same political leader, or an ordinary product review could\nconceal a secret manuscript. This uncanny state of affairs is now possible\nthanks to Large Language Models, and in this paper we present a simple and\nefficient protocol to achieve it. We show that even modest 8-billion-parameter\nopen-source LLMs are sufficient to obtain high-quality results, and a message\nas long as this abstract can be encoded and decoded locally on a laptop in\nseconds. The existence of such a protocol demonstrates a radical decoupling of\ntext from authorial intent, further eroding trust in written communication,\nalready shaken by the rise of LLM chatbots. We illustrate this with a concrete\nscenario: a company could covertly deploy an unfiltered LLM by encoding its\nanswers within the compliant responses of a safe model. This possibility raises\nurgent questions for AI safety and challenges our understanding of what it\nmeans for a Large Language Model to know something."
                },
                "authors": [
                    {
                        "name": "Antonio Norelli"
                    },
                    {
                        "name": "Michael Bronstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bronstein"
                },
                "author": "Michael Bronstein",
                "arxiv_comment": "21 pages, main paper 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19225v2",
                "updated": "2025-10-24T14:49:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    49,
                    45,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:19:37Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    19,
                    37,
                    2,
                    295,
                    0
                ],
                "title": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient\n  Reinforcement Learning on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient\n  Reinforcement Learning on LLMs"
                },
                "summary": "Reinforcement learning (RL) has become essential for unlocking advanced\nreasoning capabilities in large language models (LLMs). RL workflows involve\ninterleaving rollout and training stages with fundamentally different resource\nrequirements. Rollout typically dominates overall execution time, yet scales\nefficiently through multiple independent instances. In contrast, training\nrequires tightly-coupled GPUs with full-mesh communication. Existing RL\nframeworks fall into two categories: co-located and disaggregated\narchitectures. Co-located ones fail to address this resource tension by forcing\nboth stages to share the same GPUs. Disaggregated architectures, without\nmodifications of well-established RL algorithms, suffer from resource\nunder-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances\non public clouds and spare capacity in production clusters, present significant\ncost-saving opportunities for accelerating RL workflows, if efficiently\nharvested for rollout.\n  In this paper, we present RLBoost, a systematic solution for cost-efficient\nRL training that harvests preemptible GPU resources. Our key insight is that\nrollout's stateless and embarrassingly parallel nature aligns perfectly with\npreemptible and often fragmented resources. To efficiently utilize these\nresources despite frequent and unpredictable availability changes, RLBoost\nadopts a hybrid architecture with three key techniques: (1) adaptive rollout\noffload to dynamically adjust workloads on the reserved (on-demand) cluster,\n(2) pull-based weight transfer that quickly provisions newly available\ninstances, and (3) token-level response collection and migration for efficient\npreemption handling and continuous load balancing. Extensive experiments show\nRLBoost increases training throughput by 1.51x-1.97x while improving cost\nefficiency by 28%-49% compared to using only on-demand GPU resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become essential for unlocking advanced\nreasoning capabilities in large language models (LLMs). RL workflows involve\ninterleaving rollout and training stages with fundamentally different resource\nrequirements. Rollout typically dominates overall execution time, yet scales\nefficiently through multiple independent instances. In contrast, training\nrequires tightly-coupled GPUs with full-mesh communication. Existing RL\nframeworks fall into two categories: co-located and disaggregated\narchitectures. Co-located ones fail to address this resource tension by forcing\nboth stages to share the same GPUs. Disaggregated architectures, without\nmodifications of well-established RL algorithms, suffer from resource\nunder-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances\non public clouds and spare capacity in production clusters, present significant\ncost-saving opportunities for accelerating RL workflows, if efficiently\nharvested for rollout.\n  In this paper, we present RLBoost, a systematic solution for cost-efficient\nRL training that harvests preemptible GPU resources. Our key insight is that\nrollout's stateless and embarrassingly parallel nature aligns perfectly with\npreemptible and often fragmented resources. To efficiently utilize these\nresources despite frequent and unpredictable availability changes, RLBoost\nadopts a hybrid architecture with three key techniques: (1) adaptive rollout\noffload to dynamically adjust workloads on the reserved (on-demand) cluster,\n(2) pull-based weight transfer that quickly provisions newly available\ninstances, and (3) token-level response collection and migration for efficient\npreemption handling and continuous load balancing. Extensive experiments show\nRLBoost increases training throughput by 1.51x-1.97x while improving cost\nefficiency by 28%-49% compared to using only on-demand GPU resources."
                },
                "authors": [
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Juncheng Gu"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Z. Morley Mao"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21525v1",
                "updated": "2025-10-24T14:48:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    48,
                    57,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T14:48:57Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    48,
                    57,
                    4,
                    297,
                    0
                ],
                "title": "A Unified Model for Multi-Task Drone Routing in Post-Disaster Road\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Model for Multi-Task Drone Routing in Post-Disaster Road\n  Assessment"
                },
                "summary": "Post-disaster road assessment (PDRA) is essential for emergency response,\nenabling rapid evaluation of infrastructure conditions and efficient allocation\nof resources. Although drones provide a flexible and effective tool for PDRA,\nrouting them in large-scale networks remains challenging. Traditional\noptimization methods scale poorly and demand domain expertise, while existing\ndeep reinforcement learning (DRL) approaches adopt a single-task paradigm,\nrequiring separate models for each problem variant and lacking adaptability to\nevolving operational needs. This study proposes a unified model (UM) for drone\nrouting that simultaneously addresses eight PDRA variants. By training a single\nneural network across multiple problem configurations, UM captures shared\nstructural knowledge while adapting to variant-specific constraints through a\nmodern transformer encoder-decoder architecture. A lightweight adapter\nmechanism further enables efficient finetuning to unseen attributes without\nretraining, enhancing deployment flexibility in dynamic disaster scenarios.\nExtensive experiments demonstrate that the UM reduces training time and\nparameters by a factor of eight compared with training separate models, while\nconsistently outperforming single-task DRL methods by 6--14\\% and traditional\noptimization approaches by 24--82\\% in terms of solution quality (total\ncollected information value). The model achieves real-time solutions (1--10\nseconds) across networks of up to 1,000 nodes, with robustness confirmed\nthrough sensitivity analyses. Moreover, finetuning experiments show that unseen\nattributes can be effectively incorporated with minimal cost while retaining\nhigh solution quality. The proposed UM advances neural combinatorial\noptimization for time-critical applications, offering a computationally\nefficient, high-quality, and adaptable solution for drone-based PDRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-disaster road assessment (PDRA) is essential for emergency response,\nenabling rapid evaluation of infrastructure conditions and efficient allocation\nof resources. Although drones provide a flexible and effective tool for PDRA,\nrouting them in large-scale networks remains challenging. Traditional\noptimization methods scale poorly and demand domain expertise, while existing\ndeep reinforcement learning (DRL) approaches adopt a single-task paradigm,\nrequiring separate models for each problem variant and lacking adaptability to\nevolving operational needs. This study proposes a unified model (UM) for drone\nrouting that simultaneously addresses eight PDRA variants. By training a single\nneural network across multiple problem configurations, UM captures shared\nstructural knowledge while adapting to variant-specific constraints through a\nmodern transformer encoder-decoder architecture. A lightweight adapter\nmechanism further enables efficient finetuning to unseen attributes without\nretraining, enhancing deployment flexibility in dynamic disaster scenarios.\nExtensive experiments demonstrate that the UM reduces training time and\nparameters by a factor of eight compared with training separate models, while\nconsistently outperforming single-task DRL methods by 6--14\\% and traditional\noptimization approaches by 24--82\\% in terms of solution quality (total\ncollected information value). The model achieves real-time solutions (1--10\nseconds) across networks of up to 1,000 nodes, with robustness confirmed\nthrough sensitivity analyses. Moreover, finetuning experiments show that unseen\nattributes can be effectively incorporated with minimal cost while retaining\nhigh solution quality. The proposed UM advances neural combinatorial\noptimization for time-critical applications, offering a computationally\nefficient, high-quality, and adaptable solution for drone-based PDRA."
                },
                "authors": [
                    {
                        "name": "Huatian Gong"
                    },
                    {
                        "name": "Jiuh-Biing Sheu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Xiaoguang Yang"
                    },
                    {
                        "name": "Ran Yan"
                    }
                ],
                "author_detail": {
                    "name": "Ran Yan"
                },
                "author": "Ran Yan",
                "arxiv_comment": "34 pages, 8 figures,9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21524v1",
                "updated": "2025-10-24T14:48:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    48,
                    10,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T14:48:10Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    48,
                    10,
                    4,
                    297,
                    0
                ],
                "title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law"
                },
                "summary": "Large language models (LLMs) are increasingly deployed as agents in various\ncontexts by providing tools at their disposal. However, LLM agents can exhibit\nunpredictable behaviors, including taking undesirable and/or unsafe actions. In\norder to measure the latent propensity of LLM agents for taking illegal actions\nunder an EU legislative context, we introduce EU-Agent-Bench, a verifiable\nhuman-curated benchmark that evaluates an agent's alignment with EU legal norms\nin situations where benign user inputs could lead to unlawful actions. Our\nbenchmark spans scenarios across several categories, including data protection,\nbias/discrimination, and scientific integrity, with each user request allowing\nfor both compliant and non-compliant execution of the requested actions.\nComparing the model's function calls against a rubric exhaustively supported by\ncitations of the relevant legislature, we evaluate the legal compliance of\nfrontier LLMs, and furthermore investigate the compliance effect of providing\nthe relevant legislative excerpts in the agent's system prompt along with\nexplicit instructions to comply. We release a public preview set for the\nresearch community, while holding out a private test set to prevent data\ncontamination in evaluating upcoming models. We encourage future work extending\nagentic safety benchmarks to different legal jurisdictions and to multi-turn\nand multilingual interactions. We release our code on\n\\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed as agents in various\ncontexts by providing tools at their disposal. However, LLM agents can exhibit\nunpredictable behaviors, including taking undesirable and/or unsafe actions. In\norder to measure the latent propensity of LLM agents for taking illegal actions\nunder an EU legislative context, we introduce EU-Agent-Bench, a verifiable\nhuman-curated benchmark that evaluates an agent's alignment with EU legal norms\nin situations where benign user inputs could lead to unlawful actions. Our\nbenchmark spans scenarios across several categories, including data protection,\nbias/discrimination, and scientific integrity, with each user request allowing\nfor both compliant and non-compliant execution of the requested actions.\nComparing the model's function calls against a rubric exhaustively supported by\ncitations of the relevant legislature, we evaluate the legal compliance of\nfrontier LLMs, and furthermore investigate the compliance effect of providing\nthe relevant legislative excerpts in the agent's system prompt along with\nexplicit instructions to comply. We release a public preview set for the\nresearch community, while holding out a private test set to prevent data\ncontamination in evaluating upcoming models. We encourage future work extending\nagentic safety benchmarks to different legal jurisdictions and to multi-turn\nand multilingual interactions. We release our code on\n\\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}."
                },
                "authors": [
                    {
                        "name": "Ilija Lichkovski"
                    },
                    {
                        "name": "Alexander Mller"
                    },
                    {
                        "name": "Mariam Ibrahim"
                    },
                    {
                        "name": "Tiwai Mhundwa"
                    }
                ],
                "author_detail": {
                    "name": "Tiwai Mhundwa"
                },
                "author": "Tiwai Mhundwa",
                "arxiv_comment": "Accepted at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09702v3",
                "updated": "2025-10-24T14:48:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    48,
                    8,
                    4,
                    297,
                    0
                ],
                "published": "2025-04-13T19:35:43Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    19,
                    35,
                    43,
                    6,
                    103,
                    0
                ],
                "title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research\n  Challenges?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLRC-Bench: Can Language Agents Solve Machine Learning Research\n  Challenges?"
                },
                "summary": "We introduce MLRC-Bench, a benchmark designed to quantify how effectively\nlanguage agents can tackle challenging Machine Learning (ML) Research\nCompetitions, with a focus on open research problems that demand novel\nmethodologies. Unlike prior work, e.g., AI Scientist, which evaluates the\nend-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the\nkey steps of proposing and implementing novel research methods and evaluates\nthem with rigorous protocol and objective metrics. Our curated suite of 7\ncompetition tasks reveals significant challenges for LLM agents. Even the\nbest-performing tested agent (gemini-exp-1206 under MLAB) closes only 9.3% of\nthe gap between baseline and top human participant scores. Furthermore, our\nanalysis reveals a misalignment between the LLM-judged innovation and actual\nperformance on cutting-edge ML research problems. MLRC-Bench is a dynamic\nbenchmark, designed to grow with new ML competitions and encourage rigorous,\nobjective evaluations of AI research capabilities. Our leaderboard and code are\navailable at: https://huggingface.co/spaces/launch/MLRC_Bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MLRC-Bench, a benchmark designed to quantify how effectively\nlanguage agents can tackle challenging Machine Learning (ML) Research\nCompetitions, with a focus on open research problems that demand novel\nmethodologies. Unlike prior work, e.g., AI Scientist, which evaluates the\nend-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the\nkey steps of proposing and implementing novel research methods and evaluates\nthem with rigorous protocol and objective metrics. Our curated suite of 7\ncompetition tasks reveals significant challenges for LLM agents. Even the\nbest-performing tested agent (gemini-exp-1206 under MLAB) closes only 9.3% of\nthe gap between baseline and top human participant scores. Furthermore, our\nanalysis reveals a misalignment between the LLM-judged innovation and actual\nperformance on cutting-edge ML research problems. MLRC-Bench is a dynamic\nbenchmark, designed to grow with new ML competitions and encourage rigorous,\nobjective evaluations of AI research capabilities. Our leaderboard and code are\navailable at: https://huggingface.co/spaces/launch/MLRC_Bench"
                },
                "authors": [
                    {
                        "name": "Yunxiang Zhang"
                    },
                    {
                        "name": "Muhammad Khalifa"
                    },
                    {
                        "name": "Shitanshu Bhushan"
                    },
                    {
                        "name": "Grant D Murphy"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Jaekyeom Kim"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Honglak Lee"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "arxiv_comment": "NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04103v2",
                "updated": "2025-10-24T14:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    45,
                    40,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-05T17:12:33Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    17,
                    12,
                    33,
                    5,
                    186,
                    0
                ],
                "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Train Your LLM Web Agent: A Statistical Diagnosis"
                },
                "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models."
                },
                "authors": [
                    {
                        "name": "Dheeraj Vattikonda"
                    },
                    {
                        "name": "Santhoshi Ravichandran"
                    },
                    {
                        "name": "Emiliano Penaloza"
                    },
                    {
                        "name": "Hadi Nekoei"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Thibault Le Sellier de Chezelles"
                    },
                    {
                        "name": "Nicolas Gontier"
                    },
                    {
                        "name": "Miguel Muoz-Mrmol"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Stefania Raimondo"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Laurent Charlin"
                    },
                    {
                        "name": "Alexandre Pich"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Massimo Caccia"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Caccia"
                },
                "author": "Massimo Caccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21513v1",
                "updated": "2025-10-24T14:39:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    39,
                    23,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T14:39:23Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    39,
                    23,
                    4,
                    297,
                    0
                ],
                "title": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair"
                },
                "summary": "Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems.\n  To address this gap, we empirically compare ten individual LLMs from five\nfamilies, and three ensembles of these LLMs across three software engineering\nbenchmarks covering code generation and program repair. We assess the\ncomplementarity between models and the performance gap between the best\nindividual model and the ensembles. Next, we evaluate various selection\nheuristics to identify correct solutions from an ensemble's candidate pool.\n  We find that the theoretical upperbound for an ensemble's performance can be\n83% above the best single model. Our results show that consensus-based\nstrategies for selecting solutions fall into a \"popularity trap,\" amplifying\ncommon but incorrect outputs. In contrast, a diversity-based strategy realizes\nup to 95% of this theoretical potential, and proves effective even in small\ntwo-model ensembles, enabling a cost-efficient way to enhance performance by\nleveraging multiple LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems.\n  To address this gap, we empirically compare ten individual LLMs from five\nfamilies, and three ensembles of these LLMs across three software engineering\nbenchmarks covering code generation and program repair. We assess the\ncomplementarity between models and the performance gap between the best\nindividual model and the ensembles. Next, we evaluate various selection\nheuristics to identify correct solutions from an ensemble's candidate pool.\n  We find that the theoretical upperbound for an ensemble's performance can be\n83% above the best single model. Our results show that consensus-based\nstrategies for selecting solutions fall into a \"popularity trap,\" amplifying\ncommon but incorrect outputs. In contrast, a diversity-based strategy realizes\nup to 95% of this theoretical potential, and proves effective even in small\ntwo-model ensembles, enabling a cost-efficient way to enhance performance by\nleveraging multiple LLMs."
                },
                "authors": [
                    {
                        "name": "Fernando Vallecillos Ruiz"
                    },
                    {
                        "name": "Max Hort"
                    },
                    {
                        "name": "Leon Moonen"
                    }
                ],
                "author_detail": {
                    "name": "Leon Moonen"
                },
                "author": "Leon Moonen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21508v1",
                "updated": "2025-10-24T14:36:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    36,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T14:36:35Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    36,
                    35,
                    4,
                    297,
                    0
                ],
                "title": "Actionable Cybersecurity Notifications for Smart Homes: A User Study on\n  the Role of Length and Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Actionable Cybersecurity Notifications for Smart Homes: A User Study on\n  the Role of Length and Complexity"
                },
                "summary": "The proliferation of smart home devices has increased convenience but also\nintroduced cybersecurity risks for everyday users, as many devices lack robust\nsecurity features. Intrusion Detection Systems are a prominent approach to\ndetecting cybersecurity threats. However, their alerts often use technical\nterms and require users to interpret them correctly, which is challenging for a\ntypical smart home user. Large Language Models can bridge this gap by\ntranslating IDS alerts into actionable security notifications. However, it has\nnot yet been clear what an actionable cybersecurity notification should look\nlike. In this paper, we conduct an experimental online user study with 130\nparticipants to examine how the length and complexity of LLM-generated\nnotifications affect user likability, understandability, and motivation to act.\nOur results show that intermediate-complexity notifications are the most\neffective across all user groups, regardless of their technological\nproficiency. Across the board, users rated beginner-level messages as more\neffective when they were longer, while expert-level messages were rated\nmarginally more effective when they were shorter. These findings provide\ninsights for designing security notifications that are both actionable and\nbroadly accessible to smart home users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of smart home devices has increased convenience but also\nintroduced cybersecurity risks for everyday users, as many devices lack robust\nsecurity features. Intrusion Detection Systems are a prominent approach to\ndetecting cybersecurity threats. However, their alerts often use technical\nterms and require users to interpret them correctly, which is challenging for a\ntypical smart home user. Large Language Models can bridge this gap by\ntranslating IDS alerts into actionable security notifications. However, it has\nnot yet been clear what an actionable cybersecurity notification should look\nlike. In this paper, we conduct an experimental online user study with 130\nparticipants to examine how the length and complexity of LLM-generated\nnotifications affect user likability, understandability, and motivation to act.\nOur results show that intermediate-complexity notifications are the most\neffective across all user groups, regardless of their technological\nproficiency. Across the board, users rated beginner-level messages as more\neffective when they were longer, while expert-level messages were rated\nmarginally more effective when they were shorter. These findings provide\ninsights for designing security notifications that are both actionable and\nbroadly accessible to smart home users."
                },
                "authors": [
                    {
                        "name": "Victor Jttner"
                    },
                    {
                        "name": "Charlotte S. Lffler"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "arxiv_doi": "10.1007/978-3-032-07989-3_19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-07989-3_19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.21508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version of the article has been accepted for publication, but is\n  not the Version of Record and does not reflect post-acceptance improvements,\n  or any corrections. The Version of Record is available online at:\n  https://doi.org/10.1007/978-3-032-07989-3_19",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17006v2",
                "updated": "2025-10-24T14:36:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    36,
                    10,
                    4,
                    297,
                    0
                ],
                "published": "2024-10-22T13:25:59Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    13,
                    25,
                    59,
                    1,
                    296,
                    0
                ],
                "title": "Variational autoencoders stabilise TCN performance when classifying\n  weakly labelled bioacoustics data: an interdisciplinary approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational autoencoders stabilise TCN performance when classifying\n  weakly labelled bioacoustics data: an interdisciplinary approach"
                },
                "summary": "Passive acoustic monitoring (PAM) data is often weakly labelled, audited at\nthe scale of detection presence or absence on timescales of minutes to hours.\nMoreover, this data exhibits great variability from one deployment to the next,\ndue to differences in ambient noise and the signals across sources and\ngeographies. This study proposes a two-step solution to leverage weakly\nannotated data for training Deep Learning (DL) detection models. Our case study\ninvolves binary classification of the presence/absence of sperm whale\n(\\textit{Physeter macrocephalus}) click trains in 4-minute-long recordings from\na dataset comprising diverse sources and deployment conditions to maximise\ngeneralisability. We tested methods for extracting acoustic features from\nlengthy audio segments and integrated Temporal Convolutional Networks (TCNs)\ntrained on the extracted features for sequence classification. For feature\nextraction, we introduced a new approach using Variational AutoEncoders (VAEs)\nto extract information from both waveforms and spectrograms, which eliminates\nthe necessity for manual threshold setting or time-consuming strong labelling.\nFor classification, TCNs were trained separately on sequences of either VAE\nembeddings or handpicked acoustic features extracted from the waveform and\nspectrogram representations using classical methods, to compare the efficacy of\nthe two approaches. The TCN demonstrated robust classification capabilities on\na validation set, achieving accuracies exceeding 85\\% when applied to 4-minute\nacoustic recordings. Notably, TCNs trained on handpicked acoustic features\nexhibited greater variability in performance across recordings from diverse\ndeployment conditions, whereas those trained on VAEs showed a more consistent\nperformance, highlighting the robust transferability of VAEs for feature\nextraction across different deployment conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive acoustic monitoring (PAM) data is often weakly labelled, audited at\nthe scale of detection presence or absence on timescales of minutes to hours.\nMoreover, this data exhibits great variability from one deployment to the next,\ndue to differences in ambient noise and the signals across sources and\ngeographies. This study proposes a two-step solution to leverage weakly\nannotated data for training Deep Learning (DL) detection models. Our case study\ninvolves binary classification of the presence/absence of sperm whale\n(\\textit{Physeter macrocephalus}) click trains in 4-minute-long recordings from\na dataset comprising diverse sources and deployment conditions to maximise\ngeneralisability. We tested methods for extracting acoustic features from\nlengthy audio segments and integrated Temporal Convolutional Networks (TCNs)\ntrained on the extracted features for sequence classification. For feature\nextraction, we introduced a new approach using Variational AutoEncoders (VAEs)\nto extract information from both waveforms and spectrograms, which eliminates\nthe necessity for manual threshold setting or time-consuming strong labelling.\nFor classification, TCNs were trained separately on sequences of either VAE\nembeddings or handpicked acoustic features extracted from the waveform and\nspectrogram representations using classical methods, to compare the efficacy of\nthe two approaches. The TCN demonstrated robust classification capabilities on\na validation set, achieving accuracies exceeding 85\\% when applied to 4-minute\nacoustic recordings. Notably, TCNs trained on handpicked acoustic features\nexhibited greater variability in performance across recordings from diverse\ndeployment conditions, whereas those trained on VAEs showed a more consistent\nperformance, highlighting the robust transferability of VAEs for feature\nextraction across different deployment conditions."
                },
                "authors": [
                    {
                        "name": "Laia Garrob Fonollosa"
                    },
                    {
                        "name": "Douglas Gillespie"
                    },
                    {
                        "name": "Lina Stankovic"
                    },
                    {
                        "name": "Vladimir Stankovic"
                    },
                    {
                        "name": "Luke Rendell"
                    }
                ],
                "author_detail": {
                    "name": "Luke Rendell"
                },
                "author": "Luke Rendell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17196v2",
                "updated": "2025-10-24T14:34:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    34,
                    28,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-22T18:05:16Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    18,
                    5,
                    16,
                    3,
                    142,
                    0
                ],
                "title": "Shape it Up! Restoring LLM Safety during Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shape it Up! Restoring LLM Safety during Finetuning"
                },
                "summary": "Finetuning large language models (LLMs) enables user-specific customization\nbut introduces critical safety risks: even a few harmful examples can\ncompromise safety alignment. A common mitigation strategy is to update the\nmodel more strongly on examples deemed safe, while downweighting or excluding\nthose flagged as unsafe. However, because safety context can shift within a\nsingle example, updating the model equally on both harmful and harmless parts\nof a response is suboptimal-a coarse treatment we term static safety shaping.\nIn contrast, we propose dynamic safety shaping (DSS), a framework that uses\nfine-grained safety signals to reinforce learning from safe segments of a\nresponse while suppressing unsafe content. To enable such fine-grained control\nduring finetuning, we introduce a key insight: guardrail models, traditionally\nused for filtering, can be repurposed to evaluate partial responses, tracking\nhow safety risk evolves throughout the response, segment by segment. This leads\nto the Safety Trajectory Assessment of Response (STAR), a token-level signal\nthat enables shaping to operate dynamically over the training sequence.\nBuilding on this, we present STAR-DSS, guided by STAR scores, that robustly\nmitigates finetuning risks and delivers substantial safety improvements across\ndiverse threats, datasets, and model families-all without compromising\ncapability on intended tasks. We encourage future safety research to build on\ndynamic shaping principles for stronger mitigation against evolving finetuning\nrisks. Our code is publicly available at https://github.com/poloclub/star-dss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning large language models (LLMs) enables user-specific customization\nbut introduces critical safety risks: even a few harmful examples can\ncompromise safety alignment. A common mitigation strategy is to update the\nmodel more strongly on examples deemed safe, while downweighting or excluding\nthose flagged as unsafe. However, because safety context can shift within a\nsingle example, updating the model equally on both harmful and harmless parts\nof a response is suboptimal-a coarse treatment we term static safety shaping.\nIn contrast, we propose dynamic safety shaping (DSS), a framework that uses\nfine-grained safety signals to reinforce learning from safe segments of a\nresponse while suppressing unsafe content. To enable such fine-grained control\nduring finetuning, we introduce a key insight: guardrail models, traditionally\nused for filtering, can be repurposed to evaluate partial responses, tracking\nhow safety risk evolves throughout the response, segment by segment. This leads\nto the Safety Trajectory Assessment of Response (STAR), a token-level signal\nthat enables shaping to operate dynamically over the training sequence.\nBuilding on this, we present STAR-DSS, guided by STAR scores, that robustly\nmitigates finetuning risks and delivers substantial safety improvements across\ndiverse threats, datasets, and model families-all without compromising\ncapability on intended tasks. We encourage future safety research to build on\ndynamic shaping principles for stronger mitigation against evolving finetuning\nrisks. Our code is publicly available at https://github.com/poloclub/star-dss."
                },
                "authors": [
                    {
                        "name": "ShengYun Peng"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Jianfeng Chi"
                    },
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Duen Horng Chau"
                    }
                ],
                "author_detail": {
                    "name": "Duen Horng Chau"
                },
                "author": "Duen Horng Chau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15144v2",
                "updated": "2025-10-24T14:23:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    23,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-16T21:03:54Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    3,
                    54,
                    3,
                    289,
                    0
                ],
                "title": "HugAgent: Evaluating LLMs in Simulating Individual-Level Human Reasoning\n  on Open-Ended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HugAgent: Evaluating LLMs in Simulating Individual-Level Human Reasoning\n  on Open-Ended Tasks"
                },
                "summary": "Simulating human reasoning in open-ended tasks has been a long-standing\naspiration in AI and cognitive science. While large language models now\napproximate human responses at scale, they remain tuned to population-level\nconsensus, often erasing the individuality of reasoning styles and belief\ntrajectories. To advance the vision of more human-like reasoning in machines,\nwe introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for\naverage-to-individual reasoning adaptation. The task is to predict how a\nspecific person would reason and update their beliefs in novel scenarios, given\npartial evidence of their past views. HugAgent adopts a dual-track design: a\nsynthetic track for scale and systematic stress tests, and a human track for\necologically valid, \"out-loud\" reasoning data. This design enables scalable,\nreproducible evaluation of intra-agent fidelity: whether models can capture not\njust what people believe, but how their reasoning evolves. Experiments with\nstate-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent\nas the first extensible benchmark for aligning machine reasoning with the\nindividuality of human thought. Our benchmark and chatbot are open-sourced as\nHugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking\n(https://anonymous.4open.science/r/trace-your-thinking).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating human reasoning in open-ended tasks has been a long-standing\naspiration in AI and cognitive science. While large language models now\napproximate human responses at scale, they remain tuned to population-level\nconsensus, often erasing the individuality of reasoning styles and belief\ntrajectories. To advance the vision of more human-like reasoning in machines,\nwe introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for\naverage-to-individual reasoning adaptation. The task is to predict how a\nspecific person would reason and update their beliefs in novel scenarios, given\npartial evidence of their past views. HugAgent adopts a dual-track design: a\nsynthetic track for scale and systematic stress tests, and a human track for\necologically valid, \"out-loud\" reasoning data. This design enables scalable,\nreproducible evaluation of intra-agent fidelity: whether models can capture not\njust what people believe, but how their reasoning evolves. Experiments with\nstate-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent\nas the first extensible benchmark for aligning machine reasoning with the\nindividuality of human thought. Our benchmark and chatbot are open-sourced as\nHugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking\n(https://anonymous.4open.science/r/trace-your-thinking)."
                },
                "authors": [
                    {
                        "name": "Chance Jiajie Li"
                    },
                    {
                        "name": "Zhenze Mo"
                    },
                    {
                        "name": "Yuhan Tang"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Kaiya Ivy Zhao"
                    },
                    {
                        "name": "Yulu Gan"
                    },
                    {
                        "name": "Jie Fan"
                    },
                    {
                        "name": "Jiangbo Yu"
                    },
                    {
                        "name": "Hang Jiang"
                    },
                    {
                        "name": "Paul Pu Liang"
                    },
                    {
                        "name": "Jinhua Zhao"
                    },
                    {
                        "name": "Luis Alberto Alonso Pastor"
                    },
                    {
                        "name": "Kent Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kent Larson"
                },
                "author": "Kent Larson",
                "arxiv_comment": "To appear in NeurIPS 2025 Workshop on Bridging Language, Agent, and\n  World Models (LAW)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21490v1",
                "updated": "2025-10-24T14:14:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    14,
                    56,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T14:14:56Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    14,
                    56,
                    4,
                    297,
                    0
                ],
                "title": "Analysis and Synthesis of Switched Optimization Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Synthesis of Switched Optimization Algorithms"
                },
                "summary": "Deployment of optimization algorithms on networked systems face challenges\nassociated with time delays and corruptions. One particular instance is the\npresence of time-varying delays arising from factors such as packet drops and\nirregular sampling. Fixed time delays can destabilize gradient descent\nalgorithms, and this degradation is exacerbated by time-varying delays. This\nwork concentrates on the analysis and creation of discrete-time optimization\nalgorithms with certified exponential convergence rates that are robust against\nswitched uncertainties between the optimizer and the gradient oracle. These\noptimization algorithms are implemented by a switch-scheduled output feedback\ncontrollers. Rate variation and sawtooth behavior (packet drops) in\ntime-varying delays can be imposed through constraining switching sequences.\nAnalysis is accomplished by bisection in the convergence rate to find\nZames-Falb filter coefficents. Synthesis is performed by alternating between a\nfilter coefficient search for a fixed controller, and a controller search for\nfixed multipliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of optimization algorithms on networked systems face challenges\nassociated with time delays and corruptions. One particular instance is the\npresence of time-varying delays arising from factors such as packet drops and\nirregular sampling. Fixed time delays can destabilize gradient descent\nalgorithms, and this degradation is exacerbated by time-varying delays. This\nwork concentrates on the analysis and creation of discrete-time optimization\nalgorithms with certified exponential convergence rates that are robust against\nswitched uncertainties between the optimizer and the gradient oracle. These\noptimization algorithms are implemented by a switch-scheduled output feedback\ncontrollers. Rate variation and sawtooth behavior (packet drops) in\ntime-varying delays can be imposed through constraining switching sequences.\nAnalysis is accomplished by bisection in the convergence rate to find\nZames-Falb filter coefficents. Synthesis is performed by alternating between a\nfilter coefficient search for a fixed controller, and a controller search for\nfixed multipliers."
                },
                "authors": [
                    {
                        "name": "Jared Miller"
                    },
                    {
                        "name": "Fabian Jakob"
                    },
                    {
                        "name": "Carsten Scherer"
                    },
                    {
                        "name": "Andrea Iannelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Iannelli"
                },
                "author": "Andrea Iannelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22235v2",
                "updated": "2025-10-24T14:07:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    7,
                    40,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-28T11:11:24Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    11,
                    11,
                    24,
                    2,
                    148,
                    0
                ],
                "title": "Optimal kernel regression bounds under energy-bounded noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal kernel regression bounds under energy-bounded noise"
                },
                "summary": "Non-conservative uncertainty bounds are key for both assessing an estimation\nalgorithm's accuracy and in view of downstream tasks, such as its deployment in\nsafety-critical contexts. In this paper, we derive a tight, non-asymptotic\nuncertainty bound for kernel-based estimation, which can also handle correlated\nnoise sequences. Its computation relies on a mild norm-boundedness assumption\non the unknown function and the noise, returning the worst-case function\nrealization within the hypothesis class at an arbitrary query input location.\nThe value of this function is shown to be given in terms of the posterior mean\nand covariance of a Gaussian process for an optimal choice of the measurement\nnoise covariance. By rigorously analyzing the proposed approach and comparing\nit with other results in the literature, we show its effectiveness in returning\ntight and easy-to-compute bounds for kernel-based estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-conservative uncertainty bounds are key for both assessing an estimation\nalgorithm's accuracy and in view of downstream tasks, such as its deployment in\nsafety-critical contexts. In this paper, we derive a tight, non-asymptotic\nuncertainty bound for kernel-based estimation, which can also handle correlated\nnoise sequences. Its computation relies on a mild norm-boundedness assumption\non the unknown function and the noise, returning the worst-case function\nrealization within the hypothesis class at an arbitrary query input location.\nThe value of this function is shown to be given in terms of the posterior mean\nand covariance of a Gaussian process for an optimal choice of the measurement\nnoise covariance. By rigorously analyzing the proposed approach and comparing\nit with other results in the literature, we show its effectiveness in returning\ntight and easy-to-compute bounds for kernel-based estimates."
                },
                "authors": [
                    {
                        "name": "Amon Lahr"
                    },
                    {
                        "name": "Johannes Khler"
                    },
                    {
                        "name": "Anna Scampicchio"
                    },
                    {
                        "name": "Melanie N. Zeilinger"
                    }
                ],
                "author_detail": {
                    "name": "Melanie N. Zeilinger"
                },
                "author": "Melanie N. Zeilinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T37 (Primary) 46E22, 60G15 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11887v3",
                "updated": "2025-10-24T14:06:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    6,
                    15,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-13T15:36:22Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    22,
                    4,
                    164,
                    0
                ],
                "title": "Cascaded Language Models for Cost-effective Human-AI Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascaded Language Models for Cost-effective Human-AI Decision-Making"
                },
                "summary": "A challenge in human-AI decision-making is to balance three factors: the\ncorrectness of predictions, the cost of knowledge and reasoning complexity, and\nthe confidence about whether to abstain from automated answers or escalate to\nhuman experts. In this work, we present a cascaded LLM decision framework that\nadaptively delegates tasks across multiple tiers of expertise -- a base model\nfor initial candidate answers, a more capable and knowledgeable (but costlier)\nlarge model, and a human expert for when the model cascade abstains. Our method\nproceeds in two stages. First, a deferral policy determines whether to accept\nthe base model's answer or regenerate it with the large model based on the\nconfidence score. Second, an abstention policy decides whether the cascade\nmodel response is sufficiently certain or requires human intervention.\nMoreover, to overcome static policies and accommodate changing task difficulty,\nwe incorporate an online learning mechanism which uses human feedback. We\ndemonstrate this approach to general question-answering (ARC-Easy,\nARC-Challenge, and MMLU) and medical question-answering (MedQA and MedMCQA).\nOur results demonstrate that our cascaded strategy outperforms single-model\nbaselines in most cases, achieving higher accuracy while reducing costs and\nproviding a principled approach to handling abstentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A challenge in human-AI decision-making is to balance three factors: the\ncorrectness of predictions, the cost of knowledge and reasoning complexity, and\nthe confidence about whether to abstain from automated answers or escalate to\nhuman experts. In this work, we present a cascaded LLM decision framework that\nadaptively delegates tasks across multiple tiers of expertise -- a base model\nfor initial candidate answers, a more capable and knowledgeable (but costlier)\nlarge model, and a human expert for when the model cascade abstains. Our method\nproceeds in two stages. First, a deferral policy determines whether to accept\nthe base model's answer or regenerate it with the large model based on the\nconfidence score. Second, an abstention policy decides whether the cascade\nmodel response is sufficiently certain or requires human intervention.\nMoreover, to overcome static policies and accommodate changing task difficulty,\nwe incorporate an online learning mechanism which uses human feedback. We\ndemonstrate this approach to general question-answering (ARC-Easy,\nARC-Challenge, and MMLU) and medical question-answering (MedQA and MedMCQA).\nOur results demonstrate that our cascaded strategy outperforms single-model\nbaselines in most cases, achieving higher accuracy while reducing costs and\nproviding a principled approach to handling abstentions."
                },
                "authors": [
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21473v1",
                "updated": "2025-10-24T13:57:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    57,
                    59,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:57:59Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    57,
                    59,
                    4,
                    297,
                    0
                ],
                "title": "MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward\n  Optimization"
                },
                "summary": "Recent advances in diffusion language models (DLMs) have presented a\npromising alternative to traditional autoregressive large language models\n(LLMs). However, DLMs still lag behind LLMs in reasoning performance,\nespecially as the number of denoising steps decreases. Our analysis reveals\nthat this shortcoming arises primarily from the independent generation of\nmasked tokens across denoising steps, which fails to capture the token\ncorrelation. In this paper, we define two types of token correlation:\nintra-sequence correlation and inter-sequence correlation, and demonstrate that\nenhancing these correlations improves reasoning performance. To this end, we\npropose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to\nconsider the token correlation during the denoising process. More specifically,\nour MRO approach leverages test-time scaling, reject sampling, and\nreinforcement learning to directly optimize the token correlation with multiple\nelaborate rewards. Additionally, we introduce group step and importance\nsampling strategies to mitigate reward variance and enhance sampling\nefficiency. Through extensive experiments, we demonstrate that MRO not only\nimproves reasoning performance but also achieves significant sampling speedups\nwhile maintaining high performance on reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion language models (DLMs) have presented a\npromising alternative to traditional autoregressive large language models\n(LLMs). However, DLMs still lag behind LLMs in reasoning performance,\nespecially as the number of denoising steps decreases. Our analysis reveals\nthat this shortcoming arises primarily from the independent generation of\nmasked tokens across denoising steps, which fails to capture the token\ncorrelation. In this paper, we define two types of token correlation:\nintra-sequence correlation and inter-sequence correlation, and demonstrate that\nenhancing these correlations improves reasoning performance. To this end, we\npropose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to\nconsider the token correlation during the denoising process. More specifically,\nour MRO approach leverages test-time scaling, reject sampling, and\nreinforcement learning to directly optimize the token correlation with multiple\nelaborate rewards. Additionally, we introduce group step and importance\nsampling strategies to mitigate reward variance and enhance sampling\nefficiency. Through extensive experiments, we demonstrate that MRO not only\nimproves reasoning performance but also achieves significant sampling speedups\nwhile maintaining high performance on reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Yang Gan"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Chi Hu"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Kai Song"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Chunliang Zhang"
                    },
                    {
                        "name": "Tongran Liu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Tong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xiao"
                },
                "author": "Tong Xiao",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21464v1",
                "updated": "2025-10-24T13:46:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    46,
                    18,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:46:18Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    46,
                    18,
                    4,
                    297,
                    0
                ],
                "title": "CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray\n  Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray\n  Diagnosis"
                },
                "summary": "Deep learning models have achieved remarkable accuracy in chest X-ray\ndiagnosis, yet their widespread clinical adoption remains limited by the\nblack-box nature of their predictions. Clinicians require transparent,\nverifiable explanations to trust automated diagnoses and identify potential\nfailure modes. We introduce CXR-LanIC (Language-Grounded Interpretable\nClassifier for Chest X-rays), a novel framework that addresses this\ninterpretability challenge through task-aligned pattern discovery. Our approach\ntrains transcoder-based sparse autoencoders on a BiomedCLIP diagnostic\nclassifier to decompose medical image representations into interpretable visual\npatterns. By training an ensemble of 100 transcoders on multimodal embeddings\nfrom the MIMIC-CXR dataset, we discover approximately 5,000 monosemantic\npatterns spanning cardiac, pulmonary, pleural, structural, device, and artifact\ncategories. Each pattern exhibits consistent activation behavior across images\nsharing specific radiological features, enabling transparent attribution where\npredictions decompose into 20-50 interpretable patterns with verifiable\nactivation galleries. CXR-LanIC achieves competitive diagnostic accuracy on\nfive key findings while providing the foundation for natural language\nexplanations through planned large multimodal model annotation. Our key\ninnovation lies in extracting interpretable features from a classifier trained\non specific diagnostic objectives rather than general-purpose embeddings,\nensuring discovered patterns are directly relevant to clinical decision-making,\ndemonstrating that medical AI systems can be both accurate and interpretable,\nsupporting safer clinical deployment through transparent, clinically grounded\nexplanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have achieved remarkable accuracy in chest X-ray\ndiagnosis, yet their widespread clinical adoption remains limited by the\nblack-box nature of their predictions. Clinicians require transparent,\nverifiable explanations to trust automated diagnoses and identify potential\nfailure modes. We introduce CXR-LanIC (Language-Grounded Interpretable\nClassifier for Chest X-rays), a novel framework that addresses this\ninterpretability challenge through task-aligned pattern discovery. Our approach\ntrains transcoder-based sparse autoencoders on a BiomedCLIP diagnostic\nclassifier to decompose medical image representations into interpretable visual\npatterns. By training an ensemble of 100 transcoders on multimodal embeddings\nfrom the MIMIC-CXR dataset, we discover approximately 5,000 monosemantic\npatterns spanning cardiac, pulmonary, pleural, structural, device, and artifact\ncategories. Each pattern exhibits consistent activation behavior across images\nsharing specific radiological features, enabling transparent attribution where\npredictions decompose into 20-50 interpretable patterns with verifiable\nactivation galleries. CXR-LanIC achieves competitive diagnostic accuracy on\nfive key findings while providing the foundation for natural language\nexplanations through planned large multimodal model annotation. Our key\ninnovation lies in extracting interpretable features from a classifier trained\non specific diagnostic objectives rather than general-purpose embeddings,\nensuring discovered patterns are directly relevant to clinical decision-making,\ndemonstrating that medical AI systems can be both accurate and interpretable,\nsupporting safer clinical deployment through transparent, clinically grounded\nexplanations."
                },
                "authors": [
                    {
                        "name": "Yiming Tang"
                    },
                    {
                        "name": "Wenjia Zhong"
                    },
                    {
                        "name": "Rushi Shah"
                    },
                    {
                        "name": "Dianbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dianbo Liu"
                },
                "author": "Dianbo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01268v2",
                "updated": "2025-10-24T13:46:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    46,
                    9,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-29T10:04:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    10,
                    4,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical\n  Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical\n  Guarantees"
                },
                "summary": "We study the problem of determining whether a piece of text has been authored\nby a human or by a large language model (LLM). Existing state of the art\nlogits-based detectors make use of statistics derived from the log-probability\nof the observed text evaluated using the distribution function of a given\nsource LLM. However, relying solely on log probabilities can be sub-optimal. In\nresponse, we introduce AdaDetectGPT -- a novel classifier that adaptively\nlearns a witness function from training data to enhance the performance of\nlogits-based detectors. We provide statistical guarantees on its true positive\nrate, false positive rate, true negative rate and false negative rate.\nExtensive numerical studies show AdaDetectGPT nearly uniformly improves the\nstate-of-the-art method in various combination of datasets and LLMs, and the\nimprovement can reach up to 37\\%. A python implementation of our method is\navailable at https://github.com/Mamba413/AdaDetectGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of determining whether a piece of text has been authored\nby a human or by a large language model (LLM). Existing state of the art\nlogits-based detectors make use of statistics derived from the log-probability\nof the observed text evaluated using the distribution function of a given\nsource LLM. However, relying solely on log probabilities can be sub-optimal. In\nresponse, we introduce AdaDetectGPT -- a novel classifier that adaptively\nlearns a witness function from training data to enhance the performance of\nlogits-based detectors. We provide statistical guarantees on its true positive\nrate, false positive rate, true negative rate and false negative rate.\nExtensive numerical studies show AdaDetectGPT nearly uniformly improves the\nstate-of-the-art method in various combination of datasets and LLMs, and the\nimprovement can reach up to 37\\%. A python implementation of our method is\navailable at https://github.com/Mamba413/AdaDetectGPT."
                },
                "authors": [
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Jin Zhu"
                    },
                    {
                        "name": "Pingfan Su"
                    },
                    {
                        "name": "Kai Ye"
                    },
                    {
                        "name": "Ying Yang"
                    },
                    {
                        "name": "Shakeel A O B Gavioli-Akilagun"
                    },
                    {
                        "name": "Chengchun Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chengchun Shi"
                },
                "author": "Chengchun Shi",
                "arxiv_comment": "Accepted by NeurIPS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21460v1",
                "updated": "2025-10-24T13:43:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    43,
                    29,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:43:29Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    43,
                    29,
                    4,
                    297,
                    0
                ],
                "title": "Risk Management for Mitigating Benchmark Failure Modes: BenchRisk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Mitigating Benchmark Failure Modes: BenchRisk"
                },
                "summary": "Large language model (LLM) benchmarks inform LLM use decisions (e.g., \"is\nthis LLM safe to deploy for my use case and context?\"). However, benchmarks may\nbe rendered unreliable by various failure modes that impact benchmark bias,\nvariance, coverage, or people's capacity to understand benchmark evidence.\nUsing the National Institute of Standards and Technology's risk management\nprocess as a foundation, this research iteratively analyzed 26 popular\nbenchmarks, identifying 57 potential failure modes and 196 corresponding\nmitigation strategies. The mitigations reduce failure likelihood and/or\nseverity, providing a frame for evaluating \"benchmark risk,\" which is scored to\nprovide a metaevaluation benchmark: BenchRisk. Higher scores indicate that\nbenchmark users are less likely to reach an incorrect or unsupported conclusion\nabout an LLM. All 26 scored benchmarks present significant risk within one or\nmore of the five scored dimensions (comprehensiveness, intelligibility,\nconsistency, correctness, and longevity), which points to important open\nresearch directions for the field of LLM benchmarking. The BenchRisk workflow\nallows for comparison between benchmarks; as an open-source tool, it also\nfacilitates the identification and sharing of risks and their mitigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) benchmarks inform LLM use decisions (e.g., \"is\nthis LLM safe to deploy for my use case and context?\"). However, benchmarks may\nbe rendered unreliable by various failure modes that impact benchmark bias,\nvariance, coverage, or people's capacity to understand benchmark evidence.\nUsing the National Institute of Standards and Technology's risk management\nprocess as a foundation, this research iteratively analyzed 26 popular\nbenchmarks, identifying 57 potential failure modes and 196 corresponding\nmitigation strategies. The mitigations reduce failure likelihood and/or\nseverity, providing a frame for evaluating \"benchmark risk,\" which is scored to\nprovide a metaevaluation benchmark: BenchRisk. Higher scores indicate that\nbenchmark users are less likely to reach an incorrect or unsupported conclusion\nabout an LLM. All 26 scored benchmarks present significant risk within one or\nmore of the five scored dimensions (comprehensiveness, intelligibility,\nconsistency, correctness, and longevity), which points to important open\nresearch directions for the field of LLM benchmarking. The BenchRisk workflow\nallows for comparison between benchmarks; as an open-source tool, it also\nfacilitates the identification and sharing of risks and their mitigations."
                },
                "authors": [
                    {
                        "name": "Sean McGregor"
                    },
                    {
                        "name": "Victor Lu"
                    },
                    {
                        "name": "Vassil Tashev"
                    },
                    {
                        "name": "Armstrong Foundjem"
                    },
                    {
                        "name": "Aishwarya Ramasethu"
                    },
                    {
                        "name": "Sadegh AlMahdi Kazemi Zarkouei"
                    },
                    {
                        "name": "Chris Knotz"
                    },
                    {
                        "name": "Kongtao Chen"
                    },
                    {
                        "name": "Alicia Parrish"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Heather Frase"
                    }
                ],
                "author_detail": {
                    "name": "Heather Frase"
                },
                "author": "Heather Frase",
                "arxiv_comment": "19 pages, 7 figures, to be published in the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21459v1",
                "updated": "2025-10-24T13:41:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    41,
                    52,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:41:52Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    41,
                    52,
                    4,
                    297,
                    0
                ],
                "title": "SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM\n  Honeypots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM\n  Honeypots"
                },
                "summary": "Honeypots are decoy systems used for gathering valuable threat intelligence\nor diverting attackers away from production systems. Maximising attacker\nengagement is essential to their utility. However research has highlighted that\ncontext-awareness, such as the ability to respond to new attack types, systems\nand attacker agents, is necessary to increase engagement. Large Language Models\n(LLMs) have been shown as one approach to increase context awareness but suffer\nfrom several challenges including accuracy and timeliness of response time,\nhigh operational costs and data-protection issues due to cloud deployment. We\npropose the System-Based Attention Shell Honeypot (SBASH) framework which\nmanages data-protection issues through the use of lightweight local LLMs. We\ninvestigate the use of Retrieval Augmented Generation (RAG) supported LLMs and\nnon-RAG LLMs for Linux shell commands and evaluate them using several different\nmetrics such as response time differences, realism from human testers, and\nsimilarity to a real system calculated with Levenshtein distance, SBert, and\nBertScore. We show that RAG improves accuracy for untuned models while models\nthat have been tuned via a system prompt that tells the LLM to respond like a\nLinux system achieve without RAG a similar accuracy as untuned with RAG, while\nhaving a slightly lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honeypots are decoy systems used for gathering valuable threat intelligence\nor diverting attackers away from production systems. Maximising attacker\nengagement is essential to their utility. However research has highlighted that\ncontext-awareness, such as the ability to respond to new attack types, systems\nand attacker agents, is necessary to increase engagement. Large Language Models\n(LLMs) have been shown as one approach to increase context awareness but suffer\nfrom several challenges including accuracy and timeliness of response time,\nhigh operational costs and data-protection issues due to cloud deployment. We\npropose the System-Based Attention Shell Honeypot (SBASH) framework which\nmanages data-protection issues through the use of lightweight local LLMs. We\ninvestigate the use of Retrieval Augmented Generation (RAG) supported LLMs and\nnon-RAG LLMs for Linux shell commands and evaluate them using several different\nmetrics such as response time differences, realism from human testers, and\nsimilarity to a real system calculated with Levenshtein distance, SBert, and\nBertScore. We show that RAG improves accuracy for untuned models while models\nthat have been tuned via a system prompt that tells the LLM to respond like a\nLinux system achieve without RAG a similar accuracy as untuned with RAG, while\nhaving a slightly lower latency."
                },
                "authors": [
                    {
                        "name": "Adetayo Adebimpe"
                    },
                    {
                        "name": "Helmut Neukirchen"
                    },
                    {
                        "name": "Thomas Welsh"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Welsh"
                },
                "author": "Thomas Welsh",
                "arxiv_comment": "to be published in: The 3rd International Conference on Foundation\n  and Large Language Models (FLLM2025), IEEE, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; D.4.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10340v2",
                "updated": "2025-10-24T13:39:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    39,
                    36,
                    4,
                    297,
                    0
                ],
                "published": "2024-11-15T16:40:43Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    40,
                    43,
                    4,
                    320,
                    0
                ],
                "title": "Domain Adaptation-based Edge Computing for Cross-Conditions Fault\n  Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Adaptation-based Edge Computing for Cross-Conditions Fault\n  Diagnosis"
                },
                "summary": "Fault diagnosis of mechanical equipment provides robust support for\nindustrial production. It is worth noting that, the operation of mechanical\nequipment is accompanied by changes in factors such as speed and load, leading\nto significant differences in data distribution, which pose challenges for\nfault diagnosis. Additionally, in terms of application deployment, commonly\nused cloud-based fault diagnosis methods often encounter issues such as time\ndelays and data security concerns, while common fault diagnosis methods cannot\nbe directly applied to edge computing devices. Therefore, conducting fault\ndiagnosis under cross-operating conditions based on edge computing holds\nsignificant research value. This paper proposes a domain-adaptation-based\nlightweight fault diagnosis framework tailored for edge computing scenarios.\nIncorporating the local maximum mean discrepancy into knowledge transfer aligns\nthe feature distributions of different domains in a high-dimensional feature\nspace, to discover a common feature space across domains. The acquired fault\ndiagnosis expertise from the cloud-based deep neural network model is\ntransferred to the lightweight edge-based model (edge model) using adaptation\nknowledge transfer methods. It aims to achieve accurate fault diagnosis under\ncross-working conditions while ensuring real-time diagnosis capabilities. We\nutilized the NVIDIA Jetson Xavier NX kit as the edge computing platform and\nconducted validation experiments on two devices. In terms of diagnostic\nperformance, the proposed method significantly improved diagnostic accuracy,\nwith average increases of 34.44% and 17.33% compared to existing methods,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault diagnosis of mechanical equipment provides robust support for\nindustrial production. It is worth noting that, the operation of mechanical\nequipment is accompanied by changes in factors such as speed and load, leading\nto significant differences in data distribution, which pose challenges for\nfault diagnosis. Additionally, in terms of application deployment, commonly\nused cloud-based fault diagnosis methods often encounter issues such as time\ndelays and data security concerns, while common fault diagnosis methods cannot\nbe directly applied to edge computing devices. Therefore, conducting fault\ndiagnosis under cross-operating conditions based on edge computing holds\nsignificant research value. This paper proposes a domain-adaptation-based\nlightweight fault diagnosis framework tailored for edge computing scenarios.\nIncorporating the local maximum mean discrepancy into knowledge transfer aligns\nthe feature distributions of different domains in a high-dimensional feature\nspace, to discover a common feature space across domains. The acquired fault\ndiagnosis expertise from the cloud-based deep neural network model is\ntransferred to the lightweight edge-based model (edge model) using adaptation\nknowledge transfer methods. It aims to achieve accurate fault diagnosis under\ncross-working conditions while ensuring real-time diagnosis capabilities. We\nutilized the NVIDIA Jetson Xavier NX kit as the edge computing platform and\nconducted validation experiments on two devices. In terms of diagnostic\nperformance, the proposed method significantly improved diagnostic accuracy,\nwith average increases of 34.44% and 17.33% compared to existing methods,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jinhong Wu"
                    },
                    {
                        "name": "Chu Wang"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Tingli Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tingli Xie"
                },
                "author": "Tingli Xie",
                "arxiv_comment": "23 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14620v2",
                "updated": "2025-10-24T13:28:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    52,
                    4,
                    297,
                    0
                ],
                "published": "2025-04-20T13:58:20Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    13,
                    58,
                    20,
                    6,
                    110,
                    0
                ],
                "title": "A Hierarchical Framework for Measuring Scientific Paper Innovation via\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hierarchical Framework for Measuring Scientific Paper Innovation via\n  Large Language Models"
                },
                "summary": "Measuring scientific paper innovation is both important and challenging.\nExisting content-based methods often overlook the full-paper context, fail to\ncapture the full scope of innovation, and lack generalization. We propose\nHSPIM, a hierarchical and training-free framework based on large language\nmodels (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess\ninnovation. We segment the text by section titles and use zero-shot LLM\nprompting to implement section classification, question-answering (QA)\naugmentation, and weighted innovation scoring. The generated QA pair focuses on\nsection-level innovation and serves as additional context to improve the LLM\nscoring. For each chunk, the LLM outputs a novelty score and a confidence\nscore. We use confidence scores as weights to aggregate novelty scores into a\npaper-level innovation score. To further improve performance, we propose a\ntwo-layer question structure consisting of common and section-specific\nquestions, and apply a genetic algorithm to optimize the question-prompt\ncombinations. Furthermore, under the fine-grained structure of innovation, we\nextend HSPIM to an HSPIM$^+$ that generates novelty, contribution, and\nfeasibility scores with respective confidence scores. Comprehensive experiments\non scientific conference paper datasets show that HSPIM outperforms baseline\nmethods in effectiveness, generalization, and interpretability. Demo code is\navailable at https://github.com/Jasaxion/HSPIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring scientific paper innovation is both important and challenging.\nExisting content-based methods often overlook the full-paper context, fail to\ncapture the full scope of innovation, and lack generalization. We propose\nHSPIM, a hierarchical and training-free framework based on large language\nmodels (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess\ninnovation. We segment the text by section titles and use zero-shot LLM\nprompting to implement section classification, question-answering (QA)\naugmentation, and weighted innovation scoring. The generated QA pair focuses on\nsection-level innovation and serves as additional context to improve the LLM\nscoring. For each chunk, the LLM outputs a novelty score and a confidence\nscore. We use confidence scores as weights to aggregate novelty scores into a\npaper-level innovation score. To further improve performance, we propose a\ntwo-layer question structure consisting of common and section-specific\nquestions, and apply a genetic algorithm to optimize the question-prompt\ncombinations. Furthermore, under the fine-grained structure of innovation, we\nextend HSPIM to an HSPIM$^+$ that generates novelty, contribution, and\nfeasibility scores with respective confidence scores. Comprehensive experiments\non scientific conference paper datasets show that HSPIM outperforms baseline\nmethods in effectiveness, generalization, and interpretability. Demo code is\navailable at https://github.com/Jasaxion/HSPIM."
                },
                "authors": [
                    {
                        "name": "Hongming Tan"
                    },
                    {
                        "name": "Shaoxiong Zhan"
                    },
                    {
                        "name": "Fengwei Jia"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Wai Kin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Wai Kin Chan"
                },
                "author": "Wai Kin Chan",
                "arxiv_doi": "10.1016/j.ins.2025.122787",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ins.2025.122787",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21451v1",
                "updated": "2025-10-24T13:28:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    41,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:28:41Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    41,
                    4,
                    297,
                    0
                ],
                "title": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model\n  Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model\n  Components"
                },
                "summary": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "An Guo"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21449v1",
                "updated": "2025-10-24T13:28:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    29,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:28:29Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    29,
                    4,
                    297,
                    0
                ],
                "title": "MoniTor: Exploiting Large Language Models with Instruction for Online\n  Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoniTor: Exploiting Large Language Models with Instruction for Online\n  Video Anomaly Detection"
                },
                "summary": "Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors\nwithin videos. Recently, offline VAD has garnered substantial research\nattention, which has been invigorated by the progress in large language models\n(LLMs) and vision-language models (VLMs), offering the potential for a more\nnuanced understanding of anomalies. However, online VAD has seldom received\nattention due to real-time constraints and computational intensity. In this\npaper, we introduce a novel Memory-based online scoring queue scheme for\nTraining-free VAD (MoniTor), to address the inherent complexities in online\nVAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the\ncapabilities of pre-trained large-scale models. To capture temporal\ndependencies more effectively, we incorporate a novel prediction mechanism\ninspired by Long Short-Term Memory (LSTM) networks. This ensures the model can\neffectively model past states and leverage previous predictions to identify\nanomalous behaviors. Thereby, it better understands the current frame.\nMoreover, we design a scoring queue and an anomaly prior to dynamically store\nrecent scores and cover all anomalies in the monitoring scenario, providing\nguidance for LLMs to distinguish between normal and abnormal behaviors over\ntime. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and\nXD-Violence) containing various surveillance and real-world scenarios. The\nresults demonstrate that MoniTor outperforms state-of-the-art methods and is\ncompetitive with weakly supervised methods without training. Code is available\nat https://github.com/YsTvT/MoniTor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors\nwithin videos. Recently, offline VAD has garnered substantial research\nattention, which has been invigorated by the progress in large language models\n(LLMs) and vision-language models (VLMs), offering the potential for a more\nnuanced understanding of anomalies. However, online VAD has seldom received\nattention due to real-time constraints and computational intensity. In this\npaper, we introduce a novel Memory-based online scoring queue scheme for\nTraining-free VAD (MoniTor), to address the inherent complexities in online\nVAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the\ncapabilities of pre-trained large-scale models. To capture temporal\ndependencies more effectively, we incorporate a novel prediction mechanism\ninspired by Long Short-Term Memory (LSTM) networks. This ensures the model can\neffectively model past states and leverage previous predictions to identify\nanomalous behaviors. Thereby, it better understands the current frame.\nMoreover, we design a scoring queue and an anomaly prior to dynamically store\nrecent scores and cover all anomalies in the monitoring scenario, providing\nguidance for LLMs to distinguish between normal and abnormal behaviors over\ntime. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and\nXD-Violence) containing various surveillance and real-world scenarios. The\nresults demonstrate that MoniTor outperforms state-of-the-art methods and is\ncompetitive with weakly supervised methods without training. Code is available\nat https://github.com/YsTvT/MoniTor."
                },
                "authors": [
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Yue Feng"
                    },
                    {
                        "name": "Yingshi Liu"
                    },
                    {
                        "name": "Jingrou Zhang"
                    },
                    {
                        "name": "Jie Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jie Qin"
                },
                "author": "Jie Qin",
                "arxiv_comment": "Accepted to NeurIPS 2025. The first two authors hold equal\n  contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14828v2",
                "updated": "2025-10-24T13:25:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    25,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-02-20T18:45:01Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    45,
                    1,
                    3,
                    51,
                    0
                ],
                "title": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs"
                },
                "summary": "LLM developers have imposed technical interventions to prevent fine-tuning\nmisuse attacks, attacks where adversaries evade safeguards by fine-tuning the\nmodel using a public API. Previous work has established several successful\nattacks against specific fine-tuning API defences. In this work, we show that\ndefences of fine-tuning APIs that seek to detect individual harmful training or\ninference samples ('pointwise' detection) are fundamentally limited in their\nability to prevent fine-tuning attacks. We construct 'pointwise-undetectable'\nattacks that repurpose entropy in benign model outputs (e.g. semantic or\nsyntactic variations) to covertly transmit dangerous knowledge. Our attacks are\ncomposed solely of unsuspicious benign samples that can be collected from the\nmodel before fine-tuning, meaning training and inference samples are all\nindividually benign and low-perplexity. We test our attacks against the OpenAI\nfine-tuning API, finding they succeed in eliciting answers to harmful\nmultiple-choice questions, and that they evade an enhanced monitoring system we\ndesign that successfully detects other fine-tuning attacks. We encourage the\ncommunity to develop defences that tackle the fundamental limitations we\nuncover in pointwise fine-tuning API defences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM developers have imposed technical interventions to prevent fine-tuning\nmisuse attacks, attacks where adversaries evade safeguards by fine-tuning the\nmodel using a public API. Previous work has established several successful\nattacks against specific fine-tuning API defences. In this work, we show that\ndefences of fine-tuning APIs that seek to detect individual harmful training or\ninference samples ('pointwise' detection) are fundamentally limited in their\nability to prevent fine-tuning attacks. We construct 'pointwise-undetectable'\nattacks that repurpose entropy in benign model outputs (e.g. semantic or\nsyntactic variations) to covertly transmit dangerous knowledge. Our attacks are\ncomposed solely of unsuspicious benign samples that can be collected from the\nmodel before fine-tuning, meaning training and inference samples are all\nindividually benign and low-perplexity. We test our attacks against the OpenAI\nfine-tuning API, finding they succeed in eliciting answers to harmful\nmultiple-choice questions, and that they evade an enhanced monitoring system we\ndesign that successfully detects other fine-tuning attacks. We encourage the\ncommunity to develop defences that tackle the fundamental limitations we\nuncover in pointwise fine-tuning API defences."
                },
                "authors": [
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Eric Winsor"
                    },
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Yarin Gal"
                    }
                ],
                "author_detail": {
                    "name": "Yarin Gal"
                },
                "author": "Yarin Gal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21443v1",
                "updated": "2025-10-24T13:20:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    20,
                    30,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:20:30Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    20,
                    30,
                    4,
                    297,
                    0
                ],
                "title": "Does Model Size Matter? A Comparison of Small and Large Language Models\n  for Requirements Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Model Size Matter? A Comparison of Small and Large Language Models\n  for Requirements Classification"
                },
                "summary": "[Context and motivation] Large language models (LLMs) show notable results in\nnatural language processing (NLP) tasks for requirements engineering (RE).\nHowever, their use is compromised by high computational cost, data sharing\nrisks, and dependence on external services. In contrast, small language models\n(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]\nIt remains unclear how well SLMs perform compared to LLMs in RE tasks in terms\nof accuracy. [Results] Our preliminary study compares eight models, including\nthree LLMs and five SLMs, on requirements classification tasks using the\nPROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although\nLLMs achieve an average F1 score of 2% higher than SLMs, this difference is not\nstatistically significant. SLMs almost reach LLMs performance across all\ndatasets and even outperform them in recall on the PROMISE Reclass dataset,\ndespite being up to 300 times smaller. We also found that dataset\ncharacteristics play a more significant role in performance than model size.\n[Contribution] Our study contributes with evidence that SLMs are a valid\nalternative to LLMs for requirements classification, offering advantages in\nprivacy, cost, and local deployability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Context and motivation] Large language models (LLMs) show notable results in\nnatural language processing (NLP) tasks for requirements engineering (RE).\nHowever, their use is compromised by high computational cost, data sharing\nrisks, and dependence on external services. In contrast, small language models\n(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]\nIt remains unclear how well SLMs perform compared to LLMs in RE tasks in terms\nof accuracy. [Results] Our preliminary study compares eight models, including\nthree LLMs and five SLMs, on requirements classification tasks using the\nPROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although\nLLMs achieve an average F1 score of 2% higher than SLMs, this difference is not\nstatistically significant. SLMs almost reach LLMs performance across all\ndatasets and even outperform them in recall on the PROMISE Reclass dataset,\ndespite being up to 300 times smaller. We also found that dataset\ncharacteristics play a more significant role in performance than model size.\n[Contribution] Our study contributes with evidence that SLMs are a valid\nalternative to LLMs for requirements classification, offering advantages in\nprivacy, cost, and local deployability."
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Zadenoori"
                    },
                    {
                        "name": "Vincenzo De Martino"
                    },
                    {
                        "name": "Jacek Dabrowski"
                    },
                    {
                        "name": "Xavier Franch"
                    },
                    {
                        "name": "Alessio Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Ferrari"
                },
                "author": "Alessio Ferrari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21440v1",
                "updated": "2025-10-24T13:17:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    17,
                    0,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:17:00Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    17,
                    0,
                    4,
                    297,
                    0
                ],
                "title": "Redefining Retrieval Evaluation in the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Retrieval Evaluation in the Era of LLMs"
                },
                "summary": "Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,\nassume that human users sequentially examine documents with diminishing\nattention to lower ranks. This assumption breaks down in Retrieval Augmented\nGeneration (RAG) systems, where search results are consumed by Large Language\nModels (LLMs), which, unlike humans, process all retrieved documents as a whole\nrather than sequentially. Additionally, traditional IR metrics do not account\nfor related but irrelevant documents that actively degrade generation quality,\nrather than merely being ignored. Due to these two major misalignments, namely\nhuman vs. machine position discount and human relevance vs. machine utility,\nclassical IR metrics do not accurately predict RAG performance. We introduce a\nutility-based annotation schema that quantifies both the positive contribution\nof relevant passages and the negative impact of distracting ones. Building on\nthis foundation, we propose UDCG (Utility and Distraction-aware Cumulative\nGain), a metric using an LLM-oriented positional discount to directly optimize\nthe correlation with the end-to-end answer accuracy. Experiments on five\ndatasets and six LLMs demonstrate that UDCG improves correlation by up to 36%\ncompared to traditional metrics. Our work provides a critical step toward\naligning IR evaluation with LLM consumers and enables more reliable assessment\nof RAG components",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,\nassume that human users sequentially examine documents with diminishing\nattention to lower ranks. This assumption breaks down in Retrieval Augmented\nGeneration (RAG) systems, where search results are consumed by Large Language\nModels (LLMs), which, unlike humans, process all retrieved documents as a whole\nrather than sequentially. Additionally, traditional IR metrics do not account\nfor related but irrelevant documents that actively degrade generation quality,\nrather than merely being ignored. Due to these two major misalignments, namely\nhuman vs. machine position discount and human relevance vs. machine utility,\nclassical IR metrics do not accurately predict RAG performance. We introduce a\nutility-based annotation schema that quantifies both the positive contribution\nof relevant passages and the negative impact of distracting ones. Building on\nthis foundation, we propose UDCG (Utility and Distraction-aware Cumulative\nGain), a metric using an LLM-oriented positional discount to directly optimize\nthe correlation with the end-to-end answer accuracy. Experiments on five\ndatasets and six LLMs demonstrate that UDCG improves correlation by up to 36%\ncompared to traditional metrics. Our work provides a critical step toward\naligning IR evaluation with LLM consumers and enables more reliable assessment\nof RAG components"
                },
                "authors": [
                    {
                        "name": "Giovanni Trappolini"
                    },
                    {
                        "name": "Florin Cuconasu"
                    },
                    {
                        "name": "Simone Filice"
                    },
                    {
                        "name": "Yoelle Maarek"
                    },
                    {
                        "name": "Fabrizio Silvestri"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Silvestri"
                },
                "author": "Fabrizio Silvestri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21438v1",
                "updated": "2025-10-24T13:16:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    16,
                    1,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:16:01Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    16,
                    1,
                    4,
                    297,
                    0
                ],
                "title": "PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for\n  Mobile Robotic Chemists using Multi-Modal Behavior Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for\n  Mobile Robotic Chemists using Multi-Modal Behavior Trees"
                },
                "summary": "Mobile robotic chemists are a fast growing trend in the field of chemistry\nand materials research. However, so far these mobile robots lack workflow\nawareness skills. This poses the risk that even a small anomaly, such as an\nimproperly capped sample vial could disrupt the entire workflow. This wastes\ntime, and resources, and could pose risks to human researchers, such as\nexposure to toxic materials. Existing perception mechanisms can be used to\npredict anomalies but they often generate excessive false positives. This may\nhalt workflow execution unnecessarily, requiring researchers to intervene and\nto resume the workflow when no problem actually exists, negating the benefits\nof autonomous operation. To address this problem, we propose PREVENT a system\ncomprising navigation and manipulation skills based on a multimodal Behavior\nTree (BT) approach that can be integrated into existing software architectures\nwith minimal modifications. Our approach involves a hierarchical perception\nmechanism that exploits AI techniques and sensory feedback through Dexterous\nVision and Navigational Vision cameras and an IoT gas sensor module for\nexecution-related decision-making. Experimental evaluations show that the\nproposed approach is comparatively efficient and completely avoids both false\nnegatives and false positives when tested in simulated risk scenarios within\nour robotic chemistry workflow. The results also show that the proposed\nmulti-modal perception skills achieved deployment accuracies that were higher\nthan the average of the corresponding uni-modal skills, both for navigation and\nfor manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile robotic chemists are a fast growing trend in the field of chemistry\nand materials research. However, so far these mobile robots lack workflow\nawareness skills. This poses the risk that even a small anomaly, such as an\nimproperly capped sample vial could disrupt the entire workflow. This wastes\ntime, and resources, and could pose risks to human researchers, such as\nexposure to toxic materials. Existing perception mechanisms can be used to\npredict anomalies but they often generate excessive false positives. This may\nhalt workflow execution unnecessarily, requiring researchers to intervene and\nto resume the workflow when no problem actually exists, negating the benefits\nof autonomous operation. To address this problem, we propose PREVENT a system\ncomprising navigation and manipulation skills based on a multimodal Behavior\nTree (BT) approach that can be integrated into existing software architectures\nwith minimal modifications. Our approach involves a hierarchical perception\nmechanism that exploits AI techniques and sensory feedback through Dexterous\nVision and Navigational Vision cameras and an IoT gas sensor module for\nexecution-related decision-making. Experimental evaluations show that the\nproposed approach is comparatively efficient and completely avoids both false\nnegatives and false positives when tested in simulated risk scenarios within\nour robotic chemistry workflow. The results also show that the proposed\nmulti-modal perception skills achieved deployment accuracies that were higher\nthan the average of the corresponding uni-modal skills, both for navigation and\nfor manipulation."
                },
                "authors": [
                    {
                        "name": "Satheeshkumar Veeramani"
                    },
                    {
                        "name": "Zhengxue Zhou"
                    },
                    {
                        "name": "Francisco Munguia-Galeano"
                    },
                    {
                        "name": "Hatem Fakhruldeen"
                    },
                    {
                        "name": "Thomas Roddelkopf"
                    },
                    {
                        "name": "Mohammed Faeik Ruzaij Al-Okby"
                    },
                    {
                        "name": "Kerstin Thurow"
                    },
                    {
                        "name": "Andrew Ian Cooper"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Ian Cooper"
                },
                "author": "Andrew Ian Cooper",
                "arxiv_comment": "25 pages, 8 figures, paper submitted to Robotics and Autonomous\n  Systems Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21436v1",
                "updated": "2025-10-24T13:14:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    14,
                    53,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:14:53Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    14,
                    53,
                    4,
                    297,
                    0
                ],
                "title": "AutoOpt: A Dataset and a Unified Framework for Automating Optimization\n  Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoOpt: A Dataset and a Unified Framework for Automating Optimization\n  Problem Solving"
                },
                "summary": "This study presents AutoOpt-11k, a unique image dataset of over 11,000\nhandwritten and printed mathematical optimization models corresponding to\nsingle-objective, multi-objective, multi-level, and stochastic optimization\nproblems exhibiting various types of complexities such as non-linearity,\nnon-convexity, non-differentiability, discontinuity, and high-dimensionality.\nThe labels consist of the LaTeX representation for all the images and modeling\nlanguage representation for a subset of images. The dataset is created by 25\nexperts following ethical data creation guidelines and verified in two-phases\nto avoid errors. Further, we develop AutoOpt framework, a machine learning\nbased automated approach for solving optimization problems, where the user just\nneeds to provide an image of the formulation and AutoOpt solves it efficiently\nwithout any further human intervention. AutoOpt framework consists of three\nModules: (i) M1 (Image_to_Text)- a deep learning model performs the\nMathematical Expression Recognition (MER) task to generate the LaTeX code\ncorresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-\na small-scale fine-tuned LLM generates the PYOMO script (optimization modeling\nlanguage) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization\nbased Decomposition (BOBD) method solves the optimization formulation described\nin the PYOMO script. We use AutoOpt-11k dataset for training and testing of\ndeep learning models employed in AutoOpt. The deep learning model for MER task\n(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method\n(M3), which is a hybrid approach, yields better results on complex test\nproblems compared to common approaches, like interior-point algorithm and\ngenetic algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents AutoOpt-11k, a unique image dataset of over 11,000\nhandwritten and printed mathematical optimization models corresponding to\nsingle-objective, multi-objective, multi-level, and stochastic optimization\nproblems exhibiting various types of complexities such as non-linearity,\nnon-convexity, non-differentiability, discontinuity, and high-dimensionality.\nThe labels consist of the LaTeX representation for all the images and modeling\nlanguage representation for a subset of images. The dataset is created by 25\nexperts following ethical data creation guidelines and verified in two-phases\nto avoid errors. Further, we develop AutoOpt framework, a machine learning\nbased automated approach for solving optimization problems, where the user just\nneeds to provide an image of the formulation and AutoOpt solves it efficiently\nwithout any further human intervention. AutoOpt framework consists of three\nModules: (i) M1 (Image_to_Text)- a deep learning model performs the\nMathematical Expression Recognition (MER) task to generate the LaTeX code\ncorresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-\na small-scale fine-tuned LLM generates the PYOMO script (optimization modeling\nlanguage) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization\nbased Decomposition (BOBD) method solves the optimization formulation described\nin the PYOMO script. We use AutoOpt-11k dataset for training and testing of\ndeep learning models employed in AutoOpt. The deep learning model for MER task\n(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method\n(M3), which is a hybrid approach, yields better results on complex test\nproblems compared to common approaches, like interior-point algorithm and\ngenetic algorithm."
                },
                "authors": [
                    {
                        "name": "Ankur Sinha"
                    },
                    {
                        "name": "Shobhit Arora"
                    },
                    {
                        "name": "Dhaval Pujara"
                    }
                ],
                "author_detail": {
                    "name": "Dhaval Pujara"
                },
                "author": "Dhaval Pujara",
                "arxiv_comment": "NeurIPS 2025, 28 pages, 11 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21425v1",
                "updated": "2025-10-24T13:05:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    5,
                    50,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T13:05:50Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    5,
                    50,
                    4,
                    297,
                    0
                ],
                "title": "Advancing Symbolic Integration in Large Language Models: Beyond\n  Conventional Neurosymbolic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Symbolic Integration in Large Language Models: Beyond\n  Conventional Neurosymbolic AI"
                },
                "summary": "LLMs have demonstrated highly effective learning, human-like response\ngeneration,and decision-making capabilities in high-risk sectors. However,\nthese models remain black boxes because they struggle to ensure transparency in\nresponses. The literature has explored numerous approaches to address\ntransparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI\napproaches were primarily developed for conventional neural networks and are\nnot well-suited to the unique features of LLMs. Consequently, there is a\nlimited systematic understanding of how symbolic AI can be effectively\nintegrated into LLMs. This paper aims to address this gap by first reviewing\nestablished NeSy AI methods and then proposing a novel taxonomy of symbolic\nintegration in LLMs, along with a roadmap to merge symbolic techniques with\nLLMs. The roadmap introduces a new categorisation framework across four\ndimensions by organising existing literature within these categories. These\ninclude symbolic integration across various stages of LLM, coupling mechanisms,\narchitectural paradigms, as well as algorithmic and application-level\nperspectives. The paper thoroughly identifies current benchmarks, cutting-edge\nadvancements, and critical gaps within the field to propose a roadmap for\nfuture research. By highlighting the latest developments and notable gaps in\nthe literature, it offers practical insights for implementing frameworks for\nsymbolic integration into LLMs to enhance transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated highly effective learning, human-like response\ngeneration,and decision-making capabilities in high-risk sectors. However,\nthese models remain black boxes because they struggle to ensure transparency in\nresponses. The literature has explored numerous approaches to address\ntransparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI\napproaches were primarily developed for conventional neural networks and are\nnot well-suited to the unique features of LLMs. Consequently, there is a\nlimited systematic understanding of how symbolic AI can be effectively\nintegrated into LLMs. This paper aims to address this gap by first reviewing\nestablished NeSy AI methods and then proposing a novel taxonomy of symbolic\nintegration in LLMs, along with a roadmap to merge symbolic techniques with\nLLMs. The roadmap introduces a new categorisation framework across four\ndimensions by organising existing literature within these categories. These\ninclude symbolic integration across various stages of LLM, coupling mechanisms,\narchitectural paradigms, as well as algorithmic and application-level\nperspectives. The paper thoroughly identifies current benchmarks, cutting-edge\nadvancements, and critical gaps within the field to propose a roadmap for\nfuture research. By highlighting the latest developments and notable gaps in\nthe literature, it offers practical insights for implementing frameworks for\nsymbolic integration into LLMs to enhance transparency."
                },
                "authors": [
                    {
                        "name": "Maneeha Rani"
                    },
                    {
                        "name": "Bhupesh Kumar Mishra"
                    },
                    {
                        "name": "Dhavalkumar Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Dhavalkumar Thakker"
                },
                "author": "Dhavalkumar Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03516v2",
                "updated": "2025-10-24T12:53:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    53,
                    50,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-03T21:02:34Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    21,
                    2,
                    34,
                    4,
                    276,
                    0
                ],
                "title": "COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC\n  Techniques"
                },
                "summary": "Convolutional Neural Networks (CNNs) are highly effective for computer vision\nand pattern recognition tasks; however, their computational intensity and\nreliance on hardware such as FPGAs pose challenges for deployment on low-power\nedge devices. In this work, we present COMET, a framework of CNN designs that\nemploy efficient hardware offset-binary coding (OBC) techniques to enable\nco-optimization of performance and resource utilization. The approach\nformulates CNN inference with OBC representations of inputs (Scheme A) and\nweights (Scheme B) separately, enabling exploitation of bit-width asymmetry.\nThe shift-accumulate operation is modified by incorporating the offset term\nwith the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we\nintroduce four novel look-up table (LUT) techniques -- parallel, shared, split,\nand hybrid -- and analyze them to identify the most efficient options. Building\non this foundation, we develop an OBC-based general matrix multiplication core\nusing the im2col transformation, enabling efficient acceleration of a\nfixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the\nproposed co-optimization approach significantly reduces resource utilization\ncompared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) are highly effective for computer vision\nand pattern recognition tasks; however, their computational intensity and\nreliance on hardware such as FPGAs pose challenges for deployment on low-power\nedge devices. In this work, we present COMET, a framework of CNN designs that\nemploy efficient hardware offset-binary coding (OBC) techniques to enable\nco-optimization of performance and resource utilization. The approach\nformulates CNN inference with OBC representations of inputs (Scheme A) and\nweights (Scheme B) separately, enabling exploitation of bit-width asymmetry.\nThe shift-accumulate operation is modified by incorporating the offset term\nwith the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we\nintroduce four novel look-up table (LUT) techniques -- parallel, shared, split,\nand hybrid -- and analyze them to identify the most efficient options. Building\non this foundation, we develop an OBC-based general matrix multiplication core\nusing the im2col transformation, enabling efficient acceleration of a\nfixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the\nproposed co-optimization approach significantly reduces resource utilization\ncompared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on\naccuracy."
                },
                "authors": [
                    {
                        "name": "Boyang Chen"
                    },
                    {
                        "name": "Mohd Tasleem Khan"
                    },
                    {
                        "name": "George Goussetis"
                    },
                    {
                        "name": "Mathini Sellathurai"
                    },
                    {
                        "name": "Yuan Ding"
                    },
                    {
                        "name": "Joo F. C. Mota"
                    },
                    {
                        "name": "Jongeun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongeun Lee"
                },
                "author": "Jongeun Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01084v2",
                "updated": "2025-10-24T12:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-01T17:03:02Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    17,
                    3,
                    2,
                    6,
                    152,
                    0
                ],
                "title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression"
                },
                "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized on general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a novel\nmethod for achieving context-adaptive tokenization in LLMs at inference time.\nLeveraging an online data compression algorithm (Lempel-Ziv-Welch), zip2zip\ndynamically expands its active vocabulary at inference time by continuously\nreplacing fragmented token sequences with more compact hypertokens, which it\ncan immediately output during generation. In doing so, the model refines its\ninternal tokenization scheme to match the token distribution of the current\ncontext, reducing redundancy and improving representational efficiency. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\ncompression that incrementally merges co-occurring tokens into reusable\nhypertokens on the fly; (2) a dynamic embedding (and unembedding) layer that\ncomputes embeddings for newly formed hypertokens at runtime; and (3) a variant\nof autoregressive language modeling that pretrains the model to handle\nhypertokenized, compressed text sequences as inputs and outputs. We show that\nan existing LLM can be uptrained for zip2zip in 10 GPU-hours via\nparameter-efficient finetuning. The resulting LLM performs test-time\nadaptation, learning to use hypertokens in unseen contexts and reducing input\nand output tokens by 15-40%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized on general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a novel\nmethod for achieving context-adaptive tokenization in LLMs at inference time.\nLeveraging an online data compression algorithm (Lempel-Ziv-Welch), zip2zip\ndynamically expands its active vocabulary at inference time by continuously\nreplacing fragmented token sequences with more compact hypertokens, which it\ncan immediately output during generation. In doing so, the model refines its\ninternal tokenization scheme to match the token distribution of the current\ncontext, reducing redundancy and improving representational efficiency. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\ncompression that incrementally merges co-occurring tokens into reusable\nhypertokens on the fly; (2) a dynamic embedding (and unembedding) layer that\ncomputes embeddings for newly formed hypertokens at runtime; and (3) a variant\nof autoregressive language modeling that pretrains the model to handle\nhypertokenized, compressed text sequences as inputs and outputs. We show that\nan existing LLM can be uptrained for zip2zip in 10 GPU-hours via\nparameter-efficient finetuning. The resulting LLM performs test-time\nadaptation, learning to use hypertokens in unseen contexts and reducing input\nand output tokens by 15-40%."
                },
                "authors": [
                    {
                        "name": "Saibo Geng"
                    },
                    {
                        "name": "Nathan Ranchin"
                    },
                    {
                        "name": "Yunzhen yao"
                    },
                    {
                        "name": "Maxime Peyrard"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Michael Gastpar"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21408v1",
                "updated": "2025-10-24T12:52:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    52,
                    11,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:52:11Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    52,
                    11,
                    4,
                    297,
                    0
                ],
                "title": "Large Language Models as Model Organisms for Human Associative Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Model Organisms for Human Associative Learning"
                },
                "summary": "Associative learning--forming links between co-occurring items--is\nfundamental to human cognition, reshaping internal representations in complex\nways. Testing hypotheses on how representational changes occur in biological\nsystems is challenging, but large language models (LLMs) offer a scalable\nalternative. Building on LLMs' in-context learning, we adapt a cognitive\nneuroscience associative learning paradigm and investigate how representations\nevolve across six models. Our initial findings reveal a non-monotonic pattern\nconsistent with the Non-Monotonic Plasticity Hypothesis, with moderately\nsimilar items differentiating after learning. Leveraging the controllability of\nLLMs, we further show that this differentiation is modulated by the overlap of\nassociated items with the broader vocabulary--a factor we term vocabulary\ninterference, capturing how new associations compete with prior knowledge. We\nfind that higher vocabulary interference amplifies differentiation, suggesting\nthat representational change is influenced by both item similarity and global\ncompetition. Our findings position LLMs not only as powerful tools for studying\nrepresentational dynamics in human-like learning systems, but also as\naccessible and general computational models for generating new hypotheses about\nthe principles underlying memory reorganization in the brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associative learning--forming links between co-occurring items--is\nfundamental to human cognition, reshaping internal representations in complex\nways. Testing hypotheses on how representational changes occur in biological\nsystems is challenging, but large language models (LLMs) offer a scalable\nalternative. Building on LLMs' in-context learning, we adapt a cognitive\nneuroscience associative learning paradigm and investigate how representations\nevolve across six models. Our initial findings reveal a non-monotonic pattern\nconsistent with the Non-Monotonic Plasticity Hypothesis, with moderately\nsimilar items differentiating after learning. Leveraging the controllability of\nLLMs, we further show that this differentiation is modulated by the overlap of\nassociated items with the broader vocabulary--a factor we term vocabulary\ninterference, capturing how new associations compete with prior knowledge. We\nfind that higher vocabulary interference amplifies differentiation, suggesting\nthat representational change is influenced by both item similarity and global\ncompetition. Our findings position LLMs not only as powerful tools for studying\nrepresentational dynamics in human-like learning systems, but also as\naccessible and general computational models for generating new hypotheses about\nthe principles underlying memory reorganization in the brain."
                },
                "authors": [
                    {
                        "name": "Camila Kolling"
                    },
                    {
                        "name": "Vy Ai Vo"
                    },
                    {
                        "name": "Mariya Toneva"
                    }
                ],
                "author_detail": {
                    "name": "Mariya Toneva"
                },
                "author": "Mariya Toneva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21407v1",
                "updated": "2025-10-24T12:50:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    50,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:50:35Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    50,
                    35,
                    4,
                    297,
                    0
                ],
                "title": "REvolution: An Evolutionary Framework for RTL Generation driven by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REvolution: An Evolutionary Framework for RTL Generation driven by Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are used for Register-Transfer Level (RTL) code\ngeneration, but they face two main challenges: functional correctness and\nPower, Performance, and Area (PPA) optimization. Iterative, feedback-based\nmethods partially address these, but they are limited to local search,\nhindering the discovery of a global optimum. This paper introduces REvolution,\na framework that combines Evolutionary Computation (EC) with LLMs for automatic\nRTL generation and optimization. REvolution evolves a population of candidates\nin parallel, each defined by a design strategy, RTL implementation, and\nevaluation feedback. The framework includes a dual-population algorithm that\ndivides candidates into Fail and Success groups for bug fixing and PPA\noptimization, respectively. An adaptive mechanism further improves search\nefficiency by dynamically adjusting the selection probability of each prompt\nstrategy according to its success rate. Experiments on the VerilogEval and\nRTLLM benchmarks show that REvolution increased the initial pass rate of\nvarious LLMs by up to 24.0 percentage points. The DeepSeek-V3 model achieved a\nfinal pass rate of 95.5\\%, comparable to state-of-the-art results, without the\nneed for separate training or domain-specific tools. Additionally, the\ngenerated RTL designs showed significant PPA improvements over reference\ndesigns. This work introduces a new RTL design approach by combining LLMs'\ngenerative capabilities with EC's broad search power, overcoming the\nlocal-search limitations of previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are used for Register-Transfer Level (RTL) code\ngeneration, but they face two main challenges: functional correctness and\nPower, Performance, and Area (PPA) optimization. Iterative, feedback-based\nmethods partially address these, but they are limited to local search,\nhindering the discovery of a global optimum. This paper introduces REvolution,\na framework that combines Evolutionary Computation (EC) with LLMs for automatic\nRTL generation and optimization. REvolution evolves a population of candidates\nin parallel, each defined by a design strategy, RTL implementation, and\nevaluation feedback. The framework includes a dual-population algorithm that\ndivides candidates into Fail and Success groups for bug fixing and PPA\noptimization, respectively. An adaptive mechanism further improves search\nefficiency by dynamically adjusting the selection probability of each prompt\nstrategy according to its success rate. Experiments on the VerilogEval and\nRTLLM benchmarks show that REvolution increased the initial pass rate of\nvarious LLMs by up to 24.0 percentage points. The DeepSeek-V3 model achieved a\nfinal pass rate of 95.5\\%, comparable to state-of-the-art results, without the\nneed for separate training or domain-specific tools. Additionally, the\ngenerated RTL designs showed significant PPA improvements over reference\ndesigns. This work introduces a new RTL design approach by combining LLMs'\ngenerative capabilities with EC's broad search power, overcoming the\nlocal-search limitations of previous methods."
                },
                "authors": [
                    {
                        "name": "Kyungjun Min"
                    },
                    {
                        "name": "Kyumin Cho"
                    },
                    {
                        "name": "Junhwan Jang"
                    },
                    {
                        "name": "Seokhyeong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Seokhyeong Kang"
                },
                "author": "Seokhyeong Kang",
                "arxiv_comment": "Accepted for publication at the 2026 Asia and South Pacific Design\n  Automation Conference (ASP-DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19092v2",
                "updated": "2025-10-24T12:45:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    45,
                    47,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-25T11:03:45Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    11,
                    3,
                    45,
                    6,
                    145,
                    0
                ],
                "title": "Reinforced Latent Reasoning for LLM-based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Latent Reasoning for LLM-based Recommendation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities in complex problem-solving tasks, sparking growing interest in\ntheir application to preference reasoning in recommendation systems. Existing\nmethods typically rely on fine-tuning with explicit chain-of-thought (CoT)\ndata. However, these methods face significant practical limitations due to (1)\nthe difficulty of obtaining high-quality CoT data in recommendation and (2) the\nhigh inference latency caused by generating CoT reasoning. In this work, we\nexplore an alternative approach that shifts from explicit CoT reasoning to\ncompact, information-dense latent reasoning. This approach eliminates the need\nfor explicit CoT generation and improves inference efficiency, as few latent\ntokens can effectively capture the entire reasoning process. Building on this\nidea, we propose \\textit{\\underline{R}einforced \\underline{Latent}\n\\underline{R}easoning for \\underline{R}ecommendation} (LatentR$^3$), a novel\nend-to-end training framework that leverages reinforcement learning (RL) to\noptimize latent reasoning without relying on any CoT data. LatentR$^3$ adopts a\ntwo-stage training strategy: first, supervised fine-tuning to initialize the\nlatent reasoning module, followed by pure RL training to encourage exploration\nthrough a rule-based reward design. Our RL implementation is based on a\nmodified GRPO algorithm, which reduces computational overhead during training\nand introduces continuous reward signals for more efficient learning. Extensive\nexperiments demonstrate that LatentR$^3$ enables effective latent reasoning\nwithout any direct supervision of the reasoning process, significantly\nimproving performance when integrated with different LLM-based recommendation\nmethods. Our codes are available at https://github.com/xuwenxinedu/R3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities in complex problem-solving tasks, sparking growing interest in\ntheir application to preference reasoning in recommendation systems. Existing\nmethods typically rely on fine-tuning with explicit chain-of-thought (CoT)\ndata. However, these methods face significant practical limitations due to (1)\nthe difficulty of obtaining high-quality CoT data in recommendation and (2) the\nhigh inference latency caused by generating CoT reasoning. In this work, we\nexplore an alternative approach that shifts from explicit CoT reasoning to\ncompact, information-dense latent reasoning. This approach eliminates the need\nfor explicit CoT generation and improves inference efficiency, as few latent\ntokens can effectively capture the entire reasoning process. Building on this\nidea, we propose \\textit{\\underline{R}einforced \\underline{Latent}\n\\underline{R}easoning for \\underline{R}ecommendation} (LatentR$^3$), a novel\nend-to-end training framework that leverages reinforcement learning (RL) to\noptimize latent reasoning without relying on any CoT data. LatentR$^3$ adopts a\ntwo-stage training strategy: first, supervised fine-tuning to initialize the\nlatent reasoning module, followed by pure RL training to encourage exploration\nthrough a rule-based reward design. Our RL implementation is based on a\nmodified GRPO algorithm, which reduces computational overhead during training\nand introduces continuous reward signals for more efficient learning. Extensive\nexperiments demonstrate that LatentR$^3$ enables effective latent reasoning\nwithout any direct supervision of the reasoning process, significantly\nimproving performance when integrated with different LLM-based recommendation\nmethods. Our codes are available at https://github.com/xuwenxinedu/R3."
                },
                "authors": [
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Wenxin Xu"
                    },
                    {
                        "name": "Xiaoyan Zhao"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13732v2",
                "updated": "2025-10-24T12:44:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    44,
                    14,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-15T16:36:06Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    36,
                    6,
                    2,
                    288,
                    0
                ],
                "title": "Pilot Assignment for Distributed Massive MIMO Based on Channel\n  Estimation Error Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pilot Assignment for Distributed Massive MIMO Based on Channel\n  Estimation Error Minimization"
                },
                "summary": "Pilot contamination remains a major bottleneck in realizing the full\npotential of distributed massive MIMO systems. We propose two dynamic and\nscalable pilot assignment schemes designed for practical deployment in such\nnetworks. First, we present a low-complexity centralized scheme that\nsequentially assigns pilots to user equipments (UEs) to minimize the global\nchannel estimation errors across serving access points (APs). This improves the\nchannel estimation quality and reduces interference among UEs, enhancing the\nspectral efficiency. Second, we develop a fully distributed scheme that uses a\npriority-based pilot selection approach. In this scheme, each selected AP\nminimizes the channel estimation error using only local information and offers\ncandidate pilots to the UEs. Every UE then selects a suitable pilot based on\nits AP priority. This approach ensures consistency and minimizes interference\nwhile significantly reducing pilot contamination. The method requires no global\ncoordination, maintains low signaling overhead, and adapts dynamically to the\nUE deployment. Numerical simulations demonstrate the superiority of the\nproposed schemes in terms of network throughput when compared to the existing\nstate-of-the-art schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pilot contamination remains a major bottleneck in realizing the full\npotential of distributed massive MIMO systems. We propose two dynamic and\nscalable pilot assignment schemes designed for practical deployment in such\nnetworks. First, we present a low-complexity centralized scheme that\nsequentially assigns pilots to user equipments (UEs) to minimize the global\nchannel estimation errors across serving access points (APs). This improves the\nchannel estimation quality and reduces interference among UEs, enhancing the\nspectral efficiency. Second, we develop a fully distributed scheme that uses a\npriority-based pilot selection approach. In this scheme, each selected AP\nminimizes the channel estimation error using only local information and offers\ncandidate pilots to the UEs. Every UE then selects a suitable pilot based on\nits AP priority. This approach ensures consistency and minimizes interference\nwhile significantly reducing pilot contamination. The method requires no global\ncoordination, maintains low signaling overhead, and adapts dynamically to the\nUE deployment. Numerical simulations demonstrate the superiority of the\nproposed schemes in terms of network throughput when compared to the existing\nstate-of-the-art schemes."
                },
                "authors": [
                    {
                        "name": "Mohd Saif Ali Khan"
                    },
                    {
                        "name": "Karthik RM"
                    },
                    {
                        "name": "Samar Agnihotri"
                    }
                ],
                "author_detail": {
                    "name": "Samar Agnihotri"
                },
                "author": "Samar Agnihotri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21401v1",
                "updated": "2025-10-24T12:44:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    44,
                    8,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:44:08Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    44,
                    8,
                    4,
                    297,
                    0
                ],
                "title": "FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract\n  Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract\n  Security"
                },
                "summary": "Smart contract vulnerabilities cost billions of dollars annually, yet\nexisting automated analysis tools fail to generate deployable defenses. We\npresent FLAMES, a novel automated approach that synthesizes executable runtime\nguards as Solidity \"require\" statements to harden smart contracts against\nexploits. Unlike prior work that relies on vulnerability labels, symbolic\nanalysis, or natural language specifications, FLAMES employs domain-adapted\nlarge language models trained through fill-in-the-middle supervised fine-tuning\non real-world invariants extracted from 514,506 verified contracts. Our\nextensive evaluation across three dimensions demonstrates FLAMES's\neffectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for\nsynthesized invariant (2) Semantic Quality: on a curated test set of 5,000\nchallenging invariants, FLAMES produces exact or semantically equivalent\nmatches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES\nprevents 22 out of 108 real exploits (20.4%) while preserving contract\nfunctionality, and (4) FLAMES successfully blocks the real-world APEMAGA\nincident by synthesizing a pre-condition that mitigates the attack. FLAMES\nestablishes that domain-adapted LLMs can automatically generate\nproduction-ready security defenses for smart contracts without requiring\nvulnerability detection, formal specifications, or human intervention. We\nrelease our code, model weights, datasets, and evaluation infrastructure to\nenable reproducible research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract vulnerabilities cost billions of dollars annually, yet\nexisting automated analysis tools fail to generate deployable defenses. We\npresent FLAMES, a novel automated approach that synthesizes executable runtime\nguards as Solidity \"require\" statements to harden smart contracts against\nexploits. Unlike prior work that relies on vulnerability labels, symbolic\nanalysis, or natural language specifications, FLAMES employs domain-adapted\nlarge language models trained through fill-in-the-middle supervised fine-tuning\non real-world invariants extracted from 514,506 verified contracts. Our\nextensive evaluation across three dimensions demonstrates FLAMES's\neffectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for\nsynthesized invariant (2) Semantic Quality: on a curated test set of 5,000\nchallenging invariants, FLAMES produces exact or semantically equivalent\nmatches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES\nprevents 22 out of 108 real exploits (20.4%) while preserving contract\nfunctionality, and (4) FLAMES successfully blocks the real-world APEMAGA\nincident by synthesizing a pre-condition that mitigates the attack. FLAMES\nestablishes that domain-adapted LLMs can automatically generate\nproduction-ready security defenses for smart contracts without requiring\nvulnerability detection, formal specifications, or human intervention. We\nrelease our code, model weights, datasets, and evaluation infrastructure to\nenable reproducible research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Mojtaba Eshghie"
                    },
                    {
                        "name": "Gabriele Morello"
                    },
                    {
                        "name": "Matteo Lauretano"
                    },
                    {
                        "name": "Alexandre Bartel"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21398v1",
                "updated": "2025-10-24T12:39:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    39,
                    15,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:39:15Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    39,
                    15,
                    4,
                    297,
                    0
                ],
                "title": "Boosting Accuracy and Efficiency of Budget Forcing in LLMs via\n  Reinforcement Learning for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Accuracy and Efficiency of Budget Forcing in LLMs via\n  Reinforcement Learning for Mathematical Reasoning"
                },
                "summary": "Test-time scaling methods have seen a rapid increase in popularity for its\ncomputational efficiency and parameter-independent training to improve\nreasoning performance on Large Language Models. One such method is called\nbudget forcing, a decoding intervention strategy which allocates extra compute\nbudget for thinking and elicits the inherent self-correcting behavior of the\nmodel. However, this relies on supervised fine-tuning (SFT) on long-context\nreasoning traces which causes performance degradation on smaller models due to\nverbose responses. For this reason, we offer a framework integrating\nreinforcement learning (RL) to improve token efficiency and boost the\nperformance of a 1.5B model for mathematical reasoning. We demonstrate this\nusing only 1.5K training samples and found that our SFT+RL model performed\nbetter on the GSM8K dataset with varying compute budgets. Our main findings\nshowed an overall higher accuracy while significantly reducing its token usage\nby over 40% compared to the SFT model, revealing how RL can recover the losses\ndue to long-context training and altogether improving performance in\nmathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling methods have seen a rapid increase in popularity for its\ncomputational efficiency and parameter-independent training to improve\nreasoning performance on Large Language Models. One such method is called\nbudget forcing, a decoding intervention strategy which allocates extra compute\nbudget for thinking and elicits the inherent self-correcting behavior of the\nmodel. However, this relies on supervised fine-tuning (SFT) on long-context\nreasoning traces which causes performance degradation on smaller models due to\nverbose responses. For this reason, we offer a framework integrating\nreinforcement learning (RL) to improve token efficiency and boost the\nperformance of a 1.5B model for mathematical reasoning. We demonstrate this\nusing only 1.5K training samples and found that our SFT+RL model performed\nbetter on the GSM8K dataset with varying compute budgets. Our main findings\nshowed an overall higher accuracy while significantly reducing its token usage\nby over 40% compared to the SFT model, revealing how RL can recover the losses\ndue to long-context training and altogether improving performance in\nmathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Ravindra Aribowo Tarunokusumo"
                    },
                    {
                        "name": "Rafael Fernandes Cunha"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Fernandes Cunha"
                },
                "author": "Rafael Fernandes Cunha",
                "arxiv_comment": "Submitted to the European Conference on Artificial Intelligence\n  (ECAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06958v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06958v3",
                "updated": "2025-10-24T12:32:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    32,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-08T00:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    0,
                    59,
                    2,
                    6,
                    159,
                    0
                ],
                "title": "Simulating Society Requires Simulating Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Society Requires Simulating Thought"
                },
                "summary": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior; it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior, primarily through prompting and\nsupervised fine-tuning. Yet current simulations remain grounded in a\nbehaviorist \"demographics in, behavior out\" paradigm, focusing on surface-level\nplausibility. As a result, they often lack internal coherence, causal\nreasoning, and belief traceability, making them unreliable for modeling how\npeople reason, deliberate, and respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought, not just\nlanguage, for social simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior; it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior, primarily through prompting and\nsupervised fine-tuning. Yet current simulations remain grounded in a\nbehaviorist \"demographics in, behavior out\" paradigm, focusing on surface-level\nplausibility. As a result, they often lack internal coherence, causal\nreasoning, and belief traceability, making them unreliable for modeling how\npeople reason, deliberate, and respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought, not just\nlanguage, for social simulations."
                },
                "authors": [
                    {
                        "name": "Chance Jiajie Li"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Zhenze Mo"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Yuhan Tang"
                    },
                    {
                        "name": "Kaiya Ivy Zhao"
                    },
                    {
                        "name": "Yulu Gan"
                    },
                    {
                        "name": "Jie Fan"
                    },
                    {
                        "name": "Jiangbo Yu"
                    },
                    {
                        "name": "Jinhua Zhao"
                    },
                    {
                        "name": "Paul Liang"
                    },
                    {
                        "name": "Luis Alonso"
                    },
                    {
                        "name": "Kent Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kent Larson"
                },
                "author": "Kent Larson",
                "arxiv_comment": "NeurIPS 2025 (Position Paper Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06958v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06958v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18631v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18631v4",
                "updated": "2025-10-24T12:32:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    32,
                    0,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-23T13:36:24Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    36,
                    24,
                    0,
                    174,
                    0
                ],
                "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDit: Reward Dithering for Improved LLM Policy Optimization"
                },
                "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Jiarui Yu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "arxiv_comment": "34 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18631v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18631v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15795v2",
                "updated": "2025-10-24T12:30:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    30,
                    19,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-21T17:48:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    48,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "Reverse Engineering Human Preferences with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Engineering Human Preferences with Reinforcement Learning"
                },
                "summary": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks."
                },
                "authors": [
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Tan Yi-Chern"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "arxiv_comment": "NeurIPS 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21388v1",
                "updated": "2025-10-24T12:19:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    19,
                    19,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T12:19:19Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    19,
                    19,
                    4,
                    297,
                    0
                ],
                "title": "Compressing Quaternion Convolutional Neural Networks for Audio\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Quaternion Convolutional Neural Networks for Audio\n  Classification"
                },
                "summary": "Conventional Convolutional Neural Networks (CNNs) in the real domain have\nbeen widely used for audio classification. However, their convolution\noperations process multi-channel inputs independently, limiting the ability to\ncapture correlations among channels. This can lead to suboptimal feature\nlearning, particularly for complex audio patterns such as multi-channel\nspectrogram representations. Quaternion Convolutional Neural Networks (QCNNs)\naddress this limitation by employing quaternion algebra to jointly capture\ninter-channel dependencies, enabling more compact models with fewer learnable\nparameters while better exploiting the multi-dimensional nature of audio\nsignals. However, QCNNs exhibit higher computational complexity due to the\noverhead of quaternion operations, resulting in increased inference latency and\nreduced efficiency compared to conventional CNNs, posing challenges for\ndeployment on resource-constrained platforms. To address this challenge, this\nstudy explores knowledge distillation (KD) and pruning, to reduce the\ncomputational complexity of QCNNs while maintaining performance. Our\nexperiments on audio classification reveal that pruning QCNNs achieves similar\nor superior performance compared to KD while requiring less computational\neffort. Compared to conventional CNNs and Transformer-based architectures,\npruned QCNNs achieve competitive performance with a reduced learnable parameter\ncount and computational complexity. On the AudioSet dataset, pruned QCNNs\nreduce computational cost by 50\\% and parameter count by 80\\%, while\nmaintaining performance comparable to the conventional CNNs. Furthermore,\npruned QCNNs generalize well across multiple audio classification benchmarks,\nincluding GTZAN for music genre recognition, ESC-50 for environmental sound\nclassification and RAVDESS for speech emotion recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional Convolutional Neural Networks (CNNs) in the real domain have\nbeen widely used for audio classification. However, their convolution\noperations process multi-channel inputs independently, limiting the ability to\ncapture correlations among channels. This can lead to suboptimal feature\nlearning, particularly for complex audio patterns such as multi-channel\nspectrogram representations. Quaternion Convolutional Neural Networks (QCNNs)\naddress this limitation by employing quaternion algebra to jointly capture\ninter-channel dependencies, enabling more compact models with fewer learnable\nparameters while better exploiting the multi-dimensional nature of audio\nsignals. However, QCNNs exhibit higher computational complexity due to the\noverhead of quaternion operations, resulting in increased inference latency and\nreduced efficiency compared to conventional CNNs, posing challenges for\ndeployment on resource-constrained platforms. To address this challenge, this\nstudy explores knowledge distillation (KD) and pruning, to reduce the\ncomputational complexity of QCNNs while maintaining performance. Our\nexperiments on audio classification reveal that pruning QCNNs achieves similar\nor superior performance compared to KD while requiring less computational\neffort. Compared to conventional CNNs and Transformer-based architectures,\npruned QCNNs achieve competitive performance with a reduced learnable parameter\ncount and computational complexity. On the AudioSet dataset, pruned QCNNs\nreduce computational cost by 50\\% and parameter count by 80\\%, while\nmaintaining performance comparable to the conventional CNNs. Furthermore,\npruned QCNNs generalize well across multiple audio classification benchmarks,\nincluding GTZAN for music genre recognition, ESC-50 for environmental sound\nclassification and RAVDESS for speech emotion recognition."
                },
                "authors": [
                    {
                        "name": "Arshdeep Singh"
                    },
                    {
                        "name": "Vinayak Abrol"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    }
                ],
                "author_detail": {
                    "name": "Mark D. Plumbley"
                },
                "author": "Mark D. Plumbley",
                "arxiv_comment": "Under review in IEEE TASLPRO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12171v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12171v3",
                "updated": "2025-10-24T12:16:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    16,
                    12,
                    4,
                    297,
                    0
                ],
                "published": "2025-02-13T10:33:58Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    10,
                    33,
                    58,
                    3,
                    44,
                    0
                ],
                "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoRA: Gradient-driven Adaptive Low Rank Adaptation"
                },
                "summary": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework--GoRA (Gradient-driven Adaptive Low Rank Adaptation)--that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches--which often focus on either rank\nselection or initialization in isolation--but also unifies both aspects within\na single framework, enabling more effective and efficient adaptation. Extensive\nexperiments across various architectures and modalities show that GoRA\nconsistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings. Code is available at: https://github.com/hhnqqq/MyTransformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework--GoRA (Gradient-driven Adaptive Low Rank Adaptation)--that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches--which often focus on either rank\nselection or initialization in isolation--but also unifies both aspects within\na single framework, enabling more effective and efficient adaptation. Extensive\nexperiments across various architectures and modalities show that GoRA\nconsistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings. Code is available at: https://github.com/hhnqqq/MyTransformers."
                },
                "authors": [
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Yuchen Ren"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Luyang Zhou"
                    },
                    {
                        "name": "Shucun Ju"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12171v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12171v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18988v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18988v3",
                "updated": "2025-10-24T12:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    12,
                    5,
                    14,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-21T18:10:45Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    18,
                    10,
                    45,
                    1,
                    294,
                    0
                ],
                "title": "Timely Clinical Diagnosis through Active Test Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Clinical Diagnosis through Active Test Selection"
                },
                "summary": "There is growing interest in using machine learning (ML) to support clinical\ndiagnosis, but most approaches rely on static, fully observed datasets and fail\nto reflect the sequential, resource-aware reasoning clinicians use in practice.\nDiagnosis remains complex and error prone, especially in high-pressure or\nresource-limited settings, underscoring the need for frameworks that help\nclinicians make timely and cost-effective decisions. We propose ACTMED\n(Adaptive Clinical Test selection via Model-based Experimental Design), a\ndiagnostic framework that integrates Bayesian Experimental Design (BED) with\nlarge language models (LLMs) to better emulate real-world diagnostic reasoning.\nAt each step, ACTMED selects the test expected to yield the greatest reduction\nin diagnostic uncertainty for a given patient. LLMs act as flexible simulators,\ngenerating plausible patient state distributions and supporting belief updates\nwithout requiring structured, task-specific training data. Clinicians can\nremain in the loop; reviewing test suggestions, interpreting intermediate\noutputs, and applying clinical judgment throughout. We evaluate ACTMED on\nreal-world datasets and show it can optimize test selection to improve\ndiagnostic accuracy, interpretability, and resource use. This represents a step\ntoward transparent, adaptive, and clinician-aligned diagnostic systems that\ngeneralize across settings with reduced reliance on domain-specific data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest in using machine learning (ML) to support clinical\ndiagnosis, but most approaches rely on static, fully observed datasets and fail\nto reflect the sequential, resource-aware reasoning clinicians use in practice.\nDiagnosis remains complex and error prone, especially in high-pressure or\nresource-limited settings, underscoring the need for frameworks that help\nclinicians make timely and cost-effective decisions. We propose ACTMED\n(Adaptive Clinical Test selection via Model-based Experimental Design), a\ndiagnostic framework that integrates Bayesian Experimental Design (BED) with\nlarge language models (LLMs) to better emulate real-world diagnostic reasoning.\nAt each step, ACTMED selects the test expected to yield the greatest reduction\nin diagnostic uncertainty for a given patient. LLMs act as flexible simulators,\ngenerating plausible patient state distributions and supporting belief updates\nwithout requiring structured, task-specific training data. Clinicians can\nremain in the loop; reviewing test suggestions, interpreting intermediate\noutputs, and applying clinical judgment throughout. We evaluate ACTMED on\nreal-world datasets and show it can optimize test selection to improve\ndiagnostic accuracy, interpretability, and resource use. This represents a step\ntoward transparent, adaptive, and clinician-aligned diagnostic systems that\ngeneralize across settings with reduced reliance on domain-specific data."
                },
                "authors": [
                    {
                        "name": "Silas Ruhrberg Estvez"
                    },
                    {
                        "name": "Nicols Astorga"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "None",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18988v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18988v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12854v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12854v3",
                "updated": "2025-10-24T11:56:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    56,
                    39,
                    4,
                    297,
                    0
                ],
                "published": "2024-10-10T22:22:05Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    22,
                    22,
                    5,
                    3,
                    284,
                    0
                ],
                "title": "TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees"
                },
                "summary": "In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets. Our code is publicly available at\nhttps://github.com/MrBlankness/TPO.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets. Our code is publicly available at\nhttps://github.com/MrBlankness/TPO.git."
                },
                "authors": [
                    {
                        "name": "Weibin Liao"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12854v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12854v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19949v2",
                "updated": "2025-10-24T11:52:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    52,
                    29,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T18:21:52Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    18,
                    21,
                    52,
                    2,
                    295,
                    0
                ],
                "title": "Surfer 2: The Next Generation of Cross-Platform Computer Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfer 2: The Next Generation of Cross-Platform Computer Use Agents"
                },
                "summary": "Building agents that generalize across web, desktop, and mobile environments\nremains an open challenge, as prior systems rely on environment-specific\ninterfaces that limit cross-platform deployment. We introduce Surfer 2, a\nunified architecture operating purely from visual observations that achieves\nstate-of-the-art performance across all three environments. Surfer 2 integrates\nhierarchical context management, decoupled planning and execution, and\nself-verification with adaptive recovery, enabling reliable operation over long\ntask horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on\nWebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior\nsystems without task-specific fine-tuning. With multiple attempts, Surfer 2\nexceeds human performance on all benchmarks. These results demonstrate that\nsystematic orchestration amplifies foundation model capabilities and enables\ngeneral-purpose computer control through visual interaction alone, while\ncalling for a next-generation vision language model to achieve Pareto-optimal\ncost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building agents that generalize across web, desktop, and mobile environments\nremains an open challenge, as prior systems rely on environment-specific\ninterfaces that limit cross-platform deployment. We introduce Surfer 2, a\nunified architecture operating purely from visual observations that achieves\nstate-of-the-art performance across all three environments. Surfer 2 integrates\nhierarchical context management, decoupled planning and execution, and\nself-verification with adaptive recovery, enabling reliable operation over long\ntask horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on\nWebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior\nsystems without task-specific fine-tuning. With multiple attempts, Surfer 2\nexceeds human performance on all benchmarks. These results demonstrate that\nsystematic orchestration amplifies foundation model capabilities and enables\ngeneral-purpose computer control through visual interaction alone, while\ncalling for a next-generation vision language model to achieve Pareto-optimal\ncost-efficiency."
                },
                "authors": [
                    {
                        "name": "Mathieu Andreux"
                    },
                    {
                        "name": "Mrt Bakler"
                    },
                    {
                        "name": "Yanael Barbier"
                    },
                    {
                        "name": "Hamza Benchekroun"
                    },
                    {
                        "name": "Emilien Bir"
                    },
                    {
                        "name": "Antoine Bonnet"
                    },
                    {
                        "name": "Riaz Bordie"
                    },
                    {
                        "name": "Nathan Bout"
                    },
                    {
                        "name": "Matthias Brunel"
                    },
                    {
                        "name": "Aleix Cambray"
                    },
                    {
                        "name": "Pierre-Louis Cedoz"
                    },
                    {
                        "name": "Antoine Chassang"
                    },
                    {
                        "name": "Gautier Cloix"
                    },
                    {
                        "name": "Ethan Connelly"
                    },
                    {
                        "name": "Alexandra Constantinou"
                    },
                    {
                        "name": "Ramzi De Coster"
                    },
                    {
                        "name": "Hubert de la Jonquiere"
                    },
                    {
                        "name": "Aurlien Delfosse"
                    },
                    {
                        "name": "Maxime Delpit"
                    },
                    {
                        "name": "Alexis Deprez"
                    },
                    {
                        "name": "Augustin Derupti"
                    },
                    {
                        "name": "Mathieu Diaz"
                    },
                    {
                        "name": "Shannon D'Souza"
                    },
                    {
                        "name": "Julie Dujardin"
                    },
                    {
                        "name": "Abai Edmund"
                    },
                    {
                        "name": "Michael Eickenberg"
                    },
                    {
                        "name": "Armand Fatalot"
                    },
                    {
                        "name": "Wissem Felissi"
                    },
                    {
                        "name": "Isaac Herring"
                    },
                    {
                        "name": "Xavier Koegler"
                    },
                    {
                        "name": "Erwan Le Jumeau de Kergaradec"
                    },
                    {
                        "name": "Aurlien Lac"
                    },
                    {
                        "name": "Maxime Langevin"
                    },
                    {
                        "name": "Corentin Lauverjat"
                    },
                    {
                        "name": "Antonio Loison"
                    },
                    {
                        "name": "Avshalom Manevich"
                    },
                    {
                        "name": "Axel Moyal"
                    },
                    {
                        "name": "Axel Nguyen Kerbel"
                    },
                    {
                        "name": "Marinela Parovic"
                    },
                    {
                        "name": "Julien Revelle"
                    },
                    {
                        "name": "Guillaume Richard"
                    },
                    {
                        "name": "Mats Richter"
                    },
                    {
                        "name": "Ronan Riochet"
                    },
                    {
                        "name": "Mara Santos"
                    },
                    {
                        "name": "Romain Savidan"
                    },
                    {
                        "name": "Laurent Sifre"
                    },
                    {
                        "name": "Maxime Theillard"
                    },
                    {
                        "name": "Marc Thibault"
                    },
                    {
                        "name": "Ivan Valentini"
                    },
                    {
                        "name": "Tony Wu"
                    },
                    {
                        "name": "Laura Yie"
                    },
                    {
                        "name": "Kai Yuan"
                    },
                    {
                        "name": "Jevgenij Zubovskij"
                    }
                ],
                "author_detail": {
                    "name": "Jevgenij Zubovskij"
                },
                "author": "Jevgenij Zubovskij",
                "arxiv_comment": "21 pages, 9 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08730v2",
                "updated": "2025-10-24T11:50:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    50,
                    54,
                    4,
                    297,
                    0
                ],
                "published": "2025-08-12T08:21:58Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    21,
                    58,
                    1,
                    224,
                    0
                ],
                "title": "Magical: Medical Lay Language Generation via Semantic Invariance and\n  Layperson-tailored Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magical: Medical Lay Language Generation via Semantic Invariance and\n  Layperson-tailored Adaptation"
                },
                "summary": "Medical Lay Language Generation (MLLG) plays a vital role in improving the\naccessibility of complex scientific content for broader audiences. Recent\nliterature to MLLG commonly employ parameter-efficient fine-tuning methods such\nas Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using\npaired expert-lay language datasets. However, LoRA struggles with the\nchallenges posed by multi-source heterogeneous MLLG datasets. Specifically,\nthrough a series of exploratory experiments, we reveal that standard LoRA fail\nto meet the requirement for semantic fidelity and diverse lay-style generation\nin MLLG task. To address these limitations, we propose Magical, an asymmetric\nLoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical\nemploys a shared matrix $A$ for abstractive summarization, along with multiple\nisolated matrices $B$ for diverse lay-style generation. To preserve semantic\nfidelity during the lay language generation process, Magical introduces a\nSemantic Invariance Constraint to mitigate semantic subspace shifts on matrix\n$A$. Furthermore, to better adapt to diverse lay-style generation, Magical\nincorporates the Recommendation-guided Switch, an externally interface to\nprompt the LLM to switch between different matrices $B$. Experimental results\non three real-world lay language generation datasets demonstrate that Magical\nconsistently outperforms prompt-based methods, vanilla LoRA, and its recent\nvariants, while also reducing trainable parameters by 31.66%. Our code is\npublicly available at https://github.com/tianlwang/Magical.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Lay Language Generation (MLLG) plays a vital role in improving the\naccessibility of complex scientific content for broader audiences. Recent\nliterature to MLLG commonly employ parameter-efficient fine-tuning methods such\nas Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using\npaired expert-lay language datasets. However, LoRA struggles with the\nchallenges posed by multi-source heterogeneous MLLG datasets. Specifically,\nthrough a series of exploratory experiments, we reveal that standard LoRA fail\nto meet the requirement for semantic fidelity and diverse lay-style generation\nin MLLG task. To address these limitations, we propose Magical, an asymmetric\nLoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical\nemploys a shared matrix $A$ for abstractive summarization, along with multiple\nisolated matrices $B$ for diverse lay-style generation. To preserve semantic\nfidelity during the lay language generation process, Magical introduces a\nSemantic Invariance Constraint to mitigate semantic subspace shifts on matrix\n$A$. Furthermore, to better adapt to diverse lay-style generation, Magical\nincorporates the Recommendation-guided Switch, an externally interface to\nprompt the LLM to switch between different matrices $B$. Experimental results\non three real-world lay language generation datasets demonstrate that Magical\nconsistently outperforms prompt-based methods, vanilla LoRA, and its recent\nvariants, while also reducing trainable parameters by 31.66%. Our code is\npublicly available at https://github.com/tianlwang/Magical.git."
                },
                "authors": [
                    {
                        "name": "Weibin Liao"
                    },
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Liantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Liantao Ma"
                },
                "author": "Liantao Ma",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11128v2",
                "updated": "2025-10-24T11:47:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    47,
                    35,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-10T17:04:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    4,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "Theory-Grounded Evaluation of Human-Like Fallacy Patterns in LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-Grounded Evaluation of Human-Like Fallacy Patterns in LLM\n  Reasoning"
                },
                "summary": "We study logical reasoning in language models by asking whether their errors\nfollow established human fallacy patterns. Using the Erotetic Theory of\nReasoning (ETR) and its open-source implementation, PyETR, we programmatically\ngenerate 383 formally specified reasoning problems and evaluate 38 models. For\neach response, we judge logical correctness and, when incorrect, whether it\nmatches an ETR-predicted fallacy. Two results stand out: (i) as a capability\nproxy (Chatbot Arena Elo) increases, a larger share of a model's incorrect\nanswers are ETR-predicted fallacies $(\\rho=0.360, p=0.0265)$, while overall\ncorrectness on this dataset shows no correlation with capability; (ii)\nreversing premise order significantly reduces fallacy production for many\nmodels, mirroring human order effects. Methodologically, PyETR provides an\nopen-source pipeline for unbounded, synthetic, contamination-resistant\nreasoning tests linked to a cognitive theory, enabling analyses that focus on\nerror composition rather than error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study logical reasoning in language models by asking whether their errors\nfollow established human fallacy patterns. Using the Erotetic Theory of\nReasoning (ETR) and its open-source implementation, PyETR, we programmatically\ngenerate 383 formally specified reasoning problems and evaluate 38 models. For\neach response, we judge logical correctness and, when incorrect, whether it\nmatches an ETR-predicted fallacy. Two results stand out: (i) as a capability\nproxy (Chatbot Arena Elo) increases, a larger share of a model's incorrect\nanswers are ETR-predicted fallacies $(\\rho=0.360, p=0.0265)$, while overall\ncorrectness on this dataset shows no correlation with capability; (ii)\nreversing premise order significantly reduces fallacy production for many\nmodels, mirroring human order effects. Methodologically, PyETR provides an\nopen-source pipeline for unbounded, synthetic, contamination-resistant\nreasoning tests linked to a cognitive theory, enabling analyses that focus on\nerror composition rather than error rate."
                },
                "authors": [
                    {
                        "name": "Andrew Keenan Richardson"
                    },
                    {
                        "name": "Ryan Othniel Kearns"
                    },
                    {
                        "name": "Sean Moss"
                    },
                    {
                        "name": "Vincent Wang-Mascianica"
                    },
                    {
                        "name": "Philipp Koralus"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Koralus"
                },
                "author": "Philipp Koralus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02422v2",
                "updated": "2025-10-24T11:43:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    43,
                    40,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-02T16:40:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    40,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "Dynamic Target Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Target Attack"
                },
                "summary": "Existing gradient-based jailbreak attacks typically optimize an adversarial\nsuffix to induce a fixed affirmative response. However, this fixed target\nusually resides in an extremely low-density region of a safety-aligned LLM's\noutput distribution conditioned on diverse harmful inputs. Due to the\nsubstantial discrepancy between the target and the original output, existing\nattacks require numerous iterations to optimize the adversarial prompt, which\nmight still fail to induce the low-probability target response from the target\nLLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking\nframework relying on the target LLM's own responses as targets to optimize the\nadversarial prompts. In each optimization round, DTA iteratively samples\nmultiple candidate responses directly from the output distribution conditioned\non the current prompt, and selects the most harmful response as a temporary\ntarget for prompt optimization. In contrast to existing attacks, DTA\nsignificantly reduces the discrepancy between the target and the output\ndistribution, substantially easing the optimization process to search for an\neffective adversarial prompt.\n  Extensive experiments demonstrate the superior effectiveness and efficiency\nof DTA: under the white-box setting, DTA only needs 200 optimization iterations\nto achieve an average attack success rate (ASR) of over 87\\% on recent\nsafety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\\%. The\ntime cost of DTA is 2-26 times less than existing baselines. Under the\nblack-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target\nsampling and achieves an ASR of 85\\% against the black-box target model\nLlama-3-70B-Instruct, exceeding its counterparts by over 25\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing gradient-based jailbreak attacks typically optimize an adversarial\nsuffix to induce a fixed affirmative response. However, this fixed target\nusually resides in an extremely low-density region of a safety-aligned LLM's\noutput distribution conditioned on diverse harmful inputs. Due to the\nsubstantial discrepancy between the target and the original output, existing\nattacks require numerous iterations to optimize the adversarial prompt, which\nmight still fail to induce the low-probability target response from the target\nLLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking\nframework relying on the target LLM's own responses as targets to optimize the\nadversarial prompts. In each optimization round, DTA iteratively samples\nmultiple candidate responses directly from the output distribution conditioned\non the current prompt, and selects the most harmful response as a temporary\ntarget for prompt optimization. In contrast to existing attacks, DTA\nsignificantly reduces the discrepancy between the target and the output\ndistribution, substantially easing the optimization process to search for an\neffective adversarial prompt.\n  Extensive experiments demonstrate the superior effectiveness and efficiency\nof DTA: under the white-box setting, DTA only needs 200 optimization iterations\nto achieve an average attack success rate (ASR) of over 87\\% on recent\nsafety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\\%. The\ntime cost of DTA is 2-26 times less than existing baselines. Under the\nblack-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target\nsampling and achieves an ASR of 85\\% against the black-box target model\nLlama-3-70B-Instruct, exceeding its counterparts by over 25\\%."
                },
                "authors": [
                    {
                        "name": "Kedong Xiu"
                    },
                    {
                        "name": "Churui Zeng"
                    },
                    {
                        "name": "Tianhang Zheng"
                    },
                    {
                        "name": "Xinzhe Huang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Puning Zhao"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06041v2",
                "updated": "2025-10-24T11:37:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    37,
                    52,
                    4,
                    297,
                    0
                ],
                "published": "2025-08-08T05:57:04Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    5,
                    57,
                    4,
                    4,
                    220,
                    0
                ],
                "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision\n  Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision\n  Assignment"
                },
                "summary": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding steps. Building on this insight,\nwe introduce DP-LLM, a novel mechanism that dynamically assigns precision to\neach layer based on input values. Experimental results across multiple models\nand benchmarks demonstrate that DP-LLM achieves a superior performance-latency\ntrade-off, outperforming prior approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding steps. Building on this insight,\nwe introduce DP-LLM, a novel mechanism that dynamically assigns precision to\neach layer based on input values. Experimental results across multiple models\nand benchmarks demonstrate that DP-LLM achieves a superior performance-latency\ntrade-off, outperforming prior approaches."
                },
                "authors": [
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Seong Hoon Seo"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Yeonhong Park"
                    }
                ],
                "author_detail": {
                    "name": "Yeonhong Park"
                },
                "author": "Yeonhong Park",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21357v1",
                "updated": "2025-10-24T11:36:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    36,
                    7,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:36:07Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    36,
                    7,
                    4,
                    297,
                    0
                ],
                "title": "Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search\n  and Rescue Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search\n  and Rescue Operations"
                },
                "summary": "In recent years, consumer-grade UAVs have been widely adopted by first\nresponders. In general, they are operated manually, which requires trained\npilots, especially in unknown GNSS-denied environments and in the vicinity of\nstructures. Autonomous flight can facilitate the application of UAVs and reduce\noperator strain. However, autonomous systems usually require special\nprogramming interfaces, custom sensor setups, and strong onboard computers,\nwhich limits a broader deployment.\n  We present a system for autonomous flight using lightweight consumer-grade\nDJI drones. They are controlled by an Android app for state estimation and\nobstacle avoidance directly running on the UAV's remote control. Our ground\ncontrol station enables a single operator to configure and supervise multiple\nheterogeneous UAVs at once. Furthermore, it combines the observations of all\nUAVs into a joint 3D environment model for improved situational awareness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, consumer-grade UAVs have been widely adopted by first\nresponders. In general, they are operated manually, which requires trained\npilots, especially in unknown GNSS-denied environments and in the vicinity of\nstructures. Autonomous flight can facilitate the application of UAVs and reduce\noperator strain. However, autonomous systems usually require special\nprogramming interfaces, custom sensor setups, and strong onboard computers,\nwhich limits a broader deployment.\n  We present a system for autonomous flight using lightweight consumer-grade\nDJI drones. They are controlled by an Android app for state estimation and\nobstacle avoidance directly running on the UAV's remote control. Our ground\ncontrol station enables a single operator to configure and supervise multiple\nheterogeneous UAVs at once. Furthermore, it combines the observations of all\nUAVs into a joint 3D environment model for improved situational awareness."
                },
                "authors": [
                    {
                        "name": "Daniel Schleich"
                    },
                    {
                        "name": "Jan Quenzel"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "Accepted final version. IEEE International Symposium on Safety,\n  Security, and Rescue Robotics (SSRR), Galway, Ireland, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21798v2",
                "updated": "2025-10-24T11:33:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    33,
                    1,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-26T02:56:06Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    2,
                    56,
                    6,
                    4,
                    269,
                    0
                ],
                "title": "Evaluating and Improving Cultural Awareness of Reward Models for LLM\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving Cultural Awareness of Reward Models for LLM\n  Alignment"
                },
                "summary": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith diverse cultures. Consequently, evaluating their cultural awareness is\nessential for further advancing global alignment of LLMs. However, existing RM\nevaluations fall short in assessing cultural awareness due to the scarcity of\nculturally relevant evaluation datasets. To fill this gap, we propose Cultural\nAwareness Reward modeling Benchmark (CARB), covering 10 distinct cultures\nacross 4 cultural domains. Our extensive evaluation of state-of-the-art RMs\nreveals their deficiencies in modeling cultural awareness and demonstrates a\npositive correlation between performance on CARB and downstream multilingual\ncultural alignment tasks. Further analysis identifies the spurious correlations\nwithin culture-aware reward modeling, wherein RM's scoring relies predominantly\non surface-level features rather than authentic cultural nuance understanding.\nTo address these, we propose Think-as-Locals to elicit deeper culturally\ngrounded reasoning from generative RMs via reinforcement learning from\nverifiable rewards (RLVR) and employ well-designed rewards to ensure accurate\npreference judgments and high-quality structured evaluation criteria\ngeneration. Experimental results validate its efficacy in mitigating spurious\nfeatures interference and advancing culture-aware reward modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith diverse cultures. Consequently, evaluating their cultural awareness is\nessential for further advancing global alignment of LLMs. However, existing RM\nevaluations fall short in assessing cultural awareness due to the scarcity of\nculturally relevant evaluation datasets. To fill this gap, we propose Cultural\nAwareness Reward modeling Benchmark (CARB), covering 10 distinct cultures\nacross 4 cultural domains. Our extensive evaluation of state-of-the-art RMs\nreveals their deficiencies in modeling cultural awareness and demonstrates a\npositive correlation between performance on CARB and downstream multilingual\ncultural alignment tasks. Further analysis identifies the spurious correlations\nwithin culture-aware reward modeling, wherein RM's scoring relies predominantly\non surface-level features rather than authentic cultural nuance understanding.\nTo address these, we propose Think-as-Locals to elicit deeper culturally\ngrounded reasoning from generative RMs via reinforcement learning from\nverifiable rewards (RLVR) and employ well-designed rewards to ensure accurate\npreference judgments and high-quality structured evaluation criteria\ngeneration. Experimental results validate its efficacy in mitigating spurious\nfeatures interference and advancing culture-aware reward modeling."
                },
                "authors": [
                    {
                        "name": "Hongbin Zhang"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Under review;Work in progress;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21352v1",
                "updated": "2025-10-24T11:28:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    28,
                    8,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:28:08Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    28,
                    8,
                    4,
                    297,
                    0
                ],
                "title": "SciNUP: Natural Language User Interest Profiles for Scientific\n  Literature Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciNUP: Natural Language User Interest Profiles for Scientific\n  Literature Recommendation"
                },
                "summary": "The use of natural language (NL) user profiles in recommender systems offers\ngreater transparency and user control compared to traditional representations.\nHowever, there is scarcity of large-scale, publicly available test collections\nfor evaluating NL profile-based recommendation. To address this gap, we\nintroduce SciNUP, a novel synthetic dataset for scholarly recommendation that\nleverages authors' publication histories to generate NL profiles and\ncorresponding ground truth items. We use this dataset to conduct a comparison\nof baseline methods, ranging from sparse and dense retrieval approaches to\nstate-of-the-art LLM-based rerankers. Our results show that while baseline\nmethods achieve comparable performance, they often retrieve different items,\nindicating complementary behaviors. At the same time, considerable headroom for\nimprovement remains, highlighting the need for effective NL-based\nrecommendation approaches. The SciNUP dataset thus serves as a valuable\nresource for fostering future research and development in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of natural language (NL) user profiles in recommender systems offers\ngreater transparency and user control compared to traditional representations.\nHowever, there is scarcity of large-scale, publicly available test collections\nfor evaluating NL profile-based recommendation. To address this gap, we\nintroduce SciNUP, a novel synthetic dataset for scholarly recommendation that\nleverages authors' publication histories to generate NL profiles and\ncorresponding ground truth items. We use this dataset to conduct a comparison\nof baseline methods, ranging from sparse and dense retrieval approaches to\nstate-of-the-art LLM-based rerankers. Our results show that while baseline\nmethods achieve comparable performance, they often retrieve different items,\nindicating complementary behaviors. At the same time, considerable headroom for\nimprovement remains, highlighting the need for effective NL-based\nrecommendation approaches. The SciNUP dataset thus serves as a valuable\nresource for fostering future research and development in this area."
                },
                "authors": [
                    {
                        "name": "Mariam Arustashvili"
                    },
                    {
                        "name": "Krisztian Balog"
                    }
                ],
                "author_detail": {
                    "name": "Krisztian Balog"
                },
                "author": "Krisztian Balog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21351v1",
                "updated": "2025-10-24T11:28:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    28,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:28:06Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    28,
                    6,
                    4,
                    297,
                    0
                ],
                "title": "Dynamic Semantic-Aware Correlation Modeling for UAV Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Semantic-Aware Correlation Modeling for UAV Tracking"
                },
                "summary": "UAV tracking can be widely applied in scenarios such as disaster rescue,\nenvironmental monitoring, and logistics transportation. However, existing UAV\ntracking methods predominantly emphasize speed and lack exploration in semantic\nawareness, which hinders the search region from extracting accurate\nlocalization information from the template. The limitation results in\nsuboptimal performance under typical UAV tracking challenges such as camera\nmotion, fast motion, and low resolution, etc. To address this issue, we propose\na dynamic semantic aware correlation modeling tracking framework. The core of\nour framework is a Dynamic Semantic Relevance Generator, which, in combination\nwith the correlation map from the Transformer, explore semantic relevance. The\napproach enhances the search region's ability to extract important information\nfrom the template, improving accuracy and robustness under the aforementioned\nchallenges. Additionally, to enhance the tracking speed, we design a pruning\nmethod for the proposed framework. Therefore, we present multiple model\nvariants that achieve trade-offs between speed and accuracy, enabling flexible\ndeployment according to the available computational resources. Experimental\nresults validate the effectiveness of our method, achieving competitive\nperformance on multiple UAV tracking datasets. The code is available at\nhttps://github.com/zxyyxzz/DSATrack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV tracking can be widely applied in scenarios such as disaster rescue,\nenvironmental monitoring, and logistics transportation. However, existing UAV\ntracking methods predominantly emphasize speed and lack exploration in semantic\nawareness, which hinders the search region from extracting accurate\nlocalization information from the template. The limitation results in\nsuboptimal performance under typical UAV tracking challenges such as camera\nmotion, fast motion, and low resolution, etc. To address this issue, we propose\na dynamic semantic aware correlation modeling tracking framework. The core of\nour framework is a Dynamic Semantic Relevance Generator, which, in combination\nwith the correlation map from the Transformer, explore semantic relevance. The\napproach enhances the search region's ability to extract important information\nfrom the template, improving accuracy and robustness under the aforementioned\nchallenges. Additionally, to enhance the tracking speed, we design a pruning\nmethod for the proposed framework. Therefore, we present multiple model\nvariants that achieve trade-offs between speed and accuracy, enabling flexible\ndeployment according to the available computational resources. Experimental\nresults validate the effectiveness of our method, achieving competitive\nperformance on multiple UAV tracking datasets. The code is available at\nhttps://github.com/zxyyxzz/DSATrack."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Tongxin Pan"
                    },
                    {
                        "name": "Lingyi Hong"
                    },
                    {
                        "name": "Pinxue Guo"
                    },
                    {
                        "name": "Haijing Guo"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Kaixun Jiang"
                    },
                    {
                        "name": "Wenqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Zhang"
                },
                "author": "Wenqiang Zhang",
                "arxiv_comment": "Accepted by NeurIPS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13937v2",
                "updated": "2025-10-24T11:26:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    26,
                    28,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-18T14:09:45Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    9,
                    45,
                    4,
                    199,
                    0
                ],
                "title": "Marcel: A Lightweight and Open-Source Conversational Agent for\n  University Student Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marcel: A Lightweight and Open-Source Conversational Agent for\n  University Student Support"
                },
                "summary": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. We introduce a Frequently Asked Question (FAQ) retriever\nthat maps user questions to knowledge-base entries, which allows administrators\nto steer retrieval, and improves over standard dense/hybrid retrieval\nstrategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. We introduce a Frequently Asked Question (FAQ) retriever\nthat maps user questions to knowledge-base entries, which allows administrators\nto steer retrieval, and improves over standard dense/hybrid retrieval\nstrategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Jan Trienes"
                    },
                    {
                        "name": "Anastasiia Derzhanskaia"
                    },
                    {
                        "name": "Roland Schwarzkopf"
                    },
                    {
                        "name": "Markus Mhling"
                    },
                    {
                        "name": "Jrg Schltterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "arxiv_comment": "Accepted at EMNLP 2025 (System Demonstrations)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03317v2",
                "updated": "2025-10-24T11:24:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    24,
                    57,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-01T01:18:27Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    18,
                    27,
                    2,
                    274,
                    0
                ],
                "title": "Photorealistic Inpainting for Perturbation-based Explanations in\n  Ecological Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photorealistic Inpainting for Perturbation-based Explanations in\n  Ecological Monitoring"
                },
                "summary": "Ecological monitoring is increasingly automated by vision models, yet opaque\npredictions limit trust and field adoption. We present an inpainting-guided,\nperturbation-based explanation technique that produces photorealistic,\nmask-localized edits that preserve scene context. Unlike masking or blurring,\nthese edits stay in-distribution and reveal which fine-grained morphological\ncues drive predictions in tasks such as species recognition and trait\nattribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for\nharbor seal detection in Glacier Bay drone imagery, using\nSegment-Anything-Model-refined masks to support two interventions: (i) object\nremoval/replacement (e.g., replacing seals with plausible ice/water or boats)\nand (ii) background replacement with original animals composited onto new\nscenes. Explanations are assessed by re-scoring perturbed images (flip rate,\nconfidence drop) and by expert review for ecological plausibility and\ninterpretability. The resulting explanations localize diagnostic structures,\navoid deletion artifacts common to traditional perturbations, and yield\ndomain-relevant insights that support expert validation and more trustworthy\ndeployment of AI in ecology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecological monitoring is increasingly automated by vision models, yet opaque\npredictions limit trust and field adoption. We present an inpainting-guided,\nperturbation-based explanation technique that produces photorealistic,\nmask-localized edits that preserve scene context. Unlike masking or blurring,\nthese edits stay in-distribution and reveal which fine-grained morphological\ncues drive predictions in tasks such as species recognition and trait\nattribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for\nharbor seal detection in Glacier Bay drone imagery, using\nSegment-Anything-Model-refined masks to support two interventions: (i) object\nremoval/replacement (e.g., replacing seals with plausible ice/water or boats)\nand (ii) background replacement with original animals composited onto new\nscenes. Explanations are assessed by re-scoring perturbed images (flip rate,\nconfidence drop) and by expert review for ecological plausibility and\ninterpretability. The resulting explanations localize diagnostic structures,\navoid deletion artifacts common to traditional perturbations, and yield\ndomain-relevant insights that support expert validation and more trustworthy\ndeployment of AI in ecology."
                },
                "authors": [
                    {
                        "name": "Gnel Aghakishiyeva"
                    },
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Saagar Arya"
                    },
                    {
                        "name": "Julian Dale"
                    },
                    {
                        "name": "James David Poling"
                    },
                    {
                        "name": "Holly R. Houliston"
                    },
                    {
                        "name": "Jamie N. Womble"
                    },
                    {
                        "name": "Gregory D. Larsen"
                    },
                    {
                        "name": "David W. Johnston"
                    },
                    {
                        "name": "Brinnae Bent"
                    }
                ],
                "author_detail": {
                    "name": "Brinnae Bent"
                },
                "author": "Brinnae Bent",
                "arxiv_comment": "NeurIPS 2025 Imageomics Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21345v1",
                "updated": "2025-10-24T11:19:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    19,
                    33,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:19:33Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    19,
                    33,
                    4,
                    297,
                    0
                ],
                "title": "$$-LoRA: Effective Fine-Tuning via Base Model Rescaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$$-LoRA: Effective Fine-Tuning via Base Model Rescaling"
                },
                "summary": "Fine-tuning has proven to be highly effective in adapting pre-trained models\nto perform better on new desired tasks with minimal data samples. Among the\nmost widely used approaches are reparameterization methods, which update a\ntarget module by augmenting its frozen weight matrix with an additional\ntrainable weight matrix. The most prominent example is Low Rank Adaption\n(LoRA), which gained significant attention in recent years. In this paper, we\nintroduce a new class of reparameterization methods for transfer learning,\ndesigned to enhance the generalization ability of fine-tuned models. We\nestablish the effectiveness of our approach in a high-dimensional binary\nclassification setting using tools from Random Matrix Theory, and further\nvalidate our theoretical findings through more realistic experiments, such as\nfine-tuning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning has proven to be highly effective in adapting pre-trained models\nto perform better on new desired tasks with minimal data samples. Among the\nmost widely used approaches are reparameterization methods, which update a\ntarget module by augmenting its frozen weight matrix with an additional\ntrainable weight matrix. The most prominent example is Low Rank Adaption\n(LoRA), which gained significant attention in recent years. In this paper, we\nintroduce a new class of reparameterization methods for transfer learning,\ndesigned to enhance the generalization ability of fine-tuned models. We\nestablish the effectiveness of our approach in a high-dimensional binary\nclassification setting using tools from Random Matrix Theory, and further\nvalidate our theoretical findings through more realistic experiments, such as\nfine-tuning LLMs."
                },
                "authors": [
                    {
                        "name": "Aymane El Firdoussi"
                    },
                    {
                        "name": "El Mahdi Chayti"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21341v1",
                "updated": "2025-10-24T11:09:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    9,
                    59,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:09:59Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    9,
                    59,
                    4,
                    297,
                    0
                ],
                "title": "Magellan: Guided MCTS for Latent Space Exploration and Novelty\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magellan: Guided MCTS for Latent Space Exploration and Novelty\n  Generation"
                },
                "summary": "Large Language Models (LLMs) often struggle with generating truly innovative\nideas, typically defaulting to high-probability, familiar concepts within their\ntraining data's \"gravity wells.\" While advanced search-based methods like Tree\nof Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by\ntheir reliance on unprincipled, inconsistent self-evaluation heuristics to\nguide exploration. To address this gap, we introduce \\textbf{Magellan}, a novel\nframework that reframes creative generation as a principled, guided exploration\nof an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo\nTree Search (MCTS) governed by a hierarchical guidance system. For long-range\ndirection, a \"semantic compass\" vector, formulated via orthogonal projection,\nsteers the search towards relevant novelty. For local, step-by-step decisions,\na landscape-aware value function replaces flawed self-evaluation with an\nexplicit reward structure that balances intrinsic coherence, extrinsic novelty,\nand narrative progress. Extensive experiments demonstrate that Magellan\nsignificantly outperforms strong baselines, including ReAct and ToT, in\ngenerating scientific ideas with superior plausibility and innovation. Our work\nshows that for creative discovery, a principled, guided search is more\neffective than unconstrained agency, paving the way for LLMs to become more\ncapable partners in innovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with generating truly innovative\nideas, typically defaulting to high-probability, familiar concepts within their\ntraining data's \"gravity wells.\" While advanced search-based methods like Tree\nof Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by\ntheir reliance on unprincipled, inconsistent self-evaluation heuristics to\nguide exploration. To address this gap, we introduce \\textbf{Magellan}, a novel\nframework that reframes creative generation as a principled, guided exploration\nof an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo\nTree Search (MCTS) governed by a hierarchical guidance system. For long-range\ndirection, a \"semantic compass\" vector, formulated via orthogonal projection,\nsteers the search towards relevant novelty. For local, step-by-step decisions,\na landscape-aware value function replaces flawed self-evaluation with an\nexplicit reward structure that balances intrinsic coherence, extrinsic novelty,\nand narrative progress. Extensive experiments demonstrate that Magellan\nsignificantly outperforms strong baselines, including ReAct and ToT, in\ngenerating scientific ideas with superior plausibility and innovation. Our work\nshows that for creative discovery, a principled, guided search is more\neffective than unconstrained agency, paving the way for LLMs to become more\ncapable partners in innovation."
                },
                "authors": [
                    {
                        "name": "Lufan Chang"
                    }
                ],
                "author_detail": {
                    "name": "Lufan Chang"
                },
                "author": "Lufan Chang",
                "arxiv_comment": "Accepted to 1st Open Conference on AI Agents for Science\n  (agents4science 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21339v1",
                "updated": "2025-10-24T11:08:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    8,
                    32,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:08:32Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    8,
                    32,
                    4,
                    297,
                    0
                ],
                "title": "Multi-turn Training with Basic Human Feedback Helps Little on LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn Training with Basic Human Feedback Helps Little on LLM\n  Reasoning"
                },
                "summary": "The reasoning capabilities of Large Language Models (LLMs) are typically\ndeveloped through the single-turn reinforcement learning, whereas real-world\napplications often involve multi-turn interactions with human feedback, leading\nto a potential mismatch between training and deployment conditions. In this\nwork, we study whether multi-turn training with human feedback is necessary for\nreasoning tasks. We compare conventional single-turn training with three\nmulti-turn strategies and reach contrary conclusions to previous research. We\nfind that models trained in a single-turn setting generalize effectively to\nboth single- and multi-turn evaluations, while models trained with multi-turn\nstrategies exhibit a significant degradation in single-turn reasoning\nperformance. These results suggest that for tasks with complete information,\nrobust single-turn training remains more effective and reliable, as multi-turn\ntraining with basic feedback provides limited benefits and can even degrade\nreasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of Large Language Models (LLMs) are typically\ndeveloped through the single-turn reinforcement learning, whereas real-world\napplications often involve multi-turn interactions with human feedback, leading\nto a potential mismatch between training and deployment conditions. In this\nwork, we study whether multi-turn training with human feedback is necessary for\nreasoning tasks. We compare conventional single-turn training with three\nmulti-turn strategies and reach contrary conclusions to previous research. We\nfind that models trained in a single-turn setting generalize effectively to\nboth single- and multi-turn evaluations, while models trained with multi-turn\nstrategies exhibit a significant degradation in single-turn reasoning\nperformance. These results suggest that for tasks with complete information,\nrobust single-turn training remains more effective and reliable, as multi-turn\ntraining with basic feedback provides limited benefits and can even degrade\nreasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Wuganjing Song"
                    },
                    {
                        "name": "Zhenzhou Lin"
                    },
                    {
                        "name": "Feifan Chen"
                    },
                    {
                        "name": "Qiaolong Cai"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Yongduo Sui"
                    }
                ],
                "author_detail": {
                    "name": "Yongduo Sui"
                },
                "author": "Yongduo Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11546v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11546v5",
                "updated": "2025-10-24T11:05:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    5,
                    50,
                    4,
                    297,
                    0
                ],
                "published": "2025-02-17T08:28:29Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    28,
                    29,
                    0,
                    48,
                    0
                ],
                "title": "DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data\n  Cleaning as Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data\n  Cleaning as Anomaly Detection"
                },
                "summary": "The rapid development of multilingual large language models (LLMs) highlights\nthe need for high-quality, diverse, and well-curated multilingual datasets. In\nthis paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a\nlarge-scale multilingual corpus constructed from newly extracted Common Crawl\ndata and existing multilingual sources. DCAD-2000 covers 2,282 languages,\n46.72TB of text, and 8.63 billion documents, spanning 155 high- and\nmedium-resource languages and 159 writing scripts. To overcome the limitations\nof existing data cleaning approaches, which rely on manually designed heuristic\nthresholds, we reframe data cleaning as an anomaly detection problem. This\ndynamic filtering paradigm substantially improves data quality by automatically\nidentifying and removing noisy or anomalous content. By fine-tuning LLMs on\nDCAD-2000, we demonstrate notable improvements in data quality, robustness of\nthe cleaning pipeline, and downstream performance, particularly for\nlow-resource languages across multiple multilingual benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multilingual large language models (LLMs) highlights\nthe need for high-quality, diverse, and well-curated multilingual datasets. In\nthis paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a\nlarge-scale multilingual corpus constructed from newly extracted Common Crawl\ndata and existing multilingual sources. DCAD-2000 covers 2,282 languages,\n46.72TB of text, and 8.63 billion documents, spanning 155 high- and\nmedium-resource languages and 159 writing scripts. To overcome the limitations\nof existing data cleaning approaches, which rely on manually designed heuristic\nthresholds, we reframe data cleaning as an anomaly detection problem. This\ndynamic filtering paradigm substantially improves data quality by automatically\nidentifying and removing noisy or anomalous content. By fine-tuning LLMs on\nDCAD-2000, we demonstrate notable improvements in data quality, robustness of\nthe cleaning pipeline, and downstream performance, particularly for\nlow-resource languages across multiple multilingual benchmarks."
                },
                "authors": [
                    {
                        "name": "Yingli Shen"
                    },
                    {
                        "name": "Wen Lai"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xueren Zhang"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Alexander Fraser"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11546v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11546v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14604v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14604v3",
                "updated": "2025-10-24T11:03:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    3,
                    24,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T16:53:40Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    53,
                    40,
                    1,
                    140,
                    0
                ],
                "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let LLMs Break Free from Overthinking via Self-Braking Tuning"
                },
                "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models."
                },
                "authors": [
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Accepted to NeurIPS 2025; Camera ready version, 10 pages.\n  Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page:\n  https://ZJU-REAL.github.io/SBT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14604v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14604v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21329v1",
                "updated": "2025-10-24T10:39:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    10,
                    39,
                    55,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T10:39:55Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    10,
                    39,
                    55,
                    4,
                    297,
                    0
                ],
                "title": "TripTide: A Benchmark for Adaptive Travel Planning under Disruptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TripTide: A Benchmark for Adaptive Travel Planning under Disruptions"
                },
                "summary": "Recent efforts like TripCraft and TravelPlanner have advanced the use of\nLarge Language Models ( LLMs) for personalized, constraint aware travel\nitinerary generation. Yet, real travel often faces disruptions. To address\nthis, we present TripTide, the first benchmark evaluating LLM's ability to\nrevise itineraries under realistic disruptions. TripTide models key dimensions\nsuch as disruption severity and traveler tolerance, enabling nuanced assessment\nof LLM adaptability to events like flight cancellations, weather closures, or\noverbooked attractions. We conduct a threefold evaluation. First, we introduce\nautomatic metrics including Preservation of Intent (how well the revised plan\nmaintains feasibility and goals), Responsiveness (promptness and\nappropriateness of disruption handling), and Adaptability (semantic, spatial,\nand sequential divergence between original and revised plans). Second, we apply\nan LLM-as-a-judge approach to automatically assess revision quality. Third, we\nperform manual expert evaluation to verify whether revisions preserve semantic,\nspatial, sequential, and responsive aspects. Our experiments show that LLMs\nmaintain strong sequential consistency and semantic stability, while spatial\ndeviations are larger for shorter trips but decrease with longer ones,\nindicating that extended plans encourage better geographic coherence. However,\ndisruption-handling ability declines as plan length increases, highlighting\nlimits in LLM robustness. TripTide establishes a benchmark for evaluating\nadaptability, personalization, and resilience in LLM-based travel planning\nunder real-world uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts like TripCraft and TravelPlanner have advanced the use of\nLarge Language Models ( LLMs) for personalized, constraint aware travel\nitinerary generation. Yet, real travel often faces disruptions. To address\nthis, we present TripTide, the first benchmark evaluating LLM's ability to\nrevise itineraries under realistic disruptions. TripTide models key dimensions\nsuch as disruption severity and traveler tolerance, enabling nuanced assessment\nof LLM adaptability to events like flight cancellations, weather closures, or\noverbooked attractions. We conduct a threefold evaluation. First, we introduce\nautomatic metrics including Preservation of Intent (how well the revised plan\nmaintains feasibility and goals), Responsiveness (promptness and\nappropriateness of disruption handling), and Adaptability (semantic, spatial,\nand sequential divergence between original and revised plans). Second, we apply\nan LLM-as-a-judge approach to automatically assess revision quality. Third, we\nperform manual expert evaluation to verify whether revisions preserve semantic,\nspatial, sequential, and responsive aspects. Our experiments show that LLMs\nmaintain strong sequential consistency and semantic stability, while spatial\ndeviations are larger for shorter trips but decrease with longer ones,\nindicating that extended plans encourage better geographic coherence. However,\ndisruption-handling ability declines as plan length increases, highlighting\nlimits in LLM robustness. TripTide establishes a benchmark for evaluating\nadaptability, personalization, and resilience in LLM-based travel planning\nunder real-world uncertainty."
                },
                "authors": [
                    {
                        "name": "Priyanshu Karmakar"
                    },
                    {
                        "name": "Soumyabrata Chaudhuri"
                    },
                    {
                        "name": "Shubhojit Mallick"
                    },
                    {
                        "name": "Manish Gupta"
                    },
                    {
                        "name": "Abhik Jana"
                    },
                    {
                        "name": "Shreya Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Shreya Ghosh"
                },
                "arxiv_affiliation": "School of Electrical and Computer Sciences, IIT Bhubaneswar, India",
                "author": "Shreya Ghosh",
                "arxiv_comment": "12 pages, 12 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21324v1",
                "updated": "2025-10-24T10:31:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    10,
                    31,
                    30,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T10:31:30Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    10,
                    31,
                    30,
                    4,
                    297,
                    0
                ],
                "title": "CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray\n  Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray\n  Interpretation"
                },
                "summary": "Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety\nof task-specific and foundation models have been developed for automatic CXR\ninterpretation. However, these models often struggle to adapt to new diagnostic\ntasks and complex reasoning scenarios. Recently, LLM-based agent models have\nemerged as a promising paradigm for CXR analysis, enhancing model's capability\nthrough tool coordination, multi-step reasoning, and team collaboration, etc.\nHowever, existing agents often rely on a single diagnostic pipeline and lack\nmechanisms for assessing tools' reliability, limiting their adaptability and\ncredibility. To this end, we propose CXRAgent, a director-orchestrated,\nmulti-stage agent for CXR interpretation, where a central director coordinates\nthe following stages: (1) Tool Invocation: The agent strategically orchestrates\na set of CXR-analysis tools, with outputs normalized and verified by the\nEvidence-driven Validator (EDV), which grounds diagnostic outputs with visual\nevidence to support reliable downstream diagnosis; (2) Diagnostic Planning:\nGuided by task requirements and intermediate findings, the agent formulates a\ntargeted diagnostic plan. It then assembles an expert team accordingly,\ndefining member roles and coordinating their interactions to enable adaptive\nand collaborative reasoning; (3) Collaborative Decision-making: The agent\nintegrates insights from the expert team with accumulated contextual memories,\nsynthesizing them into an evidence-backed diagnostic conclusion. Experiments on\nvarious CXR interpretation tasks show that CXRAgent delivers strong\nperformance, providing visual evidence and generalizes well to clinical tasks\nof different complexity. Code and data are valuable at this\n\\href{https://github.com/laojiahuo2003/CXRAgent/}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety\nof task-specific and foundation models have been developed for automatic CXR\ninterpretation. However, these models often struggle to adapt to new diagnostic\ntasks and complex reasoning scenarios. Recently, LLM-based agent models have\nemerged as a promising paradigm for CXR analysis, enhancing model's capability\nthrough tool coordination, multi-step reasoning, and team collaboration, etc.\nHowever, existing agents often rely on a single diagnostic pipeline and lack\nmechanisms for assessing tools' reliability, limiting their adaptability and\ncredibility. To this end, we propose CXRAgent, a director-orchestrated,\nmulti-stage agent for CXR interpretation, where a central director coordinates\nthe following stages: (1) Tool Invocation: The agent strategically orchestrates\na set of CXR-analysis tools, with outputs normalized and verified by the\nEvidence-driven Validator (EDV), which grounds diagnostic outputs with visual\nevidence to support reliable downstream diagnosis; (2) Diagnostic Planning:\nGuided by task requirements and intermediate findings, the agent formulates a\ntargeted diagnostic plan. It then assembles an expert team accordingly,\ndefining member roles and coordinating their interactions to enable adaptive\nand collaborative reasoning; (3) Collaborative Decision-making: The agent\nintegrates insights from the expert team with accumulated contextual memories,\nsynthesizing them into an evidence-backed diagnostic conclusion. Experiments on\nvarious CXR interpretation tasks show that CXRAgent delivers strong\nperformance, providing visual evidence and generalizes well to clinical tasks\nof different complexity. Code and data are valuable at this\n\\href{https://github.com/laojiahuo2003/CXRAgent/}{link}."
                },
                "authors": [
                    {
                        "name": "Jinhui Lou"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Zhenqi Fu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Qingming Huang"
                    },
                    {
                        "name": "Jun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Yu"
                },
                "author": "Jun Yu",
                "arxiv_comment": "10 pages, 4 figures, 7 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]