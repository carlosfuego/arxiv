[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.21590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v2",
                "updated": "2025-11-03T11:32:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    32,
                    13,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "Accepted at NeurIPS 2025. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v3",
                "updated": "2025-11-02T20:27:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    27,
                    27,
                    6,
                    306,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26692v2",
                "updated": "2025-11-01T12:05:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    12,
                    5,
                    18,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-30T16:59:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi Linear: An Expressive, Efficient Attention Architecture"
                },
                "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Chengyin Liu"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Weizhou Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Yizhi Zhang"
                    },
                    {
                        "name": "T. Y. Liu"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Shengjun Fang"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chao Hong"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Guanduo Chen"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Siyuan Pan"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Guohong Fu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Yulun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Du"
                },
                "author": "Yulun Du",
                "arxiv_comment": "Kimi Linear tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v3",
                "updated": "2025-11-01T08:49:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    49,
                    20,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v2",
                "updated": "2025-11-01T08:26:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    26,
                    24,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/FastMAS/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22765v2",
                "updated": "2025-11-01T07:01:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    7,
                    1,
                    0,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval"
                },
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Shaolin Lu"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v7",
                "updated": "2025-11-01T04:26:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    4,
                    26,
                    3,
                    5,
                    305,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25979v2",
                "updated": "2025-10-31T18:19:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    18,
                    19,
                    55,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache"
                },
                "summary": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27641v1",
                "updated": "2025-10-31T17:12:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:12:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "SpecAttn: Speculating Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecAttn: Speculating Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation."
                },
                "authors": [
                    {
                        "name": "Harsh Shah"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Shah"
                },
                "author": "Harsh Shah",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Structured Probabilistic\n  Inference & Generative Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27617v1",
                "updated": "2025-10-31T16:40:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:40:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation"
                },
                "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training."
                },
                "authors": [
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Arijit Bhattacharjee"
                    },
                    {
                        "name": "Peiyu Zhang"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Anzhe Cheng"
                    },
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Ali Jannesari"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v2",
                "updated": "2025-10-31T05:31:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    31,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages, fixed cleveref-related issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27171v1",
                "updated": "2025-10-31T04:47:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T04:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models"
                },
                "summary": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache."
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v2",
                "updated": "2025-10-31T04:17:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    17,
                    5,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25977v2",
                "updated": "2025-10-31T01:52:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    1,
                    52,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium"
                },
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27070v1",
                "updated": "2025-10-31T00:39:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T00:39:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review"
                },
                "summary": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures."
                },
                "authors": [
                    {
                        "name": "Dong Tong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Tong"
                },
                "author": "Dong Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v2",
                "updated": "2025-10-30T21:11:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    21,
                    11,
                    33,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26944v1",
                "updated": "2025-10-30T18:58:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T18:58:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies"
                },
                "summary": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios."
                },
                "authors": [
                    {
                        "name": "Hoa Nguyen"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    },
                    {
                        "name": "Alireza Kaviani"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Kaviani"
                },
                "author": "Alireza Kaviani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26730v1",
                "updated": "2025-10-30T17:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:29:27Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference"
                },
                "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."
                },
                "authors": [
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Runxin Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v2",
                "updated": "2025-10-30T13:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    31,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif rdk"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26486v1",
                "updated": "2025-10-30T13:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks"
                },
                "summary": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    },
                    {
                        "name": "Guadalupe Correa-Cabrera"
                    }
                ],
                "author_detail": {
                    "name": "Guadalupe Correa-Cabrera"
                },
                "author": "Guadalupe Correa-Cabrera",
                "arxiv_comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25160v2",
                "updated": "2025-10-30T08:52:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    52,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T04:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    29,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "Model-Document Protocol for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Document Protocol for AI Search"
                },
                "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v2",
                "updated": "2025-10-30T08:46:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    46,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "arxiv_comment": "Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26234v1",
                "updated": "2025-10-30T08:12:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T08:12:53Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "title": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS"
                },
                "summary": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies."
                },
                "authors": [
                    {
                        "name": "Mathis Engelbart"
                    },
                    {
                        "name": "Mike Kosek"
                    },
                    {
                        "name": "Lars Eggert"
                    },
                    {
                        "name": "Jrg Ott"
                    }
                ],
                "author_detail": {
                    "name": "Jrg Ott"
                },
                "author": "Jrg Ott",
                "arxiv_doi": "10.1145/3772356.3772416",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772356.3772416",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.26234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "HotNets 2025",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25600v2",
                "updated": "2025-10-30T03:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    43,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:10:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    10,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models"
                },
                "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Sihao Liu"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26104v1",
                "updated": "2025-10-30T03:30:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T03:30:12Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender"
                },
                "summary": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests."
                },
                "authors": [
                    {
                        "name": "Zhaoqi Zhang"
                    },
                    {
                        "name": "Haolei Pei"
                    },
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yufei Feng"
                    },
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v2",
                "updated": "2025-10-29T21:56:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    56,
                    19,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26835v1",
                "updated": "2025-10-29T19:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T19:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads"
                },
                "summary": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Xunzhuo Liu"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Priya Nagpurkar"
                    },
                    {
                        "name": "Huamin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Chen"
                },
                "author": "Huamin Chen",
                "arxiv_comment": "13 pages including reference, position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25695v1",
                "updated": "2025-10-29T17:00:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:00:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate"
                },
                "summary": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices."
                },
                "authors": [
                    {
                        "name": "Emerson J. Hollar"
                    },
                    {
                        "name": "Esmat Farzana"
                    }
                ],
                "author_detail": {
                    "name": "Esmat Farzana"
                },
                "author": "Esmat Farzana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25604v1",
                "updated": "2025-10-29T15:12:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:12:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Quickest Change Point Detection with Measurements over a Lossy Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickest Change Point Detection with Measurements over a Lossy Link"
                },
                "summary": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime."
                },
                "authors": [
                    {
                        "name": "Krishna Chaythanya KV"
                    },
                    {
                        "name": "Saqib Abbas Baba"
                    },
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Arpan Chattopadhyay"
                    },
                    {
                        "name": "Rajesh Sundaresan"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sundaresan"
                },
                "author": "Rajesh Sundaresan",
                "arxiv_comment": "17 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25590v1",
                "updated": "2025-10-29T14:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
                },
                "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Bangyin Xiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "26 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v2",
                "updated": "2025-10-29T14:46:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    46,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25152v1",
                "updated": "2025-10-29T04:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T04:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "Off-Centered WoS-Type Solvers with Statistical Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Centered WoS-Type Solvers with Statistical Weighting"
                },
                "summary": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems."
                },
                "authors": [
                    {
                        "name": "Anchang Bao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Enya Shen"
                    },
                    {
                        "name": "Jianmin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Wang"
                },
                "author": "Jianmin Wang",
                "arxiv_comment": "SIGGRAPH Asia 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25122v1",
                "updated": "2025-10-29T03:00:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T03:00:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies"
                },
                "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Jiahong Chen"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Chuwei Cai"
                    },
                    {
                        "name": "Jinghui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jinghui Lu"
                },
                "author": "Jinghui Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24824v1",
                "updated": "2025-10-28T15:35:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:35:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling"
                },
                "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Fan Xia"
                    },
                    {
                        "name": "Tianqi Zhang"
                    },
                    {
                        "name": "Hongrui Zhan"
                    },
                    {
                        "name": "Zheng Zhong"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Xingyan Bin"
                    }
                ],
                "author_detail": {
                    "name": "Xingyan Bin"
                },
                "author": "Xingyan Bin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24359v1",
                "updated": "2025-10-28T12:28:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:28:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine"
                },
                "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."
                },
                "authors": [
                    {
                        "name": "Pedram Fard"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Neguine Rezaii"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "arxiv_comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24051v1",
                "updated": "2025-10-28T04:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T04:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: A Programmable Serving System for Emerging LLM Applications"
                },
                "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Zhiyao Ma"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3731569.3764814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v4",
                "updated": "2025-10-28T04:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    0,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v2",
                "updated": "2025-10-27T21:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    21,
                    48,
                    48,
                    0,
                    300,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12362v2",
                "updated": "2025-10-27T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    17,
                    31,
                    15,
                    0,
                    300,
                    0
                ],
                "published": "2024-04-18T17:45:19Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    45,
                    19,
                    3,
                    109,
                    0
                ],
                "title": "KV-weights are all you need for skipless transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-weights are all you need for skipless transformers"
                },
                "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks."
                },
                "authors": [
                    {
                        "name": "Nils Graef"
                    }
                ],
                "author_detail": {
                    "name": "Nils Graef"
                },
                "author": "Nils Graef",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v3",
                "updated": "2025-10-27T16:20:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    20,
                    28,
                    0,
                    300,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_doi": "10.1145/3721462.3770776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721462.3770776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Middleware '25",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v2",
                "updated": "2025-10-27T14:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    59,
                    46,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving"
                },
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Accepted in a computer science workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v2",
                "updated": "2025-10-27T11:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    11,
                    55,
                    7,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gatan Hadjeres"
                    },
                    {
                        "name": "Gal Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_doi": "10.5334/TISMIR.251",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5334/TISMIR.251",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transactions of the International Society for Music Information\n  Retrieval, 8(1): 334-352 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22876v1",
                "updated": "2025-10-26T23:59:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec."
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v3",
                "updated": "2025-10-26T13:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    13,
                    31,
                    41,
                    6,
                    299,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22556v1",
                "updated": "2025-10-26T07:17:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T07:17:10Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size"
                },
                "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length."
                },
                "authors": [
                    {
                        "name": "Jinhan Chen"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Shilong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Wang"
                },
                "author": "Shilong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v3",
                "updated": "2025-10-26T04:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    4,
                    25,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23657v1",
                "updated": "2025-10-26T01:25:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T01:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "title": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops"
                },
                "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture."
                },
                "authors": [
                    {
                        "name": "Saklain Niam"
                    },
                    {
                        "name": "Tashfiqur Rahman"
                    },
                    {
                        "name": "Md. Amjad Patwary"
                    },
                    {
                        "name": "Mukarram Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Mukarram Hossain"
                },
                "author": "Mukarram Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22467v1",
                "updated": "2025-10-26T00:50:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T00:50:12Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "title": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints"
                },
                "summary": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore)."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v3",
                "updated": "2025-10-25T14:12:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    14,
                    12,
                    56,
                    5,
                    298,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "38 pages, 9 figures, 17 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23649v1",
                "updated": "2025-10-25T11:43:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models"
                },
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK."
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22145v1",
                "updated": "2025-10-25T03:34:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T03:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Coded Caching with Fixed Subpacketization"
                },
                "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Jinyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyan Wang"
                },
                "author": "Jinyan Wang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v3",
                "updated": "2025-10-25T02:29:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    2,
                    29,
                    47,
                    5,
                    298,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v4",
                "updated": "2025-10-25T00:33:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    0,
                    33,
                    14,
                    5,
                    298,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22049v1",
                "updated": "2025-10-24T22:17:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders"
                },
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Shouwei Chen"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20787v2",
                "updated": "2025-10-24T16:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    56,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T17:53:03Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    17,
                    53,
                    3,
                    3,
                    296,
                    0
                ],
                "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction"
                },
                "summary": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v2",
                "updated": "2025-10-24T14:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    55,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22922v2",
                "updated": "2025-10-24T11:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-28T15:15:31Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    15,
                    15,
                    31,
                    5,
                    179,
                    0
                ],
                "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling"
                },
                "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass."
                },
                "authors": [
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v2",
                "updated": "2025-10-24T08:35:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    35,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v2",
                "updated": "2025-10-24T05:39:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    5,
                    39,
                    3,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v2",
                "updated": "2025-10-24T04:48:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    48,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v5",
                "updated": "2025-10-23T23:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    23,
                    35,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page:\n  https://github.com/tensorgi/TPA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v2",
                "updated": "2025-10-23T21:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    31,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v3",
                "updated": "2025-10-23T19:45:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    19,
                    45,
                    39,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    },
                    {
                        "name": "Panagiotis Karras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Karras"
                },
                "author": "Panagiotis Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v3",
                "updated": "2025-10-23T18:52:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    18,
                    52,
                    25,
                    3,
                    296,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20707v1",
                "updated": "2025-10-23T16:17:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T16:17:47Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models"
                },
                "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Yuchao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/MixKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v2",
                "updated": "2025-10-23T15:26:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    15,
                    26,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "arxiv_comment": "Accepted to ALENEX`26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v4",
                "updated": "2025-10-23T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    14,
                    23,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "35 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20878v1",
                "updated": "2025-10-23T12:28:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:28:58Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement"
                },
                "summary": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Yixue Yang"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "13 pages,16 figures,2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; E.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21865v1",
                "updated": "2025-10-23T10:35:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:35:35Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "title": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis"
                },
                "summary": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization."
                },
                "authors": [
                    {
                        "name": "F. I. Qowy"
                    }
                ],
                "author_detail": {
                    "name": "F. I. Qowy"
                },
                "author": "F. I. Qowy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20400v1",
                "updated": "2025-10-23T10:06:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:06:48Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels"
                },
                "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline."
                },
                "authors": [
                    {
                        "name": "Rubn Langarita"
                    },
                    {
                        "name": "Jess Alastruey-Bened"
                    },
                    {
                        "name": "Pablo Ibez-Marn"
                    },
                    {
                        "name": "Santiago Marco-Sola"
                    },
                    {
                        "name": "Miquel Moret"
                    },
                    {
                        "name": "Adri Armejach"
                    }
                ],
                "author_detail": {
                    "name": "Adri Armejach"
                },
                "author": "Adri Armejach",
                "arxiv_comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v2",
                "updated": "2025-10-23T09:55:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    55,
                    50,
                    3,
                    296,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v2",
                "updated": "2025-10-23T08:29:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    29,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20230v1",
                "updated": "2025-10-23T05:22:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T05:22:09Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "title": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$"
                },
                "summary": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Chenchao Xu"
                    },
                    {
                        "name": "Zhimian Wu"
                    },
                    {
                        "name": "Huachen Rao"
                    },
                    {
                        "name": "Zhaoyang Shan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Guanghan Cao"
                    },
                    {
                        "name": "Michael Smidman"
                    },
                    {
                        "name": "Ming Shi"
                    },
                    {
                        "name": "Huiqiu Yuan"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Xianhui Chen"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Yu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yu Song"
                },
                "author": "Yu Song",
                "arxiv_comment": "submitted to journal in July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v4",
                "updated": "2025-10-23T00:47:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    47,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "Accepted to ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v2",
                "updated": "2025-10-23T00:40:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    40,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v2",
                "updated": "2025-10-22T23:56:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    56,
                    45,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    }
                ],
                "author_detail": {
                    "name": "Fariha Tasmin"
                },
                "author": "Fariha Tasmin",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19875v1",
                "updated": "2025-10-22T09:42:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:42:29Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention"
                },
                "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "Jos Luis Redondo Garca"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Konstantina Palla"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24761v1",
                "updated": "2025-10-22T07:50:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    7,
                    50,
                    6,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T07:50:06Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    7,
                    50,
                    6,
                    2,
                    295,
                    0
                ],
                "title": "ODataX: A Progressive Evolution of the Open Data Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODataX: A Progressive Evolution of the Open Data Protocol"
                },
                "summary": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices."
                },
                "authors": [
                    {
                        "name": "Anirudh Ganesh"
                    },
                    {
                        "name": "Nitin Sood"
                    }
                ],
                "author_detail": {
                    "name": "Nitin Sood"
                },
                "author": "Nitin Sood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19264v1",
                "updated": "2025-10-22T05:47:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T05:47:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPRAD: LLM-Assisted PRotocol Attack Discovery"
                },
                "summary": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."
                },
                "authors": [
                    {
                        "name": "R. Can Aygun"
                    },
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    },
                    {
                        "name": "Leonard Kleinrock"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Kleinrock"
                },
                "arxiv_affiliation": "UCLA",
                "author": "Leonard Kleinrock",
                "arxiv_comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)",
                "arxiv_journal_ref": "Published in IFIP Networking 2025 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19183v1",
                "updated": "2025-10-22T02:41:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:41:07Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning"
                },
                "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Fengyuan Sun"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Xinhao Xu"
                    },
                    {
                        "name": "Dandan Zheng"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19171v1",
                "updated": "2025-10-22T02:09:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:09:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG"
                },
                "summary": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference."
                },
                "authors": [
                    {
                        "name": "Jihwan Bang"
                    },
                    {
                        "name": "Juntae Lee"
                    },
                    {
                        "name": "Seunghan Yang"
                    },
                    {
                        "name": "Sungha Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungha Choi"
                },
                "author": "Sungha Choi",
                "arxiv_comment": "Accepted at NeurIPS 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v3",
                "updated": "2025-10-21T22:37:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    22,
                    37,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v3",
                "updated": "2025-10-21T21:07:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    21,
                    7,
                    17,
                    1,
                    294,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "arxiv_doi": "10.1145/3772052.3772208",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772208",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the version accepted to ACM SoCC 2025. The title has been\n  updated to match the published version",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15878v2",
                "updated": "2025-10-21T16:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    32,
                    50,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-21T16:10:26Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    10,
                    26,
                    3,
                    233,
                    0
                ],
                "title": "Putting the Context back into Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putting the Context back into Memory"
                },
                "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices."
                },
                "authors": [
                    {
                        "name": "David A. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "David A. Roberts"
                },
                "author": "David A. Roberts",
                "arxiv_comment": "Fixed errors in paragraph numbering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18716v1",
                "updated": "2025-10-21T15:17:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:17:37Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Siyong Jian"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14576v3",
                "updated": "2025-10-21T15:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    13,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2024-01-26T00:27:00Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    0,
                    27,
                    0,
                    4,
                    26,
                    0
                ],
                "title": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints"
                },
                "summary": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings."
                },
                "authors": [
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Artur Podobas"
                    },
                    {
                        "name": "Niclas Jansson"
                    },
                    {
                        "name": "Stefano Markidis"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "arxiv_doi": "10.1145/3772052.3772212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SoCC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18546v1",
                "updated": "2025-10-21T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval"
                },
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v2",
                "updated": "2025-10-21T10:33:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    33,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching"
                },
                "summary": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v2",
                "updated": "2025-10-21T10:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    8,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18413v1",
                "updated": "2025-10-21T08:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T08:44:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference"
                },
                "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity."
                },
                "authors": [
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Guo-Qing Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    },
                    {
                        "name": "Ran Zhu"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v5",
                "updated": "2025-10-21T06:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    47,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was first submitted for review on Sept. 5, 2024, and the\n  initial version was uploaded to Arxiv on Sept. 30, 2024. The latest version\n  has accepted for publication by IEEE Transactions on Information Forensics\n  and Security (TIFS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v3",
                "updated": "2025-10-21T06:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    30,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Steffen Christgau"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18269v1",
                "updated": "2025-10-21T03:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T03:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "title": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding"
                },
                "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth."
                },
                "authors": [
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.08872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08872v3",
                "updated": "2025-11-03T18:54:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    54,
                    17,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-10T00:05:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    0,
                    5,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Social Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Social Welfare"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a social welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and social welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a social welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and social welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign ."
                },
                "authors": [
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "David Zhang"
                    },
                    {
                        "name": "Pedro Cisneros-Velarde"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "arxiv_doi": "10.48550/arXiv.2510.08872",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.48550/arXiv.2510.08872",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.08872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "31 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02085v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02085v6",
                "updated": "2025-11-03T18:47:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    47,
                    32,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-04T05:51:55Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    5,
                    51,
                    55,
                    0,
                    216,
                    0
                ],
                "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents"
                },
                "summary": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent."
                },
                "authors": [
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Yifu Guo"
                    },
                    {
                        "name": "Yuzhen Han"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Licheng Wang"
                    },
                    {
                        "name": "Mingguang Chen"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Huacan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huacan Wang"
                },
                "author": "Huacan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02085v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02085v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13030v2",
                "updated": "2025-11-03T18:29:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    29,
                    21,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-16T01:42:52Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    1,
                    42,
                    52,
                    0,
                    167,
                    0
                ],
                "title": "WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild"
                },
                "summary": "Despite recent advances in sparse novel view synthesis (NVS) applied to\nobject-centric scenes, scene-level NVS remains a challenge. A central issue is\nthe lack of available clean multi-view training data, beyond manually curated\ndatasets with limited diversity, camera variation, or licensing issues. On the\nother hand, an abundance of diverse and permissively-licensed data exists in\nthe wild, consisting of scenes with varying appearances (illuminations,\ntransient occlusions, etc.) from sources such as tourist photos. To this end,\nwe present WildCAT3D, a framework for generating novel views of scenes learned\nfrom diverse 2D scene image data captured in the wild. We unlock training on\nthese data sources by explicitly modeling global appearance conditions in\nimages, extending the state-of-the-art multi-view diffusion paradigm to learn\nfrom scene views of varying appearances. Our trained model generalizes to new\nscenes at inference time, enabling the generation of multiple consistent novel\nviews. WildCAT3D provides state-of-the-art results on single-view NVS in\nobject- and scene-level settings, while training on strictly less data sources\nthan prior methods. Additionally, it enables novel applications by providing\nglobal appearance control during generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in sparse novel view synthesis (NVS) applied to\nobject-centric scenes, scene-level NVS remains a challenge. A central issue is\nthe lack of available clean multi-view training data, beyond manually curated\ndatasets with limited diversity, camera variation, or licensing issues. On the\nother hand, an abundance of diverse and permissively-licensed data exists in\nthe wild, consisting of scenes with varying appearances (illuminations,\ntransient occlusions, etc.) from sources such as tourist photos. To this end,\nwe present WildCAT3D, a framework for generating novel views of scenes learned\nfrom diverse 2D scene image data captured in the wild. We unlock training on\nthese data sources by explicitly modeling global appearance conditions in\nimages, extending the state-of-the-art multi-view diffusion paradigm to learn\nfrom scene views of varying appearances. Our trained model generalizes to new\nscenes at inference time, enabling the generation of multiple consistent novel\nviews. WildCAT3D provides state-of-the-art results on single-view NVS in\nobject- and scene-level settings, while training on strictly less data sources\nthan prior methods. Additionally, it enables novel applications by providing\nglobal appearance control during generation."
                },
                "authors": [
                    {
                        "name": "Morris Alper"
                    },
                    {
                        "name": "David Novotny"
                    },
                    {
                        "name": "Filippos Kokkinos"
                    },
                    {
                        "name": "Hadar Averbuch-Elor"
                    },
                    {
                        "name": "Tom Monnier"
                    }
                ],
                "author_detail": {
                    "name": "Tom Monnier"
                },
                "author": "Tom Monnier",
                "arxiv_comment": "Accepted to NeurIPS 2025. Project page: https://wildcat3d.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12828v2",
                "updated": "2025-11-03T18:20:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    37,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-11T20:07:54Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    20,
                    7,
                    54,
                    5,
                    284,
                    0
                ],
                "title": "SimKey: A Semantically Aware Key Module for Watermarking Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimKey: A Semantically Aware Key Module for Watermarking Language Models"
                },
                "summary": "The rapid spread of text generated by large language models (LLMs) makes it\nincreasingly difficult to distinguish authentic human writing from machine\noutput. Watermarking offers a promising solution: model owners can embed an\nimperceptible signal into generated text, marking its origin. Most leading\napproaches seed an LLM's next-token sampling with a pseudo-random key that can\nlater be recovered to identify the text as machine-generated, while only\nminimally altering the model's output distribution. However, these methods\nsuffer from two related issues: (i) watermarks are brittle to simple\nsurface-level edits such as paraphrasing or reordering; and (ii) adversaries\ncan append unrelated, potentially harmful text that inherits the watermark,\nrisking reputational damage to model owners. To address these issues, we\nintroduce SimKey, a semantic key module that strengthens watermark robustness\nby tying key generation to the meaning of prior context. SimKey uses\nlocality-sensitive hashing over semantic embeddings to ensure that paraphrased\ntext yields the same watermark key, while unrelated or semantically shifted\ntext produces a different one. Integrated with state-of-the-art watermarking\nschemes, SimKey improves watermark robustness to paraphrasing and translation\nwhile preventing harmful content from false attribution, establishing\nsemantic-aware keying as a practical and extensible watermarking direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of text generated by large language models (LLMs) makes it\nincreasingly difficult to distinguish authentic human writing from machine\noutput. Watermarking offers a promising solution: model owners can embed an\nimperceptible signal into generated text, marking its origin. Most leading\napproaches seed an LLM's next-token sampling with a pseudo-random key that can\nlater be recovered to identify the text as machine-generated, while only\nminimally altering the model's output distribution. However, these methods\nsuffer from two related issues: (i) watermarks are brittle to simple\nsurface-level edits such as paraphrasing or reordering; and (ii) adversaries\ncan append unrelated, potentially harmful text that inherits the watermark,\nrisking reputational damage to model owners. To address these issues, we\nintroduce SimKey, a semantic key module that strengthens watermark robustness\nby tying key generation to the meaning of prior context. SimKey uses\nlocality-sensitive hashing over semantic embeddings to ensure that paraphrased\ntext yields the same watermark key, while unrelated or semantically shifted\ntext produces a different one. Integrated with state-of-the-art watermarking\nschemes, SimKey improves watermark robustness to paraphrasing and translation\nwhile preventing harmful content from false attribution, establishing\nsemantic-aware keying as a practical and extensible watermarking direction."
                },
                "authors": [
                    {
                        "name": "Shingo Kodama"
                    },
                    {
                        "name": "Haya Diwan"
                    },
                    {
                        "name": "Lucas Rosenblatt"
                    },
                    {
                        "name": "R. Teal Witter"
                    },
                    {
                        "name": "Niv Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Niv Cohen"
                },
                "author": "Niv Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19916v2",
                "updated": "2025-11-03T18:20:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    2,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-22T18:00:03Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    18,
                    0,
                    3,
                    2,
                    295,
                    0
                ],
                "title": "Constraining the Swift Memory Burden Effect with GW250114-like Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the Swift Memory Burden Effect with GW250114-like Events"
                },
                "summary": "Black hole spectroscopy allows to infer the properties of the remnant of a\nbinary black hole coalescence. Motivated by the recent proposal that a black\nhole's information load can alter its classical response to small\nperturbations, an effect known as the swift memory burden, we develop a minimal\nphenomenological framework to analyze the ringdown of a binary black hole\nmerger and confront it with the data from the GW250114 event. We perform a\nBayesian analysis combining the frequencies of the (220) and (440) quasi-normal\nmodes and obtain a lower bound $\\log_{10}p \\gtrsim 2$, where $p$ controls how\nthe gaps reopen when the black hole's master mode occupation departs from the\ncritical value. Moreover, using a Fisher information matrix (high\nsignal-to-noise ratio) approximation, we forecast the lower bound $\\log_{10}p\n\\gtrsim 3$ for a GW250114-like event observed with Cosmic Explorer or Einstein\nTelescope. Our results disfavour rapid gap reopening, shedding light on how the\nswift memory burden effect can be probed with current and next-generation\ndetectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black hole spectroscopy allows to infer the properties of the remnant of a\nbinary black hole coalescence. Motivated by the recent proposal that a black\nhole's information load can alter its classical response to small\nperturbations, an effect known as the swift memory burden, we develop a minimal\nphenomenological framework to analyze the ringdown of a binary black hole\nmerger and confront it with the data from the GW250114 event. We perform a\nBayesian analysis combining the frequencies of the (220) and (440) quasi-normal\nmodes and obtain a lower bound $\\log_{10}p \\gtrsim 2$, where $p$ controls how\nthe gaps reopen when the black hole's master mode occupation departs from the\ncritical value. Moreover, using a Fisher information matrix (high\nsignal-to-noise ratio) approximation, we forecast the lower bound $\\log_{10}p\n\\gtrsim 3$ for a GW250114-like event observed with Cosmic Explorer or Einstein\nTelescope. Our results disfavour rapid gap reopening, shedding light on how the\nswift memory burden effect can be probed with current and next-generation\ndetectors."
                },
                "authors": [
                    {
                        "name": "Chen Yuan"
                    },
                    {
                        "name": "Richard Brito"
                    }
                ],
                "author_detail": {
                    "name": "Richard Brito"
                },
                "author": "Richard Brito",
                "arxiv_comment": "8 pages, two figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19606v2",
                "updated": "2025-11-03T17:59:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    59,
                    28,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-22T14:00:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    0,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Revisiting the Radio Lateral Distribution Function: An amplitude\n  dependence on $X_{\\rm max}$ and primary composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Radio Lateral Distribution Function: An amplitude\n  dependence on $X_{\\rm max}$ and primary composition"
                },
                "summary": "We show that there is a strong dependence of the radio LDF electric field\namplitudes at ground level on the position of $X_{\\rm max}$ in the atmosphere,\neven accounting for differences in the EM energy of the showers. Since an\n$X_{\\rm max}$ dependence leads to a primary composition dependence, this\nimplies that information on the mass composition is encoded not only in the LDF\nshape but also in its amplitude. This $X_{\\rm max}$ dependence can be explained\nin terms of two competing scalings of the measured electric field: One goes\nwith $(1/\\rho)^J$, where $\\rho$ is the air density at $X_{\\rm max}$ and $J$ is\na zenith dependent non-linearity factor describing coherence loss. This density\nscaling tends to decrease the geomagnetic emission of deeper showers. The other\nscaling goes with $(1/R)$, where $R$ is the distance from $X_{\\rm max}$ to the\ncore at ground, and instead increases the measured electric field of deeper\nshowers. At low zenith angles, the $(1/R)$ scaling is stronger and leads to\nlarger measured electric fields as $X_{\\rm max}$ increases. The picture at\nhigher zeniths, i.e., lower densities, is more nuanced. In this region, the\ndeflections due to the Lorentz force are much larger and introduce extra time\ndelays between the particle tracks, decreasing the coherence of the emission.\nThis loss of coherence is highly dependent on the strength of the geomagnetic\nfield and can slow down, or even reverse the increase of the radio emission\nwith decreasing air density. This strong, yet historically overlooked LDF\namplitude dependence on $X_{\\rm max}$/composition could be used to directly\ninfer, even bypassing any $X_{\\rm max}$ reconstruction, the cosmic ray primary\ncomposition on an event-by-event basis. It could also have some repercussions\non other radio reconstruction methods, such as a possible $X_{\\rm\nmax}$/composition bias on shower electromagnetic energy reconstruction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that there is a strong dependence of the radio LDF electric field\namplitudes at ground level on the position of $X_{\\rm max}$ in the atmosphere,\neven accounting for differences in the EM energy of the showers. Since an\n$X_{\\rm max}$ dependence leads to a primary composition dependence, this\nimplies that information on the mass composition is encoded not only in the LDF\nshape but also in its amplitude. This $X_{\\rm max}$ dependence can be explained\nin terms of two competing scalings of the measured electric field: One goes\nwith $(1/\\rho)^J$, where $\\rho$ is the air density at $X_{\\rm max}$ and $J$ is\na zenith dependent non-linearity factor describing coherence loss. This density\nscaling tends to decrease the geomagnetic emission of deeper showers. The other\nscaling goes with $(1/R)$, where $R$ is the distance from $X_{\\rm max}$ to the\ncore at ground, and instead increases the measured electric field of deeper\nshowers. At low zenith angles, the $(1/R)$ scaling is stronger and leads to\nlarger measured electric fields as $X_{\\rm max}$ increases. The picture at\nhigher zeniths, i.e., lower densities, is more nuanced. In this region, the\ndeflections due to the Lorentz force are much larger and introduce extra time\ndelays between the particle tracks, decreasing the coherence of the emission.\nThis loss of coherence is highly dependent on the strength of the geomagnetic\nfield and can slow down, or even reverse the increase of the radio emission\nwith decreasing air density. This strong, yet historically overlooked LDF\namplitude dependence on $X_{\\rm max}$/composition could be used to directly\ninfer, even bypassing any $X_{\\rm max}$ reconstruction, the cosmic ray primary\ncomposition on an event-by-event basis. It could also have some repercussions\non other radio reconstruction methods, such as a possible $X_{\\rm\nmax}$/composition bias on shower electromagnetic energy reconstruction methods."
                },
                "authors": [
                    {
                        "name": "Washington R. Carvalho Jr."
                    },
                    {
                        "name": "Lech Wiktor Piotrowski"
                    }
                ],
                "author_detail": {
                    "name": "Lech Wiktor Piotrowski"
                },
                "author": "Lech Wiktor Piotrowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00331v2",
                "updated": "2025-11-03T17:57:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    57,
                    29,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-01T06:12:35Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    6,
                    12,
                    35,
                    3,
                    121,
                    0
                ],
                "title": "Geodesic Synthetic Control Methods for Random Objects and Functional\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geodesic Synthetic Control Methods for Random Objects and Functional\n  Data"
                },
                "summary": "We introduce a geodesic synthetic control method for causal inference that\nextends existing synthetic control methods to scenarios where outcomes are\nelements in a geodesic metric space rather than scalars. Examples of such\noutcomes include distributions, compositions, networks, trees and functional\ndata, among other data types that can be viewed as elements of a geodesic\nmetric space given a suitable metric. We extend this further to geodesic\nsynthetic difference-in-differences that builds on the established synthetic\ndifference-in-differences for Euclidean outcomes. This estimator generalizes\nboth the geodesic synthetic control method and a previously proposed geodesic\ndifference-in-differences method and exhibits a double robustness property. The\nproposed geodesic synthetic control method is illustrated through comprehensive\nsimulation studies and applications to the employment composition changes\nfollowing the 2011 Great East Japan Earthquake, and the impact of abortion\nliberalization policy on fertility patterns in East Germany. We illustrate the\nproposed geodesic synthetic difference-in-differences by studying the\nconsequences of the Soviet Union's collapse on age-at-death distributions for\nmales and females.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a geodesic synthetic control method for causal inference that\nextends existing synthetic control methods to scenarios where outcomes are\nelements in a geodesic metric space rather than scalars. Examples of such\noutcomes include distributions, compositions, networks, trees and functional\ndata, among other data types that can be viewed as elements of a geodesic\nmetric space given a suitable metric. We extend this further to geodesic\nsynthetic difference-in-differences that builds on the established synthetic\ndifference-in-differences for Euclidean outcomes. This estimator generalizes\nboth the geodesic synthetic control method and a previously proposed geodesic\ndifference-in-differences method and exhibits a double robustness property. The\nproposed geodesic synthetic control method is illustrated through comprehensive\nsimulation studies and applications to the employment composition changes\nfollowing the 2011 Great East Japan Earthquake, and the impact of abortion\nliberalization policy on fertility patterns in East Germany. We illustrate the\nproposed geodesic synthetic difference-in-differences by studying the\nconsequences of the Soviet Union's collapse on age-at-death distributions for\nmales and females."
                },
                "authors": [
                    {
                        "name": "Daisuke Kurisu"
                    },
                    {
                        "name": "Yidong Zhou"
                    },
                    {
                        "name": "Taisuke Otsu"
                    },
                    {
                        "name": "Hans-Georg Mller"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Georg Mller"
                },
                "author": "Hans-Georg Mller",
                "arxiv_comment": "58 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D20, 62R20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21041v2",
                "updated": "2025-11-03T17:16:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    16,
                    42,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-23T22:51:57Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    22,
                    51,
                    57,
                    3,
                    296,
                    0
                ],
                "title": "Gaussian Processes for Inferring Parton Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Processes for Inferring Parton Distributions"
                },
                "summary": "The extraction of parton distribution functions (PDFs) from experimental or\nlattice QCD data is an ill-posed inverse problem, where regularization strongly\nimpacts both systematic uncertainties and the reliability of the results. We\nstudy a framework based on Gaussian Process Regression (GPR) to reconstruct\nPDFs from lattice QCD matrix elements. Within a Bayesian framework, Gaussian\nprocesses serve as flexible priors that encode uncertainties, correlations, and\nconstraints without imposing rigid functional forms. We investigate a wide\nrange of kernel choices, mean functions, and hyperparameter treatments. We\nquantify information gained from the data using the Kullback Leibler\ndivergence. Synthetic data tests demonstrate the consistency and robustness of\nthe method. Our study establishes GPR as a systematic and non-parametric\napproach to PDF reconstruction, offering controlled uncertainty estimates and\nreduced model bias in lattice QCD analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extraction of parton distribution functions (PDFs) from experimental or\nlattice QCD data is an ill-posed inverse problem, where regularization strongly\nimpacts both systematic uncertainties and the reliability of the results. We\nstudy a framework based on Gaussian Process Regression (GPR) to reconstruct\nPDFs from lattice QCD matrix elements. Within a Bayesian framework, Gaussian\nprocesses serve as flexible priors that encode uncertainties, correlations, and\nconstraints without imposing rigid functional forms. We investigate a wide\nrange of kernel choices, mean functions, and hyperparameter treatments. We\nquantify information gained from the data using the Kullback Leibler\ndivergence. Synthetic data tests demonstrate the consistency and robustness of\nthe method. Our study establishes GPR as a systematic and non-parametric\napproach to PDF reconstruction, offering controlled uncertainty estimates and\nreduced model bias in lattice QCD analyses."
                },
                "authors": [
                    {
                        "name": "Yamil Cahuana Medrano"
                    },
                    {
                        "name": "Herv Dutrieux"
                    },
                    {
                        "name": "Joseph Karpie"
                    },
                    {
                        "name": "Kostas Orginos"
                    },
                    {
                        "name": "Savvas Zafeiropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Savvas Zafeiropoulos"
                },
                "author": "Savvas Zafeiropoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-lat",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06429v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06429v2",
                "updated": "2025-11-03T17:13:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    13,
                    55,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-06T18:00:02Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    0,
                    2,
                    4,
                    157,
                    0
                ],
                "title": "Maximizing Ariel's Survey Leverage for Population-Level Studies of\n  Exoplanets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximizing Ariel's Survey Leverage for Population-Level Studies of\n  Exoplanets"
                },
                "summary": "ESA's Ariel mission will be uniquely suited to performing population-level\nstudies of exoplanets. Most of these studies consist of quantifying trends\nbetween an Ariel-measured quantity, y, and an a priori planetary property, x;\nfor example, atmospheric metallicity as inferred from Ariel transit\nspectroscopy vs. planetary mass. The precision with which we can quantify such\ntrends depends on the number of targets in the survey and their variance in the\na priori parameter. We define the leverage of a survey with N targets as L =\nsqrt(N)stdev(x) and show that it quantitatively predicts the precision of\npopulation-level trends. The target selection challenge of Ariel can therefore\nbe summarized as maximizing L along some axes of diversity for a given\ncumulative observing time. To this end, we consider different schemes to select\nthe mission reference sample for a notional three year transit spectroscopy\nsurvey with Ariel. We divide the exoplanets in the mission candidate sample\ninto logarithmic classes based on radius, equilibrium temperature and host star\ntemperature. We then construct a target list by cyclically choosing the easiest\nremaining target in each class. We find that the leverage on a single axis of\ndiversity can be increased by dividing that axis into many classes, but this\nsacrifices leverage along other axes of diversity. We conclude that a modest\nnumber of classes, possibly only one, should be defined when selecting Ariel\ntargets. Lastly, we note that the statistical leverage of the Ariel transit\nsurvey would be significantly increased if current candidate planets were\nconfirmed. This highlights the urgency of vetting and confirming the easiest\ntransmission and emission spectroscopy targets in the Ariel mission candidate\nsample.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESA's Ariel mission will be uniquely suited to performing population-level\nstudies of exoplanets. Most of these studies consist of quantifying trends\nbetween an Ariel-measured quantity, y, and an a priori planetary property, x;\nfor example, atmospheric metallicity as inferred from Ariel transit\nspectroscopy vs. planetary mass. The precision with which we can quantify such\ntrends depends on the number of targets in the survey and their variance in the\na priori parameter. We define the leverage of a survey with N targets as L =\nsqrt(N)stdev(x) and show that it quantitatively predicts the precision of\npopulation-level trends. The target selection challenge of Ariel can therefore\nbe summarized as maximizing L along some axes of diversity for a given\ncumulative observing time. To this end, we consider different schemes to select\nthe mission reference sample for a notional three year transit spectroscopy\nsurvey with Ariel. We divide the exoplanets in the mission candidate sample\ninto logarithmic classes based on radius, equilibrium temperature and host star\ntemperature. We then construct a target list by cyclically choosing the easiest\nremaining target in each class. We find that the leverage on a single axis of\ndiversity can be increased by dividing that axis into many classes, but this\nsacrifices leverage along other axes of diversity. We conclude that a modest\nnumber of classes, possibly only one, should be defined when selecting Ariel\ntargets. Lastly, we note that the statistical leverage of the Ariel transit\nsurvey would be significantly increased if current candidate planets were\nconfirmed. This highlights the urgency of vetting and confirming the easiest\ntransmission and emission spectroscopy targets in the Ariel mission candidate\nsample."
                },
                "authors": [
                    {
                        "name": "Nicolas B. Cowan"
                    },
                    {
                        "name": "Ben Coull-Neveu"
                    }
                ],
                "author_detail": {
                    "name": "Ben Coull-Neveu"
                },
                "arxiv_affiliation": "McGill University",
                "author": "Ben Coull-Neveu",
                "arxiv_comment": "8 pages, 5 figures, Published in the Open Journal of Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06429v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06429v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07653v2",
                "updated": "2025-11-03T17:13:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    13,
                    50,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-12T15:22:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "JobHop: A Large-Scale Dataset of Career Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JobHop: A Large-Scale Dataset of Career Trajectories"
                },
                "summary": "Understanding labor market dynamics is essential for policymakers, employers,\nand job seekers. However, comprehensive datasets that capture real-world career\ntrajectories are scarce. In this paper, we introduce JobHop, a large-scale\npublic dataset derived from anonymized resumes provided by VDAB, the public\nemployment service in Flanders, Belgium. Utilizing Large Language Models\n(LLMs), we process unstructured resume data to extract structured career\ninformation, which is then normalized to standardized ESCO occupation codes\nusing a multi-label classification model. This results in a rich dataset of\nover 1.67 million work experiences, extracted from and grouped into more than\n361,000 user resumes and mapped to standardized ESCO occupation codes, offering\nvaluable insights into real-world occupational transitions. This dataset\nenables diverse applications, such as analyzing labor market mobility, job\nstability, and the effects of career breaks on occupational transitions. It\nalso supports career path prediction and other data-driven decision-making\nprocesses. To illustrate its potential, we explore key dataset characteristics,\nincluding job distributions, career breaks, and job transitions, demonstrating\nits value for advancing labor market research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding labor market dynamics is essential for policymakers, employers,\nand job seekers. However, comprehensive datasets that capture real-world career\ntrajectories are scarce. In this paper, we introduce JobHop, a large-scale\npublic dataset derived from anonymized resumes provided by VDAB, the public\nemployment service in Flanders, Belgium. Utilizing Large Language Models\n(LLMs), we process unstructured resume data to extract structured career\ninformation, which is then normalized to standardized ESCO occupation codes\nusing a multi-label classification model. This results in a rich dataset of\nover 1.67 million work experiences, extracted from and grouped into more than\n361,000 user resumes and mapped to standardized ESCO occupation codes, offering\nvaluable insights into real-world occupational transitions. This dataset\nenables diverse applications, such as analyzing labor market mobility, job\nstability, and the effects of career breaks on occupational transitions. It\nalso supports career path prediction and other data-driven decision-making\nprocesses. To illustrate its potential, we explore key dataset characteristics,\nincluding job distributions, career breaks, and job transitions, demonstrating\nits value for advancing labor market research."
                },
                "authors": [
                    {
                        "name": "Iman Johary"
                    },
                    {
                        "name": "Raphael Romero"
                    },
                    {
                        "name": "Alexandru C. Mara"
                    },
                    {
                        "name": "Tijl De Bie"
                    }
                ],
                "author_detail": {
                    "name": "Tijl De Bie"
                },
                "author": "Tijl De Bie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09905v2",
                "updated": "2025-11-03T17:09:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    9,
                    11,
                    0,
                    307,
                    0
                ],
                "published": "2025-07-14T04:21:23Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    4,
                    21,
                    23,
                    0,
                    195,
                    0
                ],
                "title": "Statistical Analysis of Conditional Group Distributionally Robust\n  Optimization with Cross-Entropy Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Analysis of Conditional Group Distributionally Robust\n  Optimization with Cross-Entropy Loss"
                },
                "summary": "In multi-source learning with discrete labels, distributional heterogeneity\nacross domains poses a central challenge to developing predictive models that\ntransfer reliably to unseen domains. We study multi-source unsupervised domain\nadaptation, where labeled data are available from multiple source domains and\nonly unlabeled data are observed from the target domain. To address potential\ndistribution shifts, we propose a novel Conditional Group Distributionally\nRobust Optimization (CG-DRO) framework that learns a classifier by minimizing\nthe worst-case cross-entropy loss over the convex combinations of the\nconditional outcome distributions from sources domains. We develop an efficient\nMirror Prox algorithm for solving the minimax problem and employ a double\nmachine learning procedure to estimate the risk function, ensuring that errors\nin nuisance estimation contribute only at higher-order rates. We establish fast\nstatistical convergence rates for the empirical CG-DRO estimator by\nconstructing two surrogate minimax optimization problems that serve as\ntheoretical bridges. A distinguishing challenge for CG-DRO is the emergence of\nnonstandard asymptotics: the empirical CG-DRO estimator may fail to converge to\na standard limiting distribution due to boundary effects and system\ninstability. To address this, we introduce a perturbation-based inference\nprocedure that enables uniformly valid inference, including confidence interval\nconstruction and hypothesis testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-source learning with discrete labels, distributional heterogeneity\nacross domains poses a central challenge to developing predictive models that\ntransfer reliably to unseen domains. We study multi-source unsupervised domain\nadaptation, where labeled data are available from multiple source domains and\nonly unlabeled data are observed from the target domain. To address potential\ndistribution shifts, we propose a novel Conditional Group Distributionally\nRobust Optimization (CG-DRO) framework that learns a classifier by minimizing\nthe worst-case cross-entropy loss over the convex combinations of the\nconditional outcome distributions from sources domains. We develop an efficient\nMirror Prox algorithm for solving the minimax problem and employ a double\nmachine learning procedure to estimate the risk function, ensuring that errors\nin nuisance estimation contribute only at higher-order rates. We establish fast\nstatistical convergence rates for the empirical CG-DRO estimator by\nconstructing two surrogate minimax optimization problems that serve as\ntheoretical bridges. A distinguishing challenge for CG-DRO is the emergence of\nnonstandard asymptotics: the empirical CG-DRO estimator may fail to converge to\na standard limiting distribution due to boundary effects and system\ninstability. To address this, we introduce a perturbation-based inference\nprocedure that enables uniformly valid inference, including confidence interval\nconstruction and hypothesis testing."
                },
                "authors": [
                    {
                        "name": "Zijian Guo"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Francis Bach"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bach"
                },
                "author": "Francis Bach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15715v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15715v2",
                "updated": "2025-11-03T17:03:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    3,
                    30,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-21T16:24:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    24,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with\n  Large Language Models for Mental Health Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with\n  Large Language Models for Mental Health Counseling"
                },
                "summary": "Large language models (LLMs) hold significant potential for mental health\nsupport, capable of generating empathetic responses and simulating therapeutic\nconversations. However, existing LLM-based approaches often lack the clinical\ngrounding necessary for real-world psychological counseling, particularly in\nexplicit diagnostic reasoning aligned with standards like the DSM/ICD and\nincorporating diverse therapeutic modalities beyond basic empathy or single\nstrategies. To address these critical limitations, we propose PsyLLM, the first\nlarge language model designed to systematically integrate both diagnostic and\ntherapeutic reasoning for mental health counseling. To develop PsyLLM, we\ndesign a novel automated data synthesis pipeline that processes real-world\nmental health posts collected from Reddit, where users frequently share\npsychological distress and seek community support. This pipeline processes\nreal-world mental health posts, generates multi-turn dialogue structures, and\nleverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and\nmultiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate\ndetailed clinical reasoning processes. Rigorous multi-dimensional filtering\nensures the generation of high-quality, clinically aligned dialogue data. In\naddition, we introduce a new benchmark and evaluation protocol, assessing\ncounseling quality across four key dimensions. Our experiments demonstrate that\nPsyLLM significantly outperforms state-of-the-art baseline models on this\nbenchmark. The model weights and dataset have been publicly released at\nhttps://github.com/Emo-gml/PsyLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold significant potential for mental health\nsupport, capable of generating empathetic responses and simulating therapeutic\nconversations. However, existing LLM-based approaches often lack the clinical\ngrounding necessary for real-world psychological counseling, particularly in\nexplicit diagnostic reasoning aligned with standards like the DSM/ICD and\nincorporating diverse therapeutic modalities beyond basic empathy or single\nstrategies. To address these critical limitations, we propose PsyLLM, the first\nlarge language model designed to systematically integrate both diagnostic and\ntherapeutic reasoning for mental health counseling. To develop PsyLLM, we\ndesign a novel automated data synthesis pipeline that processes real-world\nmental health posts collected from Reddit, where users frequently share\npsychological distress and seek community support. This pipeline processes\nreal-world mental health posts, generates multi-turn dialogue structures, and\nleverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and\nmultiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate\ndetailed clinical reasoning processes. Rigorous multi-dimensional filtering\nensures the generation of high-quality, clinically aligned dialogue data. In\naddition, we introduce a new benchmark and evaluation protocol, assessing\ncounseling quality across four key dimensions. Our experiments demonstrate that\nPsyLLM significantly outperforms state-of-the-art baseline models on this\nbenchmark. The model weights and dataset have been publicly released at\nhttps://github.com/Emo-gml/PsyLLM."
                },
                "authors": [
                    {
                        "name": "He Hu"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Juzheng Si"
                    },
                    {
                        "name": "Qianning Wang"
                    },
                    {
                        "name": "Hengheng Zhang"
                    },
                    {
                        "name": "Fuji Ren"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Laizhong Cui"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15715v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15715v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18418v2",
                "updated": "2025-11-03T16:59:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    59,
                    59,
                    0,
                    307,
                    0
                ],
                "published": "2025-07-24T13:57:24Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    57,
                    24,
                    3,
                    205,
                    0
                ],
                "title": "Distributing Retractions, Weak Distributive Laws and Applications to\n  Monads of Hyperspaces, Continuous Valuations and Measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributing Retractions, Weak Distributive Laws and Applications to\n  Monads of Hyperspaces, Continuous Valuations and Measures"
                },
                "summary": "Given two monads $S$, $T$ on a category where idempotents split, and a weak\ndistributive law between them, one can build a combined monad $U$. Making\nexplicit what this monad $U$ is requires some effort. When we already have an\nidea what $U$ should be, we show how to recognize that $U$ is indeed the\ncombined monad obtained from $S$ and $T$: it suffices to exhibit what we call a\ndistributing retraction of $ST$ onto $U$. We show that distributing retractions\nand weak distributive laws are in one-to-one correspondence, in a 2-categorical\nsetting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin\nhyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad\nof previsions or of forks, depending on the case. As a byproduct, this allows\nus to describe the algebras of monads of superlinear, resp. sublinear\nprevisions. In the category of compact Hausdorff spaces, the Plotkin hyperspace\nmonad is sometimes known as the Vietoris monad, the monad of probability\nvaluations coincides with the Radon monad, and we infer that the associated\ncombined monad is the monad of normalized forks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given two monads $S$, $T$ on a category where idempotents split, and a weak\ndistributive law between them, one can build a combined monad $U$. Making\nexplicit what this monad $U$ is requires some effort. When we already have an\nidea what $U$ should be, we show how to recognize that $U$ is indeed the\ncombined monad obtained from $S$ and $T$: it suffices to exhibit what we call a\ndistributing retraction of $ST$ onto $U$. We show that distributing retractions\nand weak distributive laws are in one-to-one correspondence, in a 2-categorical\nsetting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin\nhyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad\nof previsions or of forks, depending on the case. As a byproduct, this allows\nus to describe the algebras of monads of superlinear, resp. sublinear\nprevisions. In the category of compact Hausdorff spaces, the Plotkin hyperspace\nmonad is sometimes known as the Vietoris monad, the monad of probability\nvaluations coincides with the Radon monad, and we infer that the associated\ncombined monad is the monad of normalized forks."
                },
                "authors": [
                    {
                        "name": "Jean Goubault-Larrecq"
                    }
                ],
                "author_detail": {
                    "name": "Jean Goubault-Larrecq"
                },
                "author": "Jean Goubault-Larrecq",
                "arxiv_comment": "47 pages. Fixed a minor bug by adding Lemma 2.15",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18N15, 18C15 (Primary) 54B20, 28A33, 46E27 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24448v2",
                "updated": "2025-11-03T16:32:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    32,
                    22,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-28T14:12:11Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    12,
                    11,
                    1,
                    301,
                    0
                ],
                "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Intelligence: Insights from Video Pretraining"
                },
                "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models."
                },
                "authors": [
                    {
                        "name": "Pablo Acuaviva"
                    },
                    {
                        "name": "Aram Davtyan"
                    },
                    {
                        "name": "Mariam Hassan"
                    },
                    {
                        "name": "Sebastian Stapf"
                    },
                    {
                        "name": "Ahmad Rahimi"
                    },
                    {
                        "name": "Alexandre Alahi"
                    },
                    {
                        "name": "Paolo Favaro"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Favaro"
                },
                "author": "Paolo Favaro",
                "arxiv_comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T45, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8; I.5.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17103v2",
                "updated": "2025-11-03T16:31:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    31,
                    16,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-21T08:50:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    50,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "Forging Time Series with Language: A Large Language Model Approach to\n  Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forging Time Series with Language: A Large Language Model Approach to\n  Synthetic Data Generation"
                },
                "summary": "SDForger is a flexible and efficient framework for generating high-quality\nmultivariate time series using LLMs. Leveraging a compact data representation,\nSDForger provides synthetic time series generation from a few samples and\nlow-computation fine-tuning of any autoregressive LLM. Specifically, the\nframework transforms univariate and multivariate signals into tabular\nembeddings, which are then encoded into text and used to fine-tune the LLM. At\ninference, new textual embeddings are sampled and decoded into synthetic time\nseries that retain the original data's statistical properties and temporal\ndynamics. Across a diverse range of datasets, SDForger outperforms existing\ngenerative models in many scenarios, both in similarity-based evaluations and\ndownstream forecasting tasks. By enabling textual conditioning in the\ngeneration process, SDForger paves the way for multimodal modeling and the\nstreamlined integration of time series with textual information. The model is\nopen-sourced at\nhttps://github.com/IBM/fms-dgt/tree/main/fms_dgt/public/databuilders/time_series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDForger is a flexible and efficient framework for generating high-quality\nmultivariate time series using LLMs. Leveraging a compact data representation,\nSDForger provides synthetic time series generation from a few samples and\nlow-computation fine-tuning of any autoregressive LLM. Specifically, the\nframework transforms univariate and multivariate signals into tabular\nembeddings, which are then encoded into text and used to fine-tune the LLM. At\ninference, new textual embeddings are sampled and decoded into synthetic time\nseries that retain the original data's statistical properties and temporal\ndynamics. Across a diverse range of datasets, SDForger outperforms existing\ngenerative models in many scenarios, both in similarity-based evaluations and\ndownstream forecasting tasks. By enabling textual conditioning in the\ngeneration process, SDForger paves the way for multimodal modeling and the\nstreamlined integration of time series with textual information. The model is\nopen-sourced at\nhttps://github.com/IBM/fms-dgt/tree/main/fms_dgt/public/databuilders/time_series."
                },
                "authors": [
                    {
                        "name": "Ccile Rousseau"
                    },
                    {
                        "name": "Tobia Boschi"
                    },
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Dhaval Salwala"
                    },
                    {
                        "name": "Alessandra Pascale"
                    },
                    {
                        "name": "Juan Bernabe Moreno"
                    }
                ],
                "author_detail": {
                    "name": "Juan Bernabe Moreno"
                },
                "author": "Juan Bernabe Moreno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13986v2",
                "updated": "2025-11-03T16:28:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    28,
                    24,
                    0,
                    307,
                    0
                ],
                "published": "2025-03-18T07:44:01Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    44,
                    1,
                    1,
                    77,
                    0
                ],
                "title": "Stratified Permutational Berry--Esseen Bounds and Their Applications to\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stratified Permutational Berry--Esseen Bounds and Their Applications to\n  Statistics"
                },
                "summary": "The stratified linear permutation statistic arises in various statistics\nproblems, including stratified and post-stratified survey sampling, stratified\nand post-stratified experiments, conditional permutation tests, etc. Although\nwe can derive the Berry--Esseen bounds for the stratified linear permutation\nstatistic based on existing bounds for the non-stratified statistics, those\nbounds are not sharp, and moreover, this strategy does not work in general\nsettings with heterogeneous strata with varying sizes. We first use Stein's\nmethod to obtain a unified stratified permutational Berry--Esseen bound that\ncan accommodate heterogeneous strata. We then apply the bound to various\nstatistics problems, leading to stronger theoretical quantifications and\nthereby facilitating statistical inference in those problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The stratified linear permutation statistic arises in various statistics\nproblems, including stratified and post-stratified survey sampling, stratified\nand post-stratified experiments, conditional permutation tests, etc. Although\nwe can derive the Berry--Esseen bounds for the stratified linear permutation\nstatistic based on existing bounds for the non-stratified statistics, those\nbounds are not sharp, and moreover, this strategy does not work in general\nsettings with heterogeneous strata with varying sizes. We first use Stein's\nmethod to obtain a unified stratified permutational Berry--Esseen bound that\ncan accommodate heterogeneous strata. We then apply the bound to various\nstatistics problems, leading to stronger theoretical quantifications and\nthereby facilitating statistical inference in those problems."
                },
                "authors": [
                    {
                        "name": "Pengfei Tian"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Peng Ding"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ding"
                },
                "author": "Peng Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62E17, 60F05 (Primary) 62K10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20172v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20172v3",
                "updated": "2025-11-03T16:12:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    12,
                    9,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-24T14:36:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    36,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Benchmarking LLMs in Web API Integration Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs in Web API Integration Tasks"
                },
                "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models (LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present\nWAPIIBench, a dataset and evaluation pipeline designed to assess the ability of\nLLMs to generate web API invocation code. Our experiments with several\nopen-source LLMs reveal that generating API invocations poses a significant\nchallenge, resulting in hallucinated endpoints, incorrect argument usage, and\nother errors. None of the evaluated open-source models was able to solve more\nthan 40% of the tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models (LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present\nWAPIIBench, a dataset and evaluation pipeline designed to assess the ability of\nLLMs to generate web API invocation code. Our experiments with several\nopen-source LLMs reveal that generating API invocations poses a significant\nchallenge, resulting in hallucinated endpoints, incorrect argument usage, and\nother errors. None of the evaluated open-source models was able to solve more\nthan 40% of the tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Maninger"
                    },
                    {
                        "name": "Leon Chemnitz"
                    },
                    {
                        "name": "Amir Molzam Sharifloo"
                    },
                    {
                        "name": "Jannis Brugger"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "To be published in Proceedings of 2025 2nd IEEE/ACM International\n  Conference on AI-powered Software (AIware), Data & Benchmark Track; switched\n  to IEEE conference template",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20172v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20172v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16406v2",
                "updated": "2025-11-03T15:40:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    40,
                    21,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-22T14:13:16Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    13,
                    16,
                    4,
                    234,
                    0
                ],
                "title": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak\n  Prevention for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak\n  Prevention for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner."
                },
                "authors": [
                    {
                        "name": "Guangyu Yang"
                    },
                    {
                        "name": "Jinghong Chen"
                    },
                    {
                        "name": "Jingbiao Mei"
                    },
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Bill Byrne"
                    }
                ],
                "author_detail": {
                    "name": "Bill Byrne"
                },
                "author": "Bill Byrne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21043v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21043v4",
                "updated": "2025-11-03T15:40:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    40,
                    2,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-25T11:48:37Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    11,
                    48,
                    37,
                    3,
                    268,
                    0
                ],
                "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Creativity: A New Frontier in Generalization Abilities"
                },
                "summary": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence."
                },
                "authors": [
                    {
                        "name": "Samuel Schapiro"
                    },
                    {
                        "name": "Sumuk Shashidhar"
                    },
                    {
                        "name": "Alexi Gladstone"
                    },
                    {
                        "name": "Jonah Black"
                    },
                    {
                        "name": "Royce Moon"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "arxiv_comment": "Preprint. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21043v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21043v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08150v2",
                "updated": "2025-11-03T15:39:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    39,
                    18,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-09T21:14:44Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    21,
                    14,
                    44,
                    1,
                    252,
                    0
                ],
                "title": "Verbalized Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Algorithms"
                },
                "summary": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks."
                },
                "authors": [
                    {
                        "name": "Supriya Lall"
                    },
                    {
                        "name": "Christian Farrell"
                    },
                    {
                        "name": "Hari Pathanjaly"
                    },
                    {
                        "name": "Marko Pavic"
                    },
                    {
                        "name": "Sarvesh Chezhian"
                    },
                    {
                        "name": "Masataro Asai"
                    }
                ],
                "author_detail": {
                    "name": "Masataro Asai"
                },
                "author": "Masataro Asai",
                "arxiv_comment": "Accepted in NeurIPS 2025 Workshop on Efficient Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21006v2",
                "updated": "2025-11-03T15:30:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    30,
                    58,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-28T17:08:11Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    8,
                    11,
                    3,
                    240,
                    0
                ],
                "title": "Practical indistinguishability in a gene regulatory network inference\n  problem, a case study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical indistinguishability in a gene regulatory network inference\n  problem, a case study"
                },
                "summary": "Determining mechanistic models of gene regulation, especially underlying\nphenotypic variation, is a central goal of both mathematical biology and modern\nevolutionary biology. However, several challenges, involving both common\ncharacteristics of experimental data and the model development process, remain\nthat limit the discovery of general principles. Even the highest-quality\nexperimental data come with challenges. There are always sources of noise, a\nlimit to how often we can measure the system in time, and it is impossible to\nmeasure all the relevant states that participate in the full underlying\ncomplexity. Additionally, there are usually sources of uncertainty in the\nunderlying biological mechanisms, which give rise to multiple competing model\nstructures. We walk through a case study involving inference of a regulatory\nnetwork structure involved in a developmental decision in the nematode,\n\\textit{Pristonchus pacificus}. In this study, we fit 13,824 distinct\nregulatory network models to gene expression data from three experimental\nconditions to determine which regulatory features are supported by the data. We\ndiscover \\textit{model sets}, or collections of models with shared regulatory\nnetwork features that best fit the data, for each of the three experiments we\nconsidered, and identify a regulatory network in the intersection of the three\nmodel sets. This model describes the data across the experimental conditions\nand exhibits a high degree of positive regulation and interconnectivity between\nthe key regulators, \\textit{eud-1}, \\textit{sult-1}, and \\textit{nhr-40}. While\nthe biological results are specific to the molecular biology of development in\n\\textit{Pristonchus pacificus}, the comparative modeling framework introduced\nhere can be applied to other systems of gene regulation in an evolutionary\ndevelopmental context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining mechanistic models of gene regulation, especially underlying\nphenotypic variation, is a central goal of both mathematical biology and modern\nevolutionary biology. However, several challenges, involving both common\ncharacteristics of experimental data and the model development process, remain\nthat limit the discovery of general principles. Even the highest-quality\nexperimental data come with challenges. There are always sources of noise, a\nlimit to how often we can measure the system in time, and it is impossible to\nmeasure all the relevant states that participate in the full underlying\ncomplexity. Additionally, there are usually sources of uncertainty in the\nunderlying biological mechanisms, which give rise to multiple competing model\nstructures. We walk through a case study involving inference of a regulatory\nnetwork structure involved in a developmental decision in the nematode,\n\\textit{Pristonchus pacificus}. In this study, we fit 13,824 distinct\nregulatory network models to gene expression data from three experimental\nconditions to determine which regulatory features are supported by the data. We\ndiscover \\textit{model sets}, or collections of models with shared regulatory\nnetwork features that best fit the data, for each of the three experiments we\nconsidered, and identify a regulatory network in the intersection of the three\nmodel sets. This model describes the data across the experimental conditions\nand exhibits a high degree of positive regulation and interconnectivity between\nthe key regulators, \\textit{eud-1}, \\textit{sult-1}, and \\textit{nhr-40}. While\nthe biological results are specific to the molecular biology of development in\n\\textit{Pristonchus pacificus}, the comparative modeling framework introduced\nhere can be applied to other systems of gene regulation in an evolutionary\ndevelopmental context."
                },
                "authors": [
                    {
                        "name": "Cody E. FitzGerald"
                    },
                    {
                        "name": "Shelley Reich"
                    },
                    {
                        "name": "Victor Agaba"
                    },
                    {
                        "name": "Arjun Mathur"
                    },
                    {
                        "name": "Michael S. Werner"
                    },
                    {
                        "name": "Niall M. Mangan"
                    }
                ],
                "author_detail": {
                    "name": "Niall M. Mangan"
                },
                "author": "Niall M. Mangan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03665v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03665v4",
                "updated": "2025-11-03T15:21:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    21,
                    13,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-05T17:24:50Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    17,
                    24,
                    50,
                    1,
                    217,
                    0
                ],
                "title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design"
                },
                "summary": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts."
                },
                "authors": [
                    {
                        "name": "Claudiu Leoveanu-Condrei"
                    }
                ],
                "author_detail": {
                    "name": "Claudiu Leoveanu-Condrei"
                },
                "author": "Claudiu Leoveanu-Condrei",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03665v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03665v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.2; I.1.2; D.1.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20318v2",
                "updated": "2025-11-03T15:12:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    12,
                    58,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-24T17:01:50Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    50,
                    2,
                    267,
                    0
                ],
                "title": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices"
                },
                "summary": "The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies such as hunting,\nfencing, use of repellents, and scare tactics. This underscores a critical need\nfor intelligent, autonomous solutions capable of real-time deer detection and\ndeterrence. But the progress in this field is impeded by a significant gap in\nthe literature, mainly the lack of a domain-specific, practical dataset and\nlimited study on the viability of deer detection systems on edge devices. To\naddress this gap, this study presents a comprehensive evaluation of\nstate-of-the-art deep learning models for deer detection in challenging\nreal-world scenarios. We introduce a curated, publicly available dataset of\n3,095 annotated images with bounding box annotations of deer. Then, we provide\nan extensive comparative analysis of 12 model variants across four recent YOLO\narchitectures (v8 to v11). Finally, we evaluated their performance on two\nrepresentative edge computing platforms: the CPU-based Raspberry Pi 5 and the\nGPU-accelerated NVIDIA Jetson AGX Xavier to assess feasibility for real-world\nfield deployment. Results show that the real-time detection performance is not\nfeasible on Raspberry Pi without hardware-specific model optimization, while\nNVIDIA Jetson provides greater than 30 frames per second (FPS) with 's' and 'n'\nseries models. This study also reveals that smaller, architecturally advanced\nmodels such as YOLOv11n, YOLOv8s, and YOLOv9s offer the optimal balance of high\naccuracy (Average Precision (AP) > 0.85) and computational efficiency\n(Inference Time < 34 milliseconds).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies such as hunting,\nfencing, use of repellents, and scare tactics. This underscores a critical need\nfor intelligent, autonomous solutions capable of real-time deer detection and\ndeterrence. But the progress in this field is impeded by a significant gap in\nthe literature, mainly the lack of a domain-specific, practical dataset and\nlimited study on the viability of deer detection systems on edge devices. To\naddress this gap, this study presents a comprehensive evaluation of\nstate-of-the-art deep learning models for deer detection in challenging\nreal-world scenarios. We introduce a curated, publicly available dataset of\n3,095 annotated images with bounding box annotations of deer. Then, we provide\nan extensive comparative analysis of 12 model variants across four recent YOLO\narchitectures (v8 to v11). Finally, we evaluated their performance on two\nrepresentative edge computing platforms: the CPU-based Raspberry Pi 5 and the\nGPU-accelerated NVIDIA Jetson AGX Xavier to assess feasibility for real-world\nfield deployment. Results show that the real-time detection performance is not\nfeasible on Raspberry Pi without hardware-specific model optimization, while\nNVIDIA Jetson provides greater than 30 frames per second (FPS) with 's' and 'n'\nseries models. This study also reveals that smaller, architecturally advanced\nmodels such as YOLOv11n, YOLOv8s, and YOLOv9s offer the optimal balance of high\naccuracy (Average Precision (AP) > 0.85) and computational efficiency\n(Inference Time < 34 milliseconds)."
                },
                "authors": [
                    {
                        "name": "Bishal Adhikari"
                    },
                    {
                        "name": "Jiajia Li"
                    },
                    {
                        "name": "Eric S. Michel"
                    },
                    {
                        "name": "Jacob Dykes"
                    },
                    {
                        "name": "Te-Ming Paul Tseng"
                    },
                    {
                        "name": "Mary Love Tagert"
                    },
                    {
                        "name": "Dong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dong Chen"
                },
                "author": "Dong Chen",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v6",
                "updated": "2025-11-03T15:04:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    4,
                    26,
                    0,
                    307,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gal Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gal Varoquaux"
                },
                "author": "Gal Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09913v2",
                "updated": "2025-11-03T14:11:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    11,
                    25,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-15T02:52:30Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    2,
                    52,
                    30,
                    3,
                    135,
                    0
                ],
                "title": "Probing the Dynamics of Gaussian Dark Energy Equation of State Using\n  DESI BAO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Dynamics of Gaussian Dark Energy Equation of State Using\n  DESI BAO"
                },
                "summary": "We present an updated reconstruction of the DE equation of state (EoS),\n$w(a)$, employing the newly released DESI DR2 Baryon Acoustic Oscillation data.\nThis analysis constrains the cosmological scenarios influenced by different\nmodels through the joint examination of a range of recently available\ncosmological probes, specifically the Pantheon+ sample and the DESY5 sample of\nType Ia Supernovae, baryon acoustic oscillations, Hubble parameter measurements\nderived from cosmic chronometers, and cosmic microwave background distance\npriors based on the Planck 2018 data. Furthermore, we provide a concise\nperspective on the dynamical evolution of all models (CPL, PADE, GEDE, GDE,\nBellDE) and their interrelations. A Bayesian inference procedure is adopted to\nestimate the models parameters that yield the best fit to the data. The EoS\nremains within the phantom regime at higher redshifts, while favoring the\nquintessence regime in the current epoch. In this context, we propose a new\nGaussian-like form of EoS, termed BellDE, which avoids phantom behavior (\\(w\n\\geq -1\\)) at higher redshifts while remaining precisely calibrated at lower\nredshifts. Interestingly, BellDE exhibits a transient phantom nature (\\(w <\n-1\\)) around the transition redshift \\(z \\sim 0.5\\), subsequently evolving into\na quintessential regime (\\(w > -1\\)). In particular, the BellDE model provides\ncompetitive statistical preference while offering greater flexibility in the\nredshift regime $z \\sim 0.5-1$, where DE is observationally significant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an updated reconstruction of the DE equation of state (EoS),\n$w(a)$, employing the newly released DESI DR2 Baryon Acoustic Oscillation data.\nThis analysis constrains the cosmological scenarios influenced by different\nmodels through the joint examination of a range of recently available\ncosmological probes, specifically the Pantheon+ sample and the DESY5 sample of\nType Ia Supernovae, baryon acoustic oscillations, Hubble parameter measurements\nderived from cosmic chronometers, and cosmic microwave background distance\npriors based on the Planck 2018 data. Furthermore, we provide a concise\nperspective on the dynamical evolution of all models (CPL, PADE, GEDE, GDE,\nBellDE) and their interrelations. A Bayesian inference procedure is adopted to\nestimate the models parameters that yield the best fit to the data. The EoS\nremains within the phantom regime at higher redshifts, while favoring the\nquintessence regime in the current epoch. In this context, we propose a new\nGaussian-like form of EoS, termed BellDE, which avoids phantom behavior (\\(w\n\\geq -1\\)) at higher redshifts while remaining precisely calibrated at lower\nredshifts. Interestingly, BellDE exhibits a transient phantom nature (\\(w <\n-1\\)) around the transition redshift \\(z \\sim 0.5\\), subsequently evolving into\na quintessential regime (\\(w > -1\\)). In particular, the BellDE model provides\ncompetitive statistical preference while offering greater flexibility in the\nredshift regime $z \\sim 0.5-1$, where DE is observationally significant."
                },
                "authors": [
                    {
                        "name": "Saddam Hussain"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Anzhong Wang"
                    },
                    {
                        "name": "Ben Rose"
                    }
                ],
                "author_detail": {
                    "name": "Ben Rose"
                },
                "author": "Ben Rose",
                "arxiv_comment": "13 pages, 26 figures, Accepted for the publication in MNRS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21236v2",
                "updated": "2025-11-03T14:09:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    9,
                    54,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-27T14:19:06Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    19,
                    6,
                    1,
                    147,
                    0
                ],
                "title": "Breaking the Performance Ceiling in Reinforcement Learning requires\n  Inference Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Performance Ceiling in Reinforcement Learning requires\n  Inference Strategies"
                },
                "summary": "Reinforcement learning (RL) systems have countless applications, from\nenergy-grid management to protein design. However, such real-world scenarios\nare often extremely difficult, combinatorial in nature, and require complex\ncoordination between multiple agents. This level of complexity can cause even\nstate-of-the-art RL systems, trained until convergence, to hit a performance\nceiling which they are unable to break out of with zero-shot inference.\nMeanwhile, many digital or simulation-based applications allow for an inference\nphase that utilises a specific time and compute budget to explore multiple\nattempts before outputting a final solution. In this work, we show that such an\ninference phase employed at execution time, and the choice of a corresponding\ninference strategy, are key to breaking the performance ceiling observed in\ncomplex multi-agent RL problems. Our main result is striking: we can obtain up\nto a 126% and, on average, a 45% improvement over the previous state-of-the-art\nacross 17 tasks, using only a couple seconds of extra wall-clock time during\nexecution. We also demonstrate promising compute scaling properties, supported\nby over 60k experiments, making it the largest study on inference strategies\nfor complex RL to date. Our experimental data and code are available at\nhttps://sites.google.com/view/inference-strategies-rl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) systems have countless applications, from\nenergy-grid management to protein design. However, such real-world scenarios\nare often extremely difficult, combinatorial in nature, and require complex\ncoordination between multiple agents. This level of complexity can cause even\nstate-of-the-art RL systems, trained until convergence, to hit a performance\nceiling which they are unable to break out of with zero-shot inference.\nMeanwhile, many digital or simulation-based applications allow for an inference\nphase that utilises a specific time and compute budget to explore multiple\nattempts before outputting a final solution. In this work, we show that such an\ninference phase employed at execution time, and the choice of a corresponding\ninference strategy, are key to breaking the performance ceiling observed in\ncomplex multi-agent RL problems. Our main result is striking: we can obtain up\nto a 126% and, on average, a 45% improvement over the previous state-of-the-art\nacross 17 tasks, using only a couple seconds of extra wall-clock time during\nexecution. We also demonstrate promising compute scaling properties, supported\nby over 60k experiments, making it the largest study on inference strategies\nfor complex RL to date. Our experimental data and code are available at\nhttps://sites.google.com/view/inference-strategies-rl."
                },
                "authors": [
                    {
                        "name": "Felix Chalumeau"
                    },
                    {
                        "name": "Daniel Rajaonarivonivelomanantsoa"
                    },
                    {
                        "name": "Ruan de Kock"
                    },
                    {
                        "name": "Claude Formanek"
                    },
                    {
                        "name": "Sasha Abramowitz"
                    },
                    {
                        "name": "Oumayma Mahjoub"
                    },
                    {
                        "name": "Wiem Khlifi"
                    },
                    {
                        "name": "Simon Du Toit"
                    },
                    {
                        "name": "Louay Ben Nessir"
                    },
                    {
                        "name": "Refiloe Shabe"
                    },
                    {
                        "name": "Arnol Fokam"
                    },
                    {
                        "name": "Siddarth Singh"
                    },
                    {
                        "name": "Ulrich Mbou Sob"
                    },
                    {
                        "name": "Arnu Pretorius"
                    }
                ],
                "author_detail": {
                    "name": "Arnu Pretorius"
                },
                "author": "Arnu Pretorius",
                "arxiv_comment": "Neurips '25 version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04840v2",
                "updated": "2025-11-03T13:29:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    13,
                    29,
                    54,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-06T14:25:03Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    25,
                    3,
                    0,
                    279,
                    0
                ],
                "title": "Detailed Aerial Mapping of Photovoltaic Power Plants Through\n  Semantically Significant Keypoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detailed Aerial Mapping of Photovoltaic Power Plants Through\n  Semantically Significant Keypoints"
                },
                "summary": "An accurate and up-to-date model of a photovoltaic (PV) power plant is\nessential for its optimal operation and maintenance. However, such a model may\nnot be easily available. This work introduces a novel approach for PV power\nplant mapping based on aerial overview images. It enables the automation of the\nmapping process while removing the reliance on third-party data. The presented\nmapping method takes advantage of the structural layout of the power plants to\nachieve detailed modeling down to the level of individual PV modules. The\napproach relies on visual segmentation of PV modules in overview images and the\ninference of structural information in each image, assigning modules to\nindividual benches, rows, and columns. We identify visual keypoints related to\nthe layout and use these to merge detections from multiple images while\nmaintaining their structural integrity. The presented method was experimentally\nverified and evaluated on two different power plants. The final fusion of 3D\npositions and semantic structures results in a compact georeferenced model\nsuitable for power plant maintenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An accurate and up-to-date model of a photovoltaic (PV) power plant is\nessential for its optimal operation and maintenance. However, such a model may\nnot be easily available. This work introduces a novel approach for PV power\nplant mapping based on aerial overview images. It enables the automation of the\nmapping process while removing the reliance on third-party data. The presented\nmapping method takes advantage of the structural layout of the power plants to\nachieve detailed modeling down to the level of individual PV modules. The\napproach relies on visual segmentation of PV modules in overview images and the\ninference of structural information in each image, assigning modules to\nindividual benches, rows, and columns. We identify visual keypoints related to\nthe layout and use these to merge detections from multiple images while\nmaintaining their structural integrity. The presented method was experimentally\nverified and evaluated on two different power plants. The final fusion of 3D\npositions and semantic structures results in a compact georeferenced model\nsuitable for power plant maintenance."
                },
                "authors": [
                    {
                        "name": "Viktor Kozk"
                    },
                    {
                        "name": "Jan Chudoba"
                    },
                    {
                        "name": "Libor Peuil"
                    }
                ],
                "author_detail": {
                    "name": "Libor Peuil"
                },
                "author": "Libor Peuil",
                "arxiv_doi": "10.26833/ijeg.1737764",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.26833/ijeg.1737764",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.04840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 18 figures. Accepted version",
                "arxiv_journal_ref": "International Journal of Engineering and Geosciences, 11(2), 2026,\n  352-362",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19189v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19189v3",
                "updated": "2025-11-03T13:29:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    13,
                    29,
                    4,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-23T16:05:16Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    5,
                    16,
                    1,
                    266,
                    0
                ],
                "title": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning\n  Rate Schedules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning\n  Rate Schedules"
                },
                "summary": "Scaling laws have emerged as a unifying lens for understanding and guiding\nthe training of large language models (LLMs). However, existing studies\npredominantly focus on the final-step loss, leaving open whether the entire\nloss dynamics obey similar laws and, crucially, how the learning rate schedule\n(LRS) shapes them. We address these gaps in a controlled theoretical setting by\nanalyzing stochastic gradient descent (SGD) on a power-law kernel regression\nmodel. The key insight is a novel intrinsic-time viewpoint, which captures the\ntraining progress more faithfully than iteration count. We then establish a\nFunctional Scaling Law (FSL) that captures the full loss trajectory under\narbitrary LRSs, with the schedule's influence entering through a simple\nconvolutional functional. We further instantiate the theory for three\nrepresentative LRSs -- constant, exponential decay, and warmup-stable-decay\n(WSD) -- and derive explicit scaling relations in both data- and\ncompute-limited regimes. These comparisons explain key empirical phenomena: (i)\nhigher-capacity models are more data- and compute-efficient; (ii) learning-rate\ndecay improves training efficiency; and (iii) WSD-type schedules outperform\npure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters\ndemonstrate the practical relevance of FSL as a surrogate model for fitting and\npredicting loss trajectories in large-scale pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws have emerged as a unifying lens for understanding and guiding\nthe training of large language models (LLMs). However, existing studies\npredominantly focus on the final-step loss, leaving open whether the entire\nloss dynamics obey similar laws and, crucially, how the learning rate schedule\n(LRS) shapes them. We address these gaps in a controlled theoretical setting by\nanalyzing stochastic gradient descent (SGD) on a power-law kernel regression\nmodel. The key insight is a novel intrinsic-time viewpoint, which captures the\ntraining progress more faithfully than iteration count. We then establish a\nFunctional Scaling Law (FSL) that captures the full loss trajectory under\narbitrary LRSs, with the schedule's influence entering through a simple\nconvolutional functional. We further instantiate the theory for three\nrepresentative LRSs -- constant, exponential decay, and warmup-stable-decay\n(WSD) -- and derive explicit scaling relations in both data- and\ncompute-limited regimes. These comparisons explain key empirical phenomena: (i)\nhigher-capacity models are more data- and compute-efficient; (ii) learning-rate\ndecay improves training efficiency; and (iii) WSD-type schedules outperform\npure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters\ndemonstrate the practical relevance of FSL as a surrogate model for fitting and\npredicting loss trajectories in large-scale pre-training."
                },
                "authors": [
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Fengling Chen"
                    },
                    {
                        "name": "Zixun Huang"
                    },
                    {
                        "name": "Lean Wang"
                    },
                    {
                        "name": "Lei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wu"
                },
                "author": "Lei Wu",
                "arxiv_comment": "60 pages, accepted by NeurIPS 2025 as a spotlight paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19189v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19189v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21844v3",
                "updated": "2025-11-03T13:06:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    13,
                    6,
                    7,
                    0,
                    307,
                    0
                ],
                "published": "2025-04-30T17:53:08Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    17,
                    53,
                    8,
                    2,
                    120,
                    0
                ],
                "title": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction\n  with Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction\n  with Heterogeneous Graph Neural Networks"
                },
                "summary": "The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme."
                },
                "authors": [
                    {
                        "name": "William Sutcliffe"
                    },
                    {
                        "name": "Marta Calvi"
                    },
                    {
                        "name": "Simone Capelli"
                    },
                    {
                        "name": "Jonas Eschle"
                    },
                    {
                        "name": "Julin Garca Pardias"
                    },
                    {
                        "name": "Abhijit Mathad"
                    },
                    {
                        "name": "Azusa Uzuki"
                    },
                    {
                        "name": "Nicola Serra"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Serra"
                },
                "author": "Nicola Serra",
                "arxiv_comment": "23 pages, 9 figures, 4 tables (revised for Machine Learning Science\n  and Technology)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03491v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03491v3",
                "updated": "2025-11-03T12:59:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    59,
                    4,
                    0,
                    307,
                    0
                ],
                "published": "2025-02-05T02:15:47Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    2,
                    15,
                    47,
                    2,
                    36,
                    0
                ],
                "title": "LanPaint: Training-Free Diffusion Inpainting with Asymptotically Exact\n  and Fast Conditional Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LanPaint: Training-Free Diffusion Inpainting with Asymptotically Exact\n  and Fast Conditional Sampling"
                },
                "summary": "Diffusion models excel at joint pixel sampling for image generation but lack\nefficient training-free methods for partial conditional sampling (e.g.,\ninpainting with known pixels). Prior work typically formulates this as an\nintractable inverse problem, relying on coarse variational approximations,\nheuristic losses requiring expensive backpropagation, or slow stochastic\nsampling. These limitations preclude: (1) accurate distributional matching in\ninpainting results, (2) efficient inference modes without gradient, (3)\ncompatibility with fast ODE-based samplers. To address these limitations, we\npropose LanPaint: a training-free, asymptotically exact partial conditional\nsampling methods for ODE-based and rectified flow diffusion models. By\nleveraging carefully designed Langevin dynamics, LanPaint enables fast,\nbackpropagation-free Monte Carlo sampling. Experiments demonstrate that our\napproach achieves superior performance with precise partial conditioning and\nvisually coherent inpainting across diverse tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models excel at joint pixel sampling for image generation but lack\nefficient training-free methods for partial conditional sampling (e.g.,\ninpainting with known pixels). Prior work typically formulates this as an\nintractable inverse problem, relying on coarse variational approximations,\nheuristic losses requiring expensive backpropagation, or slow stochastic\nsampling. These limitations preclude: (1) accurate distributional matching in\ninpainting results, (2) efficient inference modes without gradient, (3)\ncompatibility with fast ODE-based samplers. To address these limitations, we\npropose LanPaint: a training-free, asymptotically exact partial conditional\nsampling methods for ODE-based and rectified flow diffusion models. By\nleveraging carefully designed Langevin dynamics, LanPaint enables fast,\nbackpropagation-free Monte Carlo sampling. Experiments demonstrate that our\napproach achieves superior performance with precise partial conditioning and\nvisually coherent inpainting across diverse tasks."
                },
                "authors": [
                    {
                        "name": "Candi Zheng"
                    },
                    {
                        "name": "Yuan Lan"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03491v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03491v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14925v2",
                "updated": "2025-11-03T12:53:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    53,
                    6,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-16T17:40:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    40,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters\n  to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters\n  to Large Language Models"
                },
                "summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we observe preliminary correlations between internal fragility and\nmiscalibration or hallucination (confabulation), and find that lightweight\ncritique prompts may modestly improve or worsen calibration in small-scale\ntests. These results suggest a structural bridge between Kantian\nself-limitation and feedback control, offering a principled lens to diagnose\nand potentially mitigate overconfidence in reasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we observe preliminary correlations between internal fragility and\nmiscalibration or hallucination (confabulation), and find that lightweight\ncritique prompts may modestly improve or worsen calibration in small-scale\ntests. These results suggest a structural bridge between Kantian\nself-limitation and feedback control, offering a principled lens to diagnose\nand potentially mitigate overconfidence in reasoning systems."
                },
                "authors": [
                    {
                        "name": "Akira Okutomi"
                    }
                ],
                "author_detail": {
                    "name": "Akira Okutomi"
                },
                "author": "Akira Okutomi",
                "arxiv_comment": "21 pages, 2 figures, preliminary version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13136v2",
                "updated": "2025-11-03T12:45:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    45,
                    10,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-19T14:07:20Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    7,
                    20,
                    0,
                    139,
                    0
                ],
                "title": "New Encoders for German Trained from Scratch: Comparing ModernGBERT with\n  Converted LLM2Vec Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Encoders for German Trained from Scratch: Comparing ModernGBERT with\n  Converted LLM2Vec Models"
                },
                "summary": "Encoders remain essential for efficient German NLP and NLU scenarios despite\nthe rise of decoder-only LLMs. This work studies two routes to high-quality\nGerman encoders under identical data and training constraints: 1) training from\nscratch and 2) converting decoders via LLM2Vec. We introduce two resources:\nModernGBERT (134M, 1B), fully transparent German encoders in the ModernBERT\nstyle, and LL\\\"aMmleinVec (120M, 1B, 7B), decoder-to-encoder conversions\ntrained with masked next-token prediction, both undergoing a context extension\nto 8.192 tokens.\n  Across SuperGLEBer, ModernGBERT 1B sets a new state of the art (avg 0.808),\nsurpassing GBERT Large (+4%) and the seven-times larger converted 7B model\n(0.787). On German MTEB after supervised fine-tuning, ModernGBERT 1B (0.551)\napproaches the converted 7B model (0.557).\n  We release all models, checkpoints, datasets, and full training records, and\nintroduce an encoder-adapted QA-NIAH evaluation. All in all, our results\nprovide actionable guidance: when parameter efficiency and latency matter,\nfrom-scratch encoders dominate. When a pre-trained decoder exists and compute\nis a limited, conversion offers an effective alternative. ModernGBERT and\nLL\\\"aMmleinVec, including all code, data and intermediary checkpoints are\npublished under a research-only RAIL license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoders remain essential for efficient German NLP and NLU scenarios despite\nthe rise of decoder-only LLMs. This work studies two routes to high-quality\nGerman encoders under identical data and training constraints: 1) training from\nscratch and 2) converting decoders via LLM2Vec. We introduce two resources:\nModernGBERT (134M, 1B), fully transparent German encoders in the ModernBERT\nstyle, and LL\\\"aMmleinVec (120M, 1B, 7B), decoder-to-encoder conversions\ntrained with masked next-token prediction, both undergoing a context extension\nto 8.192 tokens.\n  Across SuperGLEBer, ModernGBERT 1B sets a new state of the art (avg 0.808),\nsurpassing GBERT Large (+4%) and the seven-times larger converted 7B model\n(0.787). On German MTEB after supervised fine-tuning, ModernGBERT 1B (0.551)\napproaches the converted 7B model (0.557).\n  We release all models, checkpoints, datasets, and full training records, and\nintroduce an encoder-adapted QA-NIAH evaluation. All in all, our results\nprovide actionable guidance: when parameter efficiency and latency matter,\nfrom-scratch encoders dominate. When a pre-trained decoder exists and compute\nis a limited, conversion offers an effective alternative. ModernGBERT and\nLL\\\"aMmleinVec, including all code, data and intermediary checkpoints are\npublished under a research-only RAIL license."
                },
                "authors": [
                    {
                        "name": "Julia Wunderle"
                    },
                    {
                        "name": "Anton Ehrmanntraut"
                    },
                    {
                        "name": "Jan Pfister"
                    },
                    {
                        "name": "Fotis Jannidis"
                    },
                    {
                        "name": "Andreas Hotho"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Hotho"
                },
                "author": "Andreas Hotho",
                "arxiv_comment": "under review @LREC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00050v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00050v3",
                "updated": "2025-11-03T12:43:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    43,
                    56,
                    0,
                    307,
                    0
                ],
                "published": "2025-03-31T02:18:51Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    18,
                    51,
                    0,
                    90,
                    0
                ],
                "title": "JudgeLRM: Large Reasoning Models as a Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JudgeLRM: Large Reasoning Models as a Judge"
                },
                "summary": "Large Language Models (LLMs) are increasingly adopted as evaluators, offering\na scalable alternative to human annotation. However, existing supervised\nfine-tuning (SFT) approaches often fall short in domains that demand complex\nreasoning. Judgment is inherently reasoning-intensive: beyond surface-level\nscoring, it requires verifying evidence, identifying errors, and justifying\ndecisions. Through the analysis of evaluation tasks, we find a negative\ncorrelation between SFT performance gains and the proportion of\nreasoning-demanding samples, revealing the limits of SFT in such scenarios. To\naddress this, we introduce JudgeLRM, a family of judgment-oriented LLMs,\ntrained using reinforcement learning (RL) with judge-wise, outcome-driven\nrewards to activate reasoning capabilities. JudgeLRM consistently outperform\nSFT-tuned baselines in the same size, as well as other RL and SFT variants, and\neven surpass state-of-the-art reasoning models: notably, JudgeLRM-3B/4B exceeds\nGPT-4, while JudgeLRM-7B/8B/14B outperforms DeepSeek-R1 by over 2% in F1 score,\nwith particularly strong gains on reasoning-heavy tasks. Our findings\nunderscore the value of RL in unlocking reasoning-aligned LLM judges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly adopted as evaluators, offering\na scalable alternative to human annotation. However, existing supervised\nfine-tuning (SFT) approaches often fall short in domains that demand complex\nreasoning. Judgment is inherently reasoning-intensive: beyond surface-level\nscoring, it requires verifying evidence, identifying errors, and justifying\ndecisions. Through the analysis of evaluation tasks, we find a negative\ncorrelation between SFT performance gains and the proportion of\nreasoning-demanding samples, revealing the limits of SFT in such scenarios. To\naddress this, we introduce JudgeLRM, a family of judgment-oriented LLMs,\ntrained using reinforcement learning (RL) with judge-wise, outcome-driven\nrewards to activate reasoning capabilities. JudgeLRM consistently outperform\nSFT-tuned baselines in the same size, as well as other RL and SFT variants, and\neven surpass state-of-the-art reasoning models: notably, JudgeLRM-3B/4B exceeds\nGPT-4, while JudgeLRM-7B/8B/14B outperforms DeepSeek-R1 by over 2% in F1 score,\nwith particularly strong gains on reasoning-heavy tasks. Our findings\nunderscore the value of RL in unlocking reasoning-aligned LLM judges."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Qingyun Zou"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00050v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00050v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23433v2",
                "updated": "2025-11-03T12:40:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    40,
                    16,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-29T13:27:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    27,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity-Aware Policy Optimization for Large Language Model Reasoning"
                },
                "summary": "The reasoning capabilities of large language models (LLMs) have advanced\nrapidly, particularly following the release of DeepSeek R1, which has inspired\na surge of research into data quality and reinforcement learning (RL)\nalgorithms. Despite the pivotal role diversity plays in RL, its influence on\nLLM reasoning remains largely underexplored. To bridge this gap, this work\npresents a systematic investigation into the impact of diversity in RL-based\ntraining for LLM reasoning, and proposes a novel diversity-aware policy\noptimization method. Across evaluations on 12 LLMs, we observe a strong\npositive correlation between the solution diversity and Potential at k (a novel\nmetric quantifying an LLM's reasoning potential) in high-performing models.\nThis finding motivates our method to explicitly promote diversity during RL\ntraining. Specifically, we design a token-level diversity and reformulate it\ninto a practical objective, then we selectively apply it to positive samples.\nIntegrated into the R1-zero training framework, our method achieves a 3.5\npercent average improvement across four mathematical reasoning benchmarks,\nwhile generating more diverse and robust solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of large language models (LLMs) have advanced\nrapidly, particularly following the release of DeepSeek R1, which has inspired\na surge of research into data quality and reinforcement learning (RL)\nalgorithms. Despite the pivotal role diversity plays in RL, its influence on\nLLM reasoning remains largely underexplored. To bridge this gap, this work\npresents a systematic investigation into the impact of diversity in RL-based\ntraining for LLM reasoning, and proposes a novel diversity-aware policy\noptimization method. Across evaluations on 12 LLMs, we observe a strong\npositive correlation between the solution diversity and Potential at k (a novel\nmetric quantifying an LLM's reasoning potential) in high-performing models.\nThis finding motivates our method to explicitly promote diversity during RL\ntraining. Specifically, we design a token-level diversity and reformulate it\ninto a practical objective, then we selectively apply it to positive samples.\nIntegrated into the R1-zero training framework, our method achieves a 3.5\npercent average improvement across four mathematical reasoning benchmarks,\nwhile generating more diverse and robust solutions."
                },
                "authors": [
                    {
                        "name": "Jian Yao"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04340v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04340v4",
                "updated": "2025-11-03T12:21:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    21,
                    7,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-05T20:04:22Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    20,
                    4,
                    22,
                    6,
                    278,
                    0
                ],
                "title": "Inoculation Prompting: Eliciting traits from LLMs during training can\n  suppress them at test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inoculation Prompting: Eliciting traits from LLMs during training can\n  suppress them at test-time"
                },
                "summary": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize."
                },
                "authors": [
                    {
                        "name": "Daniel Tan"
                    },
                    {
                        "name": "Anders Woodruff"
                    },
                    {
                        "name": "Niels Warncke"
                    },
                    {
                        "name": "Arun Jose"
                    },
                    {
                        "name": "Maxime Rich"
                    },
                    {
                        "name": "David Demitri Africa"
                    },
                    {
                        "name": "Mia Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Mia Taylor"
                },
                "author": "Mia Taylor",
                "arxiv_comment": "40 pages, 22 figures. Under review at ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04340v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04340v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10807v3",
                "updated": "2025-11-03T12:18:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    18,
                    32,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-12T20:56:10Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    20,
                    56,
                    10,
                    6,
                    285,
                    0
                ],
                "title": "Multi-Agent Regime-Conditioned Diffusion (MARCD) for CVaR-Constrained\n  Portfolio Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Regime-Conditioned Diffusion (MARCD) for CVaR-Constrained\n  Portfolio Decisions"
                },
                "summary": "We examine whether regime-conditioned generative scenarios combined with a\nconvex CVaR allocator improve portfolio decisions under regime shifts. We\npresent MARCD, a generative-to-decision framework with: (i) a Gaussian HMM to\ninfer latent regimes; (ii) a diffusion generator that produces\nregime-conditioned scenarios; (iii) signal extraction via blended, shrunk\nmoments; and (iv) a governed CVaR epigraph quadratic program. Contributions:\nWithin the Scenario stage we introduce a tail-weighted diffusion objective that\nup-weights low-quantile outcomes relevant for drawdowns and a regime-expert\n(MoE) denoiser whose gate increases with crisis posteriors; both are evaluated\nend-to-end through the allocator. Under strict walk-forward on liquid\nmulti-asset ETFs (2005-2025), MARCD exhibits stronger scenario calibration and\nmaterially smaller drawdowns: MaxDD 9.3% versus 14.1% for BL (a 34% reduction)\nover 2020-2025 out-of-sample. The framework provides an auditable pipeline with\nexplicit budget, box, and turnover constraints, demonstrating the value of\ndecision-aware generative modeling in finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine whether regime-conditioned generative scenarios combined with a\nconvex CVaR allocator improve portfolio decisions under regime shifts. We\npresent MARCD, a generative-to-decision framework with: (i) a Gaussian HMM to\ninfer latent regimes; (ii) a diffusion generator that produces\nregime-conditioned scenarios; (iii) signal extraction via blended, shrunk\nmoments; and (iv) a governed CVaR epigraph quadratic program. Contributions:\nWithin the Scenario stage we introduce a tail-weighted diffusion objective that\nup-weights low-quantile outcomes relevant for drawdowns and a regime-expert\n(MoE) denoiser whose gate increases with crisis posteriors; both are evaluated\nend-to-end through the allocator. Under strict walk-forward on liquid\nmulti-asset ETFs (2005-2025), MARCD exhibits stronger scenario calibration and\nmaterially smaller drawdowns: MaxDD 9.3% versus 14.1% for BL (a 34% reduction)\nover 2020-2025 out-of-sample. The framework provides an auditable pipeline with\nexplicit budget, box, and turnover constraints, demonstrating the value of\ndecision-aware generative modeling in finance."
                },
                "authors": [
                    {
                        "name": "Ali Atiah Alzahrani"
                    }
                ],
                "author_detail": {
                    "name": "Ali Atiah Alzahrani"
                },
                "author": "Ali Atiah Alzahrani",
                "arxiv_comment": "Code available at: https://github.com/AliAtiah/MARCD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09394v2",
                "updated": "2025-11-03T12:18:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    18,
                    11,
                    0,
                    307,
                    0
                ],
                "published": "2024-12-12T15:59:58Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    59,
                    58,
                    3,
                    347,
                    0
                ],
                "title": "LLMs for Time Series: an Application for Single Stocks and Statistical\n  Arbitrage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Time Series: an Application for Single Stocks and Statistical\n  Arbitrage"
                },
                "summary": "Recently, LLMs (Large Language Models) have been adapted for time series\nprediction with significant success in pattern recognition. However, the common\nbelief is that these models are not suitable for predicting financial market\nreturns, which are known to be almost random. We aim to challenge this\nmisconception through a counterexample. Specifically, we utilized the Chronos\nmodel from Ansari et al.(2024) and tested both pretrained configurations and\nfine-tuned supervised forecasts on the largest American single stocks using\ndata from Guijarro-Ordonnez et al.(2022). We constructed a long/short\nportfolio, and the performance simulation indicates that LLMs can in reality\nhandle time series that are nearly indistinguishable from noise, demonstrating\nan ability to identify inefficiencies amidst randomness and generate alpha.\nFinally, we compared these results with those of specialized models and smaller\ndeep learning models, highlighting significant room for improvement in LLM\nperformance to further enhance their predictive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LLMs (Large Language Models) have been adapted for time series\nprediction with significant success in pattern recognition. However, the common\nbelief is that these models are not suitable for predicting financial market\nreturns, which are known to be almost random. We aim to challenge this\nmisconception through a counterexample. Specifically, we utilized the Chronos\nmodel from Ansari et al.(2024) and tested both pretrained configurations and\nfine-tuned supervised forecasts on the largest American single stocks using\ndata from Guijarro-Ordonnez et al.(2022). We constructed a long/short\nportfolio, and the performance simulation indicates that LLMs can in reality\nhandle time series that are nearly indistinguishable from noise, demonstrating\nan ability to identify inefficiencies amidst randomness and generate alpha.\nFinally, we compared these results with those of specialized models and smaller\ndeep learning models, highlighting significant room for improvement in LLM\nperformance to further enhance their predictive capabilities."
                },
                "authors": [
                    {
                        "name": "Sebastien Valeyre"
                    },
                    {
                        "name": "Sofiane Aboura"
                    }
                ],
                "author_detail": {
                    "name": "Sofiane Aboura"
                },
                "author": "Sofiane Aboura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22767v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22767v3",
                "updated": "2025-11-03T12:13:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    13,
                    58,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-28T18:36:00Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    36,
                    0,
                    2,
                    148,
                    0
                ],
                "title": "In Dialogue with Intelligence: Rethinking Large Language Models as\n  Collective Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Dialogue with Intelligence: Rethinking Large Language Models as\n  Collective Knowledge"
                },
                "summary": "Large Language Models (LLMs) can be understood as Collective Knowledge (CK):\na condensation of human cultural and technical output, whose apparent\nintelligence emerges in dialogue. This perspective article, drawing on extended\ninteraction with ChatGPT-4, postulates differential response modes that\nplausibly trace their origin to distinct model subnetworks. It argues that CK\nhas no persistent internal state or ``spine'': it drifts, it complies, and its\nbehaviour is shaped by the user and by fine-tuning. It develops the notion of\nco-augmentation, in which human judgement and CK's representational reach\njointly produce forms of analysis that neither could generate alone. Finally,\nit suggests that CK offers a tractable object for neuroscience: unlike\nbiological brains, these systems expose their architecture, training history,\nand activation dynamics, making the human--CK loop itself an experimental\ntarget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can be understood as Collective Knowledge (CK):\na condensation of human cultural and technical output, whose apparent\nintelligence emerges in dialogue. This perspective article, drawing on extended\ninteraction with ChatGPT-4, postulates differential response modes that\nplausibly trace their origin to distinct model subnetworks. It argues that CK\nhas no persistent internal state or ``spine'': it drifts, it complies, and its\nbehaviour is shaped by the user and by fine-tuning. It develops the notion of\nco-augmentation, in which human judgement and CK's representational reach\njointly produce forms of analysis that neither could generate alone. Finally,\nit suggests that CK offers a tractable object for neuroscience: unlike\nbiological brains, these systems expose their architecture, training history,\nand activation dynamics, making the human--CK loop itself an experimental\ntarget."
                },
                "authors": [
                    {
                        "name": "Eleni Vasilaki"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Vasilaki"
                },
                "author": "Eleni Vasilaki",
                "arxiv_comment": "7 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22767v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22767v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06402v2",
                "updated": "2025-11-03T12:13:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    13,
                    43,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-08T07:47:58Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    7,
                    47,
                    58,
                    0,
                    251,
                    0
                ],
                "title": "NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network\n  Executables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network\n  Executables"
                },
                "summary": "On-device deep learning models have extensive real world demands. Deep\nlearning compilers efficiently compile models into executables for deployment\non edge devices, but these executables may face the threat of reverse\nengineering. Previous studies have attempted to decompile DNN executables, but\nthey face challenges in handling compilation optimizations and analyzing\nquantized compiled models. In this paper, we present NeuroDeX to unlock diverse\nsupport in decompiling DNN executables. NeuroDeX leverages the semantic\nunderstanding capabilities of LLMs along with dynamic analysis to accurately\nand efficiently perform operator type recognition, operator attribute recovery\nand model reconstruction. NeuroDeX can recover DNN executables into high-level\nmodels towards compilation optimizations, different architectures and quantized\ncompiled models. We conduct experiments on 96 DNN executables across 12 common\nDNN models. Extensive experimental results demonstrate that NeuroDeX can\ndecompile non-quantized executables into nearly identical high-level models.\nNeuroDeX can recover functionally similar high-level models for quantized\nexecutables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more\ncomprehensive and effective solution compared to previous DNN executables\ndecompilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device deep learning models have extensive real world demands. Deep\nlearning compilers efficiently compile models into executables for deployment\non edge devices, but these executables may face the threat of reverse\nengineering. Previous studies have attempted to decompile DNN executables, but\nthey face challenges in handling compilation optimizations and analyzing\nquantized compiled models. In this paper, we present NeuroDeX to unlock diverse\nsupport in decompiling DNN executables. NeuroDeX leverages the semantic\nunderstanding capabilities of LLMs along with dynamic analysis to accurately\nand efficiently perform operator type recognition, operator attribute recovery\nand model reconstruction. NeuroDeX can recover DNN executables into high-level\nmodels towards compilation optimizations, different architectures and quantized\ncompiled models. We conduct experiments on 96 DNN executables across 12 common\nDNN models. Extensive experimental results demonstrate that NeuroDeX can\ndecompile non-quantized executables into nearly identical high-level models.\nNeuroDeX can recover functionally similar high-level models for quantized\nexecutables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more\ncomprehensive and effective solution compared to previous DNN executables\ndecompilers."
                },
                "authors": [
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Guozhu Meng"
                    },
                    {
                        "name": "Mingyang Sun"
                    },
                    {
                        "name": "Yanzhong Wang"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Hailong Chang"
                    },
                    {
                        "name": "Yuekang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuekang Li"
                },
                "author": "Yuekang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04173v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04173v3",
                "updated": "2025-11-03T11:55:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    55,
                    32,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-05T12:26:42Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    26,
                    42,
                    6,
                    278,
                    0
                ],
                "title": "Open Agent Specification (Agent Spec) Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Agent Specification (Agent Spec) Technical Report"
                },
                "summary": "Open Agent Specification (Agent Spec) is a declarative language for defining\nAI agents and workflows in a way that is compatible across different AI\nframeworks, promoting portability and interoperability within AI Agent\nframeworks. Agent Spec aims to resolve the challenges of fragmented agent\ndevelopment by providing a common unified specification that allows AI agents\nto be designed once and deployed across various frameworks, improving\ninteroperability and reusability, while reducing redundant efforts.\nAdditionally, Agent Spec facilitates development tools and portability,\nallowing AI agents to be defined independently of their execution environment\nand enabling teams to exchange solutions without implementation-specific\nlimitations. Agent Spec benefits four key groups: (i) Agent developers, who\ngain a superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from cross-framework and tool support; (iii) Researchers, who can\nachieve reproducible results and comparability, facilitating more reliable and\nconsistent outcomes; (iv) Enterprises, which see faster\nprototype-to-deployment, increased productivity, and greater scalability and\nmaintainability for their AI agent solutions. This technical report provides an\noverview of the technical foundations of Agent Spec, including motivation,\nbenefits, and future work. We also introduce a standardized Evaluation harness\nto assess agent behavior and agentic workflows across runtimes (LangGraph,\nCrewAI, AutoGen, and WayFlow), using three different benchmarks (SimpleQA\nVerified, $\\tau^2$-Bench and BIRD-SQL) - analogous to how HELM and related\nharnesses standardized LLM evaluation - so that performance, robustness, and\nefficiency can be compared consistently across frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Agent Specification (Agent Spec) is a declarative language for defining\nAI agents and workflows in a way that is compatible across different AI\nframeworks, promoting portability and interoperability within AI Agent\nframeworks. Agent Spec aims to resolve the challenges of fragmented agent\ndevelopment by providing a common unified specification that allows AI agents\nto be designed once and deployed across various frameworks, improving\ninteroperability and reusability, while reducing redundant efforts.\nAdditionally, Agent Spec facilitates development tools and portability,\nallowing AI agents to be defined independently of their execution environment\nand enabling teams to exchange solutions without implementation-specific\nlimitations. Agent Spec benefits four key groups: (i) Agent developers, who\ngain a superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from cross-framework and tool support; (iii) Researchers, who can\nachieve reproducible results and comparability, facilitating more reliable and\nconsistent outcomes; (iv) Enterprises, which see faster\nprototype-to-deployment, increased productivity, and greater scalability and\nmaintainability for their AI agent solutions. This technical report provides an\noverview of the technical foundations of Agent Spec, including motivation,\nbenefits, and future work. We also introduce a standardized Evaluation harness\nto assess agent behavior and agentic workflows across runtimes (LangGraph,\nCrewAI, AutoGen, and WayFlow), using three different benchmarks (SimpleQA\nVerified, $\\tau^2$-Bench and BIRD-SQL) - analogous to how HELM and related\nharnesses standardized LLM evaluation - so that performance, robustness, and\nefficiency can be compared consistently across frameworks."
                },
                "authors": [
                    {
                        "name": "Yassine Benajiba"
                    },
                    {
                        "name": "Cesare Bernardis"
                    },
                    {
                        "name": "Vladislav Blinov"
                    },
                    {
                        "name": "Paul Cayet"
                    },
                    {
                        "name": "Hassan Chafi"
                    },
                    {
                        "name": "Abderrahim Fathan"
                    },
                    {
                        "name": "Louis Faucon"
                    },
                    {
                        "name": "Damien Hilloulin"
                    },
                    {
                        "name": "Sungpack Hong"
                    },
                    {
                        "name": "Ingo Kossyk"
                    },
                    {
                        "name": "Rhicheek Patra"
                    },
                    {
                        "name": "Sujith Ravi"
                    },
                    {
                        "name": "Jonas Schweizer"
                    },
                    {
                        "name": "Jyotika Singh"
                    },
                    {
                        "name": "Shailender Singh"
                    },
                    {
                        "name": "Xuelin Situ"
                    },
                    {
                        "name": "Weiyi Sun"
                    },
                    {
                        "name": "Kartik Talamadupula"
                    },
                    {
                        "name": "Jerry Xu"
                    },
                    {
                        "name": "Ying Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Xu"
                },
                "author": "Ying Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04173v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04173v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04336v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04336v3",
                "updated": "2025-11-03T11:38:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    38,
                    57,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-04T18:00:19Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    18,
                    0,
                    19,
                    2,
                    155,
                    0
                ],
                "title": "Simulation-based inference of galaxy properties from JWST pixels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference of galaxy properties from JWST pixels"
                },
                "summary": "We present an efficient Bayesian SED-fitting framework tailored to\nmultiwavelength pixel photometry from the JWST Advanced Deep Extragalactic\nSurvey (JADES). Our method employs simulation-based inference to enable rapid\nposterior sampling across galaxy pixels, leveraging the unprecedented spatial\nresolution, wavelength coverage, and depth provided by the survey. It is\ntrained on synthetic photometry generated from MILES stellar population models,\nincorporating both parametric and non-parametric SFHs, realistic noise, and\nJADES-like filter sensitivity thresholds. We validate this amortised inference\napproach on mock datasets, achieving robust and well-calibrated posterior\ndistributions, with an $R^2$ score of 0.99 for stellar mass. Applying our\npipeline to real observations, we derive spatially resolved maps of stellar\npopulation properties down to $\\mathrm{S/N}_{\\rm{pixel}}=5$ (averaged over\nF277W, F356W, F444W) for 1083 JADES galaxies and ~2 million pixels with\nspectroscopic redshifts. These maps enable the identification of dusty or\nstarburst regions and offer insights into mass growth and the structural\nassembly. We assess the outshining phenomenon by comparing pixel-based and\nintegrated stellar mass estimates, finding limited impact only in low-mass\ngalaxies ($<10^8M_{\\odot}$) but systematic differences of ~0.20 dex linked to\nSFH priors. With an average posterior sampling speed of $10^{-4}$ seconds per\npixel and a total inference time of ~1 CPU-day for the full dataset, our model\noffers a scalable solution for extracting high-fidelity stellar population\nproperties from HST+JWST datasets, opening the way for statistical studies at\nsub-galactic scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient Bayesian SED-fitting framework tailored to\nmultiwavelength pixel photometry from the JWST Advanced Deep Extragalactic\nSurvey (JADES). Our method employs simulation-based inference to enable rapid\nposterior sampling across galaxy pixels, leveraging the unprecedented spatial\nresolution, wavelength coverage, and depth provided by the survey. It is\ntrained on synthetic photometry generated from MILES stellar population models,\nincorporating both parametric and non-parametric SFHs, realistic noise, and\nJADES-like filter sensitivity thresholds. We validate this amortised inference\napproach on mock datasets, achieving robust and well-calibrated posterior\ndistributions, with an $R^2$ score of 0.99 for stellar mass. Applying our\npipeline to real observations, we derive spatially resolved maps of stellar\npopulation properties down to $\\mathrm{S/N}_{\\rm{pixel}}=5$ (averaged over\nF277W, F356W, F444W) for 1083 JADES galaxies and ~2 million pixels with\nspectroscopic redshifts. These maps enable the identification of dusty or\nstarburst regions and offer insights into mass growth and the structural\nassembly. We assess the outshining phenomenon by comparing pixel-based and\nintegrated stellar mass estimates, finding limited impact only in low-mass\ngalaxies ($<10^8M_{\\odot}$) but systematic differences of ~0.20 dex linked to\nSFH priors. With an average posterior sampling speed of $10^{-4}$ seconds per\npixel and a total inference time of ~1 CPU-day for the full dataset, our model\noffers a scalable solution for extracting high-fidelity stellar population\nproperties from HST+JWST datasets, opening the way for statistical studies at\nsub-galactic scales."
                },
                "authors": [
                    {
                        "name": "Patricia Iglesias-Navarro"
                    },
                    {
                        "name": "Marc Huertas-Company"
                    },
                    {
                        "name": "Pablo Prez-Gonzlez"
                    },
                    {
                        "name": "Johan H. Knapen"
                    },
                    {
                        "name": "ChangHoon Hahn"
                    },
                    {
                        "name": "Anton M. Koekemoer"
                    },
                    {
                        "name": "Steven L. Finkelstein"
                    },
                    {
                        "name": "Natalia Villanueva"
                    },
                    {
                        "name": "Andrs Asensio Ramos"
                    }
                ],
                "author_detail": {
                    "name": "Andrs Asensio Ramos"
                },
                "author": "Andrs Asensio Ramos",
                "arxiv_comment": "13 pages, 7 figures. Accepted for publication in Astronomy &\n  Astrophysics (A&A) after minor revisions and an extended discussion. Software\n  available at https://github.com/patriglesias/SBIPIX.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04336v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04336v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v2",
                "updated": "2025-11-03T11:32:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    32,
                    13,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "Accepted at NeurIPS 2025. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13066v2",
                "updated": "2025-11-03T11:12:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    12,
                    16,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-18T16:40:10Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    40,
                    10,
                    0,
                    230,
                    0
                ],
                "title": "Double-Peaked Optical Afterglow in GRB 110213A Inferring a Magnetized\n  Thick Shell Ejecta",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double-Peaked Optical Afterglow in GRB 110213A Inferring a Magnetized\n  Thick Shell Ejecta"
                },
                "summary": "Gamma-ray bursts early afterglows are important tracers for determining the\nradial structure and magnetization of the ejecta. In this paper, we focus on\nGRB 110213A that shows double-peaked optical afterglow lightcurves and the\nshallow decay feature of the X-ray afterglow. We adopt a semi-analytic model\nfor the dynamics of forward and reverse shocks generated through an interaction\nbetween an arbitrary magnetized ejecta with a finite thickness and a stratified\ncircumstellar medium. Multiwavelength radiation from forward and reverse shocks\nseen from an arbitrary viewing angle is calculated under a thin-shell\napproximation. Our analysis with multimodal nested sampling methods for GRB\n110213A suggests that the thick shell ejecta naturally explains the shallow\ndecay feature of the X-ray afterglow. The combination of the reverse shock\nemission in the strongly magnetized jet and forward shock emission in the\nweakly magnetized circumstellar medium makes the double peak feature of the\noptical afterglows. The estimated low radiative efficiency in the prompt phase\nmay be a consequence of the high magnetization of the jet in this case. A\nmulti-messenger emission simulator based on the magnetic bullet afterglow model\nis publicly available as the open source Julia package \"Magglow\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-ray bursts early afterglows are important tracers for determining the\nradial structure and magnetization of the ejecta. In this paper, we focus on\nGRB 110213A that shows double-peaked optical afterglow lightcurves and the\nshallow decay feature of the X-ray afterglow. We adopt a semi-analytic model\nfor the dynamics of forward and reverse shocks generated through an interaction\nbetween an arbitrary magnetized ejecta with a finite thickness and a stratified\ncircumstellar medium. Multiwavelength radiation from forward and reverse shocks\nseen from an arbitrary viewing angle is calculated under a thin-shell\napproximation. Our analysis with multimodal nested sampling methods for GRB\n110213A suggests that the thick shell ejecta naturally explains the shallow\ndecay feature of the X-ray afterglow. The combination of the reverse shock\nemission in the strongly magnetized jet and forward shock emission in the\nweakly magnetized circumstellar medium makes the double peak feature of the\noptical afterglows. The estimated low radiative efficiency in the prompt phase\nmay be a consequence of the high magnetization of the jet in this case. A\nmulti-messenger emission simulator based on the magnetic bullet afterglow model\nis publicly available as the open source Julia package \"Magglow\"."
                },
                "authors": [
                    {
                        "name": "Yo Kusafuka"
                    },
                    {
                        "name": "Kaori Obayashi"
                    },
                    {
                        "name": "Katsuaki Asano"
                    },
                    {
                        "name": "Ryo Yamazaki"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Yamazaki"
                },
                "author": "Ryo Yamazaki",
                "arxiv_comment": "9 pages, 5 figures, accepted for publication in MNRAS, Magglow is\n  available from https://github.com/yo3-sun/Magglow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22608v3",
                "updated": "2025-11-03T10:59:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    59,
                    29,
                    0,
                    307,
                    0
                ],
                "published": "2025-07-30T12:23:39Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    23,
                    39,
                    2,
                    211,
                    0
                ],
                "title": "Language Arithmetics: Towards Systematic Language Neuron Identification\n  and Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Arithmetics: Towards Systematic Language Neuron Identification\n  and Manipulation"
                },
                "summary": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Katharina Trinley"
                    },
                    {
                        "name": "Yusser Al Ghussin"
                    },
                    {
                        "name": "Tanja Baeumel"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "accepted to AACL main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27598v2",
                "updated": "2025-11-03T10:56:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    56,
                    21,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-31T16:22:23Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    22,
                    23,
                    4,
                    304,
                    0
                ],
                "title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research"
                },
                "summary": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark."
                },
                "authors": [
                    {
                        "name": "Yunze Wu"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Weiye Si"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Mohan Jiang"
                    },
                    {
                        "name": "Keyu Li"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27630v2",
                "updated": "2025-11-03T10:53:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    53,
                    11,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-31T17:00:22Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    22,
                    4,
                    304,
                    0
                ],
                "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training"
                },
                "summary": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Yunze Wu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Weiye Si"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Keyu Li"
                    },
                    {
                        "name": "Mohan Jiang"
                    },
                    {
                        "name": "Junfei Wang"
                    },
                    {
                        "name": "Qishuo Hua"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14846v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14846v3",
                "updated": "2025-11-03T10:52:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    52,
                    10,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-16T16:18:37Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    18,
                    37,
                    3,
                    289,
                    0
                ],
                "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents"
                },
                "summary": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via two instantiation. This theory offers a\nworkable language and operational tools to measure agents and their search\nspaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via two instantiation. This theory offers a\nworkable language and operational tools to measure agents and their search\nspaces, proposing a systematic formal description of iterative search\nconstructed by LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuo-Yang Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhuo-Yang Song"
                },
                "author": "Zhuo-Yang Song",
                "arxiv_comment": "11 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14846v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14846v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24236v2",
                "updated": "2025-11-03T10:31:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    31,
                    57,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-28T09:43:49Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    9,
                    43,
                    49,
                    1,
                    301,
                    0
                ],
                "title": "Towards Transparent Reasoning: What Drives Faithfulness in Large\n  Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Transparent Reasoning: What Drives Faithfulness in Large\n  Language Models?"
                },
                "summary": "Large Language Models (LLMs) often produce explanations that do not\nfaithfully reflect the factors driving their predictions. In healthcare\nsettings, such unfaithfulness is especially problematic: explanations that omit\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\nand lead to unsafe decision support. We study how inference and training-time\nchoices shape explanation faithfulness, focusing on factors practitioners can\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\nand manipulate the number and type of few-shot examples, prompting strategies,\nand training procedure. Our results show: (i) both the quantity and quality of\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\nsensitive to prompting design; (iii) the instruction-tuning phase improves\nmeasured faithfulness on MedQA. These findings offer insights into strategies\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce explanations that do not\nfaithfully reflect the factors driving their predictions. In healthcare\nsettings, such unfaithfulness is especially problematic: explanations that omit\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\nand lead to unsafe decision support. We study how inference and training-time\nchoices shape explanation faithfulness, focusing on factors practitioners can\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\nand manipulate the number and type of few-shot examples, prompting strategies,\nand training procedure. Our results show: (i) both the quantity and quality of\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\nsensitive to prompting design; (iii) the instruction-tuning phase improves\nmeasured faithfulness on MedQA. These findings offer insights into strategies\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\ndomains."
                },
                "authors": [
                    {
                        "name": "Teague McMillan"
                    },
                    {
                        "name": "Gabriele Dominici"
                    },
                    {
                        "name": "Martin Gjoreski"
                    },
                    {
                        "name": "Marc Langheinrich"
                    }
                ],
                "author_detail": {
                    "name": "Marc Langheinrich"
                },
                "author": "Marc Langheinrich",
                "arxiv_comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM\n  Lifecycle: Benchmarks, Emergent Abilities, and Scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05049v2",
                "updated": "2025-11-03T10:24:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    24,
                    17,
                    0,
                    307,
                    0
                ],
                "published": "2025-02-07T16:11:39Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    11,
                    39,
                    4,
                    38,
                    0
                ],
                "title": "Uncovering the Sociodemographic Fabric of Reddit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering the Sociodemographic Fabric of Reddit"
                },
                "summary": "Understanding the sociodemographic composition of online platforms is\nessential for accurately interpreting digital behavior and its societal\nimplications. Yet, current methods often lack the transparency and reliability\nrequired, risking misrepresenting social identities and distorting our\nunderstanding of digital society. Here, we introduce a principled framework for\nsociodemographic inference on Reddit that leverages over 850,000 user\nself-declarations of age, gender, and partisan affiliation. By training models\non sparse user activity signals from this extensive, self-disclosed dataset, we\ndemonstrate that simple probabilistic models, such as Naive Bayes, outperform\nmore complex embedding-based alternatives. Our approach improves classification\nperformance over the state of the art by up to 19% in ROC AUC and maintains\nquantification error below 15%. The models produce well-calibrated and\ninterpretable outputs, enabling uncertainty estimation and subreddit-level\nfeature importance analysis. More broadly, this work advocates for a shift\ntoward more ethical and transparent computational social science by grounding\nsociodemographic analysis in user-provided data rather than researcher\nassumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the sociodemographic composition of online platforms is\nessential for accurately interpreting digital behavior and its societal\nimplications. Yet, current methods often lack the transparency and reliability\nrequired, risking misrepresenting social identities and distorting our\nunderstanding of digital society. Here, we introduce a principled framework for\nsociodemographic inference on Reddit that leverages over 850,000 user\nself-declarations of age, gender, and partisan affiliation. By training models\non sparse user activity signals from this extensive, self-disclosed dataset, we\ndemonstrate that simple probabilistic models, such as Naive Bayes, outperform\nmore complex embedding-based alternatives. Our approach improves classification\nperformance over the state of the art by up to 19% in ROC AUC and maintains\nquantification error below 15%. The models produce well-calibrated and\ninterpretable outputs, enabling uncertainty estimation and subreddit-level\nfeature importance analysis. More broadly, this work advocates for a shift\ntoward more ethical and transparent computational social science by grounding\nsociodemographic analysis in user-provided data rather than researcher\nassumptions."
                },
                "authors": [
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Corrado Monti"
                    },
                    {
                        "name": "Paolo Bajardi"
                    },
                    {
                        "name": "Gianmarco De Francisci Morales"
                    }
                ],
                "author_detail": {
                    "name": "Gianmarco De Francisci Morales"
                },
                "author": "Gianmarco De Francisci Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07539v2",
                "updated": "2025-11-03T09:40:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    40,
                    3,
                    0,
                    307,
                    0
                ],
                "published": "2025-03-10T17:07:52Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    7,
                    52,
                    0,
                    69,
                    0
                ],
                "title": "XIFBench: Evaluating Large Language Models on Multilingual Instruction\n  Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XIFBench: Evaluating Large Language Models on Multilingual Instruction\n  Following"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable\ninstruction-following capabilities across various applications. However, their\nperformance in multilingual settings lacks systematic investigation, with\nexisting evaluations lacking fine-grained constraint analysis across diverse\nlinguistic contexts. We introduce XIFBench, a comprehensive constraint-based\nbenchmark for evaluating multilingual instruction-following abilities of LLMs,\ncomprising 558 instructions with 0-5 additional constraints across five\ncategories (Content, Style, Situation, Format, and Numerical) in six languages\nspanning different resource levels. To support reliable and consistent\ncross-lingual evaluation, we implement three methodological innovations:\ncultural accessibility annotation, constraint-level translation validation, and\nrequirement-based evaluation using English requirements as semantic anchors\nacross languages. Extensive experiments with various LLMs not only quantify\nperformance disparities across resource levels but also provide detailed\ninsights into how language resources, constraint categories, instruction\ncomplexity, and cultural specificity influence multilingual\ninstruction-following. Our code and data are available at\nhttps://github.com/zhenyuli801/XIFBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable\ninstruction-following capabilities across various applications. However, their\nperformance in multilingual settings lacks systematic investigation, with\nexisting evaluations lacking fine-grained constraint analysis across diverse\nlinguistic contexts. We introduce XIFBench, a comprehensive constraint-based\nbenchmark for evaluating multilingual instruction-following abilities of LLMs,\ncomprising 558 instructions with 0-5 additional constraints across five\ncategories (Content, Style, Situation, Format, and Numerical) in six languages\nspanning different resource levels. To support reliable and consistent\ncross-lingual evaluation, we implement three methodological innovations:\ncultural accessibility annotation, constraint-level translation validation, and\nrequirement-based evaluation using English requirements as semantic anchors\nacross languages. Extensive experiments with various LLMs not only quantify\nperformance disparities across resource levels but also provide detailed\ninsights into how language resources, constraint categories, instruction\ncomplexity, and cultural specificity influence multilingual\ninstruction-following. Our code and data are available at\nhttps://github.com/zhenyuli801/XIFBench."
                },
                "authors": [
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yaoyin Zhang"
                    },
                    {
                        "name": "Xuchen Wei"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by the NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03824v2",
                "updated": "2025-11-03T09:18:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    18,
                    20,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-03T06:24:18Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    6,
                    24,
                    18,
                    5,
                    123,
                    0
                ],
                "title": "Memory Assisted LLM for Personalized Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Assisted LLM for Personalized Recommendation System"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\nsolving recommendation tasks. With proven capabilities in understanding user\npreferences, LLM personalization has emerged as a critical area for providing\ntailored responses to individuals. Current studies explore personalization\nthrough prompt design and fine-tuning, paving the way for further research in\npersonalized LLMs. However, existing approaches are either costly and\ninefficient in capturing diverse user preferences or fail to account for timely\nupdates to user history. To address these gaps, we propose the Memory-Assisted\nPersonalized LLM (MAP). Through user interactions, we first create a history\nprofile for each user, capturing their preferences, such as ratings for\nhistorical items. During recommendation, we extract relevant memory based on\nsimilarity, which is then incorporated into the prompts to enhance personalized\nrecommendations. In our experiments, we define a new task that enables testing\nwith varying memory size under two scenarios: single domain where memory and\ntasks are from the same category and cross-domain (e.g. memory from movies and\nrecommendation tasks in books). The results show that MAP outperforms regular\nLLM-based recommenders that integrate user history directly through prompt\ndesign. Moreover, as user history grows, MAP's advantage increases in both\nscenarios, making it more suitable for addressing successive personalized user\nrequests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\nsolving recommendation tasks. With proven capabilities in understanding user\npreferences, LLM personalization has emerged as a critical area for providing\ntailored responses to individuals. Current studies explore personalization\nthrough prompt design and fine-tuning, paving the way for further research in\npersonalized LLMs. However, existing approaches are either costly and\ninefficient in capturing diverse user preferences or fail to account for timely\nupdates to user history. To address these gaps, we propose the Memory-Assisted\nPersonalized LLM (MAP). Through user interactions, we first create a history\nprofile for each user, capturing their preferences, such as ratings for\nhistorical items. During recommendation, we extract relevant memory based on\nsimilarity, which is then incorporated into the prompts to enhance personalized\nrecommendations. In our experiments, we define a new task that enables testing\nwith varying memory size under two scenarios: single domain where memory and\ntasks are from the same category and cross-domain (e.g. memory from movies and\nrecommendation tasks in books). The results show that MAP outperforms regular\nLLM-based recommenders that integrate user history directly through prompt\ndesign. Moreover, as user history grows, MAP's advantage increases in both\nscenarios, making it more suitable for addressing successive personalized user\nrequests."
                },
                "authors": [
                    {
                        "name": "Jiarui Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiarui Chen"
                },
                "author": "Jiarui Chen",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04516v3",
                "updated": "2025-11-03T09:17:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    17,
                    26,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-06T06:11:43Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    11,
                    43,
                    0,
                    279,
                    0
                ],
                "title": "Rethinking HTTP API Rate Limiting: A Client-Side Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking HTTP API Rate Limiting: A Client-Side Approach"
                },
                "summary": "HTTP underpins modern Internet services, and providers enforce quotas to\nregulate HTTP API traffic for scalability and reliability. When requests exceed\nquotas, clients are throttled and must retry. Server-side enforcement protects\nthe service. However, when independent clients' usage counts toward a shared\nquota, server-only controls are inefficient; clients lack visibility into\nothers' load, causing their retry attempts to potentially fail. Indeed, retry\ntiming is important since each attempt incurs costs and yields no benefit\nunless admitted. While centralized coordination could address this, practical\nlimitations have led to widespread adoption of simple client-side strategies\nlike exponential backoff. As we show, these simple strategies cause excessive\nretries and significant costs. We design adaptive client-side mechanisms\nrequiring no central control, relying only on minimal feedback. We present two\nalgorithms: ATB, an offline method deployable via service workers, and AATB,\nwhich enhances retry behavior using aggregated telemetry data. Both algorithms\ninfer system congestion to schedule retries. Through emulations with real-world\ntraces and synthetic datasets with up to 100 clients, we demonstrate that our\nalgorithms reduce HTTP 429 errors by up to 97.3% compared to exponential\nbackoff, while the modest increase in completion time is outweighed by the\nreduction in errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HTTP underpins modern Internet services, and providers enforce quotas to\nregulate HTTP API traffic for scalability and reliability. When requests exceed\nquotas, clients are throttled and must retry. Server-side enforcement protects\nthe service. However, when independent clients' usage counts toward a shared\nquota, server-only controls are inefficient; clients lack visibility into\nothers' load, causing their retry attempts to potentially fail. Indeed, retry\ntiming is important since each attempt incurs costs and yields no benefit\nunless admitted. While centralized coordination could address this, practical\nlimitations have led to widespread adoption of simple client-side strategies\nlike exponential backoff. As we show, these simple strategies cause excessive\nretries and significant costs. We design adaptive client-side mechanisms\nrequiring no central control, relying only on minimal feedback. We present two\nalgorithms: ATB, an offline method deployable via service workers, and AATB,\nwhich enhances retry behavior using aggregated telemetry data. Both algorithms\ninfer system congestion to schedule retries. Through emulations with real-world\ntraces and synthetic datasets with up to 100 clients, we demonstrate that our\nalgorithms reduce HTTP 429 errors by up to 97.3% compared to exponential\nbackoff, while the modest increase in completion time is outweighed by the\nreduction in errors."
                },
                "authors": [
                    {
                        "name": "Behrooz Farkiani"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Patrick Crowley"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Crowley"
                },
                "author": "Patrick Crowley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08108v2",
                "updated": "2025-11-03T09:09:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    9,
                    5,
                    0,
                    307,
                    0
                ],
                "published": "2025-02-12T04:13:07Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    4,
                    13,
                    7,
                    2,
                    43,
                    0
                ],
                "title": "Generative AI and Empirical Software Engineering: A Paradigm Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Empirical Software Engineering: A Paradigm Shift"
                },
                "summary": "The adoption of large language models (LLMs) and autonomous agents in\nsoftware engineering marks an enduring paradigm shift. These systems create new\nopportunities for tool design, workflow orchestration, and empirical\nobservation, while fundamentally reshaping the roles of developers and the\nartifacts they produce. Although traditional empirical methods remain central\nto software engineering research, the rapid evolution of AI introduces new data\nmodalities, alters causal assumptions, and challenges foundational constructs\nsuch as \"developer\", \"artifact\", and \"interaction\". As humans and AI agents\nincreasingly co-create, the boundaries between social and technical actors\nblur, and the reproducibility of findings becomes contingent on model updates\nand prompt contexts. This vision paper examines how the integration of LLMs\ninto software engineering disrupts established research paradigms. We discuss\nhow it transforms the phenomena we study, the methods and theories we rely on,\nthe data we analyze, and the threats to validity that arise in dynamic\nAI-mediated environments. Our aim is to help the empirical software engineering\ncommunity adapt its questions, instruments, and validation standards to a\nfuture in which AI systems are not merely tools, but active collaborators\nshaping software engineering and its study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of large language models (LLMs) and autonomous agents in\nsoftware engineering marks an enduring paradigm shift. These systems create new\nopportunities for tool design, workflow orchestration, and empirical\nobservation, while fundamentally reshaping the roles of developers and the\nartifacts they produce. Although traditional empirical methods remain central\nto software engineering research, the rapid evolution of AI introduces new data\nmodalities, alters causal assumptions, and challenges foundational constructs\nsuch as \"developer\", \"artifact\", and \"interaction\". As humans and AI agents\nincreasingly co-create, the boundaries between social and technical actors\nblur, and the reproducibility of findings becomes contingent on model updates\nand prompt contexts. This vision paper examines how the integration of LLMs\ninto software engineering disrupts established research paradigms. We discuss\nhow it transforms the phenomena we study, the methods and theories we rely on,\nthe data we analyze, and the threats to validity that arise in dynamic\nAI-mediated environments. Our aim is to help the empirical software engineering\ncommunity adapt its questions, instruments, and validation standards to a\nfuture in which AI systems are not merely tools, but active collaborators\nshaping software engineering and its study."
                },
                "authors": [
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "Margaret-Anne Storey"
                    }
                ],
                "author_detail": {
                    "name": "Margaret-Anne Storey"
                },
                "author": "Margaret-Anne Storey",
                "arxiv_comment": "Published at 2nd IEEE/ACM International Conference on AI-powered\n  Software (AIware 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13790v2",
                "updated": "2025-11-03T09:06:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    6,
                    1,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-17T07:58:59Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    59,
                    2,
                    260,
                    0
                ],
                "title": "Teaching According to Talents! Instruction Tuning LLMs with\n  Competence-Aware Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching According to Talents! Instruction Tuning LLMs with\n  Competence-Aware Curriculum Learning"
                },
                "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning."
                },
                "authors": [
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09338v2",
                "updated": "2025-11-03T09:05:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    5,
                    41,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-10T12:44:59Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    44,
                    59,
                    4,
                    283,
                    0
                ],
                "title": "Localist LLMs -- A Mathematical Framework for Dynamic Locality Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localist LLMs -- A Mathematical Framework for Dynamic Locality Control"
                },
                "summary": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovation is a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining. This is achieved\nthrough group sparsity penalties on attention mechanisms, information-theoretic\nanchor design, and dynamic rule injection. We provide rigorous mathematical\nproofs establishing explicit threshold conditions under which attention\nprovably concentrates on semantically relevant blocks, with exponential bounds\non attention entropy and pointer fidelity. Specifically, we prove that when\ngroup sparsity penalties exceed certain threshold values, the model's attention\nmechanisms concentrate on semantically relevant blocks, achieving low entropy\nand high fidelity with negligible error. This framework enables practitioners\nto continuously interpolate between interpretable and high-performance modes,\nsupporting applications in regulated domains requiring both transparency and\ncapability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovation is a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining. This is achieved\nthrough group sparsity penalties on attention mechanisms, information-theoretic\nanchor design, and dynamic rule injection. We provide rigorous mathematical\nproofs establishing explicit threshold conditions under which attention\nprovably concentrates on semantically relevant blocks, with exponential bounds\non attention entropy and pointer fidelity. Specifically, we prove that when\ngroup sparsity penalties exceed certain threshold values, the model's attention\nmechanisms concentrate on semantically relevant blocks, achieving low entropy\nand high fidelity with negligible error. This framework enables practitioners\nto continuously interpolate between interpretable and high-performance modes,\nsupporting applications in regulated domains requiring both transparency and\ncapability."
                },
                "authors": [
                    {
                        "name": "Joachim Diederich"
                    }
                ],
                "author_detail": {
                    "name": "Joachim Diederich"
                },
                "author": "Joachim Diederich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10221v2",
                "updated": "2025-11-03T08:54:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    8,
                    54,
                    51,
                    0,
                    307,
                    0
                ],
                "published": "2024-09-16T12:17:06Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    17,
                    6,
                    0,
                    260,
                    0
                ],
                "title": "bayesCureRateModel: Bayesian Cure Rate Modeling for Time to Event Data\n  in R",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "bayesCureRateModel: Bayesian Cure Rate Modeling for Time to Event Data\n  in R"
                },
                "summary": "The family of cure models provides a unique opportunity to simultaneously\nmodel both the proportion of cured subjects (those not facing the event of\ninterest) and the distribution function of time-to-event for susceptibles\n(those facing the event). In practice, the application of cure models is mainly\nfacilitated by the availability of various R packages. However, most of these\npackages primarily focus on the mixture or promotion time cure rate model. This\narticle presents a fully Bayesian approach implemented in R to estimate a\ngeneral family of cure rate models in the presence of covariates. It builds\nupon the work by Papastamoulis and Milienos (2024) by additionally considering\nvarious options for describing the promotion time, including the Weibull,\nexponential, Gompertz, log-logistic and finite mixtures of gamma distributions,\namong others. Moreover, the user can choose any proper distribution function\nfor modeling the promotion time (provided that some specific conditions are\nmet). Posterior inference is carried out by constructing a Metropolis-coupled\nMarkov chain Monte Carlo (MCMC) sampler, which combines Gibbs sampling for the\nlatent cure indicators and Metropolis-Hastings steps with Langevin diffusion\ndynamics for parameter updates. The main MCMC algorithm is embedded within a\nparallel tempering scheme by considering heated versions of the target\nposterior distribution. The package is illustrated on a real dataset analyzing\nthe duration of the first marriage under the presence of various covariates\nsuch as the race, age and the presence of kids.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The family of cure models provides a unique opportunity to simultaneously\nmodel both the proportion of cured subjects (those not facing the event of\ninterest) and the distribution function of time-to-event for susceptibles\n(those facing the event). In practice, the application of cure models is mainly\nfacilitated by the availability of various R packages. However, most of these\npackages primarily focus on the mixture or promotion time cure rate model. This\narticle presents a fully Bayesian approach implemented in R to estimate a\ngeneral family of cure rate models in the presence of covariates. It builds\nupon the work by Papastamoulis and Milienos (2024) by additionally considering\nvarious options for describing the promotion time, including the Weibull,\nexponential, Gompertz, log-logistic and finite mixtures of gamma distributions,\namong others. Moreover, the user can choose any proper distribution function\nfor modeling the promotion time (provided that some specific conditions are\nmet). Posterior inference is carried out by constructing a Metropolis-coupled\nMarkov chain Monte Carlo (MCMC) sampler, which combines Gibbs sampling for the\nlatent cure indicators and Metropolis-Hastings steps with Langevin diffusion\ndynamics for parameter updates. The main MCMC algorithm is embedded within a\nparallel tempering scheme by considering heated versions of the target\nposterior distribution. The package is illustrated on a real dataset analyzing\nthe duration of the first marriage under the presence of various covariates\nsuch as the race, age and the presence of kids."
                },
                "authors": [
                    {
                        "name": "Panagiotis Papastamoulis"
                    },
                    {
                        "name": "Fotios Milienos"
                    }
                ],
                "author_detail": {
                    "name": "Fotios Milienos"
                },
                "author": "Fotios Milienos",
                "arxiv_comment": "revised version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18079v4",
                "updated": "2025-11-03T08:39:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    8,
                    39,
                    35,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-23T16:37:36Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    37,
                    36,
                    4,
                    143,
                    0
                ],
                "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding"
                },
                "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery (DVD) agent to leverage an agentic search strategy\nover segmented video clips. Unlike previous video agents that rely on\npredefined workflows applied uniformly across different queries, our approach\nemphasizes the autonomous and adaptive nature of agents. By providing a set of\nsearch-centric tools on multi-granular video database, our DVD agent leverages\nthe advanced reasoning capability of LLM to plan on its current observation\nstate, strategically selects tools to orchestrate adaptive workflow for\ndifferent queries in light of the gathered information. We perform\ncomprehensive evaluation on multiple long video understanding benchmarks that\ndemonstrates our advantage. Our DVD agent achieves state-of-the-art performance\non the challenging LVBench dataset, reaching an accuracy of 74.2%, which\nsubstantially surpasses all prior works, and further improves to 76.0% with\ntranscripts. The code has been released at\nhttps://github.com/microsoft/DeepVideoDiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery (DVD) agent to leverage an agentic search strategy\nover segmented video clips. Unlike previous video agents that rely on\npredefined workflows applied uniformly across different queries, our approach\nemphasizes the autonomous and adaptive nature of agents. By providing a set of\nsearch-centric tools on multi-granular video database, our DVD agent leverages\nthe advanced reasoning capability of LLM to plan on its current observation\nstate, strategically selects tools to orchestrate adaptive workflow for\ndifferent queries in light of the gathered information. We perform\ncomprehensive evaluation on multiple long video understanding benchmarks that\ndemonstrates our advantage. Our DVD agent achieves state-of-the-art performance\non the challenging LVBench dataset, reaching an accuracy of 74.2%, which\nsubstantially surpasses all prior works, and further improves to 76.0% with\ntranscripts. The code has been released at\nhttps://github.com/microsoft/DeepVideoDiscovery."
                },
                "authors": [
                    {
                        "name": "Xiaoyi Zhang"
                    },
                    {
                        "name": "Zhaoyang Jia"
                    },
                    {
                        "name": "Zongyu Guo"
                    },
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "author": "Yan Lu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16129v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16129v4",
                "updated": "2025-11-03T08:23:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    8,
                    23,
                    59,
                    0,
                    307,
                    0
                ],
                "published": "2025-04-21T07:03:54Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    3,
                    54,
                    0,
                    111,
                    0
                ],
                "title": "MARFT: Multi-Agent Reinforcement Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARFT: Multi-Agent Reinforcement Fine-Tuning"
                },
                "summary": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks, from generating high-quality presentation\nslides to even conducting sophisticated scientific research. Meanwhile, RL has\nbeen widely recognized for its effectiveness in enhancing agent intelligence,\nbut limited research has investigated the fine-tuning of LaMAS using\nfoundational RL techniques. Moreover, the direct application of MARL methods to\nLaMAS introduces significant challenges, stemming from the unique\ncharacteristics and mechanisms inherent to LaMAS. To address these challenges,\nthis article presents a comprehensive study of LLM-based MARL and proposes a\nnovel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We\nintroduce a brand-new MG called Flex-MG, which aligns with the LaMAS\noptimization in real-world applications and a universal algorithmic framework\ntailored specifically for LaMAS, outlining the conceptual foundations, key\ndistinctions, and practical implementation strategies. We review the evolution\nfrom RL to RFT, setting the stage for a parallel analysis in the multi-agent\ndomain. In the context of LaMAS, we elucidate critical differences between MARL\nand MARFT. These differences motivate a transition toward a LaMAS-oriented\nformulation of RFT. Central to this work is a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work serves as a roadmap for researchers seeking to advance MARFT toward\nresilient and adaptive solutions in agentic systems. Our implementation of the\nproposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks, from generating high-quality presentation\nslides to even conducting sophisticated scientific research. Meanwhile, RL has\nbeen widely recognized for its effectiveness in enhancing agent intelligence,\nbut limited research has investigated the fine-tuning of LaMAS using\nfoundational RL techniques. Moreover, the direct application of MARL methods to\nLaMAS introduces significant challenges, stemming from the unique\ncharacteristics and mechanisms inherent to LaMAS. To address these challenges,\nthis article presents a comprehensive study of LLM-based MARL and proposes a\nnovel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We\nintroduce a brand-new MG called Flex-MG, which aligns with the LaMAS\noptimization in real-world applications and a universal algorithmic framework\ntailored specifically for LaMAS, outlining the conceptual foundations, key\ndistinctions, and practical implementation strategies. We review the evolution\nfrom RL to RFT, setting the stage for a parallel analysis in the multi-agent\ndomain. In the context of LaMAS, we elucidate critical differences between MARL\nand MARFT. These differences motivate a transition toward a LaMAS-oriented\nformulation of RFT. Central to this work is a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work serves as a roadmap for researchers seeking to advance MARFT toward\nresilient and adaptive solutions in agentic systems. Our implementation of the\nproposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT."
                },
                "authors": [
                    {
                        "name": "Junwei Liao"
                    },
                    {
                        "name": "Muning Wen"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16129v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16129v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13500v2",
                "updated": "2025-11-03T08:12:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    8,
                    12,
                    8,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-15T12:50:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    50,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts"
                },
                "summary": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, an enhanced benchmark\nwith broader coverage of medical subjects, designed to evaluate both single and\nbatch edits under strict locality constraints. We then propose MedREK, a\nretrieval-based editing framework that integrates a shared query-key module for\nprecise matching with an attention-based prompt encoder for informative\nguidance. Experimental results on various medical benchmarks demonstrate that\nour MedREK achieves superior performance across different core metrics and\nprovides the first validated solution for batch-editing in medical LLMs. Our\ncode and dataset are available at https://github.com/mylittleriver/MedREK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, an enhanced benchmark\nwith broader coverage of medical subjects, designed to evaluate both single and\nbatch edits under strict locality constraints. We then propose MedREK, a\nretrieval-based editing framework that integrates a shared query-key module for\nprecise matching with an attention-based prompt encoder for informative\nguidance. Experimental results on various medical benchmarks demonstrate that\nour MedREK achieves superior performance across different core metrics and\nprovides the first validated solution for batch-editing in medical LLMs. Our\ncode and dataset are available at https://github.com/mylittleriver/MedREK."
                },
                "authors": [
                    {
                        "name": "Shujun Xia"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Yinan Zhou"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Zhenan Sun"
                    },
                    {
                        "name": "Quanzheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Quanzheng Li"
                },
                "author": "Quanzheng Li",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09802v2",
                "updated": "2025-11-03T07:39:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    39,
                    35,
                    0,
                    307,
                    0
                ],
                "published": "2025-04-14T02:03:54Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    2,
                    3,
                    54,
                    0,
                    104,
                    0
                ],
                "title": "Enhancing Reasoning Abilities of Small LLMs with Cognitive Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reasoning Abilities of Small LLMs with Cognitive Alignment"
                },
                "summary": "The reasoning capabilities of large reasoning models (LRMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need for training effective small reasoning models. A critical\nchallenge is that small models possess different reasoning capacities and\ncognitive trajectories compared with their larger counterparts. Hence, directly\ndistilling chain-of-thought (CoT) rationales from large LRMs to smaller ones\ncan sometimes be ineffective and often requires a substantial amount of\nannotated data. In this paper, we first introduce a novel\nCritique-Rethink-Verify (CRV) system, designed for training smaller yet\npowerful LRMs. Our CRV system consists of multiple LLM agents, each\nspecializing in unique tasks: (i) critiquing the CoT rationales according to\nthe cognitive capabilities of smaller models, (ii) rethinking and refining\nthese CoTs based on the critiques, and (iii) verifying the correctness of the\nrefined results. Building on the CRV system, we further propose the Cognitive\nPreference Optimization (CogPO) algorithm to continuously enhance the reasoning\nabilities of smaller models by aligning their reasoning processes with their\ncognitive capacities. Comprehensive evaluations on challenging reasoning\nbenchmarks demonstrate the efficacy of our CRV+CogPO framework, which\noutperforms other methods by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of large reasoning models (LRMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need for training effective small reasoning models. A critical\nchallenge is that small models possess different reasoning capacities and\ncognitive trajectories compared with their larger counterparts. Hence, directly\ndistilling chain-of-thought (CoT) rationales from large LRMs to smaller ones\ncan sometimes be ineffective and often requires a substantial amount of\nannotated data. In this paper, we first introduce a novel\nCritique-Rethink-Verify (CRV) system, designed for training smaller yet\npowerful LRMs. Our CRV system consists of multiple LLM agents, each\nspecializing in unique tasks: (i) critiquing the CoT rationales according to\nthe cognitive capabilities of smaller models, (ii) rethinking and refining\nthese CoTs based on the critiques, and (iii) verifying the correctness of the\nrefined results. Building on the CRV system, we further propose the Cognitive\nPreference Optimization (CogPO) algorithm to continuously enhance the reasoning\nabilities of smaller models by aligning their reasoning processes with their\ncognitive capacities. Comprehensive evaluations on challenging reasoning\nbenchmarks demonstrate the efficacy of our CRV+CogPO framework, which\noutperforms other methods by a large margin."
                },
                "authors": [
                    {
                        "name": "Wenrui Cai"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Junbing Yan"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Xiangzhong Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzhong Fang"
                },
                "author": "Xiangzhong Fang",
                "arxiv_comment": "emnlp 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04251v4",
                "updated": "2025-11-03T07:37:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    37,
                    51,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-01T06:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    6,
                    46,
                    49,
                    6,
                    152,
                    0
                ],
                "title": "Language-Driven Coordination and Learning in Multi-Agent Simulation\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Driven Coordination and Learning in Multi-Agent Simulation\n  Environments"
                },
                "summary": "This paper introduces LLM-MARL, a unified framework that incorporates large\nlanguage models (LLMs) into multi-agent reinforcement learning (MARL) to\nenhance coordination, communication, and generalization in simulated game\nenvironments. The framework features three modular components of Coordinator,\nCommunicator, and Memory, which dynamically generate subgoals, facilitate\nsymbolic inter-agent messaging, and support episodic recall. Training combines\nPPO with a language-conditioned loss and LLM query gating. LLM-MARL is\nevaluated in Google Research Football, MAgent Battle, and StarCraft II. Results\nshow consistent improvements over MAPPO and QMIX in win rate, coordination\nscore, and zero-shot generalization. Ablation studies demonstrate that subgoal\ngeneration and language-based messaging each contribute significantly to\nperformance gains. Qualitative analysis reveals emergent behaviors such as role\nspecialization and communication-driven tactics. By bridging language modeling\nand policy learning, this work contributes to the design of intelligent,\ncooperative agents in interactive simulations. It offers a path forward for\nleveraging LLMs in multi-agent systems used for training, games, and human-AI\ncollaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLM-MARL, a unified framework that incorporates large\nlanguage models (LLMs) into multi-agent reinforcement learning (MARL) to\nenhance coordination, communication, and generalization in simulated game\nenvironments. The framework features three modular components of Coordinator,\nCommunicator, and Memory, which dynamically generate subgoals, facilitate\nsymbolic inter-agent messaging, and support episodic recall. Training combines\nPPO with a language-conditioned loss and LLM query gating. LLM-MARL is\nevaluated in Google Research Football, MAgent Battle, and StarCraft II. Results\nshow consistent improvements over MAPPO and QMIX in win rate, coordination\nscore, and zero-shot generalization. Ablation studies demonstrate that subgoal\ngeneration and language-based messaging each contribute significantly to\nperformance gains. Qualitative analysis reveals emergent behaviors such as role\nspecialization and communication-driven tactics. By bridging language modeling\nand policy learning, this work contributes to the design of intelligent,\ncooperative agents in interactive simulations. It offers a path forward for\nleveraging LLMs in multi-agent systems used for training, games, and human-AI\ncollaboration."
                },
                "authors": [
                    {
                        "name": "Zhengyang Li"
                    },
                    {
                        "name": "Sawyer Campos"
                    },
                    {
                        "name": "Nana Wang"
                    }
                ],
                "author_detail": {
                    "name": "Nana Wang"
                },
                "author": "Nana Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22323v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22323v3",
                "updated": "2025-11-03T07:21:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    21,
                    22,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-28T13:09:47Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    9,
                    47,
                    2,
                    148,
                    0
                ],
                "title": "Advancing Expert Specialization for Better MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Expert Specialization for Better MoE"
                },
                "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community."
                },
                "authors": [
                    {
                        "name": "Hongcan Guo"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Bolun Chu"
                    },
                    {
                        "name": "Jialin Zhuang"
                    },
                    {
                        "name": "Yuan Yang"
                    },
                    {
                        "name": "Wenhao Che"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Qimei Cui"
                    },
                    {
                        "name": "Xudong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Jiang"
                },
                "author": "Xudong Jiang",
                "arxiv_comment": "33pages, 6figures(Accepted by Neurips 2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22323v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22323v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04405v2",
                "updated": "2025-11-03T07:08:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    8,
                    2,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-06T12:47:05Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    47,
                    5,
                    2,
                    218,
                    0
                ],
                "title": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via\n  Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via\n  Algorithm-System Co-Design"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration. In this paper, we propose FlexQ, a\nnovel post-training INT6 quantization framework combining algorithmic\ninnovation with system-level optimizations. FlexQ employs uniform 6-bit weight\nquantization across all layers, with adaptive retention of 8-bit activations in\nlayers identified through layer-wise sensitivity analysis. To maximize hardware\nefficiency, we develop a specialized high-performance GPU kernel supporting\nmatrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core\n(BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores.\nEvaluations on LLaMA family models show FlexQ maintains near-FP16 accuracy,\nwith perplexity increases of no more than 0.1 on WikiText2. The proposed kernel\nachieves an average 1.39$\\times$ speedup over ABQ-LLM on LLaMA-2-70B linear\nlayers. End-to-end, FlexQ delivers 1.33$\\times$ inference acceleration and\n1.21$\\times$ memory savings over SmoothQuant. Code is released at\nhttps://github.com/FlyFoxPlayer/FlexQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration. In this paper, we propose FlexQ, a\nnovel post-training INT6 quantization framework combining algorithmic\ninnovation with system-level optimizations. FlexQ employs uniform 6-bit weight\nquantization across all layers, with adaptive retention of 8-bit activations in\nlayers identified through layer-wise sensitivity analysis. To maximize hardware\nefficiency, we develop a specialized high-performance GPU kernel supporting\nmatrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core\n(BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores.\nEvaluations on LLaMA family models show FlexQ maintains near-FP16 accuracy,\nwith perplexity increases of no more than 0.1 on WikiText2. The proposed kernel\nachieves an average 1.39$\\times$ speedup over ABQ-LLM on LLaMA-2-70B linear\nlayers. End-to-end, FlexQ delivers 1.33$\\times$ inference acceleration and\n1.21$\\times$ memory savings over SmoothQuant. Code is released at\nhttps://github.com/FlyFoxPlayer/FlexQ."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Aining Jia"
                    },
                    {
                        "name": "Weifeng Bu"
                    },
                    {
                        "name": "Yushu Cai"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25741v2",
                "updated": "2025-11-03T06:54:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    54,
                    49,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-29T17:45:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    45,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "Scaling Latent Reasoning via Looped Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Latent Reasoning via Looped Language Models"
                },
                "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model is available here: http://ouro-llm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model is available here: http://ouro-llm.github.io."
                },
                "authors": [
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Kai Hua"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Boyi Wei"
                    },
                    {
                        "name": "Zixin Wen"
                    },
                    {
                        "name": "Fan Yin"
                    },
                    {
                        "name": "He Xing"
                    },
                    {
                        "name": "Lu Li"
                    },
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Taylor Kergan"
                    },
                    {
                        "name": "Andrew Smith"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Mude Hui"
                    },
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Qiyang Min"
                    },
                    {
                        "name": "Hongzhi Huang"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Yunfeng Shi"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Enduo Zhao"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Jason Eshraghian"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eshraghian"
                },
                "author": "Jason Eshraghian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05887v3",
                "updated": "2025-11-03T06:50:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    50,
                    31,
                    0,
                    307,
                    0
                ],
                "published": "2024-03-09T11:59:10Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    11,
                    59,
                    10,
                    5,
                    69,
                    0
                ],
                "title": "Aligning Speech to Languages to Enhance Code-switching Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Speech to Languages to Enhance Code-switching Speech\n  Recognition"
                },
                "summary": "Code-switching (CS) refers to the switching of languages within a speech\nsignal and results in language confusion for automatic speech recognition\n(ASR). To address language confusion, we propose a language alignment loss\n(LAL) that aligns acoustic features to pseudo-language labels learned from the\nASR decoder during ASR training. This approach enables frame-level language\nidentification without the need for frame-level language annotations. To\nfurther tackle the complex token alternatives for language modeling in\nbilingual scenarios, we propose to employ large language models via a\ngenerative error correction method. A linguistic hint, derived from LAL outputs\nand decoded hypotheses, is introduced to guide the prompting and enhance the\nLLM-based generative error correction for CS-ASR. The proposed methods are\nevaluated on the SEAME dataset and data from the ASRU 2019 Mandarin-English\ncode-switching speech recognition challenge. The incorporation of the proposed\nlanguage alignment loss improves CS-ASR performance for both hybrid\nCTC/attention and Whisper models on both datasets, with only a negligible\nincrease in the number of parameters. This work also highlights the efficacy of\nlanguage alignment loss in balancing primary-language-dominant bilingual data\nduring training, with an 8.6% relative improvement on the ASRU dataset compared\nto the baseline model. Performance evaluation using large language models\nreveals the advantage of the linguistic hint by achieving 14.1% and 5.5%\nrelative improvement on test sets of the ASRU and SEAME datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-switching (CS) refers to the switching of languages within a speech\nsignal and results in language confusion for automatic speech recognition\n(ASR). To address language confusion, we propose a language alignment loss\n(LAL) that aligns acoustic features to pseudo-language labels learned from the\nASR decoder during ASR training. This approach enables frame-level language\nidentification without the need for frame-level language annotations. To\nfurther tackle the complex token alternatives for language modeling in\nbilingual scenarios, we propose to employ large language models via a\ngenerative error correction method. A linguistic hint, derived from LAL outputs\nand decoded hypotheses, is introduced to guide the prompting and enhance the\nLLM-based generative error correction for CS-ASR. The proposed methods are\nevaluated on the SEAME dataset and data from the ASRU 2019 Mandarin-English\ncode-switching speech recognition challenge. The incorporation of the proposed\nlanguage alignment loss improves CS-ASR performance for both hybrid\nCTC/attention and Whisper models on both datasets, with only a negligible\nincrease in the number of parameters. This work also highlights the efficacy of\nlanguage alignment loss in balancing primary-language-dominant bilingual data\nduring training, with an 8.6% relative improvement on the ASRU dataset compared\nto the baseline model. Performance evaluation using large language models\nreveals the advantage of the linguistic hint by achieving 14.1% and 5.5%\nrelative improvement on test sets of the ASRU and SEAME datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Hexin Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Leibny Paola Garcia"
                    },
                    {
                        "name": "Andy W. H. Khong"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Shinji Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shinji Watanabe"
                },
                "author": "Shinji Watanabe",
                "arxiv_comment": "Accepted to IEEE Trans. Audio Speech Lang. Process., copyright has\n  been transferred to IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03659v2",
                "updated": "2025-11-03T06:48:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    48,
                    4,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-04T07:48:10Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    7,
                    48,
                    10,
                    2,
                    155,
                    0
                ],
                "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey"
                },
                "summary": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Baiyang Wang"
                    },
                    {
                        "name": "Robert E. Mercer"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Sudipta Singha Roy"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Xindi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xindi Wang"
                },
                "author": "Xindi Wang",
                "arxiv_comment": "accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06350v2",
                "updated": "2025-11-03T06:40:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    40,
                    4,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-08T14:30:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    30,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Effective Tokens with Video Anomaly in Large Language Models"
                },
                "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks."
                },
                "authors": [
                    {
                        "name": "Yingxian Chen"
                    },
                    {
                        "name": "Jiahui Liu"
                    },
                    {
                        "name": "Ruidi Fan"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Chirui Chang"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Wilton W. T. Fok"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Yik-Chung Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yik-Chung Wu"
                },
                "author": "Yik-Chung Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26781v2",
                "updated": "2025-11-03T06:01:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    1,
                    32,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-30T17:56:31Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    56,
                    31,
                    3,
                    303,
                    0
                ],
                "title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment"
                },
                "summary": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models."
                },
                "authors": [
                    {
                        "name": "Aniruddh Bansal"
                    },
                    {
                        "name": "Davit Soselia"
                    },
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.09051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.09051v5",
                "updated": "2025-11-03T05:24:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    5,
                    24,
                    8,
                    0,
                    307,
                    0
                ],
                "published": "2023-02-17T18:31:31Z",
                "published_parsed": [
                    2023,
                    2,
                    17,
                    18,
                    31,
                    31,
                    4,
                    48,
                    0
                ],
                "title": "Complex QA and language models hybrid architectures, Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex QA and language models hybrid architectures, Survey"
                },
                "summary": "This paper reviews the state-of-the-art of large language models (LLM)\narchitectures and strategies for \"complex\" question-answering with a focus on\nhybrid architectures. LLM based chatbot services have allowed anyone to grasp\nthe potential of LLM to solve many common problems, but soon discovered their\nlimitations for complex questions. Addressing more specific, complex questions\n(e.g., \"What is the best mix of power-generation methods to reduce climate\nchange ?\") often requires specialized architectures, domain knowledge, new\nskills, decomposition and multi-step resolution, deep reasoning, sensitive data\nprotection, explainability, and human-in-the-loop processes. Therefore, we\nreview: (1) necessary skills and tasks for handling complex questions and\ncommon LLM limits to overcome; (2) dataset, cost functions and evaluation\nmetrics for measuring and improving (e.g. accuracy, explainability, fairness,\nrobustness, groundedness, faithfulness, toxicity...); (3) family of solutions\nto overcome LLM limitations by (a) training and reinforcement (b)\nhybridization, (c) prompting, (d) agentic-architectures (agents, tools) and\nextended reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reviews the state-of-the-art of large language models (LLM)\narchitectures and strategies for \"complex\" question-answering with a focus on\nhybrid architectures. LLM based chatbot services have allowed anyone to grasp\nthe potential of LLM to solve many common problems, but soon discovered their\nlimitations for complex questions. Addressing more specific, complex questions\n(e.g., \"What is the best mix of power-generation methods to reduce climate\nchange ?\") often requires specialized architectures, domain knowledge, new\nskills, decomposition and multi-step resolution, deep reasoning, sensitive data\nprotection, explainability, and human-in-the-loop processes. Therefore, we\nreview: (1) necessary skills and tasks for handling complex questions and\ncommon LLM limits to overcome; (2) dataset, cost functions and evaluation\nmetrics for measuring and improving (e.g. accuracy, explainability, fairness,\nrobustness, groundedness, faithfulness, toxicity...); (3) family of solutions\nto overcome LLM limitations by (a) training and reinforcement (b)\nhybridization, (c) prompting, (d) agentic-architectures (agents, tools) and\nextended reasoning."
                },
                "authors": [
                    {
                        "name": "Xavier Daull"
                    },
                    {
                        "name": "Patrice Bellot"
                    },
                    {
                        "name": "Emmanuel Bruno"
                    },
                    {
                        "name": "Vincent Martin"
                    },
                    {
                        "name": "Elisabeth Murisasco"
                    }
                ],
                "author_detail": {
                    "name": "Elisabeth Murisasco"
                },
                "author": "Elisabeth Murisasco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.09051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.09051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26466v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26466v2",
                "updated": "2025-11-03T05:03:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    5,
                    3,
                    18,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-30T13:11:23Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    11,
                    23,
                    3,
                    303,
                    0
                ],
                "title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot\n  Recognition"
                },
                "summary": "Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning."
                },
                "authors": [
                    {
                        "name": "Pei Peng"
                    },
                    {
                        "name": "MingKun Xie"
                    },
                    {
                        "name": "Hang Hao"
                    },
                    {
                        "name": "Tong Jin"
                    },
                    {
                        "name": "ShengJun Huang"
                    }
                ],
                "author_detail": {
                    "name": "ShengJun Huang"
                },
                "author": "ShengJun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26466v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26466v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13957v2",
                "updated": "2025-11-03T04:26:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    4,
                    26,
                    49,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-17T13:28:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    28,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "Enhancing Time Awareness in Generative Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Time Awareness in Generative Recommendation"
                },
                "summary": "Generative recommendation has emerged as a promising paradigm that formulates\nthe recommendations into a text-to-text generation task, harnessing the vast\nknowledge of large language models. However, existing studies focus on\nconsidering the sequential order of items and neglect to handle the temporal\ndynamics across items, which can imply evolving user preferences. To address\nthis limitation, we propose a novel model, Generative Recommender Using Time\nawareness (GRUT), effectively capturing hidden user preferences via various\ntemporal signals. We first introduce Time-aware Prompting, consisting of two\nkey contexts. The user-level temporal context models personalized temporal\npatterns across timestamps and time intervals, while the item-level transition\ncontext provides transition patterns across users. We also devise Trend-aware\nInference, a training-free method that enhances rankings by incorporating trend\ninformation about items with generation likelihood. Extensive experiments\ndemonstrate that GRUT outperforms state-of-the-art models, with gains of up to\n15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The\nsource code is available at https://github.com/skleee/GRUT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation has emerged as a promising paradigm that formulates\nthe recommendations into a text-to-text generation task, harnessing the vast\nknowledge of large language models. However, existing studies focus on\nconsidering the sequential order of items and neglect to handle the temporal\ndynamics across items, which can imply evolving user preferences. To address\nthis limitation, we propose a novel model, Generative Recommender Using Time\nawareness (GRUT), effectively capturing hidden user preferences via various\ntemporal signals. We first introduce Time-aware Prompting, consisting of two\nkey contexts. The user-level temporal context models personalized temporal\npatterns across timestamps and time intervals, while the item-level transition\ncontext provides transition patterns across users. We also devise Trend-aware\nInference, a training-free method that enhances rankings by incorporating trend\ninformation about items with generation likelihood. Extensive experiments\ndemonstrate that GRUT outperforms state-of-the-art models, with gains of up to\n15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The\nsource code is available at https://github.com/skleee/GRUT."
                },
                "authors": [
                    {
                        "name": "Sunkyung Lee"
                    },
                    {
                        "name": "Seongmin Park"
                    },
                    {
                        "name": "Jonghyo Kim"
                    },
                    {
                        "name": "Mincheol Yoon"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "arxiv_comment": "EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11671v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11671v3",
                "updated": "2025-11-03T03:47:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    3,
                    47,
                    20,
                    0,
                    307,
                    0
                ],
                "published": "2025-04-16T00:02:28Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    0,
                    2,
                    28,
                    2,
                    106,
                    0
                ],
                "title": "Computational Basis of LLM's Decision Making in Social Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Basis of LLM's Decision Making in Social Simulation"
                },
                "summary": "Large language models (LLMs) increasingly serve as human-like decision-making\nagents in social science and applied settings. These LLM-agents are typically\nassigned human-like characters and placed in real-life contexts. However, how\nthese characters and contexts shape an LLM's behavior remains underexplored.\nThis study proposes and tests methods for probing, quantifying, and modifying\nan LLM's internal representations in a Dictator Game -- a classic behavioral\nexperiment on fairness and prosocial behavior. We extract \"vectors of variable\nvariations\" (e.g., \"male\" to \"female\") from the LLM's internal state.\nManipulating these vectors during the model's inference can substantially alter\nhow those variables relate to the model's decision-making. This approach offers\na principled way to study and regulate how social concepts can be encoded and\nengineered within transformer-based models, with implications for alignment,\ndebiasing, and designing AI agents for social simulations in both academic and\ncommercial applications, strengthening sociological theory and measurement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly serve as human-like decision-making\nagents in social science and applied settings. These LLM-agents are typically\nassigned human-like characters and placed in real-life contexts. However, how\nthese characters and contexts shape an LLM's behavior remains underexplored.\nThis study proposes and tests methods for probing, quantifying, and modifying\nan LLM's internal representations in a Dictator Game -- a classic behavioral\nexperiment on fairness and prosocial behavior. We extract \"vectors of variable\nvariations\" (e.g., \"male\" to \"female\") from the LLM's internal state.\nManipulating these vectors during the model's inference can substantially alter\nhow those variables relate to the model's decision-making. This approach offers\na principled way to study and regulate how social concepts can be encoded and\nengineered within transformer-based models, with implications for alignment,\ndebiasing, and designing AI agents for social simulations in both academic and\ncommercial applications, strengthening sociological theory and measurement."
                },
                "authors": [
                    {
                        "name": "Ji Ma"
                    }
                ],
                "author_detail": {
                    "name": "Ji Ma"
                },
                "author": "Ji Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11671v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11671v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15685v2",
                "updated": "2025-11-03T03:44:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    3,
                    44,
                    12,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-21T16:01:11Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    1,
                    11,
                    2,
                    141,
                    0
                ],
                "title": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems"
                },
                "summary": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions."
                },
                "authors": [
                    {
                        "name": "Xiuchao Sui"
                    },
                    {
                        "name": "Daiying Tian"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Kenneth Kwok"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "EMNLP 2025 camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20772v2",
                "updated": "2025-11-03T03:21:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    3,
                    21,
                    16,
                    0,
                    307,
                    0
                ],
                "published": "2025-02-28T06:46:21Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    6,
                    46,
                    21,
                    4,
                    59,
                    0
                ],
                "title": "Damper-B-PINN: Damper Characteristics-Based Bayesian Physics-Informed\n  Neural Network for Vehicle State Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Damper-B-PINN: Damper Characteristics-Based Bayesian Physics-Informed\n  Neural Network for Vehicle State Estimation"
                },
                "summary": "Accurate state estimation is fundamental to intelligent vehicles. Wheel load,\none of the most important chassis states, serves as an essential input for\nadvanced driver assistance systems (ADAS) and exerts a direct influence on\nvehicle stability and safety. However, wheel load estimation remains\nchallenging due to the complexity of chassis modeling and the susceptibility of\nnonlinear systems to noise. To address these issues, this paper first\nintroduces a refined suspension linkage-level modeling approach that constructs\na nonlinear instantaneous dynamic model by explicitly considering the complex\ngeometric structure of the suspension. Building upon this, we propose a damper\ncharacteristics-based Bayesian physics-informed neural network (Damper-B-PINN)\nframework to estimate dynamic wheel load, which leverages the suspension\ndynamics as physical guidance of PINN while employing Bayesian inference to\nmitigate the effects of system noise and uncertainty. Moreover, a\ndamper-characteristic physics conditioning (DPC) module is designed for\nembedding physical prior. The proposed Damper-B-PINN is evaluated using both\nhigh-fidelity simulation datasets generated by CarSim software and real-world\ndatasets collected from a Formula Student race car. Experimental results\ndemonstrate that our Damper-B-PINN consistently outperforms existing methods\nacross various test conditions, particularly extreme ones. These findings\nhighlight the potential of the proposed Damper-B-PINN framework to enhance the\naccuracy and robustness of dynamic wheel load estimation, thereby improving the\nreliability and safety of ADAS applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate state estimation is fundamental to intelligent vehicles. Wheel load,\none of the most important chassis states, serves as an essential input for\nadvanced driver assistance systems (ADAS) and exerts a direct influence on\nvehicle stability and safety. However, wheel load estimation remains\nchallenging due to the complexity of chassis modeling and the susceptibility of\nnonlinear systems to noise. To address these issues, this paper first\nintroduces a refined suspension linkage-level modeling approach that constructs\na nonlinear instantaneous dynamic model by explicitly considering the complex\ngeometric structure of the suspension. Building upon this, we propose a damper\ncharacteristics-based Bayesian physics-informed neural network (Damper-B-PINN)\nframework to estimate dynamic wheel load, which leverages the suspension\ndynamics as physical guidance of PINN while employing Bayesian inference to\nmitigate the effects of system noise and uncertainty. Moreover, a\ndamper-characteristic physics conditioning (DPC) module is designed for\nembedding physical prior. The proposed Damper-B-PINN is evaluated using both\nhigh-fidelity simulation datasets generated by CarSim software and real-world\ndatasets collected from a Formula Student race car. Experimental results\ndemonstrate that our Damper-B-PINN consistently outperforms existing methods\nacross various test conditions, particularly extreme ones. These findings\nhighlight the potential of the proposed Damper-B-PINN framework to enhance the\naccuracy and robustness of dynamic wheel load estimation, thereby improving the\nreliability and safety of ADAS applications."
                },
                "authors": [
                    {
                        "name": "Tianyi Zeng"
                    },
                    {
                        "name": "Tianyi Wang"
                    },
                    {
                        "name": "Zimo Zeng"
                    },
                    {
                        "name": "Feiyang Zhang"
                    },
                    {
                        "name": "Jiseop Byeon"
                    },
                    {
                        "name": "Yujin Wang"
                    },
                    {
                        "name": "Yajie Zou"
                    },
                    {
                        "name": "Yangyang Wang"
                    },
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Christian Claudel"
                    },
                    {
                        "name": "Xinbo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinbo Chen"
                },
                "author": "Xinbo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21432v3",
                "updated": "2025-11-03T02:58:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    58,
                    46,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-29T09:01:34Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    1,
                    34,
                    4,
                    241,
                    0
                ],
                "title": "RepoMark: A Data-Usage Auditing Framework for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoMark: A Data-Usage Auditing Framework for Code Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables auditors to verify\nwhether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect model\nbehavior difference on trained data. Compared to prior data auditing\napproaches, RepoMark significantly enhances data efficiency, allowing effective\nauditing even when the user's repository possesses only a small number of code\nfiles.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of code authors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables auditors to verify\nwhether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect model\nbehavior difference on trained data. Compared to prior data auditing\napproaches, RepoMark significantly enhances data efficiency, allowing effective\nauditing even when the user's repository possesses only a small number of code\nfiles.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of code authors."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Yuguang Zhou"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Lionel Z. Wang"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00311v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00311v3",
                "updated": "2025-11-03T02:52:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    52,
                    22,
                    0,
                    307,
                    0
                ],
                "published": "2024-11-30T01:29:23Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    1,
                    29,
                    23,
                    5,
                    335,
                    0
                ],
                "title": "Characterizing the Effects of Environmental Exposures on Social\n  Mobility: Bayesian Semi-parametrics for Principal Stratification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Effects of Environmental Exposures on Social\n  Mobility: Bayesian Semi-parametrics for Principal Stratification"
                },
                "summary": "Understanding the causal effects of air pollution exposures on social\nmobility is attracting increasing attention. At the same time, education is\nwidely recognized as a key driver of social mobility. However, the causal\npathways linking fine particulate matter (PM2.5) exposure, educational\nattainment, and social mobility remain largely unexplored. To address this, we\nadopt the principal stratification approach, which rigorously defines causal\neffects when a post-treatment variable--educational attainment--is affected by\nexposure--PM2.5--and may, in turn, affect the primary outcome--social mobility.\nTo estimate the causal effects, we propose a Bayesian semi-parametric method\nleveraging infinite mixtures for modeling the primary outcome. The proposed\nmethod (i) allows flexible modeling of the distribution of the primary\npotential outcomes, (ii) improves the accuracy of counterfactual imputation--a\nfundamental problem in causal inference framework--, and (iii) enables the\ncharacterization of treatment effects across different values of the\npost-treatment variable. We evaluate the performance of the proposed\nmethodology through a Monte Carlo simulation study, demonstrating its\nadvantages over existing approaches. Finally, we apply our method to a national\ndataset of 3,009 counties in the United States to estimate the causal effect of\nPM2.5 on social mobility, taking into account educational attainment as a\npost-treatment variable. Our findings indicate that in counties where higher\nPM2.5 exposure significantly reduces educational attainment social mobility\ndecreases by approximately 5% compared to counties with lower PM2.5 exposure.\nWe also find that in counties where exposure to PM2.5 does not affect\neducational attainment, social mobility is reduced by approximately 2% hinting\nat the possibility of further, yet unexplored, pathways connecting air\npollution and social mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the causal effects of air pollution exposures on social\nmobility is attracting increasing attention. At the same time, education is\nwidely recognized as a key driver of social mobility. However, the causal\npathways linking fine particulate matter (PM2.5) exposure, educational\nattainment, and social mobility remain largely unexplored. To address this, we\nadopt the principal stratification approach, which rigorously defines causal\neffects when a post-treatment variable--educational attainment--is affected by\nexposure--PM2.5--and may, in turn, affect the primary outcome--social mobility.\nTo estimate the causal effects, we propose a Bayesian semi-parametric method\nleveraging infinite mixtures for modeling the primary outcome. The proposed\nmethod (i) allows flexible modeling of the distribution of the primary\npotential outcomes, (ii) improves the accuracy of counterfactual imputation--a\nfundamental problem in causal inference framework--, and (iii) enables the\ncharacterization of treatment effects across different values of the\npost-treatment variable. We evaluate the performance of the proposed\nmethodology through a Monte Carlo simulation study, demonstrating its\nadvantages over existing approaches. Finally, we apply our method to a national\ndataset of 3,009 counties in the United States to estimate the causal effect of\nPM2.5 on social mobility, taking into account educational attainment as a\npost-treatment variable. Our findings indicate that in counties where higher\nPM2.5 exposure significantly reduces educational attainment social mobility\ndecreases by approximately 5% compared to counties with lower PM2.5 exposure.\nWe also find that in counties where exposure to PM2.5 does not affect\neducational attainment, social mobility is reduced by approximately 2% hinting\nat the possibility of further, yet unexplored, pathways connecting air\npollution and social mobility."
                },
                "authors": [
                    {
                        "name": "Dafne Zorzetto"
                    },
                    {
                        "name": "Paolo Dalla Torre"
                    },
                    {
                        "name": "Sonia Petrone"
                    },
                    {
                        "name": "Francesca Dominici"
                    },
                    {
                        "name": "Falco J. Bargagli-Stoffi"
                    }
                ],
                "author_detail": {
                    "name": "Falco J. Bargagli-Stoffi"
                },
                "author": "Falco J. Bargagli-Stoffi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00311v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00311v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10987v2",
                "updated": "2025-11-03T02:23:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    23,
                    39,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-13T03:53:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    53,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge\n  Distillation"
                },
                "summary": "The promise of LLM watermarking rests on a core assumption that a specific\nwatermark proves authorship by a specific model. We demonstrate that this\nassumption is dangerously flawed. We introduce the threat of watermark\nspoofing, a sophisticated attack that allows a malicious model to generate text\ncontaining the authentic-looking watermark of a trusted, victim model. This\nenables the seamless misattribution of harmful content, such as disinformation,\nto reputable sources. The key to our attack is repurposing watermark\nradioactivity, the unintended inheritance of data patterns during fine-tuning,\nfrom a discoverable trait into an attack vector. By distilling knowledge from a\nwatermarked teacher model, our framework allows an attacker to steal and\nreplicate the watermarking signal of the victim model. This work reveals a\ncritical security gap in text authorship verification and calls for a paradigm\nshift towards technologies capable of distinguishing authentic watermarks from\nexpertly imitated ones. Our code is available at\nhttps://github.com/hsannn/ditto.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promise of LLM watermarking rests on a core assumption that a specific\nwatermark proves authorship by a specific model. We demonstrate that this\nassumption is dangerously flawed. We introduce the threat of watermark\nspoofing, a sophisticated attack that allows a malicious model to generate text\ncontaining the authentic-looking watermark of a trusted, victim model. This\nenables the seamless misattribution of harmful content, such as disinformation,\nto reputable sources. The key to our attack is repurposing watermark\nradioactivity, the unintended inheritance of data patterns during fine-tuning,\nfrom a discoverable trait into an attack vector. By distilling knowledge from a\nwatermarked teacher model, our framework allows an attacker to steal and\nreplicate the watermarking signal of the victim model. This work reveals a\ncritical security gap in text authorship verification and calls for a paradigm\nshift towards technologies capable of distinguishing authentic watermarks from\nexpertly imitated ones. Our code is available at\nhttps://github.com/hsannn/ditto.git."
                },
                "authors": [
                    {
                        "name": "Hyeseon Ahn"
                    },
                    {
                        "name": "Shinwoo Park"
                    },
                    {
                        "name": "Suyeon Woo"
                    },
                    {
                        "name": "Yo-Sub Han"
                    }
                ],
                "author_detail": {
                    "name": "Yo-Sub Han"
                },
                "author": "Yo-Sub Han",
                "arxiv_comment": "14 pages, 4 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23488v3",
                "updated": "2025-11-03T02:18:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    18,
                    28,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-27T20:23:13Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    23,
                    13,
                    5,
                    270,
                    0
                ],
                "title": "Mapping Overlaps in Benchmarks through Perplexity in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping Overlaps in Benchmarks through Perplexity in the Wild"
                },
                "summary": "We develop signatures of capacity familiarity to characterize large language\nmodel (LLM) benchmarks and their meaningful overlaps. Benchmark signatures\nprobe the capacity required for benchmark performance. We formally define them\nas a set of salient tokens drawn from in-the-wild, naturally authored corpora,\nwhere LLM token perplexity, reflecting more or less pre-training exposure,\nbecomes highly predictive of LLM benchmark performance. Through a large-scale\nmeta-evaluation, we extract benchmark signatures via stepwise forward selection\nwith linear regressions across 32 LLMs and 88 benchmarks spanning diverse\nknowledge, coding, logic, instruction following, math, language, reasoning, and\nworld modeling. Our analysis situates signatures in relation to both the\nsemantic similarity of benchmark questions and the correlation of model\nperformance. While performance overlaps are universally high and semantic\noverlaps remain confined to a narrow mid-range, benchmark signatures prove\nhighly informative in capturing variation, overlap, and divergence. We observe\noverlap in knowledge and reasoning subtasks, whereas multilingual and cultural\nbenchmarks exhibit less similarity, even compared to cross-task overlap.\nNotably, performance-level results are strongly influenced by\nbenchmark-orthogonal factors such as question format, highlighting limitations\nin LLM generalization, the conflation of performance with ability, and issues\ninherent in current mainstream benchmark agreement studies. Benchmark\nsignatures, however, remain robust to such effects. Ultimately, we identify\ncross-functional overlaps across logic, math, language, instruction following,\nand world modeling, with coding emerging as the least overlapping domain.\nTogether, these findings provide mechanistic insights into benchmark validity\nand LLM sensitivities, and sketch the underlying landscape of interconnected\nLLM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop signatures of capacity familiarity to characterize large language\nmodel (LLM) benchmarks and their meaningful overlaps. Benchmark signatures\nprobe the capacity required for benchmark performance. We formally define them\nas a set of salient tokens drawn from in-the-wild, naturally authored corpora,\nwhere LLM token perplexity, reflecting more or less pre-training exposure,\nbecomes highly predictive of LLM benchmark performance. Through a large-scale\nmeta-evaluation, we extract benchmark signatures via stepwise forward selection\nwith linear regressions across 32 LLMs and 88 benchmarks spanning diverse\nknowledge, coding, logic, instruction following, math, language, reasoning, and\nworld modeling. Our analysis situates signatures in relation to both the\nsemantic similarity of benchmark questions and the correlation of model\nperformance. While performance overlaps are universally high and semantic\noverlaps remain confined to a narrow mid-range, benchmark signatures prove\nhighly informative in capturing variation, overlap, and divergence. We observe\noverlap in knowledge and reasoning subtasks, whereas multilingual and cultural\nbenchmarks exhibit less similarity, even compared to cross-task overlap.\nNotably, performance-level results are strongly influenced by\nbenchmark-orthogonal factors such as question format, highlighting limitations\nin LLM generalization, the conflation of performance with ability, and issues\ninherent in current mainstream benchmark agreement studies. Benchmark\nsignatures, however, remain robust to such effects. Ultimately, we identify\ncross-functional overlaps across logic, math, language, instruction following,\nand world modeling, with coding emerging as the least overlapping domain.\nTogether, these findings provide mechanistic insights into benchmark validity\nand LLM sensitivities, and sketch the underlying landscape of interconnected\nLLM capabilities."
                },
                "authors": [
                    {
                        "name": "Siyang Wu"
                    },
                    {
                        "name": "Honglin Bao"
                    },
                    {
                        "name": "Sida Li"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "James A. Evans"
                    }
                ],
                "author_detail": {
                    "name": "James A. Evans"
                },
                "author": "James A. Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17669v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17669v3",
                "updated": "2025-11-03T02:16:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    16,
                    30,
                    0,
                    307,
                    0
                ],
                "published": "2024-05-27T21:47:41Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    21,
                    47,
                    41,
                    0,
                    148,
                    0
                ],
                "title": "Bayesian Nonparametrics for Principal Stratification with Continuous\n  Post-Treatment Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Nonparametrics for Principal Stratification with Continuous\n  Post-Treatment Variables"
                },
                "summary": "Principal stratification provides a causal inference framework for\ninvestigating treatment effects in the presence of a post-treatment variable.\nPrincipal strata play a key role in characterizing the treatment effect by\nidentifying groups of units with the same or similar values for the potential\npost-treatment variable at all treatment levels. The literature has focused\nmainly on binary post-treatment variables. Few papers considered continuous\npost-treatment variables. In the presence of a continuous post-treatment, a\nchallenge is how to identify and characterize meaningful coarsening of the\nlatent principal strata that lead to interpretable principal causal effects.\nThis paper introduces the Confounders-Aware SHared atoms BAyesian mixture\n(CASBAH), a novel approach for principal stratification with binary treatment\nand continuous post-treatment variables. CASBAH leverages Bayesian\nnonparametric priors with an innovative hierarchical structure for the\npotential post-treatment outcomes that overcomes some of the limitations of\nprevious works. Specifically, the novel features of our method allow for (i)\nidentifying coarsened principal strata through a data-adaptive approach and\n(ii) providing a comprehensive quantification of the uncertainty surrounding\nstratum membership. Through Monte Carlo simulations, we show that the proposed\nmethodology performs better than existing methods in characterizing the\nprincipal strata and estimating principal effects of the treatment. Finally,\nCASBAH is applied to a case study in which we estimate the causal effects of US\nnational air quality regulations on pollution levels and health outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principal stratification provides a causal inference framework for\ninvestigating treatment effects in the presence of a post-treatment variable.\nPrincipal strata play a key role in characterizing the treatment effect by\nidentifying groups of units with the same or similar values for the potential\npost-treatment variable at all treatment levels. The literature has focused\nmainly on binary post-treatment variables. Few papers considered continuous\npost-treatment variables. In the presence of a continuous post-treatment, a\nchallenge is how to identify and characterize meaningful coarsening of the\nlatent principal strata that lead to interpretable principal causal effects.\nThis paper introduces the Confounders-Aware SHared atoms BAyesian mixture\n(CASBAH), a novel approach for principal stratification with binary treatment\nand continuous post-treatment variables. CASBAH leverages Bayesian\nnonparametric priors with an innovative hierarchical structure for the\npotential post-treatment outcomes that overcomes some of the limitations of\nprevious works. Specifically, the novel features of our method allow for (i)\nidentifying coarsened principal strata through a data-adaptive approach and\n(ii) providing a comprehensive quantification of the uncertainty surrounding\nstratum membership. Through Monte Carlo simulations, we show that the proposed\nmethodology performs better than existing methods in characterizing the\nprincipal strata and estimating principal effects of the treatment. Finally,\nCASBAH is applied to a case study in which we estimate the causal effects of US\nnational air quality regulations on pollution levels and health outcomes."
                },
                "authors": [
                    {
                        "name": "Dafne Zorzetto"
                    },
                    {
                        "name": "Antonio Canale"
                    },
                    {
                        "name": "Fabrizia Mealli"
                    },
                    {
                        "name": "Francesca Dominici"
                    },
                    {
                        "name": "Falco J. Bargagli-Stoffi"
                    }
                ],
                "author_detail": {
                    "name": "Falco J. Bargagli-Stoffi"
                },
                "author": "Falco J. Bargagli-Stoffi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17669v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17669v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24086v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24086v3",
                "updated": "2025-11-03T02:15:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    15,
                    34,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-30T17:42:22Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    42,
                    22,
                    0,
                    181,
                    0
                ],
                "title": "MotionGPT3: Human Motion as a Second Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionGPT3: Human Motion as a Second Modality"
                },
                "summary": "With the rapid progress of large language models (LLMs), multimodal\nframeworks that unify understanding and generation have become promising, yet\nthey face increasing complexity as the number of modalities and tasks grows. We\nobserve that motion quantization introduces approximation errors that cap\nmotion quality, and that unifying discrete text and continuous motion within a\nsingle-stream backbone amplifies cross-modal interference. Motivated by recent\nmulti-branch Transformer designs that separate signals from different\nmodalities, we propose MotionGPT3, a bimodal motion-language model for both\nunderstanding and generation. MotionGPT3 encodes raw motion into a continuous\nlatent space using a variational autoencoder (VAE), thereby avoiding\nquantization-induced artifacts, while leveraging the semantic prior of\npretrained language models. A dual-stream Transformer with shared attention\npreserves modality-specific routes while enabling controlled, bidirectional\ninformation flow, which reduces interference, stabilizing optimization, and\nempirically accelerates convergence without degrading fidelity. For multimodal\njoint training, a generate-then-align three-stage schedule further improves\nstability and limits cross-task interference. Experiments show that MotionGPT3\nachieves 2x faster convergence in training loss and up to 4x faster convergence\nin validation, while maintaining state-of-the-art performance on standard\nmotion understanding and motion generation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid progress of large language models (LLMs), multimodal\nframeworks that unify understanding and generation have become promising, yet\nthey face increasing complexity as the number of modalities and tasks grows. We\nobserve that motion quantization introduces approximation errors that cap\nmotion quality, and that unifying discrete text and continuous motion within a\nsingle-stream backbone amplifies cross-modal interference. Motivated by recent\nmulti-branch Transformer designs that separate signals from different\nmodalities, we propose MotionGPT3, a bimodal motion-language model for both\nunderstanding and generation. MotionGPT3 encodes raw motion into a continuous\nlatent space using a variational autoencoder (VAE), thereby avoiding\nquantization-induced artifacts, while leveraging the semantic prior of\npretrained language models. A dual-stream Transformer with shared attention\npreserves modality-specific routes while enabling controlled, bidirectional\ninformation flow, which reduces interference, stabilizing optimization, and\nempirically accelerates convergence without degrading fidelity. For multimodal\njoint training, a generate-then-align three-stage schedule further improves\nstability and limits cross-task interference. Experiments show that MotionGPT3\nachieves 2x faster convergence in training loss and up to 4x faster convergence\nin validation, while maintaining state-of-the-art performance on standard\nmotion understanding and motion generation benchmarks."
                },
                "authors": [
                    {
                        "name": "Bingfan Zhu"
                    },
                    {
                        "name": "Biao Jiang"
                    },
                    {
                        "name": "Sunyi Wang"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Linjie Luo"
                    },
                    {
                        "name": "Youyi Zheng"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24086v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24086v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21972v2",
                "updated": "2025-11-03T02:14:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    14,
                    57,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-26T06:59:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    6,
                    59,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "From Superficial Outputs to Superficial Learning: Risks of Large\n  Language Models in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Superficial Outputs to Superficial Learning: Risks of Large\n  Language Models in Education"
                },
                "summary": "Large Language Models (LLMs) are transforming education by enabling\npersonalization, feedback, and knowledge access, while also raising concerns\nabout risks to students and learning systems. Yet empirical evidence on these\nrisks remains fragmented. This paper presents a systematic review of 70\nempirical studies across computer science, education, and psychology. Guided by\nfour research questions, we examine: (i) which applications of LLMs in\neducation have been most frequently explored; (ii) how researchers have\nmeasured their impact; (iii) which risks stem from such applications; and (iv)\nwhat mitigation strategies have been proposed. We find that research on LLMs\nclusters around three domains: operational effectiveness, personalized\napplications, and interactive learning tools. Across these, model-level risks\ninclude superficial understanding, bias, limited robustness, anthropomorphism,\nhallucinations, privacy concerns, and knowledge constraints. When learners\ninteract with LLMs, these risks extend to cognitive and behavioural outcomes,\nincluding reduced neural activity, over-reliance, diminished independent\nlearning skills, and a loss of student agency. To capture this progression, we\npropose an LLM-Risk Adapted Learning Model that illustrates how technical risks\ncascade through interaction and interpretation to shape educational outcomes.\nAs the first synthesis of empirically assessed risks, this review provides a\nfoundation for responsible, human-centred integration of LLMs in education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming education by enabling\npersonalization, feedback, and knowledge access, while also raising concerns\nabout risks to students and learning systems. Yet empirical evidence on these\nrisks remains fragmented. This paper presents a systematic review of 70\nempirical studies across computer science, education, and psychology. Guided by\nfour research questions, we examine: (i) which applications of LLMs in\neducation have been most frequently explored; (ii) how researchers have\nmeasured their impact; (iii) which risks stem from such applications; and (iv)\nwhat mitigation strategies have been proposed. We find that research on LLMs\nclusters around three domains: operational effectiveness, personalized\napplications, and interactive learning tools. Across these, model-level risks\ninclude superficial understanding, bias, limited robustness, anthropomorphism,\nhallucinations, privacy concerns, and knowledge constraints. When learners\ninteract with LLMs, these risks extend to cognitive and behavioural outcomes,\nincluding reduced neural activity, over-reliance, diminished independent\nlearning skills, and a loss of student agency. To capture this progression, we\npropose an LLM-Risk Adapted Learning Model that illustrates how technical risks\ncascade through interaction and interpretation to shape educational outcomes.\nAs the first synthesis of empirically assessed risks, this review provides a\nfoundation for responsible, human-centred integration of LLMs in education."
                },
                "authors": [
                    {
                        "name": "Iris Delikoura"
                    },
                    {
                        "name": "Yi. R Fung"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13698v2",
                "updated": "2025-11-03T02:09:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    9,
                    36,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-15T15:57:17Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    57,
                    17,
                    2,
                    288,
                    0
                ],
                "title": "Risk-adaptive Activation Steering for Safe Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk-adaptive Activation Steering for Safe Multimodal Large Language\n  Models"
                },
                "summary": "One of the key challenges of modern AI models is ensuring that they provide\nhelpful responses to benign queries while refusing malicious ones. But often,\nthe models are vulnerable to multimodal queries with harmful intent embedded in\nimages. One approach for safety alignment is training with extensive safety\ndatasets at the significant costs in both dataset curation and training.\nInference-time alignment mitigates these costs, but introduces two drawbacks:\nexcessive refusals from misclassified benign queries and slower inference speed\ndue to iterative output adjustments. To overcome these limitations, we propose\nto reformulate queries to strengthen cross-modal attention to safety-critical\nimage regions, enabling accurate risk assessment at the query level. Using the\nassessed risk, it adaptively steers activations to generate responses that are\nsafe and helpful without overhead from iterative output adjustments. We call\nthis Risk-adaptive Activation Steering (RAS). Extensive experiments across\nmultiple benchmarks on multimodal safety and utility demonstrate that the RAS\nsignificantly reduces attack success rates, preserves general task performance,\nand improves inference speed over prior inference-time defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key challenges of modern AI models is ensuring that they provide\nhelpful responses to benign queries while refusing malicious ones. But often,\nthe models are vulnerable to multimodal queries with harmful intent embedded in\nimages. One approach for safety alignment is training with extensive safety\ndatasets at the significant costs in both dataset curation and training.\nInference-time alignment mitigates these costs, but introduces two drawbacks:\nexcessive refusals from misclassified benign queries and slower inference speed\ndue to iterative output adjustments. To overcome these limitations, we propose\nto reformulate queries to strengthen cross-modal attention to safety-critical\nimage regions, enabling accurate risk assessment at the query level. Using the\nassessed risk, it adaptively steers activations to generate responses that are\nsafe and helpful without overhead from iterative output adjustments. We call\nthis Risk-adaptive Activation Steering (RAS). Extensive experiments across\nmultiple benchmarks on multimodal safety and utility demonstrate that the RAS\nsignificantly reduces attack success rates, preserves general task performance,\nand improves inference speed over prior inference-time defenses."
                },
                "authors": [
                    {
                        "name": "Jonghyun Park"
                    },
                    {
                        "name": "Minhyuk Seo"
                    },
                    {
                        "name": "Jonghyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jonghyun Choi"
                },
                "author": "Jonghyun Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24028v2",
                "updated": "2025-11-03T01:49:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    1,
                    49,
                    39,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-28T03:23:53Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    3,
                    23,
                    53,
                    1,
                    301,
                    0
                ],
                "title": "OneCast: Structured Decomposition and Modular Generation for\n  Cross-Domain Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneCast: Structured Decomposition and Modular Generation for\n  Cross-Domain Time Series Forecasting"
                },
                "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Tingyue Pan"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Shilong Zhang"
                    },
                    {
                        "name": "Zhiding Liu"
                    },
                    {
                        "name": "Xiaoyu Tao"
                    },
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13918v3",
                "updated": "2025-11-03T01:42:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    1,
                    42,
                    59,
                    0,
                    307,
                    0
                ],
                "published": "2024-10-17T09:09:09Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    9,
                    9,
                    9,
                    3,
                    291,
                    0
                ],
                "title": "FTSmartAudit: A Knowledge Distillation-Enhanced Framework for Automated\n  Smart Contract Auditing Using Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FTSmartAudit: A Knowledge Distillation-Enhanced Framework for Automated\n  Smart Contract Auditing Using Fine-Tuned LLMs"
                },
                "summary": "The rapid growth of blockchain technology has driven the widespread adoption\nof smart contracts. However, their inherent vulnerabilities have led to\nsignificant financial losses. Traditional auditing methods, while essential,\nstruggle to keep pace with the increasing complexity and scale of smart\ncontracts. Large Language Models (LLMs) offer promising capabilities for\nautomating vulnerability detection, but their adoption is often limited by high\ncomputational costs. Although prior work has explored leveraging large models\nthrough agents or workflows, relatively little attention has been given to\nimproving the performance of smaller, fine-tuned models--a critical factor for\nachieving both efficiency and data privacy. In this paper, we introduce\nHKT-SmartAudit, a framework for developing lightweight models optimized for\nsmart contract auditing. It features a multi-stage knowledge distillation\npipeline that integrates classical distillation, external domain knowledge, and\nreward-guided learning to transfer high-quality insights from large teacher\nmodels. A single-task learning strategy is employed to train compact student\nmodels that maintain high accuracy and robustness while significantly reducing\ncomputational overhead. Experimental results show that our distilled models\noutperform both commercial tools and larger models in detecting complex\nvulnerabilities and logical flaws, offering a practical, secure, and scalable\nsolution for smart contract auditing. The source code is available at Github\nrepository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of blockchain technology has driven the widespread adoption\nof smart contracts. However, their inherent vulnerabilities have led to\nsignificant financial losses. Traditional auditing methods, while essential,\nstruggle to keep pace with the increasing complexity and scale of smart\ncontracts. Large Language Models (LLMs) offer promising capabilities for\nautomating vulnerability detection, but their adoption is often limited by high\ncomputational costs. Although prior work has explored leveraging large models\nthrough agents or workflows, relatively little attention has been given to\nimproving the performance of smaller, fine-tuned models--a critical factor for\nachieving both efficiency and data privacy. In this paper, we introduce\nHKT-SmartAudit, a framework for developing lightweight models optimized for\nsmart contract auditing. It features a multi-stage knowledge distillation\npipeline that integrates classical distillation, external domain knowledge, and\nreward-guided learning to transfer high-quality insights from large teacher\nmodels. A single-task learning strategy is employed to train compact student\nmodels that maintain high accuracy and robustness while significantly reducing\ncomputational overhead. Experimental results show that our distilled models\noutperform both commercial tools and larger models in detecting complex\nvulnerabilities and logical flaws, offering a practical, secure, and scalable\nsolution for smart contract auditing. The source code is available at Github\nrepository."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wei"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Xianhao Zhang"
                    },
                    {
                        "name": "Zhe Hou"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Hou"
                },
                "author": "Zhe Hou",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07229v2",
                "updated": "2025-11-03T01:37:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    1,
                    37,
                    36,
                    0,
                    307,
                    0
                ],
                "published": "2025-07-09T19:05:33Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    19,
                    5,
                    33,
                    2,
                    190,
                    0
                ],
                "title": "SynthTextEval: Synthetic Text Data Generation and Evaluation for\n  High-Stakes Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynthTextEval: Synthetic Text Data Generation and Evaluation for\n  High-Stakes Domains"
                },
                "summary": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development."
                },
                "authors": [
                    {
                        "name": "Krithika Ramesh"
                    },
                    {
                        "name": "Daniel Smolyak"
                    },
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Nupoor Gandhi"
                    },
                    {
                        "name": "Ritu Agarwal"
                    },
                    {
                        "name": "Margrt Bjarnadttir"
                    },
                    {
                        "name": "Anjalie Field"
                    }
                ],
                "author_detail": {
                    "name": "Anjalie Field"
                },
                "author": "Anjalie Field",
                "arxiv_comment": "EMNLP 2025 System Demonstration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02291v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02291v4",
                "updated": "2025-11-03T01:31:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    1,
                    31,
                    50,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-04T23:20:40Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    23,
                    20,
                    40,
                    6,
                    124,
                    0
                ],
                "title": "Dexterous Contact-Rich Manipulation via the Contact Trust Region",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous Contact-Rich Manipulation via the Contact Trust Region"
                },
                "summary": "What is a good local description of contact dynamics for contact-rich\nmanipulation, and where can we trust this local description? While many\napproaches often rely on the Taylor approximation of dynamics with an\nellipsoidal trust region, we argue that such approaches are fundamentally\ninconsistent with the unilateral nature of contact. As a remedy, we present the\nContact Trust Region (CTR), which captures the unilateral nature of contact\nwhile remaining efficient for computation. With CTR, we first develop a\nModel-Predictive Control (MPC) algorithm capable of synthesizing local\ncontact-rich plans. Then, we extend this capability to plan globally by\nstitching together local MPC plans, enabling efficient and dexterous\ncontact-rich manipulation. To verify the performance of our method, we perform\ncomprehensive evaluations, both in high-fidelity simulation and on hardware, on\ntwo contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand\nsystem. On both systems, our method offers a significantly lower-compute\nalternative to existing RL-based approaches to contact-rich manipulation. In\nparticular, our Allegro in-hand manipulation policy, in the form of a roadmap,\ntakes fewer than 10 minutes to build offline on a standard laptop using just\nits CPU, with online inference taking just a few seconds. Experiment data,\nvideo and code are available at ctr.theaiinstitute.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is a good local description of contact dynamics for contact-rich\nmanipulation, and where can we trust this local description? While many\napproaches often rely on the Taylor approximation of dynamics with an\nellipsoidal trust region, we argue that such approaches are fundamentally\ninconsistent with the unilateral nature of contact. As a remedy, we present the\nContact Trust Region (CTR), which captures the unilateral nature of contact\nwhile remaining efficient for computation. With CTR, we first develop a\nModel-Predictive Control (MPC) algorithm capable of synthesizing local\ncontact-rich plans. Then, we extend this capability to plan globally by\nstitching together local MPC plans, enabling efficient and dexterous\ncontact-rich manipulation. To verify the performance of our method, we perform\ncomprehensive evaluations, both in high-fidelity simulation and on hardware, on\ntwo contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand\nsystem. On both systems, our method offers a significantly lower-compute\nalternative to existing RL-based approaches to contact-rich manipulation. In\nparticular, our Allegro in-hand manipulation policy, in the form of a roadmap,\ntakes fewer than 10 minutes to build offline on a standard laptop using just\nits CPU, with online inference taking just a few seconds. Experiment data,\nvideo and code are available at ctr.theaiinstitute.com."
                },
                "authors": [
                    {
                        "name": "H. J. Terry Suh"
                    },
                    {
                        "name": "Tao Pang"
                    },
                    {
                        "name": "Tong Zhao"
                    },
                    {
                        "name": "Russ Tedrake"
                    }
                ],
                "author_detail": {
                    "name": "Russ Tedrake"
                },
                "author": "Russ Tedrake",
                "arxiv_journal_ref": "International Journal of Robotics Research 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02291v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02291v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05702v2",
                "updated": "2025-11-03T01:00:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    1,
                    0,
                    40,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-07T09:10:13Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    9,
                    10,
                    13,
                    1,
                    280,
                    0
                ],
                "title": "Uncovering Representation Bias for Investment Decisions in Open-Source\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Representation Bias for Investment Decisions in Open-Source\n  Large Language Models"
                },
                "summary": "Large Language Models are increasingly adopted in financial applications to\nsupport investment workflows. However, prior studies have seldom examined how\nthese models reflect biases related to firm size, sector, or financial\ncharacteristics, which can significantly impact decision-making. This paper\naddresses this gap by focusing on representation bias in open-source Qwen\nmodels. We propose a balanced round-robin prompting method over approximately\n150 U.S. equities, applying constrained decoding and token-logit aggregation to\nderive firm-level confidence scores across financial contexts. Using\nstatistical tests and variance analysis, we find that firm size and valuation\nconsistently increase model confidence, while risk factors tend to decrease it.\nConfidence varies significantly across sectors, with the Technology sector\nshowing the greatest variability. When models are prompted for specific\nfinancial categories, their confidence rankings best align with fundamental\ndata, moderately with technical signals, and least with growth indicators.\nThese results highlight representation bias in Qwen models and motivate\nsector-aware calibration and category-conditioned evaluation protocols for safe\nand fair financial LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly adopted in financial applications to\nsupport investment workflows. However, prior studies have seldom examined how\nthese models reflect biases related to firm size, sector, or financial\ncharacteristics, which can significantly impact decision-making. This paper\naddresses this gap by focusing on representation bias in open-source Qwen\nmodels. We propose a balanced round-robin prompting method over approximately\n150 U.S. equities, applying constrained decoding and token-logit aggregation to\nderive firm-level confidence scores across financial contexts. Using\nstatistical tests and variance analysis, we find that firm size and valuation\nconsistently increase model confidence, while risk factors tend to decrease it.\nConfidence varies significantly across sectors, with the Technology sector\nshowing the greatest variability. When models are prompted for specific\nfinancial categories, their confidence rankings best align with fundamental\ndata, moderately with technical signals, and least with growth indicators.\nThese results highlight representation bias in Qwen models and motivate\nsector-aware calibration and category-conditioned evaluation protocols for safe\nand fair financial LLM deployment."
                },
                "authors": [
                    {
                        "name": "Fabrizio Dimino"
                    },
                    {
                        "name": "Krati Saxena"
                    },
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Stefano Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Pasquali"
                },
                "author": "Stefano Pasquali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06769v3",
                "updated": "2025-11-03T00:53:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    0,
                    53,
                    34,
                    0,
                    307,
                    0
                ],
                "published": "2024-12-09T18:55:56Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    55,
                    56,
                    0,
                    344,
                    0
                ],
                "title": "Training Large Language Models to Reason in a Continuous Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models to Reason in a Continuous Latent Space"
                },
                "summary": "Large language models (LLMs) are typically constrained to reason in the\nlanguage space, where they express the reasoning process through a\nchain-of-thought (CoT) to solve complex problems. However, the language space\nmay not always be optimal for reasoning. Most word tokens primarily ensure\ntextual coherence and are not essential for reasoning, while some critical\ntokens require complex planning and pose challenges to LLMs. To explore the\npotential of reasoning beyond language, we introduce a new paradigm called\nCoconut (Chain of Continuous Thought). Coconut utilizes the last hidden state\nof the LLM as a representation of the reasoning state, termed \"continuous\nthought.\" Instead of decoding this state into words, we feed it back to the\nmodel as the next input embedding directly in the continuous space. This latent\nreasoning paradigm enables an advanced reasoning pattern, where continuous\nthoughts can encode multiple alternative next steps, allowing the model to\nperform a breadth-first search (BFS) rather than committing prematurely to a\nsingle deterministic path as in CoT. Coconut outperforms CoT on logical\nreasoning tasks that require substantial search during planning and achieves a\nbetter trade-off between accuracy and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically constrained to reason in the\nlanguage space, where they express the reasoning process through a\nchain-of-thought (CoT) to solve complex problems. However, the language space\nmay not always be optimal for reasoning. Most word tokens primarily ensure\ntextual coherence and are not essential for reasoning, while some critical\ntokens require complex planning and pose challenges to LLMs. To explore the\npotential of reasoning beyond language, we introduce a new paradigm called\nCoconut (Chain of Continuous Thought). Coconut utilizes the last hidden state\nof the LLM as a representation of the reasoning state, termed \"continuous\nthought.\" Instead of decoding this state into words, we feed it back to the\nmodel as the next input embedding directly in the continuous space. This latent\nreasoning paradigm enables an advanced reasoning pattern, where continuous\nthoughts can encode multiple alternative next steps, allowing the model to\nperform a breadth-first search (BFS) rather than committing prematurely to a\nsingle deterministic path as in CoT. Coconut outperforms CoT on logical\nreasoning tasks that require substantial search during planning and achieves a\nbetter trade-off between accuracy and efficiency."
                },
                "authors": [
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Yuandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuandong Tian"
                },
                "author": "Yuandong Tian",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12815v2",
                "updated": "2025-11-02T22:39:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    22,
                    39,
                    55,
                    6,
                    306,
                    0
                ],
                "published": "2025-08-18T10:53:20Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    10,
                    53,
                    20,
                    0,
                    230,
                    0
                ],
                "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Steer: Input-dependent Steering for Multimodal LLMs"
                },
                "summary": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines. Our code is publicly available at\nhttps://jayneelparekh.github.io/learn-to-steer/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines. Our code is publicly available at\nhttps://jayneelparekh.github.io/learn-to-steer/"
                },
                "authors": [
                    {
                        "name": "Jayneel Parekh"
                    },
                    {
                        "name": "Pegah Khayatan"
                    },
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Arnaud Dapogny"
                    },
                    {
                        "name": "Alasdair Newson"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26012v2",
                "updated": "2025-11-02T22:15:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    22,
                    15,
                    47,
                    6,
                    306,
                    0
                ],
                "published": "2025-10-29T22:57:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    22,
                    57,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "AutoSurvey2: Empowering Researchers with Next Level Automated Literature\n  Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoSurvey2: Empowering Researchers with Next Level Automated Literature\n  Surveys"
                },
                "summary": "The rapid growth of research literature, particularly in large language\nmodels (LLMs), has made producing comprehensive and current survey papers\nincreasingly difficult. This paper introduces autosurvey2, a multi-stage\npipeline that automates survey generation through retrieval-augmented synthesis\nand structured evaluation. The system integrates parallel section generation,\niterative refinement, and real-time retrieval of recent publications to ensure\nboth topical completeness and factual accuracy. Quality is assessed using a\nmulti-LLM evaluation framework that measures coverage, structure, and relevance\nin alignment with expert review standards. Experimental results demonstrate\nthat autosurvey2 consistently outperforms existing retrieval-based and\nautomated baselines, achieving higher scores in structural coherence and\ntopical relevance while maintaining strong citation fidelity. By combining\nretrieval, reasoning, and automated evaluation into a unified framework,\nautosurvey2 provides a scalable and reproducible solution for generating\nlong-form academic surveys and contributes a solid foundation for future\nresearch on automated scholarly writing. All code and resources are available\nat https://github.com/annihi1ation/auto_research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of research literature, particularly in large language\nmodels (LLMs), has made producing comprehensive and current survey papers\nincreasingly difficult. This paper introduces autosurvey2, a multi-stage\npipeline that automates survey generation through retrieval-augmented synthesis\nand structured evaluation. The system integrates parallel section generation,\niterative refinement, and real-time retrieval of recent publications to ensure\nboth topical completeness and factual accuracy. Quality is assessed using a\nmulti-LLM evaluation framework that measures coverage, structure, and relevance\nin alignment with expert review standards. Experimental results demonstrate\nthat autosurvey2 consistently outperforms existing retrieval-based and\nautomated baselines, achieving higher scores in structural coherence and\ntopical relevance while maintaining strong citation fidelity. By combining\nretrieval, reasoning, and automated evaluation into a unified framework,\nautosurvey2 provides a scalable and reproducible solution for generating\nlong-form academic surveys and contributes a solid foundation for future\nresearch on automated scholarly writing. All code and resources are available\nat https://github.com/annihi1ation/auto_research."
                },
                "authors": [
                    {
                        "name": "Siyi Wu"
                    },
                    {
                        "name": "Chiaxin Liang"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Leyi Zhao"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Xinyuan Song"
                    }
                ],
                "author_detail": {
                    "name": "Xinyuan Song"
                },
                "author": "Xinyuan Song",
                "arxiv_comment": "TKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11575v2",
                "updated": "2025-11-02T22:14:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    22,
                    14,
                    9,
                    6,
                    306,
                    0
                ],
                "published": "2025-09-15T04:39:50Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    4,
                    39,
                    50,
                    0,
                    258,
                    0
                ],
                "title": "A Survey of Reasoning and Agentic Systems in Time Series with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Reasoning and Agentic Systems in Time Series with Large\n  Language Models"
                },
                "summary": "Time series reasoning treats time as a first-class axis and incorporates\nintermediate evidence directly into the answer. This survey defines the problem\nand organizes the literature by reasoning topology with three families: direct\nreasoning in one step, linear chain reasoning with explicit intermediates, and\nbranch-structured reasoning that explores, revises, and aggregates. The\ntopology is crossed with the main objectives of the field, including\ntraditional time series analysis, explanation and understanding, causal\ninference and decision making, and time series generation, while a compact tag\nset spans these axes and captures decomposition and verification, ensembling,\ntool use, knowledge access, multimodality, agent loops, and LLM alignment\nregimes. Methods and systems are reviewed across domains, showing what each\ntopology enables and where it breaks down in faithfulness or robustness, along\nwith curated datasets, benchmarks, and resources that support study and\ndeployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\nEvaluation practices that keep evidence visible and temporally aligned are\nhighlighted, and guidance is distilled on matching topology to uncertainty,\ngrounding with observable artifacts, planning for shift and streaming, and\ntreating cost and latency as design budgets. We emphasize that reasoning\nstructures must balance capacity for grounding and self-correction against\ncomputational cost and reproducibility, while future progress will likely\ndepend on benchmarks that tie reasoning quality to utility and on closed-loop\ntestbeds that trade off cost and risk under shift-aware, streaming, and\nlong-horizon settings. Taken together, these directions mark a shift from\nnarrow accuracy toward reliability at scale, enabling systems that not only\nanalyze but also understand, explain, and act on dynamic worlds with traceable\nevidence and credible outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series reasoning treats time as a first-class axis and incorporates\nintermediate evidence directly into the answer. This survey defines the problem\nand organizes the literature by reasoning topology with three families: direct\nreasoning in one step, linear chain reasoning with explicit intermediates, and\nbranch-structured reasoning that explores, revises, and aggregates. The\ntopology is crossed with the main objectives of the field, including\ntraditional time series analysis, explanation and understanding, causal\ninference and decision making, and time series generation, while a compact tag\nset spans these axes and captures decomposition and verification, ensembling,\ntool use, knowledge access, multimodality, agent loops, and LLM alignment\nregimes. Methods and systems are reviewed across domains, showing what each\ntopology enables and where it breaks down in faithfulness or robustness, along\nwith curated datasets, benchmarks, and resources that support study and\ndeployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\nEvaluation practices that keep evidence visible and temporally aligned are\nhighlighted, and guidance is distilled on matching topology to uncertainty,\ngrounding with observable artifacts, planning for shift and streaming, and\ntreating cost and latency as design budgets. We emphasize that reasoning\nstructures must balance capacity for grounding and self-correction against\ncomputational cost and reproducibility, while future progress will likely\ndepend on benchmarks that tie reasoning quality to utility and on closed-loop\ntestbeds that trade off cost and risk under shift-aware, streaming, and\nlong-horizon settings. Taken together, these directions mark a shift from\nnarrow accuracy toward reliability at scale, enabling systems that not only\nanalyze but also understand, explain, and act on dynamic worlds with traceable\nevidence and credible outcomes."
                },
                "authors": [
                    {
                        "name": "Ching Chang"
                    },
                    {
                        "name": "Yidan Shi"
                    },
                    {
                        "name": "Defu Cao"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Jeehyun Hwang"
                    },
                    {
                        "name": "Haixin Wang"
                    },
                    {
                        "name": "Jiacheng Pang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Wen-Chih Peng"
                    },
                    {
                        "name": "Tien-Fu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tien-Fu Chen"
                },
                "author": "Tien-Fu Chen",
                "arxiv_comment": "This paper is currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11511v3",
                "updated": "2025-11-02T21:46:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    21,
                    46,
                    34,
                    6,
                    306,
                    0
                ],
                "published": "2024-07-16T08:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    49,
                    35,
                    1,
                    198,
                    0
                ],
                "title": "Multi-Step Reasoning with Large Language Models, a Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Step Reasoning with Large Language Models, a Survey"
                },
                "summary": "Large language models (LLMs) with billions of parameters exhibit in-context\nlearning abilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks. The research on LLM\nreasoning abilities started with the question whether LLMs can solve grade\nschool math word problems, and has expanded to other tasks in the past few\nyears. This article reviews the field of multi-step reasoning with LLMs. We\npropose a taxonomy that identifies different ways to generate, evaluate, and\ncontrol multi-step reasoning. We provide an in-depth coverage of core\napproaches and open problems, and we propose a research agenda for the near\nfuture. We find that multi-step reasoning approaches have progressed beyond\nmath word problems, and can now successfully solve challenges in logic,\ncombinatorial games, and robotics, sometimes by first generating code that is\nthen executed by external tools. Many studies in multi-step methods use\nreinforcement learning for finetuning, external optimization loops, in-context\nreinforcement learning, and self-reflection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters exhibit in-context\nlearning abilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks. The research on LLM\nreasoning abilities started with the question whether LLMs can solve grade\nschool math word problems, and has expanded to other tasks in the past few\nyears. This article reviews the field of multi-step reasoning with LLMs. We\npropose a taxonomy that identifies different ways to generate, evaluate, and\ncontrol multi-step reasoning. We provide an in-depth coverage of core\napproaches and open problems, and we propose a research agenda for the near\nfuture. We find that multi-step reasoning approaches have progressed beyond\nmath word problems, and can now successfully solve challenges in logic,\ncombinatorial games, and robotics, sometimes by first generating code that is\nthen executed by external tools. Many studies in multi-step methods use\nreinforcement learning for finetuning, external optimization loops, in-context\nreinforcement learning, and self-reflection."
                },
                "authors": [
                    {
                        "name": "Aske Plaat"
                    },
                    {
                        "name": "Annie Wong"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Joost Broekens"
                    },
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Back"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Back"
                },
                "author": "Thomas Back",
                "arxiv_comment": "ACM Computing Surveys",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16728v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16728v3",
                "updated": "2025-11-02T21:36:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    21,
                    36,
                    2,
                    6,
                    306,
                    0
                ],
                "published": "2025-03-20T22:12:08Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    22,
                    12,
                    8,
                    3,
                    79,
                    0
                ],
                "title": "Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation"
                },
                "summary": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text."
                },
                "authors": [
                    {
                        "name": "Emiel van Miltenburg"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "4 pages + references. Submitted for publication in the Encyclopedia\n  of Language & Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16728v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16728v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v3",
                "updated": "2025-11-02T20:27:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    27,
                    27,
                    6,
                    306,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18462v2",
                "updated": "2025-11-02T20:11:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    11,
                    6,
                    6,
                    306,
                    0
                ],
                "published": "2024-07-26T02:09:32Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    2,
                    9,
                    32,
                    4,
                    208,
                    0
                ],
                "title": "MistralBSM: Leveraging Mistral-7B for Vehicular Networks Misbehavior\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MistralBSM: Leveraging Mistral-7B for Vehicular Networks Misbehavior\n  Detection"
                },
                "summary": "Malicious attacks on vehicular networks pose a serious threat to road safety\nas well as communication reliability. A major source of these threats stems\nfrom misbehaving vehicles within the network. To address this challenge, we\npropose a Large Language Model (LLM)-empowered Misbehavior Detection System\n(MDS) within an edge-cloud detection framework. Specifically, we fine-tune\nMistral-7B, a compact and high-performing LLM, to detect misbehavior based on\nBasic Safety Messages (BSM) sequences as the edge component for real-time\ndetection, while a larger LLM deployed in the cloud validates and reinforces\nthe edge model's detection through a more comprehensive analysis. By updating\nonly 0.012% of the model parameters, our model, which we named MistralBSM,\nachieves 98% accuracy in binary classification and 96% in multiclass\nclassification on a selected set of attacks from VeReMi dataset, outperforming\nLLAMA2-7B and RoBERTa. Our results validate the potential of LLMs in MDS,\nshowing a significant promise in strengthening vehicular network security to\nbetter ensure the safety of road users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious attacks on vehicular networks pose a serious threat to road safety\nas well as communication reliability. A major source of these threats stems\nfrom misbehaving vehicles within the network. To address this challenge, we\npropose a Large Language Model (LLM)-empowered Misbehavior Detection System\n(MDS) within an edge-cloud detection framework. Specifically, we fine-tune\nMistral-7B, a compact and high-performing LLM, to detect misbehavior based on\nBasic Safety Messages (BSM) sequences as the edge component for real-time\ndetection, while a larger LLM deployed in the cloud validates and reinforces\nthe edge model's detection through a more comprehensive analysis. By updating\nonly 0.012% of the model parameters, our model, which we named MistralBSM,\nachieves 98% accuracy in binary classification and 96% in multiclass\nclassification on a selected set of attacks from VeReMi dataset, outperforming\nLLAMA2-7B and RoBERTa. Our results validate the potential of LLMs in MDS,\nshowing a significant promise in strengthening vehicular network security to\nbetter ensure the safety of road users."
                },
                "authors": [
                    {
                        "name": "Wissal Hamhoum"
                    },
                    {
                        "name": "Soumaya Cherkaoui"
                    }
                ],
                "author_detail": {
                    "name": "Soumaya Cherkaoui"
                },
                "author": "Soumaya Cherkaoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11067v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11067v3",
                "updated": "2025-11-02T19:37:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    19,
                    37,
                    7,
                    6,
                    306,
                    0
                ],
                "published": "2024-10-14T20:25:49Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    20,
                    25,
                    49,
                    0,
                    288,
                    0
                ],
                "title": "Variational Inference in Location-Scale Families: Exact Recovery of the\n  Mean and Correlation Matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Inference in Location-Scale Families: Exact Recovery of the\n  Mean and Correlation Matrix"
                },
                "summary": "Given an intractable target density $p$, variational inference (VI) attempts\nto find the best approximation $q$ from a tractable family $Q$. This is\ntypically done by minimizing the exclusive Kullback-Leibler divergence,\n$\\text{KL}(q||p)$. In practice, $Q$ is not rich enough to contain $p$, and the\napproximation is misspecified even when it is a unique global minimizer of\n$\\text{KL}(q||p)$. In this paper, we analyze the robustness of VI to these\nmisspecifications when $p$ exhibits certain symmetries and $Q$ is a\nlocation-scale family that shares these symmetries. We prove strong guarantees\nfor VI not only under mild regularity conditions but also in the face of severe\nmisspecifications. Namely, we show that (i) VI recovers the mean of $p$ when\n$p$ exhibits an \\textit{even} symmetry, and (ii) it recovers the correlation\nmatrix of $p$ when in addition~$p$ exhibits an \\textit{elliptical} symmetry.\nThese guarantees hold for the mean even when $q$ is factorized and $p$ is not,\nand for the correlation matrix even when~$q$ and~$p$ behave differently in\ntheir tails. We analyze various regimes of Bayesian inference where these\nsymmetries are useful idealizations, and we also investigate experimentally how\nVI behaves in their absence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given an intractable target density $p$, variational inference (VI) attempts\nto find the best approximation $q$ from a tractable family $Q$. This is\ntypically done by minimizing the exclusive Kullback-Leibler divergence,\n$\\text{KL}(q||p)$. In practice, $Q$ is not rich enough to contain $p$, and the\napproximation is misspecified even when it is a unique global minimizer of\n$\\text{KL}(q||p)$. In this paper, we analyze the robustness of VI to these\nmisspecifications when $p$ exhibits certain symmetries and $Q$ is a\nlocation-scale family that shares these symmetries. We prove strong guarantees\nfor VI not only under mild regularity conditions but also in the face of severe\nmisspecifications. Namely, we show that (i) VI recovers the mean of $p$ when\n$p$ exhibits an \\textit{even} symmetry, and (ii) it recovers the correlation\nmatrix of $p$ when in addition~$p$ exhibits an \\textit{elliptical} symmetry.\nThese guarantees hold for the mean even when $q$ is factorized and $p$ is not,\nand for the correlation matrix even when~$q$ and~$p$ behave differently in\ntheir tails. We analyze various regimes of Bayesian inference where these\nsymmetries are useful idealizations, and we also investigate experimentally how\nVI behaves in their absence."
                },
                "authors": [
                    {
                        "name": "Charles C. Margossian"
                    },
                    {
                        "name": "Lawrence K. Saul"
                    }
                ],
                "author_detail": {
                    "name": "Lawrence K. Saul"
                },
                "author": "Lawrence K. Saul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11067v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11067v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04103v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04103v3",
                "updated": "2025-11-02T19:03:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    19,
                    3,
                    2,
                    6,
                    306,
                    0
                ],
                "published": "2025-07-05T17:12:33Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    17,
                    12,
                    33,
                    5,
                    186,
                    0
                ],
                "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Train Your LLM Web Agent: A Statistical Diagnosis"
                },
                "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models."
                },
                "authors": [
                    {
                        "name": "Dheeraj Vattikonda"
                    },
                    {
                        "name": "Santhoshi Ravichandran"
                    },
                    {
                        "name": "Emiliano Penaloza"
                    },
                    {
                        "name": "Hadi Nekoei"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Thibault Le Sellier de Chezelles"
                    },
                    {
                        "name": "Nicolas Gontier"
                    },
                    {
                        "name": "Miguel Muoz-Mrmol"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Stefania Raimondo"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Laurent Charlin"
                    },
                    {
                        "name": "Alexandre Pich"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Massimo Caccia"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Caccia"
                },
                "author": "Massimo Caccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04103v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04103v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07927v2",
                "updated": "2025-11-02T18:49:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    18,
                    49,
                    28,
                    6,
                    306,
                    0
                ],
                "published": "2025-06-09T16:43:38Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    16,
                    43,
                    38,
                    0,
                    160,
                    0
                ],
                "title": "Solving Inequality Proofs with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Inequality Proofs with Large Language Models"
                },
                "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/."
                },
                "authors": [
                    {
                        "name": "Jiayi Sheng"
                    },
                    {
                        "name": "Luna Lyu"
                    },
                    {
                        "name": "Jikai Jin"
                    },
                    {
                        "name": "Tony Xia"
                    },
                    {
                        "name": "Alex Gu"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Pan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Pan Lu"
                },
                "author": "Pan Lu",
                "arxiv_comment": "50 pages, 24 figures, accepted as a Spotlight at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03304v4",
                "updated": "2025-11-02T18:21:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    18,
                    21,
                    13,
                    6,
                    306,
                    0
                ],
                "published": "2025-02-05T16:03:17Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    3,
                    17,
                    2,
                    36,
                    0
                ],
                "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning"
                },
                "summary": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://github.com/Skilteee/DiZO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://github.com/Skilteee/DiZO."
                },
                "authors": [
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Caiwei Ding"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Jaewoo Lee"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08619v2",
                "updated": "2025-11-02T17:15:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    17,
                    15,
                    29,
                    6,
                    306,
                    0
                ],
                "published": "2025-07-11T14:19:05Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    19,
                    5,
                    4,
                    192,
                    0
                ],
                "title": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design"
                },
                "summary": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20%). Code compatibility peaked at 100% under\nspecific 2AS settings but averaged below 50% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20%). Code compatibility peaked at 100% under\nspecific 2AS settings but averaged below 50% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted."
                },
                "authors": [
                    {
                        "name": "Soheyl Massoudi"
                    },
                    {
                        "name": "Mark Fuge"
                    }
                ],
                "author_detail": {
                    "name": "Mark Fuge"
                },
                "author": "Mark Fuge",
                "arxiv_doi": "10.1115/DETC2025-168856",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1115/DETC2025-168856",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.08872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08872v3",
                "updated": "2025-11-03T18:54:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    54,
                    17,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-10T00:05:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    0,
                    5,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Social Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Social Welfare"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a social welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and social welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a social welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and social welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign ."
                },
                "authors": [
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "David Zhang"
                    },
                    {
                        "name": "Pedro Cisneros-Velarde"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "arxiv_doi": "10.48550/arXiv.2510.08872",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.48550/arXiv.2510.08872",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.08872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "31 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02085v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02085v6",
                "updated": "2025-11-03T18:47:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    47,
                    32,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-04T05:51:55Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    5,
                    51,
                    55,
                    0,
                    216,
                    0
                ],
                "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents"
                },
                "summary": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent."
                },
                "authors": [
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Yifu Guo"
                    },
                    {
                        "name": "Yuzhen Han"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Licheng Wang"
                    },
                    {
                        "name": "Mingguang Chen"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Huacan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huacan Wang"
                },
                "author": "Huacan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02085v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02085v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12828v2",
                "updated": "2025-11-03T18:20:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    37,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-11T20:07:54Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    20,
                    7,
                    54,
                    5,
                    284,
                    0
                ],
                "title": "SimKey: A Semantically Aware Key Module for Watermarking Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimKey: A Semantically Aware Key Module for Watermarking Language Models"
                },
                "summary": "The rapid spread of text generated by large language models (LLMs) makes it\nincreasingly difficult to distinguish authentic human writing from machine\noutput. Watermarking offers a promising solution: model owners can embed an\nimperceptible signal into generated text, marking its origin. Most leading\napproaches seed an LLM's next-token sampling with a pseudo-random key that can\nlater be recovered to identify the text as machine-generated, while only\nminimally altering the model's output distribution. However, these methods\nsuffer from two related issues: (i) watermarks are brittle to simple\nsurface-level edits such as paraphrasing or reordering; and (ii) adversaries\ncan append unrelated, potentially harmful text that inherits the watermark,\nrisking reputational damage to model owners. To address these issues, we\nintroduce SimKey, a semantic key module that strengthens watermark robustness\nby tying key generation to the meaning of prior context. SimKey uses\nlocality-sensitive hashing over semantic embeddings to ensure that paraphrased\ntext yields the same watermark key, while unrelated or semantically shifted\ntext produces a different one. Integrated with state-of-the-art watermarking\nschemes, SimKey improves watermark robustness to paraphrasing and translation\nwhile preventing harmful content from false attribution, establishing\nsemantic-aware keying as a practical and extensible watermarking direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of text generated by large language models (LLMs) makes it\nincreasingly difficult to distinguish authentic human writing from machine\noutput. Watermarking offers a promising solution: model owners can embed an\nimperceptible signal into generated text, marking its origin. Most leading\napproaches seed an LLM's next-token sampling with a pseudo-random key that can\nlater be recovered to identify the text as machine-generated, while only\nminimally altering the model's output distribution. However, these methods\nsuffer from two related issues: (i) watermarks are brittle to simple\nsurface-level edits such as paraphrasing or reordering; and (ii) adversaries\ncan append unrelated, potentially harmful text that inherits the watermark,\nrisking reputational damage to model owners. To address these issues, we\nintroduce SimKey, a semantic key module that strengthens watermark robustness\nby tying key generation to the meaning of prior context. SimKey uses\nlocality-sensitive hashing over semantic embeddings to ensure that paraphrased\ntext yields the same watermark key, while unrelated or semantically shifted\ntext produces a different one. Integrated with state-of-the-art watermarking\nschemes, SimKey improves watermark robustness to paraphrasing and translation\nwhile preventing harmful content from false attribution, establishing\nsemantic-aware keying as a practical and extensible watermarking direction."
                },
                "authors": [
                    {
                        "name": "Shingo Kodama"
                    },
                    {
                        "name": "Haya Diwan"
                    },
                    {
                        "name": "Lucas Rosenblatt"
                    },
                    {
                        "name": "R. Teal Witter"
                    },
                    {
                        "name": "Niv Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Niv Cohen"
                },
                "author": "Niv Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12441v2",
                "updated": "2025-11-03T17:44:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    44,
                    10,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-15T20:48:50Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    20,
                    48,
                    50,
                    0,
                    258,
                    0
                ],
                "title": "Automatic Network Planning with Digital Radio Twin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Network Planning with Digital Radio Twin"
                },
                "summary": "Network planning seeks to determine base station parameters that maximize\ncoverage and capacity in cellular networks. However, achieving optimal planning\nremains challenging due to the diversity of deployment scenarios and the\nsignificant simulation-to-reality discrepancy. In this paper, we propose\n\\emph{AutoPlan}, a new automatic network planning framework by leveraging\ndigital radio twin (DRT) techniques. We derive the DRT by finetuning the\nparameters of building materials to reduce the sim-to-real discrepancy based on\ncrowdsource real-world user data. Leveraging the DRT, we design a Bayesian\noptimization based algorithm to optimize the deployment parameters of base\nstations efficiently. Using the field measurement from Husker-Net, we\nextensively evaluate \\emph{AutoPlan} under various deployment scenarios, in\nterms of both coverage and capacity. The evaluation results show that\n\\emph{AutoPlan} flexibly adapts to different scenarios and achieves performance\ncomparable to exhaustive search, while requiring less than 2\\% of its\ncomputation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network planning seeks to determine base station parameters that maximize\ncoverage and capacity in cellular networks. However, achieving optimal planning\nremains challenging due to the diversity of deployment scenarios and the\nsignificant simulation-to-reality discrepancy. In this paper, we propose\n\\emph{AutoPlan}, a new automatic network planning framework by leveraging\ndigital radio twin (DRT) techniques. We derive the DRT by finetuning the\nparameters of building materials to reduce the sim-to-real discrepancy based on\ncrowdsource real-world user data. Leveraging the DRT, we design a Bayesian\noptimization based algorithm to optimize the deployment parameters of base\nstations efficiently. Using the field measurement from Husker-Net, we\nextensively evaluate \\emph{AutoPlan} under various deployment scenarios, in\nterms of both coverage and capacity. The evaluation results show that\n\\emph{AutoPlan} flexibly adapts to different scenarios and achieves performance\ncomparable to exhaustive search, while requiring less than 2\\% of its\ncomputation time."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Li"
                    },
                    {
                        "name": "Yuru Zhang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Mehmet Can Vuran"
                    },
                    {
                        "name": "Nathan Huynh"
                    },
                    {
                        "name": "Li Zhao"
                    },
                    {
                        "name": "Mizan Rahman"
                    },
                    {
                        "name": "Eren Erman Ozguven"
                    }
                ],
                "author_detail": {
                    "name": "Eren Erman Ozguven"
                },
                "author": "Eren Erman Ozguven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07653v2",
                "updated": "2025-11-03T17:13:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    13,
                    50,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-12T15:22:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "JobHop: A Large-Scale Dataset of Career Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JobHop: A Large-Scale Dataset of Career Trajectories"
                },
                "summary": "Understanding labor market dynamics is essential for policymakers, employers,\nand job seekers. However, comprehensive datasets that capture real-world career\ntrajectories are scarce. In this paper, we introduce JobHop, a large-scale\npublic dataset derived from anonymized resumes provided by VDAB, the public\nemployment service in Flanders, Belgium. Utilizing Large Language Models\n(LLMs), we process unstructured resume data to extract structured career\ninformation, which is then normalized to standardized ESCO occupation codes\nusing a multi-label classification model. This results in a rich dataset of\nover 1.67 million work experiences, extracted from and grouped into more than\n361,000 user resumes and mapped to standardized ESCO occupation codes, offering\nvaluable insights into real-world occupational transitions. This dataset\nenables diverse applications, such as analyzing labor market mobility, job\nstability, and the effects of career breaks on occupational transitions. It\nalso supports career path prediction and other data-driven decision-making\nprocesses. To illustrate its potential, we explore key dataset characteristics,\nincluding job distributions, career breaks, and job transitions, demonstrating\nits value for advancing labor market research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding labor market dynamics is essential for policymakers, employers,\nand job seekers. However, comprehensive datasets that capture real-world career\ntrajectories are scarce. In this paper, we introduce JobHop, a large-scale\npublic dataset derived from anonymized resumes provided by VDAB, the public\nemployment service in Flanders, Belgium. Utilizing Large Language Models\n(LLMs), we process unstructured resume data to extract structured career\ninformation, which is then normalized to standardized ESCO occupation codes\nusing a multi-label classification model. This results in a rich dataset of\nover 1.67 million work experiences, extracted from and grouped into more than\n361,000 user resumes and mapped to standardized ESCO occupation codes, offering\nvaluable insights into real-world occupational transitions. This dataset\nenables diverse applications, such as analyzing labor market mobility, job\nstability, and the effects of career breaks on occupational transitions. It\nalso supports career path prediction and other data-driven decision-making\nprocesses. To illustrate its potential, we explore key dataset characteristics,\nincluding job distributions, career breaks, and job transitions, demonstrating\nits value for advancing labor market research."
                },
                "authors": [
                    {
                        "name": "Iman Johary"
                    },
                    {
                        "name": "Raphael Romero"
                    },
                    {
                        "name": "Alexandru C. Mara"
                    },
                    {
                        "name": "Tijl De Bie"
                    }
                ],
                "author_detail": {
                    "name": "Tijl De Bie"
                },
                "author": "Tijl De Bie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15715v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15715v2",
                "updated": "2025-11-03T17:03:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    17,
                    3,
                    30,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-21T16:24:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    24,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with\n  Large Language Models for Mental Health Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with\n  Large Language Models for Mental Health Counseling"
                },
                "summary": "Large language models (LLMs) hold significant potential for mental health\nsupport, capable of generating empathetic responses and simulating therapeutic\nconversations. However, existing LLM-based approaches often lack the clinical\ngrounding necessary for real-world psychological counseling, particularly in\nexplicit diagnostic reasoning aligned with standards like the DSM/ICD and\nincorporating diverse therapeutic modalities beyond basic empathy or single\nstrategies. To address these critical limitations, we propose PsyLLM, the first\nlarge language model designed to systematically integrate both diagnostic and\ntherapeutic reasoning for mental health counseling. To develop PsyLLM, we\ndesign a novel automated data synthesis pipeline that processes real-world\nmental health posts collected from Reddit, where users frequently share\npsychological distress and seek community support. This pipeline processes\nreal-world mental health posts, generates multi-turn dialogue structures, and\nleverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and\nmultiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate\ndetailed clinical reasoning processes. Rigorous multi-dimensional filtering\nensures the generation of high-quality, clinically aligned dialogue data. In\naddition, we introduce a new benchmark and evaluation protocol, assessing\ncounseling quality across four key dimensions. Our experiments demonstrate that\nPsyLLM significantly outperforms state-of-the-art baseline models on this\nbenchmark. The model weights and dataset have been publicly released at\nhttps://github.com/Emo-gml/PsyLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold significant potential for mental health\nsupport, capable of generating empathetic responses and simulating therapeutic\nconversations. However, existing LLM-based approaches often lack the clinical\ngrounding necessary for real-world psychological counseling, particularly in\nexplicit diagnostic reasoning aligned with standards like the DSM/ICD and\nincorporating diverse therapeutic modalities beyond basic empathy or single\nstrategies. To address these critical limitations, we propose PsyLLM, the first\nlarge language model designed to systematically integrate both diagnostic and\ntherapeutic reasoning for mental health counseling. To develop PsyLLM, we\ndesign a novel automated data synthesis pipeline that processes real-world\nmental health posts collected from Reddit, where users frequently share\npsychological distress and seek community support. This pipeline processes\nreal-world mental health posts, generates multi-turn dialogue structures, and\nleverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and\nmultiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate\ndetailed clinical reasoning processes. Rigorous multi-dimensional filtering\nensures the generation of high-quality, clinically aligned dialogue data. In\naddition, we introduce a new benchmark and evaluation protocol, assessing\ncounseling quality across four key dimensions. Our experiments demonstrate that\nPsyLLM significantly outperforms state-of-the-art baseline models on this\nbenchmark. The model weights and dataset have been publicly released at\nhttps://github.com/Emo-gml/PsyLLM."
                },
                "authors": [
                    {
                        "name": "He Hu"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Juzheng Si"
                    },
                    {
                        "name": "Qianning Wang"
                    },
                    {
                        "name": "Hengheng Zhang"
                    },
                    {
                        "name": "Fuji Ren"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Laizhong Cui"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15715v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15715v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24448v2",
                "updated": "2025-11-03T16:32:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    32,
                    22,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-28T14:12:11Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    12,
                    11,
                    1,
                    301,
                    0
                ],
                "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Intelligence: Insights from Video Pretraining"
                },
                "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models."
                },
                "authors": [
                    {
                        "name": "Pablo Acuaviva"
                    },
                    {
                        "name": "Aram Davtyan"
                    },
                    {
                        "name": "Mariam Hassan"
                    },
                    {
                        "name": "Sebastian Stapf"
                    },
                    {
                        "name": "Ahmad Rahimi"
                    },
                    {
                        "name": "Alexandre Alahi"
                    },
                    {
                        "name": "Paolo Favaro"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Favaro"
                },
                "author": "Paolo Favaro",
                "arxiv_comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T45, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8; I.5.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17103v2",
                "updated": "2025-11-03T16:31:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    31,
                    16,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-21T08:50:49Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    50,
                    49,
                    2,
                    141,
                    0
                ],
                "title": "Forging Time Series with Language: A Large Language Model Approach to\n  Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forging Time Series with Language: A Large Language Model Approach to\n  Synthetic Data Generation"
                },
                "summary": "SDForger is a flexible and efficient framework for generating high-quality\nmultivariate time series using LLMs. Leveraging a compact data representation,\nSDForger provides synthetic time series generation from a few samples and\nlow-computation fine-tuning of any autoregressive LLM. Specifically, the\nframework transforms univariate and multivariate signals into tabular\nembeddings, which are then encoded into text and used to fine-tune the LLM. At\ninference, new textual embeddings are sampled and decoded into synthetic time\nseries that retain the original data's statistical properties and temporal\ndynamics. Across a diverse range of datasets, SDForger outperforms existing\ngenerative models in many scenarios, both in similarity-based evaluations and\ndownstream forecasting tasks. By enabling textual conditioning in the\ngeneration process, SDForger paves the way for multimodal modeling and the\nstreamlined integration of time series with textual information. The model is\nopen-sourced at\nhttps://github.com/IBM/fms-dgt/tree/main/fms_dgt/public/databuilders/time_series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDForger is a flexible and efficient framework for generating high-quality\nmultivariate time series using LLMs. Leveraging a compact data representation,\nSDForger provides synthetic time series generation from a few samples and\nlow-computation fine-tuning of any autoregressive LLM. Specifically, the\nframework transforms univariate and multivariate signals into tabular\nembeddings, which are then encoded into text and used to fine-tune the LLM. At\ninference, new textual embeddings are sampled and decoded into synthetic time\nseries that retain the original data's statistical properties and temporal\ndynamics. Across a diverse range of datasets, SDForger outperforms existing\ngenerative models in many scenarios, both in similarity-based evaluations and\ndownstream forecasting tasks. By enabling textual conditioning in the\ngeneration process, SDForger paves the way for multimodal modeling and the\nstreamlined integration of time series with textual information. The model is\nopen-sourced at\nhttps://github.com/IBM/fms-dgt/tree/main/fms_dgt/public/databuilders/time_series."
                },
                "authors": [
                    {
                        "name": "Ccile Rousseau"
                    },
                    {
                        "name": "Tobia Boschi"
                    },
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Dhaval Salwala"
                    },
                    {
                        "name": "Alessandra Pascale"
                    },
                    {
                        "name": "Juan Bernabe Moreno"
                    }
                ],
                "author_detail": {
                    "name": "Juan Bernabe Moreno"
                },
                "author": "Juan Bernabe Moreno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20172v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20172v3",
                "updated": "2025-11-03T16:12:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    16,
                    12,
                    9,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-24T14:36:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    36,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Benchmarking LLMs in Web API Integration Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs in Web API Integration Tasks"
                },
                "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models (LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present\nWAPIIBench, a dataset and evaluation pipeline designed to assess the ability of\nLLMs to generate web API invocation code. Our experiments with several\nopen-source LLMs reveal that generating API invocations poses a significant\nchallenge, resulting in hallucinated endpoints, incorrect argument usage, and\nother errors. None of the evaluated open-source models was able to solve more\nthan 40% of the tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models (LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present\nWAPIIBench, a dataset and evaluation pipeline designed to assess the ability of\nLLMs to generate web API invocation code. Our experiments with several\nopen-source LLMs reveal that generating API invocations poses a significant\nchallenge, resulting in hallucinated endpoints, incorrect argument usage, and\nother errors. None of the evaluated open-source models was able to solve more\nthan 40% of the tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Maninger"
                    },
                    {
                        "name": "Leon Chemnitz"
                    },
                    {
                        "name": "Amir Molzam Sharifloo"
                    },
                    {
                        "name": "Jannis Brugger"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "To be published in Proceedings of 2025 2nd IEEE/ACM International\n  Conference on AI-powered Software (AIware), Data & Benchmark Track; switched\n  to IEEE conference template",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20172v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20172v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16406v2",
                "updated": "2025-11-03T15:40:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    40,
                    21,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-22T14:13:16Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    14,
                    13,
                    16,
                    4,
                    234,
                    0
                ],
                "title": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak\n  Prevention for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak\n  Prevention for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner."
                },
                "authors": [
                    {
                        "name": "Guangyu Yang"
                    },
                    {
                        "name": "Jinghong Chen"
                    },
                    {
                        "name": "Jingbiao Mei"
                    },
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Bill Byrne"
                    }
                ],
                "author_detail": {
                    "name": "Bill Byrne"
                },
                "author": "Bill Byrne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21043v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21043v4",
                "updated": "2025-11-03T15:40:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    40,
                    2,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-25T11:48:37Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    11,
                    48,
                    37,
                    3,
                    268,
                    0
                ],
                "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Creativity: A New Frontier in Generalization Abilities"
                },
                "summary": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in\nparticular, are increasingly employed for creative tasks like scientific idea\ngeneration, constituting a form of generalization from training data\nunaddressed by existing conceptual frameworks. Despite its similarities to\ncompositional generalization (CG), combinatorial creativity (CC) is an\nopen-ended ability. Instead of evaluating for accuracy or correctness against\nfixed targets, which would contradict the open-ended nature of CC, we propose a\ntheoretical framework and algorithmic task for evaluating outputs by their\ndegrees of novelty and utility. From here, we make several important empirical\ncontributions: (1) We obtain the first insights into the scaling behavior of\ncreativity for LLMs. (2) We discover that, for fixed compute budgets, there\nexist optimal model depths and widths for creative ability. (3) We find that\nthe ideation-execution gap, whereby LLMs excel at generating novel scientific\nideas but struggle to ensure their practical feasibility, may be explained by a\nmore fundamental novelty-utility tradeoff characteristic of creativity\nalgorithms in general. Importantly, this tradeoff remains persistent even at\nscale, casting doubt on the long-term creative potential of LLMs in their\ncurrent form. Together, our conceptual framework and empirical findings provide\na foundation for understanding and improving creativity in modern AI models,\nbridging the gap between human and machine intelligence."
                },
                "authors": [
                    {
                        "name": "Samuel Schapiro"
                    },
                    {
                        "name": "Sumuk Shashidhar"
                    },
                    {
                        "name": "Alexi Gladstone"
                    },
                    {
                        "name": "Jonah Black"
                    },
                    {
                        "name": "Royce Moon"
                    },
                    {
                        "name": "Dilek Hakkani-Tur"
                    },
                    {
                        "name": "Lav R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Lav R. Varshney"
                },
                "author": "Lav R. Varshney",
                "arxiv_comment": "Preprint. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21043v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21043v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08150v2",
                "updated": "2025-11-03T15:39:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    39,
                    18,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-09T21:14:44Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    21,
                    14,
                    44,
                    1,
                    252,
                    0
                ],
                "title": "Verbalized Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Algorithms"
                },
                "summary": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks."
                },
                "authors": [
                    {
                        "name": "Supriya Lall"
                    },
                    {
                        "name": "Christian Farrell"
                    },
                    {
                        "name": "Hari Pathanjaly"
                    },
                    {
                        "name": "Marko Pavic"
                    },
                    {
                        "name": "Sarvesh Chezhian"
                    },
                    {
                        "name": "Masataro Asai"
                    }
                ],
                "author_detail": {
                    "name": "Masataro Asai"
                },
                "author": "Masataro Asai",
                "arxiv_comment": "Accepted in NeurIPS 2025 Workshop on Efficient Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03665v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03665v4",
                "updated": "2025-11-03T15:21:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    21,
                    13,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-05T17:24:50Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    17,
                    24,
                    50,
                    1,
                    217,
                    0
                ],
                "title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design"
                },
                "summary": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts."
                },
                "authors": [
                    {
                        "name": "Claudiu Leoveanu-Condrei"
                    }
                ],
                "author_detail": {
                    "name": "Claudiu Leoveanu-Condrei"
                },
                "author": "Claudiu Leoveanu-Condrei",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03665v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03665v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.2; I.1.2; D.1.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20318v2",
                "updated": "2025-11-03T15:12:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    12,
                    58,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-24T17:01:50Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    50,
                    2,
                    267,
                    0
                ],
                "title": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices"
                },
                "summary": "The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies such as hunting,\nfencing, use of repellents, and scare tactics. This underscores a critical need\nfor intelligent, autonomous solutions capable of real-time deer detection and\ndeterrence. But the progress in this field is impeded by a significant gap in\nthe literature, mainly the lack of a domain-specific, practical dataset and\nlimited study on the viability of deer detection systems on edge devices. To\naddress this gap, this study presents a comprehensive evaluation of\nstate-of-the-art deep learning models for deer detection in challenging\nreal-world scenarios. We introduce a curated, publicly available dataset of\n3,095 annotated images with bounding box annotations of deer. Then, we provide\nan extensive comparative analysis of 12 model variants across four recent YOLO\narchitectures (v8 to v11). Finally, we evaluated their performance on two\nrepresentative edge computing platforms: the CPU-based Raspberry Pi 5 and the\nGPU-accelerated NVIDIA Jetson AGX Xavier to assess feasibility for real-world\nfield deployment. Results show that the real-time detection performance is not\nfeasible on Raspberry Pi without hardware-specific model optimization, while\nNVIDIA Jetson provides greater than 30 frames per second (FPS) with 's' and 'n'\nseries models. This study also reveals that smaller, architecturally advanced\nmodels such as YOLOv11n, YOLOv8s, and YOLOv9s offer the optimal balance of high\naccuracy (Average Precision (AP) > 0.85) and computational efficiency\n(Inference Time < 34 milliseconds).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies such as hunting,\nfencing, use of repellents, and scare tactics. This underscores a critical need\nfor intelligent, autonomous solutions capable of real-time deer detection and\ndeterrence. But the progress in this field is impeded by a significant gap in\nthe literature, mainly the lack of a domain-specific, practical dataset and\nlimited study on the viability of deer detection systems on edge devices. To\naddress this gap, this study presents a comprehensive evaluation of\nstate-of-the-art deep learning models for deer detection in challenging\nreal-world scenarios. We introduce a curated, publicly available dataset of\n3,095 annotated images with bounding box annotations of deer. Then, we provide\nan extensive comparative analysis of 12 model variants across four recent YOLO\narchitectures (v8 to v11). Finally, we evaluated their performance on two\nrepresentative edge computing platforms: the CPU-based Raspberry Pi 5 and the\nGPU-accelerated NVIDIA Jetson AGX Xavier to assess feasibility for real-world\nfield deployment. Results show that the real-time detection performance is not\nfeasible on Raspberry Pi without hardware-specific model optimization, while\nNVIDIA Jetson provides greater than 30 frames per second (FPS) with 's' and 'n'\nseries models. This study also reveals that smaller, architecturally advanced\nmodels such as YOLOv11n, YOLOv8s, and YOLOv9s offer the optimal balance of high\naccuracy (Average Precision (AP) > 0.85) and computational efficiency\n(Inference Time < 34 milliseconds)."
                },
                "authors": [
                    {
                        "name": "Bishal Adhikari"
                    },
                    {
                        "name": "Jiajia Li"
                    },
                    {
                        "name": "Eric S. Michel"
                    },
                    {
                        "name": "Jacob Dykes"
                    },
                    {
                        "name": "Te-Ming Paul Tseng"
                    },
                    {
                        "name": "Mary Love Tagert"
                    },
                    {
                        "name": "Dong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dong Chen"
                },
                "author": "Dong Chen",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v6",
                "updated": "2025-11-03T15:04:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    4,
                    26,
                    0,
                    307,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gal Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gal Varoquaux"
                },
                "author": "Gal Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26292v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26292v2",
                "updated": "2025-11-03T13:54:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    13,
                    54,
                    16,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-30T14:08:14Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    8,
                    14,
                    1,
                    273,
                    0
                ],
                "title": "Solar Low Energy X-ray Spectrometer on board Aditya-L1: Ground\n  Calibration and In-flight Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar Low Energy X-ray Spectrometer on board Aditya-L1: Ground\n  Calibration and In-flight Performance"
                },
                "summary": "The Solar Low-Energy X-ray Spectrometer (SoLEXS) on board India's Aditya-L1\nmission was launched on 2 September 2023 and commenced solar observations on 13\nDecember 2023 following successful aperture cover deployment. Operating from\nthe Sun-Earth L1 Lagrange point, SoLEXS has been providing continuous\nSun-as-a-star soft X-ray spectroscopy across 2-22 keV with 170 eV resolution at\n5.9 keV and 1-second temporal cadence since 6 January 2024. The instrument\nemploys two Silicon Drift Detectors with aperture areas of 7.1 mm$^2$ and 0.1\nmm$^2$ to accommodate the full dynamic range of solar activity from A-class to\nX-class flares. This paper presents comprehensive ground and on board\ncalibration procedures that establish SoLEXS's quantitative spectroscopic\ncapabilities. Ground calibration encompassed energy-channel relationships,\nspectral resolution characterization, instrument response functions, and\ncollimator angular response measurements, with thermo-vacuum testing validating\nperformance stability across operational temperature ranges. On board\ncalibration utilizing an internal $^{55}$Fe source demonstrated preserved\npost-launch spectral resolution (164.9-171.2 eV), while cross-calibration with\nGOES-XRS and Chandrayaan-2/XSM confirmed radiometric accuracy and flux\nagreement. The instrument's 100% observational duty cycle at L1 enables\nunprecedented continuous monitoring of solar flare evolution across all\nintensity classes, providing calibrated data for advancing coronal heating\nmechanisms, flare energetics, and flare-coronal mass ejection relationship\nstudies through soft X-ray spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Solar Low-Energy X-ray Spectrometer (SoLEXS) on board India's Aditya-L1\nmission was launched on 2 September 2023 and commenced solar observations on 13\nDecember 2023 following successful aperture cover deployment. Operating from\nthe Sun-Earth L1 Lagrange point, SoLEXS has been providing continuous\nSun-as-a-star soft X-ray spectroscopy across 2-22 keV with 170 eV resolution at\n5.9 keV and 1-second temporal cadence since 6 January 2024. The instrument\nemploys two Silicon Drift Detectors with aperture areas of 7.1 mm$^2$ and 0.1\nmm$^2$ to accommodate the full dynamic range of solar activity from A-class to\nX-class flares. This paper presents comprehensive ground and on board\ncalibration procedures that establish SoLEXS's quantitative spectroscopic\ncapabilities. Ground calibration encompassed energy-channel relationships,\nspectral resolution characterization, instrument response functions, and\ncollimator angular response measurements, with thermo-vacuum testing validating\nperformance stability across operational temperature ranges. On board\ncalibration utilizing an internal $^{55}$Fe source demonstrated preserved\npost-launch spectral resolution (164.9-171.2 eV), while cross-calibration with\nGOES-XRS and Chandrayaan-2/XSM confirmed radiometric accuracy and flux\nagreement. The instrument's 100% observational duty cycle at L1 enables\nunprecedented continuous monitoring of solar flare evolution across all\nintensity classes, providing calibrated data for advancing coronal heating\nmechanisms, flare energetics, and flare-coronal mass ejection relationship\nstudies through soft X-ray spectroscopy."
                },
                "authors": [
                    {
                        "name": "Abhilash R. Sarwade"
                    },
                    {
                        "name": "Ankur Kushwaha"
                    },
                    {
                        "name": "M. C. Ramadevi"
                    },
                    {
                        "name": "Monoj Bug"
                    },
                    {
                        "name": "Kiran Lakshmipathaiah"
                    },
                    {
                        "name": "Smrati Verma"
                    },
                    {
                        "name": "Vaishali Sharan"
                    },
                    {
                        "name": "K. Sankarasubramanian"
                    }
                ],
                "author_detail": {
                    "name": "K. Sankarasubramanian"
                },
                "author": "K. Sankarasubramanian",
                "arxiv_doi": "10.1117/1.JATIS.11.4.045005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1117/1.JATIS.11.4.045005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.26292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26292v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 24 figures",
                "arxiv_journal_ref": "J. Astron. Telesc. Instrum. Syst. 11(4), 045005 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19189v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19189v3",
                "updated": "2025-11-03T13:29:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    13,
                    29,
                    4,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-23T16:05:16Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    5,
                    16,
                    1,
                    266,
                    0
                ],
                "title": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning\n  Rate Schedules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning\n  Rate Schedules"
                },
                "summary": "Scaling laws have emerged as a unifying lens for understanding and guiding\nthe training of large language models (LLMs). However, existing studies\npredominantly focus on the final-step loss, leaving open whether the entire\nloss dynamics obey similar laws and, crucially, how the learning rate schedule\n(LRS) shapes them. We address these gaps in a controlled theoretical setting by\nanalyzing stochastic gradient descent (SGD) on a power-law kernel regression\nmodel. The key insight is a novel intrinsic-time viewpoint, which captures the\ntraining progress more faithfully than iteration count. We then establish a\nFunctional Scaling Law (FSL) that captures the full loss trajectory under\narbitrary LRSs, with the schedule's influence entering through a simple\nconvolutional functional. We further instantiate the theory for three\nrepresentative LRSs -- constant, exponential decay, and warmup-stable-decay\n(WSD) -- and derive explicit scaling relations in both data- and\ncompute-limited regimes. These comparisons explain key empirical phenomena: (i)\nhigher-capacity models are more data- and compute-efficient; (ii) learning-rate\ndecay improves training efficiency; and (iii) WSD-type schedules outperform\npure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters\ndemonstrate the practical relevance of FSL as a surrogate model for fitting and\npredicting loss trajectories in large-scale pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws have emerged as a unifying lens for understanding and guiding\nthe training of large language models (LLMs). However, existing studies\npredominantly focus on the final-step loss, leaving open whether the entire\nloss dynamics obey similar laws and, crucially, how the learning rate schedule\n(LRS) shapes them. We address these gaps in a controlled theoretical setting by\nanalyzing stochastic gradient descent (SGD) on a power-law kernel regression\nmodel. The key insight is a novel intrinsic-time viewpoint, which captures the\ntraining progress more faithfully than iteration count. We then establish a\nFunctional Scaling Law (FSL) that captures the full loss trajectory under\narbitrary LRSs, with the schedule's influence entering through a simple\nconvolutional functional. We further instantiate the theory for three\nrepresentative LRSs -- constant, exponential decay, and warmup-stable-decay\n(WSD) -- and derive explicit scaling relations in both data- and\ncompute-limited regimes. These comparisons explain key empirical phenomena: (i)\nhigher-capacity models are more data- and compute-efficient; (ii) learning-rate\ndecay improves training efficiency; and (iii) WSD-type schedules outperform\npure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters\ndemonstrate the practical relevance of FSL as a surrogate model for fitting and\npredicting loss trajectories in large-scale pre-training."
                },
                "authors": [
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Fengling Chen"
                    },
                    {
                        "name": "Zixun Huang"
                    },
                    {
                        "name": "Lean Wang"
                    },
                    {
                        "name": "Lei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wu"
                },
                "author": "Lei Wu",
                "arxiv_comment": "60 pages, accepted by NeurIPS 2025 as a spotlight paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19189v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19189v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14925v2",
                "updated": "2025-11-03T12:53:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    53,
                    6,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-16T17:40:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    40,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters\n  to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters\n  to Large Language Models"
                },
                "summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we observe preliminary correlations between internal fragility and\nmiscalibration or hallucination (confabulation), and find that lightweight\ncritique prompts may modestly improve or worsen calibration in small-scale\ntests. These results suggest a structural bridge between Kantian\nself-limitation and feedback control, offering a principled lens to diagnose\nand potentially mitigate overconfidence in reasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we observe preliminary correlations between internal fragility and\nmiscalibration or hallucination (confabulation), and find that lightweight\ncritique prompts may modestly improve or worsen calibration in small-scale\ntests. These results suggest a structural bridge between Kantian\nself-limitation and feedback control, offering a principled lens to diagnose\nand potentially mitigate overconfidence in reasoning systems."
                },
                "authors": [
                    {
                        "name": "Akira Okutomi"
                    }
                ],
                "author_detail": {
                    "name": "Akira Okutomi"
                },
                "author": "Akira Okutomi",
                "arxiv_comment": "21 pages, 2 figures, preliminary version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13136v2",
                "updated": "2025-11-03T12:45:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    45,
                    10,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-19T14:07:20Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    7,
                    20,
                    0,
                    139,
                    0
                ],
                "title": "New Encoders for German Trained from Scratch: Comparing ModernGBERT with\n  Converted LLM2Vec Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Encoders for German Trained from Scratch: Comparing ModernGBERT with\n  Converted LLM2Vec Models"
                },
                "summary": "Encoders remain essential for efficient German NLP and NLU scenarios despite\nthe rise of decoder-only LLMs. This work studies two routes to high-quality\nGerman encoders under identical data and training constraints: 1) training from\nscratch and 2) converting decoders via LLM2Vec. We introduce two resources:\nModernGBERT (134M, 1B), fully transparent German encoders in the ModernBERT\nstyle, and LL\\\"aMmleinVec (120M, 1B, 7B), decoder-to-encoder conversions\ntrained with masked next-token prediction, both undergoing a context extension\nto 8.192 tokens.\n  Across SuperGLEBer, ModernGBERT 1B sets a new state of the art (avg 0.808),\nsurpassing GBERT Large (+4%) and the seven-times larger converted 7B model\n(0.787). On German MTEB after supervised fine-tuning, ModernGBERT 1B (0.551)\napproaches the converted 7B model (0.557).\n  We release all models, checkpoints, datasets, and full training records, and\nintroduce an encoder-adapted QA-NIAH evaluation. All in all, our results\nprovide actionable guidance: when parameter efficiency and latency matter,\nfrom-scratch encoders dominate. When a pre-trained decoder exists and compute\nis a limited, conversion offers an effective alternative. ModernGBERT and\nLL\\\"aMmleinVec, including all code, data and intermediary checkpoints are\npublished under a research-only RAIL license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoders remain essential for efficient German NLP and NLU scenarios despite\nthe rise of decoder-only LLMs. This work studies two routes to high-quality\nGerman encoders under identical data and training constraints: 1) training from\nscratch and 2) converting decoders via LLM2Vec. We introduce two resources:\nModernGBERT (134M, 1B), fully transparent German encoders in the ModernBERT\nstyle, and LL\\\"aMmleinVec (120M, 1B, 7B), decoder-to-encoder conversions\ntrained with masked next-token prediction, both undergoing a context extension\nto 8.192 tokens.\n  Across SuperGLEBer, ModernGBERT 1B sets a new state of the art (avg 0.808),\nsurpassing GBERT Large (+4%) and the seven-times larger converted 7B model\n(0.787). On German MTEB after supervised fine-tuning, ModernGBERT 1B (0.551)\napproaches the converted 7B model (0.557).\n  We release all models, checkpoints, datasets, and full training records, and\nintroduce an encoder-adapted QA-NIAH evaluation. All in all, our results\nprovide actionable guidance: when parameter efficiency and latency matter,\nfrom-scratch encoders dominate. When a pre-trained decoder exists and compute\nis a limited, conversion offers an effective alternative. ModernGBERT and\nLL\\\"aMmleinVec, including all code, data and intermediary checkpoints are\npublished under a research-only RAIL license."
                },
                "authors": [
                    {
                        "name": "Julia Wunderle"
                    },
                    {
                        "name": "Anton Ehrmanntraut"
                    },
                    {
                        "name": "Jan Pfister"
                    },
                    {
                        "name": "Fotis Jannidis"
                    },
                    {
                        "name": "Andreas Hotho"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Hotho"
                },
                "author": "Andreas Hotho",
                "arxiv_comment": "under review @LREC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00050v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00050v3",
                "updated": "2025-11-03T12:43:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    43,
                    56,
                    0,
                    307,
                    0
                ],
                "published": "2025-03-31T02:18:51Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    18,
                    51,
                    0,
                    90,
                    0
                ],
                "title": "JudgeLRM: Large Reasoning Models as a Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JudgeLRM: Large Reasoning Models as a Judge"
                },
                "summary": "Large Language Models (LLMs) are increasingly adopted as evaluators, offering\na scalable alternative to human annotation. However, existing supervised\nfine-tuning (SFT) approaches often fall short in domains that demand complex\nreasoning. Judgment is inherently reasoning-intensive: beyond surface-level\nscoring, it requires verifying evidence, identifying errors, and justifying\ndecisions. Through the analysis of evaluation tasks, we find a negative\ncorrelation between SFT performance gains and the proportion of\nreasoning-demanding samples, revealing the limits of SFT in such scenarios. To\naddress this, we introduce JudgeLRM, a family of judgment-oriented LLMs,\ntrained using reinforcement learning (RL) with judge-wise, outcome-driven\nrewards to activate reasoning capabilities. JudgeLRM consistently outperform\nSFT-tuned baselines in the same size, as well as other RL and SFT variants, and\neven surpass state-of-the-art reasoning models: notably, JudgeLRM-3B/4B exceeds\nGPT-4, while JudgeLRM-7B/8B/14B outperforms DeepSeek-R1 by over 2% in F1 score,\nwith particularly strong gains on reasoning-heavy tasks. Our findings\nunderscore the value of RL in unlocking reasoning-aligned LLM judges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly adopted as evaluators, offering\na scalable alternative to human annotation. However, existing supervised\nfine-tuning (SFT) approaches often fall short in domains that demand complex\nreasoning. Judgment is inherently reasoning-intensive: beyond surface-level\nscoring, it requires verifying evidence, identifying errors, and justifying\ndecisions. Through the analysis of evaluation tasks, we find a negative\ncorrelation between SFT performance gains and the proportion of\nreasoning-demanding samples, revealing the limits of SFT in such scenarios. To\naddress this, we introduce JudgeLRM, a family of judgment-oriented LLMs,\ntrained using reinforcement learning (RL) with judge-wise, outcome-driven\nrewards to activate reasoning capabilities. JudgeLRM consistently outperform\nSFT-tuned baselines in the same size, as well as other RL and SFT variants, and\neven surpass state-of-the-art reasoning models: notably, JudgeLRM-3B/4B exceeds\nGPT-4, while JudgeLRM-7B/8B/14B outperforms DeepSeek-R1 by over 2% in F1 score,\nwith particularly strong gains on reasoning-heavy tasks. Our findings\nunderscore the value of RL in unlocking reasoning-aligned LLM judges."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Qingyun Zou"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00050v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00050v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23433v2",
                "updated": "2025-11-03T12:40:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    40,
                    16,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-29T13:27:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    27,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity-Aware Policy Optimization for Large Language Model Reasoning"
                },
                "summary": "The reasoning capabilities of large language models (LLMs) have advanced\nrapidly, particularly following the release of DeepSeek R1, which has inspired\na surge of research into data quality and reinforcement learning (RL)\nalgorithms. Despite the pivotal role diversity plays in RL, its influence on\nLLM reasoning remains largely underexplored. To bridge this gap, this work\npresents a systematic investigation into the impact of diversity in RL-based\ntraining for LLM reasoning, and proposes a novel diversity-aware policy\noptimization method. Across evaluations on 12 LLMs, we observe a strong\npositive correlation between the solution diversity and Potential at k (a novel\nmetric quantifying an LLM's reasoning potential) in high-performing models.\nThis finding motivates our method to explicitly promote diversity during RL\ntraining. Specifically, we design a token-level diversity and reformulate it\ninto a practical objective, then we selectively apply it to positive samples.\nIntegrated into the R1-zero training framework, our method achieves a 3.5\npercent average improvement across four mathematical reasoning benchmarks,\nwhile generating more diverse and robust solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of large language models (LLMs) have advanced\nrapidly, particularly following the release of DeepSeek R1, which has inspired\na surge of research into data quality and reinforcement learning (RL)\nalgorithms. Despite the pivotal role diversity plays in RL, its influence on\nLLM reasoning remains largely underexplored. To bridge this gap, this work\npresents a systematic investigation into the impact of diversity in RL-based\ntraining for LLM reasoning, and proposes a novel diversity-aware policy\noptimization method. Across evaluations on 12 LLMs, we observe a strong\npositive correlation between the solution diversity and Potential at k (a novel\nmetric quantifying an LLM's reasoning potential) in high-performing models.\nThis finding motivates our method to explicitly promote diversity during RL\ntraining. Specifically, we design a token-level diversity and reformulate it\ninto a practical objective, then we selectively apply it to positive samples.\nIntegrated into the R1-zero training framework, our method achieves a 3.5\npercent average improvement across four mathematical reasoning benchmarks,\nwhile generating more diverse and robust solutions."
                },
                "authors": [
                    {
                        "name": "Jian Yao"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04340v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04340v4",
                "updated": "2025-11-03T12:21:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    21,
                    7,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-05T20:04:22Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    20,
                    4,
                    22,
                    6,
                    278,
                    0
                ],
                "title": "Inoculation Prompting: Eliciting traits from LLMs during training can\n  suppress them at test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inoculation Prompting: Eliciting traits from LLMs during training can\n  suppress them at test-time"
                },
                "summary": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize."
                },
                "authors": [
                    {
                        "name": "Daniel Tan"
                    },
                    {
                        "name": "Anders Woodruff"
                    },
                    {
                        "name": "Niels Warncke"
                    },
                    {
                        "name": "Arun Jose"
                    },
                    {
                        "name": "Maxime Rich"
                    },
                    {
                        "name": "David Demitri Africa"
                    },
                    {
                        "name": "Mia Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Mia Taylor"
                },
                "author": "Mia Taylor",
                "arxiv_comment": "40 pages, 22 figures. Under review at ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04340v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04340v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09394v2",
                "updated": "2025-11-03T12:18:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    18,
                    11,
                    0,
                    307,
                    0
                ],
                "published": "2024-12-12T15:59:58Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    59,
                    58,
                    3,
                    347,
                    0
                ],
                "title": "LLMs for Time Series: an Application for Single Stocks and Statistical\n  Arbitrage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Time Series: an Application for Single Stocks and Statistical\n  Arbitrage"
                },
                "summary": "Recently, LLMs (Large Language Models) have been adapted for time series\nprediction with significant success in pattern recognition. However, the common\nbelief is that these models are not suitable for predicting financial market\nreturns, which are known to be almost random. We aim to challenge this\nmisconception through a counterexample. Specifically, we utilized the Chronos\nmodel from Ansari et al.(2024) and tested both pretrained configurations and\nfine-tuned supervised forecasts on the largest American single stocks using\ndata from Guijarro-Ordonnez et al.(2022). We constructed a long/short\nportfolio, and the performance simulation indicates that LLMs can in reality\nhandle time series that are nearly indistinguishable from noise, demonstrating\nan ability to identify inefficiencies amidst randomness and generate alpha.\nFinally, we compared these results with those of specialized models and smaller\ndeep learning models, highlighting significant room for improvement in LLM\nperformance to further enhance their predictive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LLMs (Large Language Models) have been adapted for time series\nprediction with significant success in pattern recognition. However, the common\nbelief is that these models are not suitable for predicting financial market\nreturns, which are known to be almost random. We aim to challenge this\nmisconception through a counterexample. Specifically, we utilized the Chronos\nmodel from Ansari et al.(2024) and tested both pretrained configurations and\nfine-tuned supervised forecasts on the largest American single stocks using\ndata from Guijarro-Ordonnez et al.(2022). We constructed a long/short\nportfolio, and the performance simulation indicates that LLMs can in reality\nhandle time series that are nearly indistinguishable from noise, demonstrating\nan ability to identify inefficiencies amidst randomness and generate alpha.\nFinally, we compared these results with those of specialized models and smaller\ndeep learning models, highlighting significant room for improvement in LLM\nperformance to further enhance their predictive capabilities."
                },
                "authors": [
                    {
                        "name": "Sebastien Valeyre"
                    },
                    {
                        "name": "Sofiane Aboura"
                    }
                ],
                "author_detail": {
                    "name": "Sofiane Aboura"
                },
                "author": "Sofiane Aboura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22767v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22767v3",
                "updated": "2025-11-03T12:13:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    13,
                    58,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-28T18:36:00Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    36,
                    0,
                    2,
                    148,
                    0
                ],
                "title": "In Dialogue with Intelligence: Rethinking Large Language Models as\n  Collective Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Dialogue with Intelligence: Rethinking Large Language Models as\n  Collective Knowledge"
                },
                "summary": "Large Language Models (LLMs) can be understood as Collective Knowledge (CK):\na condensation of human cultural and technical output, whose apparent\nintelligence emerges in dialogue. This perspective article, drawing on extended\ninteraction with ChatGPT-4, postulates differential response modes that\nplausibly trace their origin to distinct model subnetworks. It argues that CK\nhas no persistent internal state or ``spine'': it drifts, it complies, and its\nbehaviour is shaped by the user and by fine-tuning. It develops the notion of\nco-augmentation, in which human judgement and CK's representational reach\njointly produce forms of analysis that neither could generate alone. Finally,\nit suggests that CK offers a tractable object for neuroscience: unlike\nbiological brains, these systems expose their architecture, training history,\nand activation dynamics, making the human--CK loop itself an experimental\ntarget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can be understood as Collective Knowledge (CK):\na condensation of human cultural and technical output, whose apparent\nintelligence emerges in dialogue. This perspective article, drawing on extended\ninteraction with ChatGPT-4, postulates differential response modes that\nplausibly trace their origin to distinct model subnetworks. It argues that CK\nhas no persistent internal state or ``spine'': it drifts, it complies, and its\nbehaviour is shaped by the user and by fine-tuning. It develops the notion of\nco-augmentation, in which human judgement and CK's representational reach\njointly produce forms of analysis that neither could generate alone. Finally,\nit suggests that CK offers a tractable object for neuroscience: unlike\nbiological brains, these systems expose their architecture, training history,\nand activation dynamics, making the human--CK loop itself an experimental\ntarget."
                },
                "authors": [
                    {
                        "name": "Eleni Vasilaki"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Vasilaki"
                },
                "author": "Eleni Vasilaki",
                "arxiv_comment": "7 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22767v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22767v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06402v2",
                "updated": "2025-11-03T12:13:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    12,
                    13,
                    43,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-08T07:47:58Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    7,
                    47,
                    58,
                    0,
                    251,
                    0
                ],
                "title": "NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network\n  Executables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network\n  Executables"
                },
                "summary": "On-device deep learning models have extensive real world demands. Deep\nlearning compilers efficiently compile models into executables for deployment\non edge devices, but these executables may face the threat of reverse\nengineering. Previous studies have attempted to decompile DNN executables, but\nthey face challenges in handling compilation optimizations and analyzing\nquantized compiled models. In this paper, we present NeuroDeX to unlock diverse\nsupport in decompiling DNN executables. NeuroDeX leverages the semantic\nunderstanding capabilities of LLMs along with dynamic analysis to accurately\nand efficiently perform operator type recognition, operator attribute recovery\nand model reconstruction. NeuroDeX can recover DNN executables into high-level\nmodels towards compilation optimizations, different architectures and quantized\ncompiled models. We conduct experiments on 96 DNN executables across 12 common\nDNN models. Extensive experimental results demonstrate that NeuroDeX can\ndecompile non-quantized executables into nearly identical high-level models.\nNeuroDeX can recover functionally similar high-level models for quantized\nexecutables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more\ncomprehensive and effective solution compared to previous DNN executables\ndecompilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device deep learning models have extensive real world demands. Deep\nlearning compilers efficiently compile models into executables for deployment\non edge devices, but these executables may face the threat of reverse\nengineering. Previous studies have attempted to decompile DNN executables, but\nthey face challenges in handling compilation optimizations and analyzing\nquantized compiled models. In this paper, we present NeuroDeX to unlock diverse\nsupport in decompiling DNN executables. NeuroDeX leverages the semantic\nunderstanding capabilities of LLMs along with dynamic analysis to accurately\nand efficiently perform operator type recognition, operator attribute recovery\nand model reconstruction. NeuroDeX can recover DNN executables into high-level\nmodels towards compilation optimizations, different architectures and quantized\ncompiled models. We conduct experiments on 96 DNN executables across 12 common\nDNN models. Extensive experimental results demonstrate that NeuroDeX can\ndecompile non-quantized executables into nearly identical high-level models.\nNeuroDeX can recover functionally similar high-level models for quantized\nexecutables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more\ncomprehensive and effective solution compared to previous DNN executables\ndecompilers."
                },
                "authors": [
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Guozhu Meng"
                    },
                    {
                        "name": "Mingyang Sun"
                    },
                    {
                        "name": "Yanzhong Wang"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Hailong Chang"
                    },
                    {
                        "name": "Yuekang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuekang Li"
                },
                "author": "Yuekang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04173v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04173v3",
                "updated": "2025-11-03T11:55:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    55,
                    32,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-05T12:26:42Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    26,
                    42,
                    6,
                    278,
                    0
                ],
                "title": "Open Agent Specification (Agent Spec) Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Agent Specification (Agent Spec) Technical Report"
                },
                "summary": "Open Agent Specification (Agent Spec) is a declarative language for defining\nAI agents and workflows in a way that is compatible across different AI\nframeworks, promoting portability and interoperability within AI Agent\nframeworks. Agent Spec aims to resolve the challenges of fragmented agent\ndevelopment by providing a common unified specification that allows AI agents\nto be designed once and deployed across various frameworks, improving\ninteroperability and reusability, while reducing redundant efforts.\nAdditionally, Agent Spec facilitates development tools and portability,\nallowing AI agents to be defined independently of their execution environment\nand enabling teams to exchange solutions without implementation-specific\nlimitations. Agent Spec benefits four key groups: (i) Agent developers, who\ngain a superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from cross-framework and tool support; (iii) Researchers, who can\nachieve reproducible results and comparability, facilitating more reliable and\nconsistent outcomes; (iv) Enterprises, which see faster\nprototype-to-deployment, increased productivity, and greater scalability and\nmaintainability for their AI agent solutions. This technical report provides an\noverview of the technical foundations of Agent Spec, including motivation,\nbenefits, and future work. We also introduce a standardized Evaluation harness\nto assess agent behavior and agentic workflows across runtimes (LangGraph,\nCrewAI, AutoGen, and WayFlow), using three different benchmarks (SimpleQA\nVerified, $\\tau^2$-Bench and BIRD-SQL) - analogous to how HELM and related\nharnesses standardized LLM evaluation - so that performance, robustness, and\nefficiency can be compared consistently across frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Agent Specification (Agent Spec) is a declarative language for defining\nAI agents and workflows in a way that is compatible across different AI\nframeworks, promoting portability and interoperability within AI Agent\nframeworks. Agent Spec aims to resolve the challenges of fragmented agent\ndevelopment by providing a common unified specification that allows AI agents\nto be designed once and deployed across various frameworks, improving\ninteroperability and reusability, while reducing redundant efforts.\nAdditionally, Agent Spec facilitates development tools and portability,\nallowing AI agents to be defined independently of their execution environment\nand enabling teams to exchange solutions without implementation-specific\nlimitations. Agent Spec benefits four key groups: (i) Agent developers, who\ngain a superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from cross-framework and tool support; (iii) Researchers, who can\nachieve reproducible results and comparability, facilitating more reliable and\nconsistent outcomes; (iv) Enterprises, which see faster\nprototype-to-deployment, increased productivity, and greater scalability and\nmaintainability for their AI agent solutions. This technical report provides an\noverview of the technical foundations of Agent Spec, including motivation,\nbenefits, and future work. We also introduce a standardized Evaluation harness\nto assess agent behavior and agentic workflows across runtimes (LangGraph,\nCrewAI, AutoGen, and WayFlow), using three different benchmarks (SimpleQA\nVerified, $\\tau^2$-Bench and BIRD-SQL) - analogous to how HELM and related\nharnesses standardized LLM evaluation - so that performance, robustness, and\nefficiency can be compared consistently across frameworks."
                },
                "authors": [
                    {
                        "name": "Yassine Benajiba"
                    },
                    {
                        "name": "Cesare Bernardis"
                    },
                    {
                        "name": "Vladislav Blinov"
                    },
                    {
                        "name": "Paul Cayet"
                    },
                    {
                        "name": "Hassan Chafi"
                    },
                    {
                        "name": "Abderrahim Fathan"
                    },
                    {
                        "name": "Louis Faucon"
                    },
                    {
                        "name": "Damien Hilloulin"
                    },
                    {
                        "name": "Sungpack Hong"
                    },
                    {
                        "name": "Ingo Kossyk"
                    },
                    {
                        "name": "Rhicheek Patra"
                    },
                    {
                        "name": "Sujith Ravi"
                    },
                    {
                        "name": "Jonas Schweizer"
                    },
                    {
                        "name": "Jyotika Singh"
                    },
                    {
                        "name": "Shailender Singh"
                    },
                    {
                        "name": "Xuelin Situ"
                    },
                    {
                        "name": "Weiyi Sun"
                    },
                    {
                        "name": "Kartik Talamadupula"
                    },
                    {
                        "name": "Jerry Xu"
                    },
                    {
                        "name": "Ying Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Xu"
                },
                "author": "Ying Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04173v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04173v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06111v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06111v3",
                "updated": "2025-11-03T11:52:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    52,
                    57,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-09T15:11:13Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    11,
                    13,
                    4,
                    129,
                    0
                ],
                "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions"
                },
                "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning."
                },
                "authors": [
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Yanting Yang"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Shenyuan Gao"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Hongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Li"
                },
                "author": "Hongyang Li",
                "arxiv_comment": "Accepted to RSS 2025. Code is available at\n  https://github.com/OpenDriveLab/UniVLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06111v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06111v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v2",
                "updated": "2025-11-03T11:32:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    32,
                    13,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "Accepted at NeurIPS 2025. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22608v3",
                "updated": "2025-11-03T10:59:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    59,
                    29,
                    0,
                    307,
                    0
                ],
                "published": "2025-07-30T12:23:39Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    23,
                    39,
                    2,
                    211,
                    0
                ],
                "title": "Language Arithmetics: Towards Systematic Language Neuron Identification\n  and Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Arithmetics: Towards Systematic Language Neuron Identification\n  and Manipulation"
                },
                "summary": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Katharina Trinley"
                    },
                    {
                        "name": "Yusser Al Ghussin"
                    },
                    {
                        "name": "Tanja Baeumel"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "accepted to AACL main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24528v2",
                "updated": "2025-11-03T10:58:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    58,
                    42,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-30T12:36:38Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    12,
                    36,
                    38,
                    4,
                    150,
                    0
                ],
                "title": "Geospatial Foundation Models to Enable Progress on Sustainable\n  Development Goals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial Foundation Models to Enable Progress on Sustainable\n  Development Goals"
                },
                "summary": "Foundation Models (FMs) are large-scale, pre-trained artificial intelligence\n(AI) systems that have revolutionized natural language processing and computer\nvision, and are now advancing geospatial analysis and Earth Observation (EO).\nThey promise improved generalization across tasks, scalability, and efficient\nadaptation with minimal labeled data. However, despite the rapid proliferation\nof geospatial FMs, their real-world utility and alignment with global\nsustainability goals remain underexplored. We introduce SustainFM, a\ncomprehensive benchmarking framework grounded in the 17 Sustainable Development\nGoals with extremely diverse tasks ranging from asset wealth prediction to\nenvironmental hazard detection. This study provides a rigorous,\ninterdisciplinary assessment of geospatial FMs and offers critical insights\ninto their role in attaining sustainability goals. Our findings show: (1) While\nnot universally superior, FMs often outperform traditional approaches across\ndiverse tasks and datasets. (2) Evaluating FMs should go beyond accuracy to\ninclude transferability, generalization, and energy efficiency as key criteria\nfor their responsible use. (3) FMs enable scalable, SDG-grounded solutions,\noffering broad utility for tackling complex sustainability challenges.\nCritically, we advocate for a paradigm shift from model-centric development to\nimpact-driven deployment, and emphasize metrics such as energy efficiency,\nrobustness to domain shifts, and ethical considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models (FMs) are large-scale, pre-trained artificial intelligence\n(AI) systems that have revolutionized natural language processing and computer\nvision, and are now advancing geospatial analysis and Earth Observation (EO).\nThey promise improved generalization across tasks, scalability, and efficient\nadaptation with minimal labeled data. However, despite the rapid proliferation\nof geospatial FMs, their real-world utility and alignment with global\nsustainability goals remain underexplored. We introduce SustainFM, a\ncomprehensive benchmarking framework grounded in the 17 Sustainable Development\nGoals with extremely diverse tasks ranging from asset wealth prediction to\nenvironmental hazard detection. This study provides a rigorous,\ninterdisciplinary assessment of geospatial FMs and offers critical insights\ninto their role in attaining sustainability goals. Our findings show: (1) While\nnot universally superior, FMs often outperform traditional approaches across\ndiverse tasks and datasets. (2) Evaluating FMs should go beyond accuracy to\ninclude transferability, generalization, and energy efficiency as key criteria\nfor their responsible use. (3) FMs enable scalable, SDG-grounded solutions,\noffering broad utility for tackling complex sustainability challenges.\nCritically, we advocate for a paradigm shift from model-centric development to\nimpact-driven deployment, and emphasize metrics such as energy efficiency,\nrobustness to domain shifts, and ethical considerations."
                },
                "authors": [
                    {
                        "name": "Pedram Ghamisi"
                    },
                    {
                        "name": "Weikang Yu"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Aldino Rizaldy"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Chufeng Zhou"
                    },
                    {
                        "name": "Richard Gloaguen"
                    },
                    {
                        "name": "Gustau Camps-Valls"
                    }
                ],
                "author_detail": {
                    "name": "Gustau Camps-Valls"
                },
                "author": "Gustau Camps-Valls",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27598v2",
                "updated": "2025-11-03T10:56:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    56,
                    21,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-31T16:22:23Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    22,
                    23,
                    4,
                    304,
                    0
                ],
                "title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research"
                },
                "summary": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark."
                },
                "authors": [
                    {
                        "name": "Yunze Wu"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Weiye Si"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Mohan Jiang"
                    },
                    {
                        "name": "Keyu Li"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27630v2",
                "updated": "2025-11-03T10:53:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    53,
                    11,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-31T17:00:22Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    22,
                    4,
                    304,
                    0
                ],
                "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training"
                },
                "summary": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Yunze Wu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Weiye Si"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Keyu Li"
                    },
                    {
                        "name": "Mohan Jiang"
                    },
                    {
                        "name": "Junfei Wang"
                    },
                    {
                        "name": "Qishuo Hua"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14846v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14846v3",
                "updated": "2025-11-03T10:52:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    52,
                    10,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-16T16:18:37Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    18,
                    37,
                    3,
                    289,
                    0
                ],
                "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents"
                },
                "summary": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via two instantiation. This theory offers a\nworkable language and operational tools to measure agents and their search\nspaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via two instantiation. This theory offers a\nworkable language and operational tools to measure agents and their search\nspaces, proposing a systematic formal description of iterative search\nconstructed by LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuo-Yang Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhuo-Yang Song"
                },
                "author": "Zhuo-Yang Song",
                "arxiv_comment": "11 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14846v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14846v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17006v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17006v3",
                "updated": "2025-11-03T10:45:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    45,
                    42,
                    0,
                    307,
                    0
                ],
                "published": "2024-10-22T13:25:59Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    13,
                    25,
                    59,
                    1,
                    296,
                    0
                ],
                "title": "Temporal Feature Learning in Weakly Labelled Bioacoustic Cetacean\n  Datasets via a Variational Autoencoder and Temporal Convolutional Network: An\n  Interdisciplinary Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Learning in Weakly Labelled Bioacoustic Cetacean\n  Datasets via a Variational Autoencoder and Temporal Convolutional Network: An\n  Interdisciplinary Approach"
                },
                "summary": "Bioacoustics data from Passive acoustic monitoring (PAM) poses a unique set\nof challenges for classification, particularly the limited availability of\ncomplete and reliable labels in datasets due to annotation uncertainty,\nbiological complexity due the heterogeneity in duration of cetacean\nvocalizations, and masking of target sounds due to environmental and\nanthropogenic noise. This means that data is often weakly labelled, with\nannotations indicating presence/absence of species over several minutes. In\norder to effectively capture the complex temporal patterns and key features of\nlengthy continuous audio segments, we propose an interdisciplinary framework\ncomprising dataset standardisation, feature extraction via Variational\nAutoencoders (VAE) and classification via Temporal Convolutional Networks\n(TCN). This approach eliminates the necessity for manual threshold setting or\ntime-consuming strong labelling. To demonstrate the effectiveness of our\napproach, we use sperm whale (<i>Physeter macrocephalus</i>) click trains in\n4-minute recordings as a case study, from a dataset comprising diverse sources\nand deployment conditions to maximise generalisability. The value of feature\nextraction via the VAE is demonstrated by comparing classification performance\nagainst the traditional and explainable approach of expert handpicking of\nfeatures. The TCN demonstrated robust classification capabilities achieving AUC\nscores exceeding 0.9.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bioacoustics data from Passive acoustic monitoring (PAM) poses a unique set\nof challenges for classification, particularly the limited availability of\ncomplete and reliable labels in datasets due to annotation uncertainty,\nbiological complexity due the heterogeneity in duration of cetacean\nvocalizations, and masking of target sounds due to environmental and\nanthropogenic noise. This means that data is often weakly labelled, with\nannotations indicating presence/absence of species over several minutes. In\norder to effectively capture the complex temporal patterns and key features of\nlengthy continuous audio segments, we propose an interdisciplinary framework\ncomprising dataset standardisation, feature extraction via Variational\nAutoencoders (VAE) and classification via Temporal Convolutional Networks\n(TCN). This approach eliminates the necessity for manual threshold setting or\ntime-consuming strong labelling. To demonstrate the effectiveness of our\napproach, we use sperm whale (<i>Physeter macrocephalus</i>) click trains in\n4-minute recordings as a case study, from a dataset comprising diverse sources\nand deployment conditions to maximise generalisability. The value of feature\nextraction via the VAE is demonstrated by comparing classification performance\nagainst the traditional and explainable approach of expert handpicking of\nfeatures. The TCN demonstrated robust classification capabilities achieving AUC\nscores exceeding 0.9."
                },
                "authors": [
                    {
                        "name": "Laia Garrob Fonollosa"
                    },
                    {
                        "name": "Douglas Gillespie"
                    },
                    {
                        "name": "Lina Stankovic"
                    },
                    {
                        "name": "Vladimir Stankovic"
                    },
                    {
                        "name": "Luke Rendell"
                    }
                ],
                "author_detail": {
                    "name": "Luke Rendell"
                },
                "author": "Luke Rendell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17006v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17006v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24236v2",
                "updated": "2025-11-03T10:31:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    10,
                    31,
                    57,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-28T09:43:49Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    9,
                    43,
                    49,
                    1,
                    301,
                    0
                ],
                "title": "Towards Transparent Reasoning: What Drives Faithfulness in Large\n  Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Transparent Reasoning: What Drives Faithfulness in Large\n  Language Models?"
                },
                "summary": "Large Language Models (LLMs) often produce explanations that do not\nfaithfully reflect the factors driving their predictions. In healthcare\nsettings, such unfaithfulness is especially problematic: explanations that omit\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\nand lead to unsafe decision support. We study how inference and training-time\nchoices shape explanation faithfulness, focusing on factors practitioners can\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\nand manipulate the number and type of few-shot examples, prompting strategies,\nand training procedure. Our results show: (i) both the quantity and quality of\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\nsensitive to prompting design; (iii) the instruction-tuning phase improves\nmeasured faithfulness on MedQA. These findings offer insights into strategies\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce explanations that do not\nfaithfully reflect the factors driving their predictions. In healthcare\nsettings, such unfaithfulness is especially problematic: explanations that omit\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\nand lead to unsafe decision support. We study how inference and training-time\nchoices shape explanation faithfulness, focusing on factors practitioners can\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\nand manipulate the number and type of few-shot examples, prompting strategies,\nand training procedure. Our results show: (i) both the quantity and quality of\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\nsensitive to prompting design; (iii) the instruction-tuning phase improves\nmeasured faithfulness on MedQA. These findings offer insights into strategies\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\ndomains."
                },
                "authors": [
                    {
                        "name": "Teague McMillan"
                    },
                    {
                        "name": "Gabriele Dominici"
                    },
                    {
                        "name": "Martin Gjoreski"
                    },
                    {
                        "name": "Marc Langheinrich"
                    }
                ],
                "author_detail": {
                    "name": "Marc Langheinrich"
                },
                "author": "Marc Langheinrich",
                "arxiv_comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM\n  Lifecycle: Benchmarks, Emergent Abilities, and Scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07539v2",
                "updated": "2025-11-03T09:40:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    40,
                    3,
                    0,
                    307,
                    0
                ],
                "published": "2025-03-10T17:07:52Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    7,
                    52,
                    0,
                    69,
                    0
                ],
                "title": "XIFBench: Evaluating Large Language Models on Multilingual Instruction\n  Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XIFBench: Evaluating Large Language Models on Multilingual Instruction\n  Following"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable\ninstruction-following capabilities across various applications. However, their\nperformance in multilingual settings lacks systematic investigation, with\nexisting evaluations lacking fine-grained constraint analysis across diverse\nlinguistic contexts. We introduce XIFBench, a comprehensive constraint-based\nbenchmark for evaluating multilingual instruction-following abilities of LLMs,\ncomprising 558 instructions with 0-5 additional constraints across five\ncategories (Content, Style, Situation, Format, and Numerical) in six languages\nspanning different resource levels. To support reliable and consistent\ncross-lingual evaluation, we implement three methodological innovations:\ncultural accessibility annotation, constraint-level translation validation, and\nrequirement-based evaluation using English requirements as semantic anchors\nacross languages. Extensive experiments with various LLMs not only quantify\nperformance disparities across resource levels but also provide detailed\ninsights into how language resources, constraint categories, instruction\ncomplexity, and cultural specificity influence multilingual\ninstruction-following. Our code and data are available at\nhttps://github.com/zhenyuli801/XIFBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable\ninstruction-following capabilities across various applications. However, their\nperformance in multilingual settings lacks systematic investigation, with\nexisting evaluations lacking fine-grained constraint analysis across diverse\nlinguistic contexts. We introduce XIFBench, a comprehensive constraint-based\nbenchmark for evaluating multilingual instruction-following abilities of LLMs,\ncomprising 558 instructions with 0-5 additional constraints across five\ncategories (Content, Style, Situation, Format, and Numerical) in six languages\nspanning different resource levels. To support reliable and consistent\ncross-lingual evaluation, we implement three methodological innovations:\ncultural accessibility annotation, constraint-level translation validation, and\nrequirement-based evaluation using English requirements as semantic anchors\nacross languages. Extensive experiments with various LLMs not only quantify\nperformance disparities across resource levels but also provide detailed\ninsights into how language resources, constraint categories, instruction\ncomplexity, and cultural specificity influence multilingual\ninstruction-following. Our code and data are available at\nhttps://github.com/zhenyuli801/XIFBench."
                },
                "authors": [
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yaoyin Zhang"
                    },
                    {
                        "name": "Xuchen Wei"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by the NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03824v2",
                "updated": "2025-11-03T09:18:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    18,
                    20,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-03T06:24:18Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    6,
                    24,
                    18,
                    5,
                    123,
                    0
                ],
                "title": "Memory Assisted LLM for Personalized Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Assisted LLM for Personalized Recommendation System"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\nsolving recommendation tasks. With proven capabilities in understanding user\npreferences, LLM personalization has emerged as a critical area for providing\ntailored responses to individuals. Current studies explore personalization\nthrough prompt design and fine-tuning, paving the way for further research in\npersonalized LLMs. However, existing approaches are either costly and\ninefficient in capturing diverse user preferences or fail to account for timely\nupdates to user history. To address these gaps, we propose the Memory-Assisted\nPersonalized LLM (MAP). Through user interactions, we first create a history\nprofile for each user, capturing their preferences, such as ratings for\nhistorical items. During recommendation, we extract relevant memory based on\nsimilarity, which is then incorporated into the prompts to enhance personalized\nrecommendations. In our experiments, we define a new task that enables testing\nwith varying memory size under two scenarios: single domain where memory and\ntasks are from the same category and cross-domain (e.g. memory from movies and\nrecommendation tasks in books). The results show that MAP outperforms regular\nLLM-based recommenders that integrate user history directly through prompt\ndesign. Moreover, as user history grows, MAP's advantage increases in both\nscenarios, making it more suitable for addressing successive personalized user\nrequests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\nsolving recommendation tasks. With proven capabilities in understanding user\npreferences, LLM personalization has emerged as a critical area for providing\ntailored responses to individuals. Current studies explore personalization\nthrough prompt design and fine-tuning, paving the way for further research in\npersonalized LLMs. However, existing approaches are either costly and\ninefficient in capturing diverse user preferences or fail to account for timely\nupdates to user history. To address these gaps, we propose the Memory-Assisted\nPersonalized LLM (MAP). Through user interactions, we first create a history\nprofile for each user, capturing their preferences, such as ratings for\nhistorical items. During recommendation, we extract relevant memory based on\nsimilarity, which is then incorporated into the prompts to enhance personalized\nrecommendations. In our experiments, we define a new task that enables testing\nwith varying memory size under two scenarios: single domain where memory and\ntasks are from the same category and cross-domain (e.g. memory from movies and\nrecommendation tasks in books). The results show that MAP outperforms regular\nLLM-based recommenders that integrate user history directly through prompt\ndesign. Moreover, as user history grows, MAP's advantage increases in both\nscenarios, making it more suitable for addressing successive personalized user\nrequests."
                },
                "authors": [
                    {
                        "name": "Jiarui Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiarui Chen"
                },
                "author": "Jiarui Chen",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04516v3",
                "updated": "2025-11-03T09:17:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    17,
                    26,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-06T06:11:43Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    11,
                    43,
                    0,
                    279,
                    0
                ],
                "title": "Rethinking HTTP API Rate Limiting: A Client-Side Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking HTTP API Rate Limiting: A Client-Side Approach"
                },
                "summary": "HTTP underpins modern Internet services, and providers enforce quotas to\nregulate HTTP API traffic for scalability and reliability. When requests exceed\nquotas, clients are throttled and must retry. Server-side enforcement protects\nthe service. However, when independent clients' usage counts toward a shared\nquota, server-only controls are inefficient; clients lack visibility into\nothers' load, causing their retry attempts to potentially fail. Indeed, retry\ntiming is important since each attempt incurs costs and yields no benefit\nunless admitted. While centralized coordination could address this, practical\nlimitations have led to widespread adoption of simple client-side strategies\nlike exponential backoff. As we show, these simple strategies cause excessive\nretries and significant costs. We design adaptive client-side mechanisms\nrequiring no central control, relying only on minimal feedback. We present two\nalgorithms: ATB, an offline method deployable via service workers, and AATB,\nwhich enhances retry behavior using aggregated telemetry data. Both algorithms\ninfer system congestion to schedule retries. Through emulations with real-world\ntraces and synthetic datasets with up to 100 clients, we demonstrate that our\nalgorithms reduce HTTP 429 errors by up to 97.3% compared to exponential\nbackoff, while the modest increase in completion time is outweighed by the\nreduction in errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HTTP underpins modern Internet services, and providers enforce quotas to\nregulate HTTP API traffic for scalability and reliability. When requests exceed\nquotas, clients are throttled and must retry. Server-side enforcement protects\nthe service. However, when independent clients' usage counts toward a shared\nquota, server-only controls are inefficient; clients lack visibility into\nothers' load, causing their retry attempts to potentially fail. Indeed, retry\ntiming is important since each attempt incurs costs and yields no benefit\nunless admitted. While centralized coordination could address this, practical\nlimitations have led to widespread adoption of simple client-side strategies\nlike exponential backoff. As we show, these simple strategies cause excessive\nretries and significant costs. We design adaptive client-side mechanisms\nrequiring no central control, relying only on minimal feedback. We present two\nalgorithms: ATB, an offline method deployable via service workers, and AATB,\nwhich enhances retry behavior using aggregated telemetry data. Both algorithms\ninfer system congestion to schedule retries. Through emulations with real-world\ntraces and synthetic datasets with up to 100 clients, we demonstrate that our\nalgorithms reduce HTTP 429 errors by up to 97.3% compared to exponential\nbackoff, while the modest increase in completion time is outweighed by the\nreduction in errors."
                },
                "authors": [
                    {
                        "name": "Behrooz Farkiani"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Patrick Crowley"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Crowley"
                },
                "author": "Patrick Crowley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08108v2",
                "updated": "2025-11-03T09:09:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    9,
                    5,
                    0,
                    307,
                    0
                ],
                "published": "2025-02-12T04:13:07Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    4,
                    13,
                    7,
                    2,
                    43,
                    0
                ],
                "title": "Generative AI and Empirical Software Engineering: A Paradigm Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and Empirical Software Engineering: A Paradigm Shift"
                },
                "summary": "The adoption of large language models (LLMs) and autonomous agents in\nsoftware engineering marks an enduring paradigm shift. These systems create new\nopportunities for tool design, workflow orchestration, and empirical\nobservation, while fundamentally reshaping the roles of developers and the\nartifacts they produce. Although traditional empirical methods remain central\nto software engineering research, the rapid evolution of AI introduces new data\nmodalities, alters causal assumptions, and challenges foundational constructs\nsuch as \"developer\", \"artifact\", and \"interaction\". As humans and AI agents\nincreasingly co-create, the boundaries between social and technical actors\nblur, and the reproducibility of findings becomes contingent on model updates\nand prompt contexts. This vision paper examines how the integration of LLMs\ninto software engineering disrupts established research paradigms. We discuss\nhow it transforms the phenomena we study, the methods and theories we rely on,\nthe data we analyze, and the threats to validity that arise in dynamic\nAI-mediated environments. Our aim is to help the empirical software engineering\ncommunity adapt its questions, instruments, and validation standards to a\nfuture in which AI systems are not merely tools, but active collaborators\nshaping software engineering and its study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of large language models (LLMs) and autonomous agents in\nsoftware engineering marks an enduring paradigm shift. These systems create new\nopportunities for tool design, workflow orchestration, and empirical\nobservation, while fundamentally reshaping the roles of developers and the\nartifacts they produce. Although traditional empirical methods remain central\nto software engineering research, the rapid evolution of AI introduces new data\nmodalities, alters causal assumptions, and challenges foundational constructs\nsuch as \"developer\", \"artifact\", and \"interaction\". As humans and AI agents\nincreasingly co-create, the boundaries between social and technical actors\nblur, and the reproducibility of findings becomes contingent on model updates\nand prompt contexts. This vision paper examines how the integration of LLMs\ninto software engineering disrupts established research paradigms. We discuss\nhow it transforms the phenomena we study, the methods and theories we rely on,\nthe data we analyze, and the threats to validity that arise in dynamic\nAI-mediated environments. Our aim is to help the empirical software engineering\ncommunity adapt its questions, instruments, and validation standards to a\nfuture in which AI systems are not merely tools, but active collaborators\nshaping software engineering and its study."
                },
                "authors": [
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "Margaret-Anne Storey"
                    }
                ],
                "author_detail": {
                    "name": "Margaret-Anne Storey"
                },
                "author": "Margaret-Anne Storey",
                "arxiv_comment": "Published at 2nd IEEE/ACM International Conference on AI-powered\n  Software (AIware 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13790v2",
                "updated": "2025-11-03T09:06:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    6,
                    1,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-17T07:58:59Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    59,
                    2,
                    260,
                    0
                ],
                "title": "Teaching According to Talents! Instruction Tuning LLMs with\n  Competence-Aware Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching According to Talents! Instruction Tuning LLMs with\n  Competence-Aware Curriculum Learning"
                },
                "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning."
                },
                "authors": [
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09338v2",
                "updated": "2025-11-03T09:05:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    5,
                    41,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-10T12:44:59Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    44,
                    59,
                    4,
                    283,
                    0
                ],
                "title": "Localist LLMs -- A Mathematical Framework for Dynamic Locality Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localist LLMs -- A Mathematical Framework for Dynamic Locality Control"
                },
                "summary": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovation is a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining. This is achieved\nthrough group sparsity penalties on attention mechanisms, information-theoretic\nanchor design, and dynamic rule injection. We provide rigorous mathematical\nproofs establishing explicit threshold conditions under which attention\nprovably concentrates on semantically relevant blocks, with exponential bounds\non attention entropy and pointer fidelity. Specifically, we prove that when\ngroup sparsity penalties exceed certain threshold values, the model's attention\nmechanisms concentrate on semantically relevant blocks, achieving low entropy\nand high fidelity with negligible error. This framework enables practitioners\nto continuously interpolate between interpretable and high-performance modes,\nsupporting applications in regulated domains requiring both transparency and\ncapability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovation is a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining. This is achieved\nthrough group sparsity penalties on attention mechanisms, information-theoretic\nanchor design, and dynamic rule injection. We provide rigorous mathematical\nproofs establishing explicit threshold conditions under which attention\nprovably concentrates on semantically relevant blocks, with exponential bounds\non attention entropy and pointer fidelity. Specifically, we prove that when\ngroup sparsity penalties exceed certain threshold values, the model's attention\nmechanisms concentrate on semantically relevant blocks, achieving low entropy\nand high fidelity with negligible error. This framework enables practitioners\nto continuously interpolate between interpretable and high-performance modes,\nsupporting applications in regulated domains requiring both transparency and\ncapability."
                },
                "authors": [
                    {
                        "name": "Joachim Diederich"
                    }
                ],
                "author_detail": {
                    "name": "Joachim Diederich"
                },
                "author": "Joachim Diederich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18079v4",
                "updated": "2025-11-03T08:39:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    8,
                    39,
                    35,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-23T16:37:36Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    16,
                    37,
                    36,
                    4,
                    143,
                    0
                ],
                "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding"
                },
                "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery (DVD) agent to leverage an agentic search strategy\nover segmented video clips. Unlike previous video agents that rely on\npredefined workflows applied uniformly across different queries, our approach\nemphasizes the autonomous and adaptive nature of agents. By providing a set of\nsearch-centric tools on multi-granular video database, our DVD agent leverages\nthe advanced reasoning capability of LLM to plan on its current observation\nstate, strategically selects tools to orchestrate adaptive workflow for\ndifferent queries in light of the gathered information. We perform\ncomprehensive evaluation on multiple long video understanding benchmarks that\ndemonstrates our advantage. Our DVD agent achieves state-of-the-art performance\non the challenging LVBench dataset, reaching an accuracy of 74.2%, which\nsubstantially surpasses all prior works, and further improves to 76.0% with\ntranscripts. The code has been released at\nhttps://github.com/microsoft/DeepVideoDiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery (DVD) agent to leverage an agentic search strategy\nover segmented video clips. Unlike previous video agents that rely on\npredefined workflows applied uniformly across different queries, our approach\nemphasizes the autonomous and adaptive nature of agents. By providing a set of\nsearch-centric tools on multi-granular video database, our DVD agent leverages\nthe advanced reasoning capability of LLM to plan on its current observation\nstate, strategically selects tools to orchestrate adaptive workflow for\ndifferent queries in light of the gathered information. We perform\ncomprehensive evaluation on multiple long video understanding benchmarks that\ndemonstrates our advantage. Our DVD agent achieves state-of-the-art performance\non the challenging LVBench dataset, reaching an accuracy of 74.2%, which\nsubstantially surpasses all prior works, and further improves to 76.0% with\ntranscripts. The code has been released at\nhttps://github.com/microsoft/DeepVideoDiscovery."
                },
                "authors": [
                    {
                        "name": "Xiaoyi Zhang"
                    },
                    {
                        "name": "Zhaoyang Jia"
                    },
                    {
                        "name": "Zongyu Guo"
                    },
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "author": "Yan Lu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16129v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16129v4",
                "updated": "2025-11-03T08:23:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    8,
                    23,
                    59,
                    0,
                    307,
                    0
                ],
                "published": "2025-04-21T07:03:54Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    3,
                    54,
                    0,
                    111,
                    0
                ],
                "title": "MARFT: Multi-Agent Reinforcement Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARFT: Multi-Agent Reinforcement Fine-Tuning"
                },
                "summary": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks, from generating high-quality presentation\nslides to even conducting sophisticated scientific research. Meanwhile, RL has\nbeen widely recognized for its effectiveness in enhancing agent intelligence,\nbut limited research has investigated the fine-tuning of LaMAS using\nfoundational RL techniques. Moreover, the direct application of MARL methods to\nLaMAS introduces significant challenges, stemming from the unique\ncharacteristics and mechanisms inherent to LaMAS. To address these challenges,\nthis article presents a comprehensive study of LLM-based MARL and proposes a\nnovel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We\nintroduce a brand-new MG called Flex-MG, which aligns with the LaMAS\noptimization in real-world applications and a universal algorithmic framework\ntailored specifically for LaMAS, outlining the conceptual foundations, key\ndistinctions, and practical implementation strategies. We review the evolution\nfrom RL to RFT, setting the stage for a parallel analysis in the multi-agent\ndomain. In the context of LaMAS, we elucidate critical differences between MARL\nand MARFT. These differences motivate a transition toward a LaMAS-oriented\nformulation of RFT. Central to this work is a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work serves as a roadmap for researchers seeking to advance MARFT toward\nresilient and adaptive solutions in agentic systems. Our implementation of the\nproposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks, from generating high-quality presentation\nslides to even conducting sophisticated scientific research. Meanwhile, RL has\nbeen widely recognized for its effectiveness in enhancing agent intelligence,\nbut limited research has investigated the fine-tuning of LaMAS using\nfoundational RL techniques. Moreover, the direct application of MARL methods to\nLaMAS introduces significant challenges, stemming from the unique\ncharacteristics and mechanisms inherent to LaMAS. To address these challenges,\nthis article presents a comprehensive study of LLM-based MARL and proposes a\nnovel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We\nintroduce a brand-new MG called Flex-MG, which aligns with the LaMAS\noptimization in real-world applications and a universal algorithmic framework\ntailored specifically for LaMAS, outlining the conceptual foundations, key\ndistinctions, and practical implementation strategies. We review the evolution\nfrom RL to RFT, setting the stage for a parallel analysis in the multi-agent\ndomain. In the context of LaMAS, we elucidate critical differences between MARL\nand MARFT. These differences motivate a transition toward a LaMAS-oriented\nformulation of RFT. Central to this work is a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work serves as a roadmap for researchers seeking to advance MARFT toward\nresilient and adaptive solutions in agentic systems. Our implementation of the\nproposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT."
                },
                "authors": [
                    {
                        "name": "Junwei Liao"
                    },
                    {
                        "name": "Muning Wen"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16129v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16129v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13500v2",
                "updated": "2025-11-03T08:12:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    8,
                    12,
                    8,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-15T12:50:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    50,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts"
                },
                "summary": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, an enhanced benchmark\nwith broader coverage of medical subjects, designed to evaluate both single and\nbatch edits under strict locality constraints. We then propose MedREK, a\nretrieval-based editing framework that integrates a shared query-key module for\nprecise matching with an attention-based prompt encoder for informative\nguidance. Experimental results on various medical benchmarks demonstrate that\nour MedREK achieves superior performance across different core metrics and\nprovides the first validated solution for batch-editing in medical LLMs. Our\ncode and dataset are available at https://github.com/mylittleriver/MedREK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, an enhanced benchmark\nwith broader coverage of medical subjects, designed to evaluate both single and\nbatch edits under strict locality constraints. We then propose MedREK, a\nretrieval-based editing framework that integrates a shared query-key module for\nprecise matching with an attention-based prompt encoder for informative\nguidance. Experimental results on various medical benchmarks demonstrate that\nour MedREK achieves superior performance across different core metrics and\nprovides the first validated solution for batch-editing in medical LLMs. Our\ncode and dataset are available at https://github.com/mylittleriver/MedREK."
                },
                "authors": [
                    {
                        "name": "Shujun Xia"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Yinan Zhou"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Zhenan Sun"
                    },
                    {
                        "name": "Quanzheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Quanzheng Li"
                },
                "author": "Quanzheng Li",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23196v2",
                "updated": "2025-11-03T07:52:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    52,
                    56,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-27T10:32:56Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    10,
                    32,
                    56,
                    0,
                    300,
                    0
                ],
                "title": "Neural Networks for AC Optimal Power Flow: Improving Worst-Case\n  Guarantees during Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Networks for AC Optimal Power Flow: Improving Worst-Case\n  Guarantees during Training"
                },
                "summary": "The AC Optimal Power Flow (AC-OPF) problem is central to power system\noperation but challenging to solve efficiently due to its nonconvex and\nnonlinear nature. Neural networks (NNs) offer fast surrogates, yet their\nblack-box behavior raises concerns about constraint violations that can\ncompromise safety. We propose a verification-informed NN framework that\nincorporates worst-case constraint violations directly into training, producing\nmodels that are both accurate and provably safer. Through post-hoc\nverification, we achieve substantial reductions in worst-case violations and,\nfor the first time, verify all operational constraints of large-scale AC-OPF\nproxies. Practical feasibility is further enhanced via restoration and\nwarm-start strategies for infeasible operating points. Experiments on systems\nranging from 57 to 793 buses demonstrate scalability, speed, and reliability,\nbridging the gap between ML acceleration and safe, real-time deployment of\nAC-OPF solutions - and paving the way toward data-driven optimal control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AC Optimal Power Flow (AC-OPF) problem is central to power system\noperation but challenging to solve efficiently due to its nonconvex and\nnonlinear nature. Neural networks (NNs) offer fast surrogates, yet their\nblack-box behavior raises concerns about constraint violations that can\ncompromise safety. We propose a verification-informed NN framework that\nincorporates worst-case constraint violations directly into training, producing\nmodels that are both accurate and provably safer. Through post-hoc\nverification, we achieve substantial reductions in worst-case violations and,\nfor the first time, verify all operational constraints of large-scale AC-OPF\nproxies. Practical feasibility is further enhanced via restoration and\nwarm-start strategies for infeasible operating points. Experiments on systems\nranging from 57 to 793 buses demonstrate scalability, speed, and reliability,\nbridging the gap between ML acceleration and safe, real-time deployment of\nAC-OPF solutions - and paving the way toward data-driven optimal control."
                },
                "authors": [
                    {
                        "name": "Bastien Giraud"
                    },
                    {
                        "name": "Rahul Nellikath"
                    },
                    {
                        "name": "Johanna Vorwerk"
                    },
                    {
                        "name": "Maad Alowaifeer"
                    },
                    {
                        "name": "Spyros Chatzivasileiadis"
                    }
                ],
                "author_detail": {
                    "name": "Spyros Chatzivasileiadis"
                },
                "author": "Spyros Chatzivasileiadis",
                "arxiv_comment": "Submitted to PSCC 2026 (under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09802v2",
                "updated": "2025-11-03T07:39:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    39,
                    35,
                    0,
                    307,
                    0
                ],
                "published": "2025-04-14T02:03:54Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    2,
                    3,
                    54,
                    0,
                    104,
                    0
                ],
                "title": "Enhancing Reasoning Abilities of Small LLMs with Cognitive Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reasoning Abilities of Small LLMs with Cognitive Alignment"
                },
                "summary": "The reasoning capabilities of large reasoning models (LRMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need for training effective small reasoning models. A critical\nchallenge is that small models possess different reasoning capacities and\ncognitive trajectories compared with their larger counterparts. Hence, directly\ndistilling chain-of-thought (CoT) rationales from large LRMs to smaller ones\ncan sometimes be ineffective and often requires a substantial amount of\nannotated data. In this paper, we first introduce a novel\nCritique-Rethink-Verify (CRV) system, designed for training smaller yet\npowerful LRMs. Our CRV system consists of multiple LLM agents, each\nspecializing in unique tasks: (i) critiquing the CoT rationales according to\nthe cognitive capabilities of smaller models, (ii) rethinking and refining\nthese CoTs based on the critiques, and (iii) verifying the correctness of the\nrefined results. Building on the CRV system, we further propose the Cognitive\nPreference Optimization (CogPO) algorithm to continuously enhance the reasoning\nabilities of smaller models by aligning their reasoning processes with their\ncognitive capacities. Comprehensive evaluations on challenging reasoning\nbenchmarks demonstrate the efficacy of our CRV+CogPO framework, which\noutperforms other methods by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of large reasoning models (LRMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need for training effective small reasoning models. A critical\nchallenge is that small models possess different reasoning capacities and\ncognitive trajectories compared with their larger counterparts. Hence, directly\ndistilling chain-of-thought (CoT) rationales from large LRMs to smaller ones\ncan sometimes be ineffective and often requires a substantial amount of\nannotated data. In this paper, we first introduce a novel\nCritique-Rethink-Verify (CRV) system, designed for training smaller yet\npowerful LRMs. Our CRV system consists of multiple LLM agents, each\nspecializing in unique tasks: (i) critiquing the CoT rationales according to\nthe cognitive capabilities of smaller models, (ii) rethinking and refining\nthese CoTs based on the critiques, and (iii) verifying the correctness of the\nrefined results. Building on the CRV system, we further propose the Cognitive\nPreference Optimization (CogPO) algorithm to continuously enhance the reasoning\nabilities of smaller models by aligning their reasoning processes with their\ncognitive capacities. Comprehensive evaluations on challenging reasoning\nbenchmarks demonstrate the efficacy of our CRV+CogPO framework, which\noutperforms other methods by a large margin."
                },
                "authors": [
                    {
                        "name": "Wenrui Cai"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Junbing Yan"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Xiangzhong Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzhong Fang"
                },
                "author": "Xiangzhong Fang",
                "arxiv_comment": "emnlp 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03525v2",
                "updated": "2025-11-03T07:38:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    38,
                    49,
                    0,
                    307,
                    0
                ],
                "published": "2025-07-04T12:22:35Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    12,
                    22,
                    35,
                    4,
                    185,
                    0
                ],
                "title": "Limits of Safe AI Deployment: Differentiating Oversight and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of Safe AI Deployment: Differentiating Oversight and Control"
                },
                "summary": "Oversight and control, which we collectively call supervision, are often\ndiscussed as ways to ensure that AI systems are accountable, reliable, and able\nto fulfill governance and management requirements. However, the requirements\nfor \"human oversight\" risk codifying vague or inconsistent interpretations of\nkey concepts like oversight and control. This ambiguous terminology could\nundermine efforts to design or evaluate systems that must operate under\nmeaningful human supervision. This matters because the term is used by\nregulatory texts such as the EU AI Act.\n  This paper undertakes a targeted critical review of literature on supervision\noutside of AI, along with a brief summary of past work on the topic related to\nAI. We next differentiate control as ex-ante or real-time and operational\nrather than policy or governance, and oversight as performed ex-post, or a\npolicy and governance function. Control aims to prevent failures, while\noversight focuses on detection, remediation, or incentives for future\nprevention. Building on this, we make three contributions. 1) We propose a\nframework to align regulatory expectations with what is technically and\norganizationally plausible, articulating the conditions under which each\nmechanism is possible, where they fall short, and what is required to make them\nmeaningful in practice. 2) We outline how supervision methods should be\ndocumented and integrated into risk management, and drawing on the Microsoft\nResponsible AI Maturity Model, we outline a maturity model for AI supervision.\n3) We explicitly highlight boundaries of these mechanisms, including where they\napply, where they fail, and where it is clear that no existing methods suffice.\nThis foregrounds the question of whether meaningful supervision is possible in\na given deployment context, and can support regulators, auditors, and\npractitioners in identifying both present and future limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oversight and control, which we collectively call supervision, are often\ndiscussed as ways to ensure that AI systems are accountable, reliable, and able\nto fulfill governance and management requirements. However, the requirements\nfor \"human oversight\" risk codifying vague or inconsistent interpretations of\nkey concepts like oversight and control. This ambiguous terminology could\nundermine efforts to design or evaluate systems that must operate under\nmeaningful human supervision. This matters because the term is used by\nregulatory texts such as the EU AI Act.\n  This paper undertakes a targeted critical review of literature on supervision\noutside of AI, along with a brief summary of past work on the topic related to\nAI. We next differentiate control as ex-ante or real-time and operational\nrather than policy or governance, and oversight as performed ex-post, or a\npolicy and governance function. Control aims to prevent failures, while\noversight focuses on detection, remediation, or incentives for future\nprevention. Building on this, we make three contributions. 1) We propose a\nframework to align regulatory expectations with what is technically and\norganizationally plausible, articulating the conditions under which each\nmechanism is possible, where they fall short, and what is required to make them\nmeaningful in practice. 2) We outline how supervision methods should be\ndocumented and integrated into risk management, and drawing on the Microsoft\nResponsible AI Maturity Model, we outline a maturity model for AI supervision.\n3) We explicitly highlight boundaries of these mechanisms, including where they\napply, where they fail, and where it is clear that no existing methods suffice.\nThis foregrounds the question of whether meaningful supervision is possible in\na given deployment context, and can support regulators, auditors, and\npractitioners in identifying both present and future limitations."
                },
                "authors": [
                    {
                        "name": "David Manheim"
                    },
                    {
                        "name": "Aidan Homewood"
                    }
                ],
                "author_detail": {
                    "name": "Aidan Homewood"
                },
                "author": "Aidan Homewood",
                "arxiv_comment": "Revised to improve table formatting and update draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; K.6; D.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04251v4",
                "updated": "2025-11-03T07:37:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    37,
                    51,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-01T06:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    6,
                    46,
                    49,
                    6,
                    152,
                    0
                ],
                "title": "Language-Driven Coordination and Learning in Multi-Agent Simulation\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Driven Coordination and Learning in Multi-Agent Simulation\n  Environments"
                },
                "summary": "This paper introduces LLM-MARL, a unified framework that incorporates large\nlanguage models (LLMs) into multi-agent reinforcement learning (MARL) to\nenhance coordination, communication, and generalization in simulated game\nenvironments. The framework features three modular components of Coordinator,\nCommunicator, and Memory, which dynamically generate subgoals, facilitate\nsymbolic inter-agent messaging, and support episodic recall. Training combines\nPPO with a language-conditioned loss and LLM query gating. LLM-MARL is\nevaluated in Google Research Football, MAgent Battle, and StarCraft II. Results\nshow consistent improvements over MAPPO and QMIX in win rate, coordination\nscore, and zero-shot generalization. Ablation studies demonstrate that subgoal\ngeneration and language-based messaging each contribute significantly to\nperformance gains. Qualitative analysis reveals emergent behaviors such as role\nspecialization and communication-driven tactics. By bridging language modeling\nand policy learning, this work contributes to the design of intelligent,\ncooperative agents in interactive simulations. It offers a path forward for\nleveraging LLMs in multi-agent systems used for training, games, and human-AI\ncollaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLM-MARL, a unified framework that incorporates large\nlanguage models (LLMs) into multi-agent reinforcement learning (MARL) to\nenhance coordination, communication, and generalization in simulated game\nenvironments. The framework features three modular components of Coordinator,\nCommunicator, and Memory, which dynamically generate subgoals, facilitate\nsymbolic inter-agent messaging, and support episodic recall. Training combines\nPPO with a language-conditioned loss and LLM query gating. LLM-MARL is\nevaluated in Google Research Football, MAgent Battle, and StarCraft II. Results\nshow consistent improvements over MAPPO and QMIX in win rate, coordination\nscore, and zero-shot generalization. Ablation studies demonstrate that subgoal\ngeneration and language-based messaging each contribute significantly to\nperformance gains. Qualitative analysis reveals emergent behaviors such as role\nspecialization and communication-driven tactics. By bridging language modeling\nand policy learning, this work contributes to the design of intelligent,\ncooperative agents in interactive simulations. It offers a path forward for\nleveraging LLMs in multi-agent systems used for training, games, and human-AI\ncollaboration."
                },
                "authors": [
                    {
                        "name": "Zhengyang Li"
                    },
                    {
                        "name": "Sawyer Campos"
                    },
                    {
                        "name": "Nana Wang"
                    }
                ],
                "author_detail": {
                    "name": "Nana Wang"
                },
                "author": "Nana Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22323v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22323v3",
                "updated": "2025-11-03T07:21:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    21,
                    22,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-28T13:09:47Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    9,
                    47,
                    2,
                    148,
                    0
                ],
                "title": "Advancing Expert Specialization for Better MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Expert Specialization for Better MoE"
                },
                "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community."
                },
                "authors": [
                    {
                        "name": "Hongcan Guo"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Bolun Chu"
                    },
                    {
                        "name": "Jialin Zhuang"
                    },
                    {
                        "name": "Yuan Yang"
                    },
                    {
                        "name": "Wenhao Che"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Qimei Cui"
                    },
                    {
                        "name": "Xudong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Jiang"
                },
                "author": "Xudong Jiang",
                "arxiv_comment": "33pages, 6figures(Accepted by Neurips 2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22323v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22323v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04405v2",
                "updated": "2025-11-03T07:08:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    8,
                    2,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-06T12:47:05Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    47,
                    5,
                    2,
                    218,
                    0
                ],
                "title": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via\n  Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via\n  Algorithm-System Co-Design"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration. In this paper, we propose FlexQ, a\nnovel post-training INT6 quantization framework combining algorithmic\ninnovation with system-level optimizations. FlexQ employs uniform 6-bit weight\nquantization across all layers, with adaptive retention of 8-bit activations in\nlayers identified through layer-wise sensitivity analysis. To maximize hardware\nefficiency, we develop a specialized high-performance GPU kernel supporting\nmatrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core\n(BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores.\nEvaluations on LLaMA family models show FlexQ maintains near-FP16 accuracy,\nwith perplexity increases of no more than 0.1 on WikiText2. The proposed kernel\nachieves an average 1.39$\\times$ speedup over ABQ-LLM on LLaMA-2-70B linear\nlayers. End-to-end, FlexQ delivers 1.33$\\times$ inference acceleration and\n1.21$\\times$ memory savings over SmoothQuant. Code is released at\nhttps://github.com/FlyFoxPlayer/FlexQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration. In this paper, we propose FlexQ, a\nnovel post-training INT6 quantization framework combining algorithmic\ninnovation with system-level optimizations. FlexQ employs uniform 6-bit weight\nquantization across all layers, with adaptive retention of 8-bit activations in\nlayers identified through layer-wise sensitivity analysis. To maximize hardware\nefficiency, we develop a specialized high-performance GPU kernel supporting\nmatrix multiplication for W6A6 and W6A8 representations via Binary Tensor Core\n(BTC) equivalents, effectively bypassing the lack of native INT6 tensor cores.\nEvaluations on LLaMA family models show FlexQ maintains near-FP16 accuracy,\nwith perplexity increases of no more than 0.1 on WikiText2. The proposed kernel\nachieves an average 1.39$\\times$ speedup over ABQ-LLM on LLaMA-2-70B linear\nlayers. End-to-end, FlexQ delivers 1.33$\\times$ inference acceleration and\n1.21$\\times$ memory savings over SmoothQuant. Code is released at\nhttps://github.com/FlyFoxPlayer/FlexQ."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Aining Jia"
                    },
                    {
                        "name": "Weifeng Bu"
                    },
                    {
                        "name": "Yushu Cai"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25741v2",
                "updated": "2025-11-03T06:54:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    54,
                    49,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-29T17:45:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    45,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "Scaling Latent Reasoning via Looped Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Latent Reasoning via Looped Language Models"
                },
                "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model is available here: http://ouro-llm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model is available here: http://ouro-llm.github.io."
                },
                "authors": [
                    {
                        "name": "Rui-Jie Zhu"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Kai Hua"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Boyi Wei"
                    },
                    {
                        "name": "Zixin Wen"
                    },
                    {
                        "name": "Fan Yin"
                    },
                    {
                        "name": "He Xing"
                    },
                    {
                        "name": "Lu Li"
                    },
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Taylor Kergan"
                    },
                    {
                        "name": "Andrew Smith"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Mude Hui"
                    },
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Qiyang Min"
                    },
                    {
                        "name": "Hongzhi Huang"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Yunfeng Shi"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Enduo Zhao"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Jason Eshraghian"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eshraghian"
                },
                "author": "Jason Eshraghian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05887v3",
                "updated": "2025-11-03T06:50:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    50,
                    31,
                    0,
                    307,
                    0
                ],
                "published": "2024-03-09T11:59:10Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    11,
                    59,
                    10,
                    5,
                    69,
                    0
                ],
                "title": "Aligning Speech to Languages to Enhance Code-switching Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Speech to Languages to Enhance Code-switching Speech\n  Recognition"
                },
                "summary": "Code-switching (CS) refers to the switching of languages within a speech\nsignal and results in language confusion for automatic speech recognition\n(ASR). To address language confusion, we propose a language alignment loss\n(LAL) that aligns acoustic features to pseudo-language labels learned from the\nASR decoder during ASR training. This approach enables frame-level language\nidentification without the need for frame-level language annotations. To\nfurther tackle the complex token alternatives for language modeling in\nbilingual scenarios, we propose to employ large language models via a\ngenerative error correction method. A linguistic hint, derived from LAL outputs\nand decoded hypotheses, is introduced to guide the prompting and enhance the\nLLM-based generative error correction for CS-ASR. The proposed methods are\nevaluated on the SEAME dataset and data from the ASRU 2019 Mandarin-English\ncode-switching speech recognition challenge. The incorporation of the proposed\nlanguage alignment loss improves CS-ASR performance for both hybrid\nCTC/attention and Whisper models on both datasets, with only a negligible\nincrease in the number of parameters. This work also highlights the efficacy of\nlanguage alignment loss in balancing primary-language-dominant bilingual data\nduring training, with an 8.6% relative improvement on the ASRU dataset compared\nto the baseline model. Performance evaluation using large language models\nreveals the advantage of the linguistic hint by achieving 14.1% and 5.5%\nrelative improvement on test sets of the ASRU and SEAME datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-switching (CS) refers to the switching of languages within a speech\nsignal and results in language confusion for automatic speech recognition\n(ASR). To address language confusion, we propose a language alignment loss\n(LAL) that aligns acoustic features to pseudo-language labels learned from the\nASR decoder during ASR training. This approach enables frame-level language\nidentification without the need for frame-level language annotations. To\nfurther tackle the complex token alternatives for language modeling in\nbilingual scenarios, we propose to employ large language models via a\ngenerative error correction method. A linguistic hint, derived from LAL outputs\nand decoded hypotheses, is introduced to guide the prompting and enhance the\nLLM-based generative error correction for CS-ASR. The proposed methods are\nevaluated on the SEAME dataset and data from the ASRU 2019 Mandarin-English\ncode-switching speech recognition challenge. The incorporation of the proposed\nlanguage alignment loss improves CS-ASR performance for both hybrid\nCTC/attention and Whisper models on both datasets, with only a negligible\nincrease in the number of parameters. This work also highlights the efficacy of\nlanguage alignment loss in balancing primary-language-dominant bilingual data\nduring training, with an 8.6% relative improvement on the ASRU dataset compared\nto the baseline model. Performance evaluation using large language models\nreveals the advantage of the linguistic hint by achieving 14.1% and 5.5%\nrelative improvement on test sets of the ASRU and SEAME datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Hexin Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Leibny Paola Garcia"
                    },
                    {
                        "name": "Andy W. H. Khong"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Shinji Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shinji Watanabe"
                },
                "author": "Shinji Watanabe",
                "arxiv_comment": "Accepted to IEEE Trans. Audio Speech Lang. Process., copyright has\n  been transferred to IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03659v2",
                "updated": "2025-11-03T06:48:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    48,
                    4,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-04T07:48:10Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    7,
                    48,
                    10,
                    2,
                    155,
                    0
                ],
                "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey"
                },
                "summary": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Baiyang Wang"
                    },
                    {
                        "name": "Robert E. Mercer"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Sudipta Singha Roy"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Xindi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xindi Wang"
                },
                "author": "Xindi Wang",
                "arxiv_comment": "accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06350v2",
                "updated": "2025-11-03T06:40:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    40,
                    4,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-08T14:30:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    30,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Effective Tokens with Video Anomaly in Large Language Models"
                },
                "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks."
                },
                "authors": [
                    {
                        "name": "Yingxian Chen"
                    },
                    {
                        "name": "Jiahui Liu"
                    },
                    {
                        "name": "Ruidi Fan"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Chirui Chang"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Wilton W. T. Fok"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Yik-Chung Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yik-Chung Wu"
                },
                "author": "Yik-Chung Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.09051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.09051v5",
                "updated": "2025-11-03T05:24:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    5,
                    24,
                    8,
                    0,
                    307,
                    0
                ],
                "published": "2023-02-17T18:31:31Z",
                "published_parsed": [
                    2023,
                    2,
                    17,
                    18,
                    31,
                    31,
                    4,
                    48,
                    0
                ],
                "title": "Complex QA and language models hybrid architectures, Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex QA and language models hybrid architectures, Survey"
                },
                "summary": "This paper reviews the state-of-the-art of large language models (LLM)\narchitectures and strategies for \"complex\" question-answering with a focus on\nhybrid architectures. LLM based chatbot services have allowed anyone to grasp\nthe potential of LLM to solve many common problems, but soon discovered their\nlimitations for complex questions. Addressing more specific, complex questions\n(e.g., \"What is the best mix of power-generation methods to reduce climate\nchange ?\") often requires specialized architectures, domain knowledge, new\nskills, decomposition and multi-step resolution, deep reasoning, sensitive data\nprotection, explainability, and human-in-the-loop processes. Therefore, we\nreview: (1) necessary skills and tasks for handling complex questions and\ncommon LLM limits to overcome; (2) dataset, cost functions and evaluation\nmetrics for measuring and improving (e.g. accuracy, explainability, fairness,\nrobustness, groundedness, faithfulness, toxicity...); (3) family of solutions\nto overcome LLM limitations by (a) training and reinforcement (b)\nhybridization, (c) prompting, (d) agentic-architectures (agents, tools) and\nextended reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reviews the state-of-the-art of large language models (LLM)\narchitectures and strategies for \"complex\" question-answering with a focus on\nhybrid architectures. LLM based chatbot services have allowed anyone to grasp\nthe potential of LLM to solve many common problems, but soon discovered their\nlimitations for complex questions. Addressing more specific, complex questions\n(e.g., \"What is the best mix of power-generation methods to reduce climate\nchange ?\") often requires specialized architectures, domain knowledge, new\nskills, decomposition and multi-step resolution, deep reasoning, sensitive data\nprotection, explainability, and human-in-the-loop processes. Therefore, we\nreview: (1) necessary skills and tasks for handling complex questions and\ncommon LLM limits to overcome; (2) dataset, cost functions and evaluation\nmetrics for measuring and improving (e.g. accuracy, explainability, fairness,\nrobustness, groundedness, faithfulness, toxicity...); (3) family of solutions\nto overcome LLM limitations by (a) training and reinforcement (b)\nhybridization, (c) prompting, (d) agentic-architectures (agents, tools) and\nextended reasoning."
                },
                "authors": [
                    {
                        "name": "Xavier Daull"
                    },
                    {
                        "name": "Patrice Bellot"
                    },
                    {
                        "name": "Emmanuel Bruno"
                    },
                    {
                        "name": "Vincent Martin"
                    },
                    {
                        "name": "Elisabeth Murisasco"
                    }
                ],
                "author_detail": {
                    "name": "Elisabeth Murisasco"
                },
                "author": "Elisabeth Murisasco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.09051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.09051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11671v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11671v3",
                "updated": "2025-11-03T03:47:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    3,
                    47,
                    20,
                    0,
                    307,
                    0
                ],
                "published": "2025-04-16T00:02:28Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    0,
                    2,
                    28,
                    2,
                    106,
                    0
                ],
                "title": "Computational Basis of LLM's Decision Making in Social Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Basis of LLM's Decision Making in Social Simulation"
                },
                "summary": "Large language models (LLMs) increasingly serve as human-like decision-making\nagents in social science and applied settings. These LLM-agents are typically\nassigned human-like characters and placed in real-life contexts. However, how\nthese characters and contexts shape an LLM's behavior remains underexplored.\nThis study proposes and tests methods for probing, quantifying, and modifying\nan LLM's internal representations in a Dictator Game -- a classic behavioral\nexperiment on fairness and prosocial behavior. We extract \"vectors of variable\nvariations\" (e.g., \"male\" to \"female\") from the LLM's internal state.\nManipulating these vectors during the model's inference can substantially alter\nhow those variables relate to the model's decision-making. This approach offers\na principled way to study and regulate how social concepts can be encoded and\nengineered within transformer-based models, with implications for alignment,\ndebiasing, and designing AI agents for social simulations in both academic and\ncommercial applications, strengthening sociological theory and measurement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly serve as human-like decision-making\nagents in social science and applied settings. These LLM-agents are typically\nassigned human-like characters and placed in real-life contexts. However, how\nthese characters and contexts shape an LLM's behavior remains underexplored.\nThis study proposes and tests methods for probing, quantifying, and modifying\nan LLM's internal representations in a Dictator Game -- a classic behavioral\nexperiment on fairness and prosocial behavior. We extract \"vectors of variable\nvariations\" (e.g., \"male\" to \"female\") from the LLM's internal state.\nManipulating these vectors during the model's inference can substantially alter\nhow those variables relate to the model's decision-making. This approach offers\na principled way to study and regulate how social concepts can be encoded and\nengineered within transformer-based models, with implications for alignment,\ndebiasing, and designing AI agents for social simulations in both academic and\ncommercial applications, strengthening sociological theory and measurement."
                },
                "authors": [
                    {
                        "name": "Ji Ma"
                    }
                ],
                "author_detail": {
                    "name": "Ji Ma"
                },
                "author": "Ji Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11671v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11671v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15685v2",
                "updated": "2025-11-03T03:44:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    3,
                    44,
                    12,
                    0,
                    307,
                    0
                ],
                "published": "2025-05-21T16:01:11Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    1,
                    11,
                    2,
                    141,
                    0
                ],
                "title": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems"
                },
                "summary": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions."
                },
                "authors": [
                    {
                        "name": "Xiuchao Sui"
                    },
                    {
                        "name": "Daiying Tian"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Kenneth Kwok"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "EMNLP 2025 camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09378v2",
                "updated": "2025-11-03T03:01:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    3,
                    1,
                    7,
                    0,
                    307,
                    0
                ],
                "published": "2024-09-14T09:06:43Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    9,
                    6,
                    43,
                    5,
                    258,
                    0
                ],
                "title": "Prevailing Research Areas for Music AI in the Era of Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prevailing Research Areas for Music AI in the Era of Foundation Models"
                },
                "summary": "Parallel to rapid advancements in foundation model research, the past few\nyears have witnessed a surge in music AI applications. As AI-generated and\nAI-augmented music become increasingly mainstream, many researchers in the\nmusic AI community may wonder: what research frontiers remain unexplored?\n  This paper outlines several key areas within music AI research that present\nsignificant opportunities for further investigation. We begin by examining\nfoundational representation models and highlight emerging efforts toward\nexplainability and interpretability. We then discuss the evolution toward\nmultimodal systems, provide an overview of the current landscape of music\ndatasets and their limitations, and address the growing importance of model\nefficiency in both training and deployment.\n  Next, we explore applied directions, focusing first on generative models. We\nreview recent systems, their computational constraints, and persistent\nchallenges related to evaluation and controllability. We then examine\nextensions of these generative approaches to multimodal settings and their\nintegration into artists' workflows, including applications in music editing,\ncaptioning, production, transcription, source separation, performance,\ndiscovery, and education.\n  Finally, we explore copyright implications of generative music and propose\nstrategies to safeguard artist rights. While not exhaustive, this survey aims\nto illuminate promising research directions enabled by recent developments in\nmusic foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel to rapid advancements in foundation model research, the past few\nyears have witnessed a surge in music AI applications. As AI-generated and\nAI-augmented music become increasingly mainstream, many researchers in the\nmusic AI community may wonder: what research frontiers remain unexplored?\n  This paper outlines several key areas within music AI research that present\nsignificant opportunities for further investigation. We begin by examining\nfoundational representation models and highlight emerging efforts toward\nexplainability and interpretability. We then discuss the evolution toward\nmultimodal systems, provide an overview of the current landscape of music\ndatasets and their limitations, and address the growing importance of model\nefficiency in both training and deployment.\n  Next, we explore applied directions, focusing first on generative models. We\nreview recent systems, their computational constraints, and persistent\nchallenges related to evaluation and controllability. We then examine\nextensions of these generative approaches to multimodal settings and their\nintegration into artists' workflows, including applications in music editing,\ncaptioning, production, transcription, source separation, performance,\ndiscovery, and education.\n  Finally, we explore copyright implications of generative music and propose\nstrategies to safeguard artist rights. While not exhaustive, this survey aims\nto illuminate promising research directions enabled by recent developments in\nmusic foundation models."
                },
                "authors": [
                    {
                        "name": "Megan Wei"
                    },
                    {
                        "name": "Mateusz Modrzejewski"
                    },
                    {
                        "name": "Aswin Sivaraman"
                    },
                    {
                        "name": "Dorien Herremans"
                    }
                ],
                "author_detail": {
                    "name": "Dorien Herremans"
                },
                "author": "Dorien Herremans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5.4; I.2.6; I.2.7; H.5.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21432v3",
                "updated": "2025-11-03T02:58:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    58,
                    46,
                    0,
                    307,
                    0
                ],
                "published": "2025-08-29T09:01:34Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    1,
                    34,
                    4,
                    241,
                    0
                ],
                "title": "RepoMark: A Data-Usage Auditing Framework for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoMark: A Data-Usage Auditing Framework for Code Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables auditors to verify\nwhether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect model\nbehavior difference on trained data. Compared to prior data auditing\napproaches, RepoMark significantly enhances data efficiency, allowing effective\nauditing even when the user's repository possesses only a small number of code\nfiles.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of code authors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables auditors to verify\nwhether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect model\nbehavior difference on trained data. Compared to prior data auditing\napproaches, RepoMark significantly enhances data efficiency, allowing effective\nauditing even when the user's repository possesses only a small number of code\nfiles.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of code authors."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Yuguang Zhou"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Lionel Z. Wang"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16690v2",
                "updated": "2025-11-03T02:42:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    42,
                    40,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-20T02:22:21Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    22,
                    21,
                    4,
                    171,
                    0
                ],
                "title": "DepthVanish: Optimizing Adversarial Interval Structures for\n  Stereo-Depth-Invisible Patches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DepthVanish: Optimizing Adversarial Interval Structures for\n  Stereo-Depth-Invisible Patches"
                },
                "summary": "Stereo depth estimation is a critical task in autonomous driving and\nrobotics, where inaccuracies (such as misidentifying nearby objects as distant)\ncan lead to dangerous situations. Adversarial attacks against stereo depth\nestimation can help reveal vulnerabilities before deployment. Previous works\nhave shown that repeating optimized textures can effectively mislead stereo\ndepth estimation in digital settings. However, our research reveals that these\nnaively repeated textures perform poorly in physical implementations, i.e.,\nwhen deployed as patches, limiting their practical utility for stress-testing\nstereo depth estimation systems. In this work, for the first time, we discover\nthat introducing regular intervals among the repeated textures, creating a grid\nstructure, significantly enhances the patch's attack performance. Through\nextensive experimentation, we analyze how variations of this novel structure\ninfluence the adversarial effectiveness. Based on these insights, we develop a\nnovel stereo depth attack that jointly optimizes both the interval structure\nand texture elements. Our generated adversarial patches can be inserted into\nany scenes and successfully attack advanced stereo depth estimation methods of\ndifferent paradigms, i.e., RAFT-Stereo and STTR. Most critically, our patch can\nalso attack commercial RGB-D cameras (Intel RealSense) in real-world\nconditions, demonstrating their practical relevance for security assessment of\nstereo systems. The code is officially released at:\nhttps://github.com/WiWiN42/DepthVanish",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereo depth estimation is a critical task in autonomous driving and\nrobotics, where inaccuracies (such as misidentifying nearby objects as distant)\ncan lead to dangerous situations. Adversarial attacks against stereo depth\nestimation can help reveal vulnerabilities before deployment. Previous works\nhave shown that repeating optimized textures can effectively mislead stereo\ndepth estimation in digital settings. However, our research reveals that these\nnaively repeated textures perform poorly in physical implementations, i.e.,\nwhen deployed as patches, limiting their practical utility for stress-testing\nstereo depth estimation systems. In this work, for the first time, we discover\nthat introducing regular intervals among the repeated textures, creating a grid\nstructure, significantly enhances the patch's attack performance. Through\nextensive experimentation, we analyze how variations of this novel structure\ninfluence the adversarial effectiveness. Based on these insights, we develop a\nnovel stereo depth attack that jointly optimizes both the interval structure\nand texture elements. Our generated adversarial patches can be inserted into\nany scenes and successfully attack advanced stereo depth estimation methods of\ndifferent paradigms, i.e., RAFT-Stereo and STTR. Most critically, our patch can\nalso attack commercial RGB-D cameras (Intel RealSense) in real-world\nconditions, demonstrating their practical relevance for security assessment of\nstereo systems. The code is officially released at:\nhttps://github.com/WiWiN42/DepthVanish"
                },
                "authors": [
                    {
                        "name": "Yun Xing"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Nhat Chung"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Qing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qing Guo"
                },
                "author": "Qing Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10987v2",
                "updated": "2025-11-03T02:23:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    23,
                    39,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-13T03:53:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    53,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge\n  Distillation"
                },
                "summary": "The promise of LLM watermarking rests on a core assumption that a specific\nwatermark proves authorship by a specific model. We demonstrate that this\nassumption is dangerously flawed. We introduce the threat of watermark\nspoofing, a sophisticated attack that allows a malicious model to generate text\ncontaining the authentic-looking watermark of a trusted, victim model. This\nenables the seamless misattribution of harmful content, such as disinformation,\nto reputable sources. The key to our attack is repurposing watermark\nradioactivity, the unintended inheritance of data patterns during fine-tuning,\nfrom a discoverable trait into an attack vector. By distilling knowledge from a\nwatermarked teacher model, our framework allows an attacker to steal and\nreplicate the watermarking signal of the victim model. This work reveals a\ncritical security gap in text authorship verification and calls for a paradigm\nshift towards technologies capable of distinguishing authentic watermarks from\nexpertly imitated ones. Our code is available at\nhttps://github.com/hsannn/ditto.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promise of LLM watermarking rests on a core assumption that a specific\nwatermark proves authorship by a specific model. We demonstrate that this\nassumption is dangerously flawed. We introduce the threat of watermark\nspoofing, a sophisticated attack that allows a malicious model to generate text\ncontaining the authentic-looking watermark of a trusted, victim model. This\nenables the seamless misattribution of harmful content, such as disinformation,\nto reputable sources. The key to our attack is repurposing watermark\nradioactivity, the unintended inheritance of data patterns during fine-tuning,\nfrom a discoverable trait into an attack vector. By distilling knowledge from a\nwatermarked teacher model, our framework allows an attacker to steal and\nreplicate the watermarking signal of the victim model. This work reveals a\ncritical security gap in text authorship verification and calls for a paradigm\nshift towards technologies capable of distinguishing authentic watermarks from\nexpertly imitated ones. Our code is available at\nhttps://github.com/hsannn/ditto.git."
                },
                "authors": [
                    {
                        "name": "Hyeseon Ahn"
                    },
                    {
                        "name": "Shinwoo Park"
                    },
                    {
                        "name": "Suyeon Woo"
                    },
                    {
                        "name": "Yo-Sub Han"
                    }
                ],
                "author_detail": {
                    "name": "Yo-Sub Han"
                },
                "author": "Yo-Sub Han",
                "arxiv_comment": "14 pages, 4 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23488v3",
                "updated": "2025-11-03T02:18:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    18,
                    28,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-27T20:23:13Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    23,
                    13,
                    5,
                    270,
                    0
                ],
                "title": "Mapping Overlaps in Benchmarks through Perplexity in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping Overlaps in Benchmarks through Perplexity in the Wild"
                },
                "summary": "We develop signatures of capacity familiarity to characterize large language\nmodel (LLM) benchmarks and their meaningful overlaps. Benchmark signatures\nprobe the capacity required for benchmark performance. We formally define them\nas a set of salient tokens drawn from in-the-wild, naturally authored corpora,\nwhere LLM token perplexity, reflecting more or less pre-training exposure,\nbecomes highly predictive of LLM benchmark performance. Through a large-scale\nmeta-evaluation, we extract benchmark signatures via stepwise forward selection\nwith linear regressions across 32 LLMs and 88 benchmarks spanning diverse\nknowledge, coding, logic, instruction following, math, language, reasoning, and\nworld modeling. Our analysis situates signatures in relation to both the\nsemantic similarity of benchmark questions and the correlation of model\nperformance. While performance overlaps are universally high and semantic\noverlaps remain confined to a narrow mid-range, benchmark signatures prove\nhighly informative in capturing variation, overlap, and divergence. We observe\noverlap in knowledge and reasoning subtasks, whereas multilingual and cultural\nbenchmarks exhibit less similarity, even compared to cross-task overlap.\nNotably, performance-level results are strongly influenced by\nbenchmark-orthogonal factors such as question format, highlighting limitations\nin LLM generalization, the conflation of performance with ability, and issues\ninherent in current mainstream benchmark agreement studies. Benchmark\nsignatures, however, remain robust to such effects. Ultimately, we identify\ncross-functional overlaps across logic, math, language, instruction following,\nand world modeling, with coding emerging as the least overlapping domain.\nTogether, these findings provide mechanistic insights into benchmark validity\nand LLM sensitivities, and sketch the underlying landscape of interconnected\nLLM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop signatures of capacity familiarity to characterize large language\nmodel (LLM) benchmarks and their meaningful overlaps. Benchmark signatures\nprobe the capacity required for benchmark performance. We formally define them\nas a set of salient tokens drawn from in-the-wild, naturally authored corpora,\nwhere LLM token perplexity, reflecting more or less pre-training exposure,\nbecomes highly predictive of LLM benchmark performance. Through a large-scale\nmeta-evaluation, we extract benchmark signatures via stepwise forward selection\nwith linear regressions across 32 LLMs and 88 benchmarks spanning diverse\nknowledge, coding, logic, instruction following, math, language, reasoning, and\nworld modeling. Our analysis situates signatures in relation to both the\nsemantic similarity of benchmark questions and the correlation of model\nperformance. While performance overlaps are universally high and semantic\noverlaps remain confined to a narrow mid-range, benchmark signatures prove\nhighly informative in capturing variation, overlap, and divergence. We observe\noverlap in knowledge and reasoning subtasks, whereas multilingual and cultural\nbenchmarks exhibit less similarity, even compared to cross-task overlap.\nNotably, performance-level results are strongly influenced by\nbenchmark-orthogonal factors such as question format, highlighting limitations\nin LLM generalization, the conflation of performance with ability, and issues\ninherent in current mainstream benchmark agreement studies. Benchmark\nsignatures, however, remain robust to such effects. Ultimately, we identify\ncross-functional overlaps across logic, math, language, instruction following,\nand world modeling, with coding emerging as the least overlapping domain.\nTogether, these findings provide mechanistic insights into benchmark validity\nand LLM sensitivities, and sketch the underlying landscape of interconnected\nLLM capabilities."
                },
                "authors": [
                    {
                        "name": "Siyang Wu"
                    },
                    {
                        "name": "Honglin Bao"
                    },
                    {
                        "name": "Sida Li"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "James A. Evans"
                    }
                ],
                "author_detail": {
                    "name": "James A. Evans"
                },
                "author": "James A. Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24086v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24086v3",
                "updated": "2025-11-03T02:15:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    15,
                    34,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-30T17:42:22Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    42,
                    22,
                    0,
                    181,
                    0
                ],
                "title": "MotionGPT3: Human Motion as a Second Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionGPT3: Human Motion as a Second Modality"
                },
                "summary": "With the rapid progress of large language models (LLMs), multimodal\nframeworks that unify understanding and generation have become promising, yet\nthey face increasing complexity as the number of modalities and tasks grows. We\nobserve that motion quantization introduces approximation errors that cap\nmotion quality, and that unifying discrete text and continuous motion within a\nsingle-stream backbone amplifies cross-modal interference. Motivated by recent\nmulti-branch Transformer designs that separate signals from different\nmodalities, we propose MotionGPT3, a bimodal motion-language model for both\nunderstanding and generation. MotionGPT3 encodes raw motion into a continuous\nlatent space using a variational autoencoder (VAE), thereby avoiding\nquantization-induced artifacts, while leveraging the semantic prior of\npretrained language models. A dual-stream Transformer with shared attention\npreserves modality-specific routes while enabling controlled, bidirectional\ninformation flow, which reduces interference, stabilizing optimization, and\nempirically accelerates convergence without degrading fidelity. For multimodal\njoint training, a generate-then-align three-stage schedule further improves\nstability and limits cross-task interference. Experiments show that MotionGPT3\nachieves 2x faster convergence in training loss and up to 4x faster convergence\nin validation, while maintaining state-of-the-art performance on standard\nmotion understanding and motion generation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid progress of large language models (LLMs), multimodal\nframeworks that unify understanding and generation have become promising, yet\nthey face increasing complexity as the number of modalities and tasks grows. We\nobserve that motion quantization introduces approximation errors that cap\nmotion quality, and that unifying discrete text and continuous motion within a\nsingle-stream backbone amplifies cross-modal interference. Motivated by recent\nmulti-branch Transformer designs that separate signals from different\nmodalities, we propose MotionGPT3, a bimodal motion-language model for both\nunderstanding and generation. MotionGPT3 encodes raw motion into a continuous\nlatent space using a variational autoencoder (VAE), thereby avoiding\nquantization-induced artifacts, while leveraging the semantic prior of\npretrained language models. A dual-stream Transformer with shared attention\npreserves modality-specific routes while enabling controlled, bidirectional\ninformation flow, which reduces interference, stabilizing optimization, and\nempirically accelerates convergence without degrading fidelity. For multimodal\njoint training, a generate-then-align three-stage schedule further improves\nstability and limits cross-task interference. Experiments show that MotionGPT3\nachieves 2x faster convergence in training loss and up to 4x faster convergence\nin validation, while maintaining state-of-the-art performance on standard\nmotion understanding and motion generation benchmarks."
                },
                "authors": [
                    {
                        "name": "Bingfan Zhu"
                    },
                    {
                        "name": "Biao Jiang"
                    },
                    {
                        "name": "Sunyi Wang"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Linjie Luo"
                    },
                    {
                        "name": "Youyi Zheng"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24086v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24086v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21972v2",
                "updated": "2025-11-03T02:14:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    2,
                    14,
                    57,
                    0,
                    307,
                    0
                ],
                "published": "2025-09-26T06:59:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    6,
                    59,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "From Superficial Outputs to Superficial Learning: Risks of Large\n  Language Models in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Superficial Outputs to Superficial Learning: Risks of Large\n  Language Models in Education"
                },
                "summary": "Large Language Models (LLMs) are transforming education by enabling\npersonalization, feedback, and knowledge access, while also raising concerns\nabout risks to students and learning systems. Yet empirical evidence on these\nrisks remains fragmented. This paper presents a systematic review of 70\nempirical studies across computer science, education, and psychology. Guided by\nfour research questions, we examine: (i) which applications of LLMs in\neducation have been most frequently explored; (ii) how researchers have\nmeasured their impact; (iii) which risks stem from such applications; and (iv)\nwhat mitigation strategies have been proposed. We find that research on LLMs\nclusters around three domains: operational effectiveness, personalized\napplications, and interactive learning tools. Across these, model-level risks\ninclude superficial understanding, bias, limited robustness, anthropomorphism,\nhallucinations, privacy concerns, and knowledge constraints. When learners\ninteract with LLMs, these risks extend to cognitive and behavioural outcomes,\nincluding reduced neural activity, over-reliance, diminished independent\nlearning skills, and a loss of student agency. To capture this progression, we\npropose an LLM-Risk Adapted Learning Model that illustrates how technical risks\ncascade through interaction and interpretation to shape educational outcomes.\nAs the first synthesis of empirically assessed risks, this review provides a\nfoundation for responsible, human-centred integration of LLMs in education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming education by enabling\npersonalization, feedback, and knowledge access, while also raising concerns\nabout risks to students and learning systems. Yet empirical evidence on these\nrisks remains fragmented. This paper presents a systematic review of 70\nempirical studies across computer science, education, and psychology. Guided by\nfour research questions, we examine: (i) which applications of LLMs in\neducation have been most frequently explored; (ii) how researchers have\nmeasured their impact; (iii) which risks stem from such applications; and (iv)\nwhat mitigation strategies have been proposed. We find that research on LLMs\nclusters around three domains: operational effectiveness, personalized\napplications, and interactive learning tools. Across these, model-level risks\ninclude superficial understanding, bias, limited robustness, anthropomorphism,\nhallucinations, privacy concerns, and knowledge constraints. When learners\ninteract with LLMs, these risks extend to cognitive and behavioural outcomes,\nincluding reduced neural activity, over-reliance, diminished independent\nlearning skills, and a loss of student agency. To capture this progression, we\npropose an LLM-Risk Adapted Learning Model that illustrates how technical risks\ncascade through interaction and interpretation to shape educational outcomes.\nAs the first synthesis of empirically assessed risks, this review provides a\nfoundation for responsible, human-centred integration of LLMs in education."
                },
                "authors": [
                    {
                        "name": "Iris Delikoura"
                    },
                    {
                        "name": "Yi. R Fung"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13918v3",
                "updated": "2025-11-03T01:42:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    1,
                    42,
                    59,
                    0,
                    307,
                    0
                ],
                "published": "2024-10-17T09:09:09Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    9,
                    9,
                    9,
                    3,
                    291,
                    0
                ],
                "title": "FTSmartAudit: A Knowledge Distillation-Enhanced Framework for Automated\n  Smart Contract Auditing Using Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FTSmartAudit: A Knowledge Distillation-Enhanced Framework for Automated\n  Smart Contract Auditing Using Fine-Tuned LLMs"
                },
                "summary": "The rapid growth of blockchain technology has driven the widespread adoption\nof smart contracts. However, their inherent vulnerabilities have led to\nsignificant financial losses. Traditional auditing methods, while essential,\nstruggle to keep pace with the increasing complexity and scale of smart\ncontracts. Large Language Models (LLMs) offer promising capabilities for\nautomating vulnerability detection, but their adoption is often limited by high\ncomputational costs. Although prior work has explored leveraging large models\nthrough agents or workflows, relatively little attention has been given to\nimproving the performance of smaller, fine-tuned models--a critical factor for\nachieving both efficiency and data privacy. In this paper, we introduce\nHKT-SmartAudit, a framework for developing lightweight models optimized for\nsmart contract auditing. It features a multi-stage knowledge distillation\npipeline that integrates classical distillation, external domain knowledge, and\nreward-guided learning to transfer high-quality insights from large teacher\nmodels. A single-task learning strategy is employed to train compact student\nmodels that maintain high accuracy and robustness while significantly reducing\ncomputational overhead. Experimental results show that our distilled models\noutperform both commercial tools and larger models in detecting complex\nvulnerabilities and logical flaws, offering a practical, secure, and scalable\nsolution for smart contract auditing. The source code is available at Github\nrepository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of blockchain technology has driven the widespread adoption\nof smart contracts. However, their inherent vulnerabilities have led to\nsignificant financial losses. Traditional auditing methods, while essential,\nstruggle to keep pace with the increasing complexity and scale of smart\ncontracts. Large Language Models (LLMs) offer promising capabilities for\nautomating vulnerability detection, but their adoption is often limited by high\ncomputational costs. Although prior work has explored leveraging large models\nthrough agents or workflows, relatively little attention has been given to\nimproving the performance of smaller, fine-tuned models--a critical factor for\nachieving both efficiency and data privacy. In this paper, we introduce\nHKT-SmartAudit, a framework for developing lightweight models optimized for\nsmart contract auditing. It features a multi-stage knowledge distillation\npipeline that integrates classical distillation, external domain knowledge, and\nreward-guided learning to transfer high-quality insights from large teacher\nmodels. A single-task learning strategy is employed to train compact student\nmodels that maintain high accuracy and robustness while significantly reducing\ncomputational overhead. Experimental results show that our distilled models\noutperform both commercial tools and larger models in detecting complex\nvulnerabilities and logical flaws, offering a practical, secure, and scalable\nsolution for smart contract auditing. The source code is available at Github\nrepository."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wei"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Xianhao Zhang"
                    },
                    {
                        "name": "Zhe Hou"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Hou"
                },
                "author": "Zhe Hou",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07229v2",
                "updated": "2025-11-03T01:37:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    1,
                    37,
                    36,
                    0,
                    307,
                    0
                ],
                "published": "2025-07-09T19:05:33Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    19,
                    5,
                    33,
                    2,
                    190,
                    0
                ],
                "title": "SynthTextEval: Synthetic Text Data Generation and Evaluation for\n  High-Stakes Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynthTextEval: Synthetic Text Data Generation and Evaluation for\n  High-Stakes Domains"
                },
                "summary": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development."
                },
                "authors": [
                    {
                        "name": "Krithika Ramesh"
                    },
                    {
                        "name": "Daniel Smolyak"
                    },
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Nupoor Gandhi"
                    },
                    {
                        "name": "Ritu Agarwal"
                    },
                    {
                        "name": "Margrt Bjarnadttir"
                    },
                    {
                        "name": "Anjalie Field"
                    }
                ],
                "author_detail": {
                    "name": "Anjalie Field"
                },
                "author": "Anjalie Field",
                "arxiv_comment": "EMNLP 2025 System Demonstration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05702v2",
                "updated": "2025-11-03T01:00:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    1,
                    0,
                    40,
                    0,
                    307,
                    0
                ],
                "published": "2025-10-07T09:10:13Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    9,
                    10,
                    13,
                    1,
                    280,
                    0
                ],
                "title": "Uncovering Representation Bias for Investment Decisions in Open-Source\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Representation Bias for Investment Decisions in Open-Source\n  Large Language Models"
                },
                "summary": "Large Language Models are increasingly adopted in financial applications to\nsupport investment workflows. However, prior studies have seldom examined how\nthese models reflect biases related to firm size, sector, or financial\ncharacteristics, which can significantly impact decision-making. This paper\naddresses this gap by focusing on representation bias in open-source Qwen\nmodels. We propose a balanced round-robin prompting method over approximately\n150 U.S. equities, applying constrained decoding and token-logit aggregation to\nderive firm-level confidence scores across financial contexts. Using\nstatistical tests and variance analysis, we find that firm size and valuation\nconsistently increase model confidence, while risk factors tend to decrease it.\nConfidence varies significantly across sectors, with the Technology sector\nshowing the greatest variability. When models are prompted for specific\nfinancial categories, their confidence rankings best align with fundamental\ndata, moderately with technical signals, and least with growth indicators.\nThese results highlight representation bias in Qwen models and motivate\nsector-aware calibration and category-conditioned evaluation protocols for safe\nand fair financial LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly adopted in financial applications to\nsupport investment workflows. However, prior studies have seldom examined how\nthese models reflect biases related to firm size, sector, or financial\ncharacteristics, which can significantly impact decision-making. This paper\naddresses this gap by focusing on representation bias in open-source Qwen\nmodels. We propose a balanced round-robin prompting method over approximately\n150 U.S. equities, applying constrained decoding and token-logit aggregation to\nderive firm-level confidence scores across financial contexts. Using\nstatistical tests and variance analysis, we find that firm size and valuation\nconsistently increase model confidence, while risk factors tend to decrease it.\nConfidence varies significantly across sectors, with the Technology sector\nshowing the greatest variability. When models are prompted for specific\nfinancial categories, their confidence rankings best align with fundamental\ndata, moderately with technical signals, and least with growth indicators.\nThese results highlight representation bias in Qwen models and motivate\nsector-aware calibration and category-conditioned evaluation protocols for safe\nand fair financial LLM deployment."
                },
                "authors": [
                    {
                        "name": "Fabrizio Dimino"
                    },
                    {
                        "name": "Krati Saxena"
                    },
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Stefano Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Pasquali"
                },
                "author": "Stefano Pasquali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06769v3",
                "updated": "2025-11-03T00:53:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    0,
                    53,
                    34,
                    0,
                    307,
                    0
                ],
                "published": "2024-12-09T18:55:56Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    55,
                    56,
                    0,
                    344,
                    0
                ],
                "title": "Training Large Language Models to Reason in a Continuous Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models to Reason in a Continuous Latent Space"
                },
                "summary": "Large language models (LLMs) are typically constrained to reason in the\nlanguage space, where they express the reasoning process through a\nchain-of-thought (CoT) to solve complex problems. However, the language space\nmay not always be optimal for reasoning. Most word tokens primarily ensure\ntextual coherence and are not essential for reasoning, while some critical\ntokens require complex planning and pose challenges to LLMs. To explore the\npotential of reasoning beyond language, we introduce a new paradigm called\nCoconut (Chain of Continuous Thought). Coconut utilizes the last hidden state\nof the LLM as a representation of the reasoning state, termed \"continuous\nthought.\" Instead of decoding this state into words, we feed it back to the\nmodel as the next input embedding directly in the continuous space. This latent\nreasoning paradigm enables an advanced reasoning pattern, where continuous\nthoughts can encode multiple alternative next steps, allowing the model to\nperform a breadth-first search (BFS) rather than committing prematurely to a\nsingle deterministic path as in CoT. Coconut outperforms CoT on logical\nreasoning tasks that require substantial search during planning and achieves a\nbetter trade-off between accuracy and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically constrained to reason in the\nlanguage space, where they express the reasoning process through a\nchain-of-thought (CoT) to solve complex problems. However, the language space\nmay not always be optimal for reasoning. Most word tokens primarily ensure\ntextual coherence and are not essential for reasoning, while some critical\ntokens require complex planning and pose challenges to LLMs. To explore the\npotential of reasoning beyond language, we introduce a new paradigm called\nCoconut (Chain of Continuous Thought). Coconut utilizes the last hidden state\nof the LLM as a representation of the reasoning state, termed \"continuous\nthought.\" Instead of decoding this state into words, we feed it back to the\nmodel as the next input embedding directly in the continuous space. This latent\nreasoning paradigm enables an advanced reasoning pattern, where continuous\nthoughts can encode multiple alternative next steps, allowing the model to\nperform a breadth-first search (BFS) rather than committing prematurely to a\nsingle deterministic path as in CoT. Coconut outperforms CoT on logical\nreasoning tasks that require substantial search during planning and achieves a\nbetter trade-off between accuracy and efficiency."
                },
                "authors": [
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Yuandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuandong Tian"
                },
                "author": "Yuandong Tian",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12815v2",
                "updated": "2025-11-02T22:39:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    22,
                    39,
                    55,
                    6,
                    306,
                    0
                ],
                "published": "2025-08-18T10:53:20Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    10,
                    53,
                    20,
                    0,
                    230,
                    0
                ],
                "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Steer: Input-dependent Steering for Multimodal LLMs"
                },
                "summary": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines. Our code is publicly available at\nhttps://jayneelparekh.github.io/learn-to-steer/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines. Our code is publicly available at\nhttps://jayneelparekh.github.io/learn-to-steer/"
                },
                "authors": [
                    {
                        "name": "Jayneel Parekh"
                    },
                    {
                        "name": "Pegah Khayatan"
                    },
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Arnaud Dapogny"
                    },
                    {
                        "name": "Alasdair Newson"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26012v2",
                "updated": "2025-11-02T22:15:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    22,
                    15,
                    47,
                    6,
                    306,
                    0
                ],
                "published": "2025-10-29T22:57:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    22,
                    57,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "AutoSurvey2: Empowering Researchers with Next Level Automated Literature\n  Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoSurvey2: Empowering Researchers with Next Level Automated Literature\n  Surveys"
                },
                "summary": "The rapid growth of research literature, particularly in large language\nmodels (LLMs), has made producing comprehensive and current survey papers\nincreasingly difficult. This paper introduces autosurvey2, a multi-stage\npipeline that automates survey generation through retrieval-augmented synthesis\nand structured evaluation. The system integrates parallel section generation,\niterative refinement, and real-time retrieval of recent publications to ensure\nboth topical completeness and factual accuracy. Quality is assessed using a\nmulti-LLM evaluation framework that measures coverage, structure, and relevance\nin alignment with expert review standards. Experimental results demonstrate\nthat autosurvey2 consistently outperforms existing retrieval-based and\nautomated baselines, achieving higher scores in structural coherence and\ntopical relevance while maintaining strong citation fidelity. By combining\nretrieval, reasoning, and automated evaluation into a unified framework,\nautosurvey2 provides a scalable and reproducible solution for generating\nlong-form academic surveys and contributes a solid foundation for future\nresearch on automated scholarly writing. All code and resources are available\nat https://github.com/annihi1ation/auto_research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of research literature, particularly in large language\nmodels (LLMs), has made producing comprehensive and current survey papers\nincreasingly difficult. This paper introduces autosurvey2, a multi-stage\npipeline that automates survey generation through retrieval-augmented synthesis\nand structured evaluation. The system integrates parallel section generation,\niterative refinement, and real-time retrieval of recent publications to ensure\nboth topical completeness and factual accuracy. Quality is assessed using a\nmulti-LLM evaluation framework that measures coverage, structure, and relevance\nin alignment with expert review standards. Experimental results demonstrate\nthat autosurvey2 consistently outperforms existing retrieval-based and\nautomated baselines, achieving higher scores in structural coherence and\ntopical relevance while maintaining strong citation fidelity. By combining\nretrieval, reasoning, and automated evaluation into a unified framework,\nautosurvey2 provides a scalable and reproducible solution for generating\nlong-form academic surveys and contributes a solid foundation for future\nresearch on automated scholarly writing. All code and resources are available\nat https://github.com/annihi1ation/auto_research."
                },
                "authors": [
                    {
                        "name": "Siyi Wu"
                    },
                    {
                        "name": "Chiaxin Liang"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Leyi Zhao"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Xinyuan Song"
                    }
                ],
                "author_detail": {
                    "name": "Xinyuan Song"
                },
                "author": "Xinyuan Song",
                "arxiv_comment": "TKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11575v2",
                "updated": "2025-11-02T22:14:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    22,
                    14,
                    9,
                    6,
                    306,
                    0
                ],
                "published": "2025-09-15T04:39:50Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    4,
                    39,
                    50,
                    0,
                    258,
                    0
                ],
                "title": "A Survey of Reasoning and Agentic Systems in Time Series with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Reasoning and Agentic Systems in Time Series with Large\n  Language Models"
                },
                "summary": "Time series reasoning treats time as a first-class axis and incorporates\nintermediate evidence directly into the answer. This survey defines the problem\nand organizes the literature by reasoning topology with three families: direct\nreasoning in one step, linear chain reasoning with explicit intermediates, and\nbranch-structured reasoning that explores, revises, and aggregates. The\ntopology is crossed with the main objectives of the field, including\ntraditional time series analysis, explanation and understanding, causal\ninference and decision making, and time series generation, while a compact tag\nset spans these axes and captures decomposition and verification, ensembling,\ntool use, knowledge access, multimodality, agent loops, and LLM alignment\nregimes. Methods and systems are reviewed across domains, showing what each\ntopology enables and where it breaks down in faithfulness or robustness, along\nwith curated datasets, benchmarks, and resources that support study and\ndeployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\nEvaluation practices that keep evidence visible and temporally aligned are\nhighlighted, and guidance is distilled on matching topology to uncertainty,\ngrounding with observable artifacts, planning for shift and streaming, and\ntreating cost and latency as design budgets. We emphasize that reasoning\nstructures must balance capacity for grounding and self-correction against\ncomputational cost and reproducibility, while future progress will likely\ndepend on benchmarks that tie reasoning quality to utility and on closed-loop\ntestbeds that trade off cost and risk under shift-aware, streaming, and\nlong-horizon settings. Taken together, these directions mark a shift from\nnarrow accuracy toward reliability at scale, enabling systems that not only\nanalyze but also understand, explain, and act on dynamic worlds with traceable\nevidence and credible outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series reasoning treats time as a first-class axis and incorporates\nintermediate evidence directly into the answer. This survey defines the problem\nand organizes the literature by reasoning topology with three families: direct\nreasoning in one step, linear chain reasoning with explicit intermediates, and\nbranch-structured reasoning that explores, revises, and aggregates. The\ntopology is crossed with the main objectives of the field, including\ntraditional time series analysis, explanation and understanding, causal\ninference and decision making, and time series generation, while a compact tag\nset spans these axes and captures decomposition and verification, ensembling,\ntool use, knowledge access, multimodality, agent loops, and LLM alignment\nregimes. Methods and systems are reviewed across domains, showing what each\ntopology enables and where it breaks down in faithfulness or robustness, along\nwith curated datasets, benchmarks, and resources that support study and\ndeployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\nEvaluation practices that keep evidence visible and temporally aligned are\nhighlighted, and guidance is distilled on matching topology to uncertainty,\ngrounding with observable artifacts, planning for shift and streaming, and\ntreating cost and latency as design budgets. We emphasize that reasoning\nstructures must balance capacity for grounding and self-correction against\ncomputational cost and reproducibility, while future progress will likely\ndepend on benchmarks that tie reasoning quality to utility and on closed-loop\ntestbeds that trade off cost and risk under shift-aware, streaming, and\nlong-horizon settings. Taken together, these directions mark a shift from\nnarrow accuracy toward reliability at scale, enabling systems that not only\nanalyze but also understand, explain, and act on dynamic worlds with traceable\nevidence and credible outcomes."
                },
                "authors": [
                    {
                        "name": "Ching Chang"
                    },
                    {
                        "name": "Yidan Shi"
                    },
                    {
                        "name": "Defu Cao"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Jeehyun Hwang"
                    },
                    {
                        "name": "Haixin Wang"
                    },
                    {
                        "name": "Jiacheng Pang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Wen-Chih Peng"
                    },
                    {
                        "name": "Tien-Fu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tien-Fu Chen"
                },
                "author": "Tien-Fu Chen",
                "arxiv_comment": "This paper is currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11511v3",
                "updated": "2025-11-02T21:46:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    21,
                    46,
                    34,
                    6,
                    306,
                    0
                ],
                "published": "2024-07-16T08:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    49,
                    35,
                    1,
                    198,
                    0
                ],
                "title": "Multi-Step Reasoning with Large Language Models, a Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Step Reasoning with Large Language Models, a Survey"
                },
                "summary": "Large language models (LLMs) with billions of parameters exhibit in-context\nlearning abilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks. The research on LLM\nreasoning abilities started with the question whether LLMs can solve grade\nschool math word problems, and has expanded to other tasks in the past few\nyears. This article reviews the field of multi-step reasoning with LLMs. We\npropose a taxonomy that identifies different ways to generate, evaluate, and\ncontrol multi-step reasoning. We provide an in-depth coverage of core\napproaches and open problems, and we propose a research agenda for the near\nfuture. We find that multi-step reasoning approaches have progressed beyond\nmath word problems, and can now successfully solve challenges in logic,\ncombinatorial games, and robotics, sometimes by first generating code that is\nthen executed by external tools. Many studies in multi-step methods use\nreinforcement learning for finetuning, external optimization loops, in-context\nreinforcement learning, and self-reflection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters exhibit in-context\nlearning abilities, enabling few-shot learning on tasks that the model was not\nspecifically trained for. Traditional models achieve breakthrough performance\non language tasks, but do not perform well on basic reasoning benchmarks.\nHowever, a new in-context learning approach, Chain-of-thought, has demonstrated\nstrong multi-step reasoning abilities on these benchmarks. The research on LLM\nreasoning abilities started with the question whether LLMs can solve grade\nschool math word problems, and has expanded to other tasks in the past few\nyears. This article reviews the field of multi-step reasoning with LLMs. We\npropose a taxonomy that identifies different ways to generate, evaluate, and\ncontrol multi-step reasoning. We provide an in-depth coverage of core\napproaches and open problems, and we propose a research agenda for the near\nfuture. We find that multi-step reasoning approaches have progressed beyond\nmath word problems, and can now successfully solve challenges in logic,\ncombinatorial games, and robotics, sometimes by first generating code that is\nthen executed by external tools. Many studies in multi-step methods use\nreinforcement learning for finetuning, external optimization loops, in-context\nreinforcement learning, and self-reflection."
                },
                "authors": [
                    {
                        "name": "Aske Plaat"
                    },
                    {
                        "name": "Annie Wong"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Joost Broekens"
                    },
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Back"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Back"
                },
                "author": "Thomas Back",
                "arxiv_comment": "ACM Computing Surveys",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16728v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16728v3",
                "updated": "2025-11-02T21:36:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    21,
                    36,
                    2,
                    6,
                    306,
                    0
                ],
                "published": "2025-03-20T22:12:08Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    22,
                    12,
                    8,
                    3,
                    79,
                    0
                ],
                "title": "Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation"
                },
                "summary": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text."
                },
                "authors": [
                    {
                        "name": "Emiel van Miltenburg"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "4 pages + references. Submitted for publication in the Encyclopedia\n  of Language & Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16728v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16728v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18462v2",
                "updated": "2025-11-02T20:11:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    11,
                    6,
                    6,
                    306,
                    0
                ],
                "published": "2024-07-26T02:09:32Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    2,
                    9,
                    32,
                    4,
                    208,
                    0
                ],
                "title": "MistralBSM: Leveraging Mistral-7B for Vehicular Networks Misbehavior\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MistralBSM: Leveraging Mistral-7B for Vehicular Networks Misbehavior\n  Detection"
                },
                "summary": "Malicious attacks on vehicular networks pose a serious threat to road safety\nas well as communication reliability. A major source of these threats stems\nfrom misbehaving vehicles within the network. To address this challenge, we\npropose a Large Language Model (LLM)-empowered Misbehavior Detection System\n(MDS) within an edge-cloud detection framework. Specifically, we fine-tune\nMistral-7B, a compact and high-performing LLM, to detect misbehavior based on\nBasic Safety Messages (BSM) sequences as the edge component for real-time\ndetection, while a larger LLM deployed in the cloud validates and reinforces\nthe edge model's detection through a more comprehensive analysis. By updating\nonly 0.012% of the model parameters, our model, which we named MistralBSM,\nachieves 98% accuracy in binary classification and 96% in multiclass\nclassification on a selected set of attacks from VeReMi dataset, outperforming\nLLAMA2-7B and RoBERTa. Our results validate the potential of LLMs in MDS,\nshowing a significant promise in strengthening vehicular network security to\nbetter ensure the safety of road users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious attacks on vehicular networks pose a serious threat to road safety\nas well as communication reliability. A major source of these threats stems\nfrom misbehaving vehicles within the network. To address this challenge, we\npropose a Large Language Model (LLM)-empowered Misbehavior Detection System\n(MDS) within an edge-cloud detection framework. Specifically, we fine-tune\nMistral-7B, a compact and high-performing LLM, to detect misbehavior based on\nBasic Safety Messages (BSM) sequences as the edge component for real-time\ndetection, while a larger LLM deployed in the cloud validates and reinforces\nthe edge model's detection through a more comprehensive analysis. By updating\nonly 0.012% of the model parameters, our model, which we named MistralBSM,\nachieves 98% accuracy in binary classification and 96% in multiclass\nclassification on a selected set of attacks from VeReMi dataset, outperforming\nLLAMA2-7B and RoBERTa. Our results validate the potential of LLMs in MDS,\nshowing a significant promise in strengthening vehicular network security to\nbetter ensure the safety of road users."
                },
                "authors": [
                    {
                        "name": "Wissal Hamhoum"
                    },
                    {
                        "name": "Soumaya Cherkaoui"
                    }
                ],
                "author_detail": {
                    "name": "Soumaya Cherkaoui"
                },
                "author": "Soumaya Cherkaoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04103v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04103v3",
                "updated": "2025-11-02T19:03:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    19,
                    3,
                    2,
                    6,
                    306,
                    0
                ],
                "published": "2025-07-05T17:12:33Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    17,
                    12,
                    33,
                    5,
                    186,
                    0
                ],
                "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Train Your LLM Web Agent: A Statistical Diagnosis"
                },
                "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models."
                },
                "authors": [
                    {
                        "name": "Dheeraj Vattikonda"
                    },
                    {
                        "name": "Santhoshi Ravichandran"
                    },
                    {
                        "name": "Emiliano Penaloza"
                    },
                    {
                        "name": "Hadi Nekoei"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Thibault Le Sellier de Chezelles"
                    },
                    {
                        "name": "Nicolas Gontier"
                    },
                    {
                        "name": "Miguel Muoz-Mrmol"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Stefania Raimondo"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Laurent Charlin"
                    },
                    {
                        "name": "Alexandre Pich"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Massimo Caccia"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Caccia"
                },
                "author": "Massimo Caccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04103v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04103v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07927v2",
                "updated": "2025-11-02T18:49:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    18,
                    49,
                    28,
                    6,
                    306,
                    0
                ],
                "published": "2025-06-09T16:43:38Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    16,
                    43,
                    38,
                    0,
                    160,
                    0
                ],
                "title": "Solving Inequality Proofs with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Inequality Proofs with Large Language Models"
                },
                "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/."
                },
                "authors": [
                    {
                        "name": "Jiayi Sheng"
                    },
                    {
                        "name": "Luna Lyu"
                    },
                    {
                        "name": "Jikai Jin"
                    },
                    {
                        "name": "Tony Xia"
                    },
                    {
                        "name": "Alex Gu"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Pan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Pan Lu"
                },
                "author": "Pan Lu",
                "arxiv_comment": "50 pages, 24 figures, accepted as a Spotlight at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03304v4",
                "updated": "2025-11-02T18:21:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    18,
                    21,
                    13,
                    6,
                    306,
                    0
                ],
                "published": "2025-02-05T16:03:17Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    3,
                    17,
                    2,
                    36,
                    0
                ],
                "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning"
                },
                "summary": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://github.com/Skilteee/DiZO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://github.com/Skilteee/DiZO."
                },
                "authors": [
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Caiwei Ding"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Jaewoo Lee"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08023v2",
                "updated": "2025-11-02T17:17:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    17,
                    17,
                    51,
                    6,
                    306,
                    0
                ],
                "published": "2025-03-11T04:10:06Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    4,
                    10,
                    6,
                    1,
                    70,
                    0
                ],
                "title": "AdaSCALE: Adaptive Scaling for OOD Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSCALE: Adaptive Scaling for OOD Detection"
                },
                "summary": "The ability of the deep learning model to recognize when a sample falls\noutside its learned distribution is critical for safe and reliable deployment.\nRecent state-of-the-art out-of-distribution (OOD) detection methods leverage\nactivation shaping to improve the separation between in-distribution (ID) and\nOOD inputs. These approaches resort to sample-specific scaling but apply a\nstatic percentile threshold across all samples regardless of their nature,\nresulting in suboptimal ID-OOD separability. In this work, we propose\n\\textbf{AdaSCALE}, an adaptive scaling procedure that dynamically adjusts the\npercentile threshold based on a sample's estimated OOD likelihood. This\nestimation leverages our key observation: OOD samples exhibit significantly\nmore pronounced activation shifts at high-magnitude activations under minor\nperturbation compared to ID samples. AdaSCALE enables stronger scaling for\nlikely ID samples and weaker scaling for likely OOD samples, yielding highly\nseparable energy scores. Our approach achieves state-of-the-art OOD detection\nperformance, outperforming the latest rival OptFS by 14.94% in near-OOD and\n21.67% in far-OOD datasets in average FPR@95 metric on the ImageNet-1k\nbenchmark across eight diverse architectures. The code is available at:\nhttps://github.com/sudarshanregmi/AdaSCALE/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of the deep learning model to recognize when a sample falls\noutside its learned distribution is critical for safe and reliable deployment.\nRecent state-of-the-art out-of-distribution (OOD) detection methods leverage\nactivation shaping to improve the separation between in-distribution (ID) and\nOOD inputs. These approaches resort to sample-specific scaling but apply a\nstatic percentile threshold across all samples regardless of their nature,\nresulting in suboptimal ID-OOD separability. In this work, we propose\n\\textbf{AdaSCALE}, an adaptive scaling procedure that dynamically adjusts the\npercentile threshold based on a sample's estimated OOD likelihood. This\nestimation leverages our key observation: OOD samples exhibit significantly\nmore pronounced activation shifts at high-magnitude activations under minor\nperturbation compared to ID samples. AdaSCALE enables stronger scaling for\nlikely ID samples and weaker scaling for likely OOD samples, yielding highly\nseparable energy scores. Our approach achieves state-of-the-art OOD detection\nperformance, outperforming the latest rival OptFS by 14.94% in near-OOD and\n21.67% in far-OOD datasets in average FPR@95 metric on the ImageNet-1k\nbenchmark across eight diverse architectures. The code is available at:\nhttps://github.com/sudarshanregmi/AdaSCALE/"
                },
                "authors": [
                    {
                        "name": "Sudarshan Regmi"
                    }
                ],
                "author_detail": {
                    "name": "Sudarshan Regmi"
                },
                "author": "Sudarshan Regmi",
                "arxiv_comment": "https://github.com/sudarshanregmi/AdaSCALE/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08619v2",
                "updated": "2025-11-02T17:15:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    17,
                    15,
                    29,
                    6,
                    306,
                    0
                ],
                "published": "2025-07-11T14:19:05Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    19,
                    5,
                    4,
                    192,
                    0
                ],
                "title": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design"
                },
                "summary": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20%). Code compatibility peaked at 100% under\nspecific 2AS settings but averaged below 50% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20%). Code compatibility peaked at 100% under\nspecific 2AS settings but averaged below 50% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted."
                },
                "authors": [
                    {
                        "name": "Soheyl Massoudi"
                    },
                    {
                        "name": "Mark Fuge"
                    }
                ],
                "author_detail": {
                    "name": "Mark Fuge"
                },
                "author": "Mark Fuge",
                "arxiv_doi": "10.1115/DETC2025-168856",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1115/DETC2025-168856",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09658v2",
                "updated": "2025-11-02T16:58:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    16,
                    58,
                    23,
                    6,
                    306,
                    0
                ],
                "published": "2025-03-12T12:17:34Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    17,
                    34,
                    2,
                    71,
                    0
                ],
                "title": "Understanding Endogenous Data Drift in Adaptive Models with\n  Recourse-Seeking Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Endogenous Data Drift in Adaptive Models with\n  Recourse-Seeking Users"
                },
                "summary": "Deep learning models are widely used in decision-making and recommendation\nsystems, where they typically rely on the assumption of a static data\ndistribution between training and deployment. However, real-world deployment\nenvironments often violate this assumption. Users who receive negative outcomes\nmay adapt their features to meet model criteria, i.e., recourse action. These\nadaptive behaviors create shifts in the data distribution and when models are\nretrained on this shifted data, a feedback loop emerges: user behavior\ninfluences the model, and the updated model in turn reshapes future user\nbehavior. Despite its importance, this bidirectional interaction between users\nand models has received limited attention. In this work, we develop a general\nframework to model user strategic behaviors and their interactions with\ndecision-making systems under resource constraints and competitive dynamics.\nBoth the theoretical and empirical analyses show that user recourse behavior\ntends to push logistic and MLP models toward increasingly higher decision\nstandards, resulting in higher recourse costs and less reliable recourse\nactions over time. To mitigate these challenges, we propose two\nmethods--Fair-top-k and Dynamic Continual Learning (DCL)--which significantly\nreduce recourse cost and improve model robustness. Our findings draw\nconnections to economic theories, highlighting how algorithmic decision-making\ncan unintentionally reinforce a higher standard and generate endogenous\nbarriers to entry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models are widely used in decision-making and recommendation\nsystems, where they typically rely on the assumption of a static data\ndistribution between training and deployment. However, real-world deployment\nenvironments often violate this assumption. Users who receive negative outcomes\nmay adapt their features to meet model criteria, i.e., recourse action. These\nadaptive behaviors create shifts in the data distribution and when models are\nretrained on this shifted data, a feedback loop emerges: user behavior\ninfluences the model, and the updated model in turn reshapes future user\nbehavior. Despite its importance, this bidirectional interaction between users\nand models has received limited attention. In this work, we develop a general\nframework to model user strategic behaviors and their interactions with\ndecision-making systems under resource constraints and competitive dynamics.\nBoth the theoretical and empirical analyses show that user recourse behavior\ntends to push logistic and MLP models toward increasingly higher decision\nstandards, resulting in higher recourse costs and less reliable recourse\nactions over time. To mitigate these challenges, we propose two\nmethods--Fair-top-k and Dynamic Continual Learning (DCL)--which significantly\nreduce recourse cost and improve model robustness. Our findings draw\nconnections to economic theories, highlighting how algorithmic decision-making\ncan unintentionally reinforce a higher standard and generate endogenous\nbarriers to entry."
                },
                "authors": [
                    {
                        "name": "Bo-Yi Liu"
                    },
                    {
                        "name": "Zhi-Xuan Liu"
                    },
                    {
                        "name": "Kuan Lun Chen"
                    },
                    {
                        "name": "Shih-Yu Tsai"
                    },
                    {
                        "name": "Jie Gao"
                    },
                    {
                        "name": "Hao-Tsung Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao-Tsung Yang"
                },
                "author": "Hao-Tsung Yang",
                "arxiv_doi": "10.1609/aies.v8i2.36659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1609/aies.v8i2.36659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages,4 figures, 3 tables",
                "arxiv_journal_ref": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society\n  8, no. 2 (October 15, 2025): 1598-1610",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20513v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20513v7",
                "updated": "2025-11-02T16:56:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    16,
                    56,
                    25,
                    6,
                    306,
                    0
                ],
                "published": "2024-10-27T16:52:21Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    16,
                    52,
                    21,
                    6,
                    301,
                    0
                ],
                "title": "Self-correction is Not An Innate Capability in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-correction is Not An Innate Capability in Large Language Models"
                },
                "summary": "Although there has been growing interest in the self-correction capability of\nLarge Language Models (LLMs), there are varying conclusions about its\neffectiveness. Prior research has largely concentrated on intrinsic\nself-correction, extrinsic self-correction, particularly the interplay between\ninternal knowledge and external feedback, remains underexplored. In this paper,\nwe aim to comprehensively investigate the underlying mechanism of moral\nself-correction by addressing a fundamental question: is moral self-correction\nan innate capability of LLMs? Specifically, we conduct: (1) a behavioral\nanalysis of LLMs' moral sensitivity based on a self-distinguishing task; and\n(2) a mechanistic analysis of the hidden states to examine how key components\nof self-correction, such as Chain-of-Thought (CoT) and external feedback,\ninteract to facilitate moral self-correction. Drawing on empirical evidence\nfrom both behavioral and mechanistic analyses, we demonstrate that moral\nself-correction is not an inherent capability of LLMs, as they are neither\nmorally sensitive nor able to effectively incorporate external feedback during\nthe self-correction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although there has been growing interest in the self-correction capability of\nLarge Language Models (LLMs), there are varying conclusions about its\neffectiveness. Prior research has largely concentrated on intrinsic\nself-correction, extrinsic self-correction, particularly the interplay between\ninternal knowledge and external feedback, remains underexplored. In this paper,\nwe aim to comprehensively investigate the underlying mechanism of moral\nself-correction by addressing a fundamental question: is moral self-correction\nan innate capability of LLMs? Specifically, we conduct: (1) a behavioral\nanalysis of LLMs' moral sensitivity based on a self-distinguishing task; and\n(2) a mechanistic analysis of the hidden states to examine how key components\nof self-correction, such as Chain-of-Thought (CoT) and external feedback,\ninteract to facilitate moral self-correction. Drawing on empirical evidence\nfrom both behavioral and mechanistic analyses, we demonstrate that moral\nself-correction is not an inherent capability of LLMs, as they are neither\nmorally sensitive nor able to effectively incorporate external feedback during\nthe self-correction process."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Zimo Qi"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20513v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20513v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00985v2",
                "updated": "2025-11-02T16:52:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    16,
                    52,
                    20,
                    6,
                    306,
                    0
                ],
                "published": "2025-07-01T17:36:41Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    36,
                    41,
                    1,
                    182,
                    0
                ],
                "title": "Discourse Heuristics For Paradoxically Moral Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discourse Heuristics For Paradoxically Moral Self-Correction"
                },
                "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Zimo Qi"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10142v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10142v3",
                "updated": "2025-11-02T16:45:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    16,
                    45,
                    45,
                    6,
                    306,
                    0
                ],
                "published": "2025-10-11T09:48:31Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    48,
                    31,
                    5,
                    284,
                    0
                ],
                "title": "Debiasing LLMs by Masking Unfairness-Driving Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing LLMs by Masking Unfairness-Driving Attention Heads"
                },
                "summary": "Large language models (LLMs) increasingly mediate decisions in domains where\nunfair treatment of demographic groups is unacceptable. Existing work probes\nwhen biased outputs appear, but gives little insight into the mechanisms that\ngenerate them, leaving existing mitigations largely fragile. In this paper, we\nconduct a systematic investigation LLM unfairness and propose DiffHeads, a\nlightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)\nprompting to Chain-of-Thought (CoT) prompting across eight representative open-\nand closed-source LLMs. DA will trigger the nature bias part of LLM and improve\nmeasured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.\nNext, we define a token-to-head contribution score that traces each token's\ninfluence back to individual attention heads. This reveals a small cluster of\nbias heads that activate under DA but stay largely dormant with CoT, providing\nthe first causal link between prompting strategy and bias emergence. Finally,\nbuilding on this insight, we propose DiffHeads that identifies bias heads\nthrough differential activation analysis between DA and CoT, and selectively\nmasks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under\nDA and CoT, respectively, without harming model utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly mediate decisions in domains where\nunfair treatment of demographic groups is unacceptable. Existing work probes\nwhen biased outputs appear, but gives little insight into the mechanisms that\ngenerate them, leaving existing mitigations largely fragile. In this paper, we\nconduct a systematic investigation LLM unfairness and propose DiffHeads, a\nlightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)\nprompting to Chain-of-Thought (CoT) prompting across eight representative open-\nand closed-source LLMs. DA will trigger the nature bias part of LLM and improve\nmeasured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.\nNext, we define a token-to-head contribution score that traces each token's\ninfluence back to individual attention heads. This reveals a small cluster of\nbias heads that activate under DA but stay largely dormant with CoT, providing\nthe first causal link between prompting strategy and bias emergence. Finally,\nbuilding on this insight, we propose DiffHeads that identifies bias heads\nthrough differential activation analysis between DA and CoT, and selectively\nmasks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under\nDA and CoT, respectively, without harming model utility."
                },
                "authors": [
                    {
                        "name": "Tingxu Han"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Ziqi Ding"
                    },
                    {
                        "name": "Ziming Li"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Yuekang Li"
                    },
                    {
                        "name": "Dongfang Liu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenting Wang"
                },
                "author": "Zhenting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10142v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10142v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18773v2",
                "updated": "2025-11-02T16:45:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    16,
                    45,
                    19,
                    6,
                    306,
                    0
                ],
                "published": "2025-05-24T16:23:43Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    16,
                    23,
                    43,
                    5,
                    144,
                    0
                ],
                "title": "Exploring the limits of strong membership inference attacks on large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the limits of strong membership inference attacks on large\n  language models"
                },
                "summary": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training references (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small models and\ndatasets. However, weaker attacks have been shown to be brittle and insights\nfrom strong attacks in simplified settings do not translate to today's LLMs.\nThese challenges prompt an important question: are the limitations observed in\nprior work due to attack design choices, or are MIAs fundamentally ineffective\non LLMs? We address this question by scaling LiRA--one of the strongest\nMIAs--to GPT-2 architectures ranging from 10M to 1B parameters, training\nreferences on over 20B tokens from the C4 dataset. Our results advance the\nunderstanding of MIAs on LLMs in four key ways. While (1) strong MIAs can\nsucceed on pre-trained LLMs, (2) their effectiveness, remains limited (e.g.,\nAUC<0.7) in practical settings. (3) Even when strong MIAs achieve\nbetter-than-random AUC, aggregate metrics can conceal substantial per-sample\nMIA decision instability: due to training randomness, many decisions are so\nunstable that they are statistically indistinguishable from a coin flip.\nFinally, (4) the relationship between MIA success and related LLM privacy\nmetrics is not as straightforward as prior work has suggested.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training references (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small models and\ndatasets. However, weaker attacks have been shown to be brittle and insights\nfrom strong attacks in simplified settings do not translate to today's LLMs.\nThese challenges prompt an important question: are the limitations observed in\nprior work due to attack design choices, or are MIAs fundamentally ineffective\non LLMs? We address this question by scaling LiRA--one of the strongest\nMIAs--to GPT-2 architectures ranging from 10M to 1B parameters, training\nreferences on over 20B tokens from the C4 dataset. Our results advance the\nunderstanding of MIAs on LLMs in four key ways. While (1) strong MIAs can\nsucceed on pre-trained LLMs, (2) their effectiveness, remains limited (e.g.,\nAUC<0.7) in practical settings. (3) Even when strong MIAs achieve\nbetter-than-random AUC, aggregate metrics can conceal substantial per-sample\nMIA decision instability: due to training randomness, many decisions are so\nunstable that they are statistically indistinguishable from a coin flip.\nFinally, (4) the relationship between MIA success and related LLM privacy\nmetrics is not as straightforward as prior work has suggested."
                },
                "authors": [
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "George Kaissis"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Sahra Ghalebikesabi"
                    },
                    {
                        "name": "Meenatchi Sundaram Mutu Selva Annamalai"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    },
                    {
                        "name": "Katherine Lee"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    },
                    {
                        "name": "A. Feder Cooper"
                    }
                ],
                "author_detail": {
                    "name": "A. Feder Cooper"
                },
                "author": "A. Feder Cooper",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16101v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16101v4",
                "updated": "2025-11-02T16:43:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    16,
                    43,
                    52,
                    6,
                    306,
                    0
                ],
                "published": "2025-02-22T05:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    5,
                    50,
                    15,
                    5,
                    53,
                    0
                ],
                "title": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the\n  Robustness of RAG Against Misleading Retrievals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the\n  Robustness of RAG Against Misleading Retrievals"
                },
                "summary": "Retrieval-augmented generation (RAG) has shown impressive capabilities in\nmitigating hallucinations in large language models (LLMs). However, LLMs\nstruggle to maintain consistent reasoning when exposed to misleading or\nconflicting evidence, especially in real-world domains such as politics, where\ninformation is polarized or selectively framed. Mainstream RAG benchmarks\nevaluate models under clean retrieval settings, where systems generate answers\nfrom gold-standard documents, or under synthetically perturbed settings, where\ndocuments are artificially injected with noise. These assumptions fail to\nreflect real-world conditions, often leading to an overestimation of RAG system\nperformance. To address this gap, we introduce RAGuard, the first benchmark to\nevaluate the robustness of RAG systems against misleading retrievals. Unlike\nprior benchmarks that rely on synthetic noise, our fact-checking dataset\ncaptures naturally occurring misinformation by constructing its retrieval\ncorpus from Reddit discussions. It categorizes retrieved evidence into three\ntypes: supporting, misleading, and unrelated, providing a realistic and\nchallenging testbed for assessing how well RAG systems navigate different types\nof evidence. Our experiments reveal that, when exposed to potentially\nmisleading retrievals, all tested LLM-powered RAG systems perform worse than\ntheir zero-shot baselines (i.e., no retrieval at all), while human annotators\nconsistently perform better, highlighting LLMs' susceptibility to noisy\nenvironments. To our knowledge, RAGuard is the first benchmark to\nsystematically assess the robustness of the RAG against misleading evidence. We\nexpect this benchmark to drive future research toward improving RAG systems\nbeyond idealized datasets, making them more reliable for real-world\napplications. The dataset is available at\nhttps://huggingface.co/datasets/UCSC-IRKM/RAGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has shown impressive capabilities in\nmitigating hallucinations in large language models (LLMs). However, LLMs\nstruggle to maintain consistent reasoning when exposed to misleading or\nconflicting evidence, especially in real-world domains such as politics, where\ninformation is polarized or selectively framed. Mainstream RAG benchmarks\nevaluate models under clean retrieval settings, where systems generate answers\nfrom gold-standard documents, or under synthetically perturbed settings, where\ndocuments are artificially injected with noise. These assumptions fail to\nreflect real-world conditions, often leading to an overestimation of RAG system\nperformance. To address this gap, we introduce RAGuard, the first benchmark to\nevaluate the robustness of RAG systems against misleading retrievals. Unlike\nprior benchmarks that rely on synthetic noise, our fact-checking dataset\ncaptures naturally occurring misinformation by constructing its retrieval\ncorpus from Reddit discussions. It categorizes retrieved evidence into three\ntypes: supporting, misleading, and unrelated, providing a realistic and\nchallenging testbed for assessing how well RAG systems navigate different types\nof evidence. Our experiments reveal that, when exposed to potentially\nmisleading retrievals, all tested LLM-powered RAG systems perform worse than\ntheir zero-shot baselines (i.e., no retrieval at all), while human annotators\nconsistently perform better, highlighting LLMs' susceptibility to noisy\nenvironments. To our knowledge, RAGuard is the first benchmark to\nsystematically assess the robustness of the RAG against misleading evidence. We\nexpect this benchmark to drive future research toward improving RAG systems\nbeyond idealized datasets, making them more reliable for real-world\napplications. The dataset is available at\nhttps://huggingface.co/datasets/UCSC-IRKM/RAGuard."
                },
                "authors": [
                    {
                        "name": "Linda Zeng"
                    },
                    {
                        "name": "Rithwik Gupta"
                    },
                    {
                        "name": "Divij Motwani"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Diji Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diji Yang"
                },
                "author": "Diji Yang",
                "arxiv_comment": "Advances in Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16101v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16101v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04141v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04141v4",
                "updated": "2025-11-02T16:19:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    16,
                    19,
                    12,
                    6,
                    306,
                    0
                ],
                "published": "2025-04-05T11:23:05Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    11,
                    23,
                    5,
                    5,
                    95,
                    0
                ],
                "title": "Self-Adaptive Cognitive Debiasing for Large Language Models in\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Adaptive Cognitive Debiasing for Large Language Models in\n  Decision-Making"
                },
                "summary": "Large language models (LLMs) have shown potential in supporting\ndecision-making applications, particularly as personal assistants in the\nfinancial, healthcare, and legal domains. While prompt engineering strategies\nhave enhanced the capabilities of LLMs in decision-making, cognitive biases\ninherent to LLMs present significant challenges. Cognitive biases are\nsystematic patterns of deviation from norms or rationality in decision-making\nthat can lead to the production of inaccurate outputs. Existing cognitive bias\nmitigation strategies assume that input prompts only contain one type of\ncognitive bias, limiting their effectiveness in more challenging scenarios\ninvolving multiple cognitive biases. To fill this gap, we propose a cognitive\ndebiasing approach, self-adaptive cognitive debiasing (SACD), that enhances the\nreliability of LLMs by iteratively refining prompts. Our method follows three\nsequential steps - bias determination, bias analysis, and cognitive debiasing -\nto iteratively mitigate potential cognitive biases in prompts. We evaluate SACD\non finance, healthcare, and legal decision-making tasks using both open-weight\nand closed-weight LLMs. Compared to advanced prompt engineering methods and\nexisting cognitive debiasing techniques, SACD achieves the lowest average bias\nscores in both single-bias and multi-bias settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown potential in supporting\ndecision-making applications, particularly as personal assistants in the\nfinancial, healthcare, and legal domains. While prompt engineering strategies\nhave enhanced the capabilities of LLMs in decision-making, cognitive biases\ninherent to LLMs present significant challenges. Cognitive biases are\nsystematic patterns of deviation from norms or rationality in decision-making\nthat can lead to the production of inaccurate outputs. Existing cognitive bias\nmitigation strategies assume that input prompts only contain one type of\ncognitive bias, limiting their effectiveness in more challenging scenarios\ninvolving multiple cognitive biases. To fill this gap, we propose a cognitive\ndebiasing approach, self-adaptive cognitive debiasing (SACD), that enhances the\nreliability of LLMs by iteratively refining prompts. Our method follows three\nsequential steps - bias determination, bias analysis, and cognitive debiasing -\nto iteratively mitigate potential cognitive biases in prompts. We evaluate SACD\non finance, healthcare, and legal decision-making tasks using both open-weight\nand closed-weight LLMs. Compared to advanced prompt engineering methods and\nexisting cognitive debiasing techniques, SACD achieves the lowest average bias\nscores in both single-bias and multi-bias settings."
                },
                "authors": [
                    {
                        "name": "Yougang Lyu"
                    },
                    {
                        "name": "Shijie Ren"
                    },
                    {
                        "name": "Yue Feng"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Maarten de Rijke"
                    }
                ],
                "author_detail": {
                    "name": "Maarten de Rijke"
                },
                "author": "Maarten de Rijke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04141v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04141v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18428v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18428v4",
                "updated": "2025-11-02T16:05:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    16,
                    5,
                    48,
                    6,
                    306,
                    0
                ],
                "published": "2025-04-25T15:39:04Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    39,
                    4,
                    4,
                    115,
                    0
                ],
                "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts"
                },
                "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and\nGemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%\naccuracy under the highest level From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and\nGemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%\naccuracy under the highest level From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Jialong Tang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Chenshu Sun"
                    },
                    {
                        "name": "Feitong Sun"
                    },
                    {
                        "name": "Jiran Zhang"
                    },
                    {
                        "name": "Junxuan Wu"
                    },
                    {
                        "name": "Qiqian Cang"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18428v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18428v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11925v2",
                "updated": "2025-11-02T15:47:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    15,
                    47,
                    22,
                    6,
                    306,
                    0
                ],
                "published": "2025-08-16T06:11:29Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    6,
                    11,
                    29,
                    5,
                    228,
                    0
                ],
                "title": "Optimizing Token Choice for Code Watermarking: An RL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Token Choice for Code Watermarking: An RL Approach"
                },
                "summary": "Protecting intellectual property on LLM-generated code necessitates effective\nwatermarking systems that can operate within code's highly structured,\nsyntactically constrained nature. In this work, we introduce CodeTracer, an\ninnovative adaptive code watermarking framework underpinned by a novel\nreinforcement learning training paradigm. At its core, CodeTracer features a\npolicy-driven approach that utilizes a parameterized model to intelligently\nbias token choices during next-token prediction. This strategy ensures that\nembedded watermarks maintain code functionality while exhibiting subtle yet\nstatistically detectable deviations from typical token distributions. To\nfacilitate policy learning, we devise a comprehensive reward system that\nseamlessly integrates execution feedback with watermark embedding signals,\nbalancing process-level and outcome-level rewards. Additionally, we employ\nGumbel Top-k reparameterization to enable gradient-based optimization of\ndiscrete watermarking decisions. Extensive comparative evaluations demonstrate\nCodeTracer's significant superiority over state-of-the-art baselines in both\nwatermark detectability and the preservation of generated code's functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting intellectual property on LLM-generated code necessitates effective\nwatermarking systems that can operate within code's highly structured,\nsyntactically constrained nature. In this work, we introduce CodeTracer, an\ninnovative adaptive code watermarking framework underpinned by a novel\nreinforcement learning training paradigm. At its core, CodeTracer features a\npolicy-driven approach that utilizes a parameterized model to intelligently\nbias token choices during next-token prediction. This strategy ensures that\nembedded watermarks maintain code functionality while exhibiting subtle yet\nstatistically detectable deviations from typical token distributions. To\nfacilitate policy learning, we devise a comprehensive reward system that\nseamlessly integrates execution feedback with watermark embedding signals,\nbalancing process-level and outcome-level rewards. Additionally, we employ\nGumbel Top-k reparameterization to enable gradient-based optimization of\ndiscrete watermarking decisions. Extensive comparative evaluations demonstrate\nCodeTracer's significant superiority over state-of-the-art baselines in both\nwatermark detectability and the preservation of generated code's functionality."
                },
                "authors": [
                    {
                        "name": "Zhimeng Guo"
                    },
                    {
                        "name": "Huaisheng Zhu"
                    },
                    {
                        "name": "Siyuan Xu"
                    },
                    {
                        "name": "Hangfan Zhang"
                    },
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Minhao Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minhao Cheng"
                },
                "author": "Minhao Cheng",
                "arxiv_comment": "18 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18253v2",
                "updated": "2025-11-02T15:37:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    15,
                    37,
                    19,
                    6,
                    306,
                    0
                ],
                "published": "2025-08-25T17:41:46Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    46,
                    0,
                    237,
                    0
                ],
                "title": "From BERT to LLMs: Comparing and Understanding Chinese Classifier\n  Prediction in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From BERT to LLMs: Comparing and Understanding Chinese Classifier\n  Prediction in Language Models"
                },
                "summary": "Classifiers are an important and defining feature of the Chinese language,\nand their correct prediction is key to numerous educational applications. Yet,\nwhether the most popular Large Language Models (LLMs) possess proper knowledge\nthe Chinese classifiers is an issue that has largely remain unexplored in the\nNatural Language Processing (NLP) literature.\n  To address such a question, we employ various masking strategies to evaluate\nthe LLMs' intrinsic ability, the contribution of different sentence elements,\nand the working of the attention mechanisms during prediction. Besides, we\nexplore fine-tuning for LLMs to enhance the classifier performance.\n  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning.\nThe prediction, as expected, greatly benefits from the information about the\nfollowing noun, which also explains the advantage of models with a\nbidirectional attention mechanism such as BERT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classifiers are an important and defining feature of the Chinese language,\nand their correct prediction is key to numerous educational applications. Yet,\nwhether the most popular Large Language Models (LLMs) possess proper knowledge\nthe Chinese classifiers is an issue that has largely remain unexplored in the\nNatural Language Processing (NLP) literature.\n  To address such a question, we employ various masking strategies to evaluate\nthe LLMs' intrinsic ability, the contribution of different sentence elements,\nand the working of the attention mechanisms during prediction. Besides, we\nexplore fine-tuning for LLMs to enhance the classifier performance.\n  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning.\nThe prediction, as expected, greatly benefits from the information about the\nfollowing noun, which also explains the advantage of models with a\nbidirectional attention mechanism such as BERT."
                },
                "authors": [
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Jianfei Ma"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    },
                    {
                        "name": "Jieshun You"
                    },
                    {
                        "name": "Zhaoxin Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxin Feng"
                },
                "author": "Zhaoxin Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07769v2",
                "updated": "2025-11-02T14:58:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    14,
                    58,
                    6,
                    6,
                    306,
                    0
                ],
                "published": "2024-12-10T18:59:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities"
                },
                "summary": "We introduce BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large\nMultimodal Model that supports text-based and image-based medical interactions.\nIt enables multi-turn conversation in Arabic and English and supports diverse\nmedical imaging modalities, including radiology, CT, and histology. To train\nBiMediX2, we curate BiMed-V, an extensive Arabic-English bilingual healthcare\ndataset consisting of 1.6M samples of diverse medical interactions. This\ndataset supports a range of medical Large Language Model (LLM) and Large\nMultimodal Model (LMM) tasks, including multi-turn medical conversations,\nreport generation, and visual question answering (VQA). We also introduce\nBiMed-MBench, the first Arabic-English medical LMM evaluation benchmark,\nverified by medical experts. BiMediX2 demonstrates excellent performance across\nmultiple medical LLM and LMM benchmarks, achieving state-of-the-art results\ncompared to other open-sourced models. On BiMed-MBench, BiMediX2 outperforms\nexisting methods by over 9% in English and more than 20% in Arabic evaluations.\nAdditionally, it surpasses GPT-4 by approximately 9% in UPHILL factual accuracy\nevaluations and excels in various medical VQA, report generation, and report\nsummarization tasks. Our trained models, instruction set, and source code are\navailable at https://github.com/mbzuai-oryx/BiMediX2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large\nMultimodal Model that supports text-based and image-based medical interactions.\nIt enables multi-turn conversation in Arabic and English and supports diverse\nmedical imaging modalities, including radiology, CT, and histology. To train\nBiMediX2, we curate BiMed-V, an extensive Arabic-English bilingual healthcare\ndataset consisting of 1.6M samples of diverse medical interactions. This\ndataset supports a range of medical Large Language Model (LLM) and Large\nMultimodal Model (LMM) tasks, including multi-turn medical conversations,\nreport generation, and visual question answering (VQA). We also introduce\nBiMed-MBench, the first Arabic-English medical LMM evaluation benchmark,\nverified by medical experts. BiMediX2 demonstrates excellent performance across\nmultiple medical LLM and LMM benchmarks, achieving state-of-the-art results\ncompared to other open-sourced models. On BiMed-MBench, BiMediX2 outperforms\nexisting methods by over 9% in English and more than 20% in Arabic evaluations.\nAdditionally, it surpasses GPT-4 by approximately 9% in UPHILL factual accuracy\nevaluations and excels in various medical VQA, report generation, and report\nsummarization tasks. Our trained models, instruction set, and source code are\navailable at https://github.com/mbzuai-oryx/BiMediX2"
                },
                "authors": [
                    {
                        "name": "Sahal Shaji Mullappilly"
                    },
                    {
                        "name": "Mohammed Irfan Kurpath"
                    },
                    {
                        "name": "Sara Pieri"
                    },
                    {
                        "name": "Saeed Yahya Alseiari"
                    },
                    {
                        "name": "Shanavas Cholakkal"
                    },
                    {
                        "name": "Khaled Aldahmani"
                    },
                    {
                        "name": "Fahad Khan"
                    },
                    {
                        "name": "Rao Anwer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "arxiv_comment": "Accepted to EMNLP 2025 (Findings)",
                "arxiv_journal_ref": "Findings of the Association for Computational Linguistics: EMNLP\n  2025, pages 14051-14071",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12260v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12260v3",
                "updated": "2025-11-02T11:36:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    11,
                    36,
                    40,
                    6,
                    306,
                    0
                ],
                "published": "2025-08-17T06:55:29Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    6,
                    55,
                    29,
                    6,
                    229,
                    0
                ],
                "title": "Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting"
                },
                "summary": "Infectious disease forecasting in novel outbreaks or low-resource settings is\nhampered by the need for disease-specific data, bespoke training, and expert\ntuning. We introduce Mantis, a foundation model trained entirely on mechanistic\nsimulations, which enables out-of-the-box forecasting across diseases, regions,\nand outcomes, even in settings with limited historical data. We evaluated\nMantis against 48 forecasting models across six diseases with diverse\ntransmission modes, assessing both point forecast accuracy (mean absolute\nerror) and probabilistic performance (weighted interval score and coverage).\nDespite using no real-world data during training, Mantis achieved lower mean\nabsolute error than all models in the CDC's COVID-19 Forecast Hub when\nbacktested on early pandemic forecasts. Across all other diseases tested,\nincluding respiratory, vector-borne, and waterborne pathogens, Mantis\nconsistently ranked in the top two models across all evaluation metrics.\nNotably, Mantis generalized to diseases with transmission mechanisms not\nrepresented in its training data, demonstrating that it captures fundamental\ncontagion dynamics rather than memorizing disease-specific patterns. These\ncapabilities position Mantis as a practical foundation for disease forecasting:\ngeneral-purpose, accurate, and deployable where traditional models fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infectious disease forecasting in novel outbreaks or low-resource settings is\nhampered by the need for disease-specific data, bespoke training, and expert\ntuning. We introduce Mantis, a foundation model trained entirely on mechanistic\nsimulations, which enables out-of-the-box forecasting across diseases, regions,\nand outcomes, even in settings with limited historical data. We evaluated\nMantis against 48 forecasting models across six diseases with diverse\ntransmission modes, assessing both point forecast accuracy (mean absolute\nerror) and probabilistic performance (weighted interval score and coverage).\nDespite using no real-world data during training, Mantis achieved lower mean\nabsolute error than all models in the CDC's COVID-19 Forecast Hub when\nbacktested on early pandemic forecasts. Across all other diseases tested,\nincluding respiratory, vector-borne, and waterborne pathogens, Mantis\nconsistently ranked in the top two models across all evaluation metrics.\nNotably, Mantis generalized to diseases with transmission mechanisms not\nrepresented in its training data, demonstrating that it captures fundamental\ncontagion dynamics rather than memorizing disease-specific patterns. These\ncapabilities position Mantis as a practical foundation for disease forecasting:\ngeneral-purpose, accurate, and deployable where traditional models fail."
                },
                "authors": [
                    {
                        "name": "Carson Dudley"
                    },
                    {
                        "name": "Reiden Magdaleno"
                    },
                    {
                        "name": "Christopher Harding"
                    },
                    {
                        "name": "Ananya Sharma"
                    },
                    {
                        "name": "Emily Martin"
                    },
                    {
                        "name": "Marisa Eisenberg"
                    }
                ],
                "author_detail": {
                    "name": "Marisa Eisenberg"
                },
                "author": "Marisa Eisenberg",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12260v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12260v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11090v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11090v3",
                "updated": "2025-11-02T11:34:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    11,
                    34,
                    53,
                    6,
                    306,
                    0
                ],
                "published": "2025-02-16T12:08:08Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    8,
                    8,
                    6,
                    47,
                    0
                ],
                "title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models\n  in Multi-Turn Dialogues with Diverse Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models\n  in Multi-Turn Dialogues with Diverse Jailbreak Attacks"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), the safety of\nLLMs has been a critical concern requiring precise assessment. Current\nbenchmarks primarily concentrate on single-turn dialogues or a single jailbreak\nattack method to assess the safety. Additionally, these benchmarks have not\ntaken into account the LLM's capability of identifying and handling unsafe\ninformation in detail. To address these issues, we propose a fine-grained\nbenchmark SafeDialBench for evaluating the safety of LLMs across various\njailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier\nhierarchical safety taxonomy that considers 6 safety dimensions and generates\nmore than 4000 multi-turn dialogues in both Chinese and English under 22\ndialogue scenarios. We employ 7 jailbreak attack strategies, such as reference\nattack and purpose reverse, to enhance the dataset quality for dialogue\ngeneration. Notably, we construct an innovative assessment framework of LLMs,\nmeasuring capabilities in detecting, and handling unsafe information and\nmaintaining consistency when facing jailbreak attacks. Experimental results\nacross 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior\nsafety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety\nvulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), the safety of\nLLMs has been a critical concern requiring precise assessment. Current\nbenchmarks primarily concentrate on single-turn dialogues or a single jailbreak\nattack method to assess the safety. Additionally, these benchmarks have not\ntaken into account the LLM's capability of identifying and handling unsafe\ninformation in detail. To address these issues, we propose a fine-grained\nbenchmark SafeDialBench for evaluating the safety of LLMs across various\njailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier\nhierarchical safety taxonomy that considers 6 safety dimensions and generates\nmore than 4000 multi-turn dialogues in both Chinese and English under 22\ndialogue scenarios. We employ 7 jailbreak attack strategies, such as reference\nattack and purpose reverse, to enhance the dataset quality for dialogue\ngeneration. Notably, we construct an innovative assessment framework of LLMs,\nmeasuring capabilities in detecting, and handling unsafe information and\nmaintaining consistency when facing jailbreak attacks. Experimental results\nacross 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior\nsafety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety\nvulnerabilities."
                },
                "authors": [
                    {
                        "name": "Hongye Cao"
                    },
                    {
                        "name": "Yanming Wang"
                    },
                    {
                        "name": "Sijia Jing"
                    },
                    {
                        "name": "Ziyue Peng"
                    },
                    {
                        "name": "Zhixin Bai"
                    },
                    {
                        "name": "Zhe Cao"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Fan Feng"
                    },
                    {
                        "name": "Boyan Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Tianpei Yang"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Fanyu Meng"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Junlan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Junlan Feng"
                },
                "author": "Junlan Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11090v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11090v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22603v2",
                "updated": "2025-11-02T11:33:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    11,
                    33,
                    56,
                    6,
                    306,
                    0
                ],
                "published": "2025-10-26T09:44:20Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    9,
                    44,
                    20,
                    6,
                    299,
                    0
                ],
                "title": "Mitigating Attention Sinks and Massive Activations in Audio-Visual\n  Speech Recognition with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Attention Sinks and Massive Activations in Audio-Visual\n  Speech Recognition with LLMs"
                },
                "summary": "Large language models (LLMs) have recently advanced auditory speech\nrecognition (ASR), visual speech recognition (VSR), and audio-visual speech\nrecognition (AVSR). However, understanding of their internal dynamics under\nfine-tuning remains limited. In natural language processing, recent work has\nrevealed attention sinks, tokens that attract disproportionately high\nattention, and associated massive activations in which some features of sink\ntokens exhibit huge activation in LLMs. In this work, we are the first to study\nthese phenomena in multimodal speech recognition. Through a detailed analysis\nof audio-visual LLMs, we identify attention sinks and massive activations not\nonly at the BOS token but also at intermediate low-semantic tokens across ASR,\nVSR, and AVSR. We show that massive activations originate in the MLP layers and\ncorrespond to fixed feature indices across all sink tokens. We further show\nthat intermediate sink tokens exhibit high cosine similarity to the BOS token,\nthereby amplifying attention and activation. Building on these insights, we\nintroduce a simple decorrelation loss that reduces cosine similarity between\nBOS and other tokens, effectively mitigating intermediate sinks and massive\nactivations. Furthermore, our method improves word error rate (WER) under high\naudio-visual feature downsampling while remaining stable at lower downsampling\nrates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently advanced auditory speech\nrecognition (ASR), visual speech recognition (VSR), and audio-visual speech\nrecognition (AVSR). However, understanding of their internal dynamics under\nfine-tuning remains limited. In natural language processing, recent work has\nrevealed attention sinks, tokens that attract disproportionately high\nattention, and associated massive activations in which some features of sink\ntokens exhibit huge activation in LLMs. In this work, we are the first to study\nthese phenomena in multimodal speech recognition. Through a detailed analysis\nof audio-visual LLMs, we identify attention sinks and massive activations not\nonly at the BOS token but also at intermediate low-semantic tokens across ASR,\nVSR, and AVSR. We show that massive activations originate in the MLP layers and\ncorrespond to fixed feature indices across all sink tokens. We further show\nthat intermediate sink tokens exhibit high cosine similarity to the BOS token,\nthereby amplifying attention and activation. Building on these insights, we\nintroduce a simple decorrelation loss that reduces cosine similarity between\nBOS and other tokens, effectively mitigating intermediate sinks and massive\nactivations. Furthermore, our method improves word error rate (WER) under high\naudio-visual feature downsampling while remaining stable at lower downsampling\nrates."
                },
                "authors": [
                    {
                        "name": "Anand"
                    },
                    {
                        "name": "Umberto Cappellazzo"
                    },
                    {
                        "name": "Stavros Petridis"
                    },
                    {
                        "name": "Maja Pantic"
                    }
                ],
                "author_detail": {
                    "name": "Maja Pantic"
                },
                "author": "Maja Pantic",
                "arxiv_comment": "The code is available at\n  https://github.com/umbertocappellazzo/Llama-AVSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12205v2",
                "updated": "2025-11-02T11:32:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    11,
                    32,
                    1,
                    6,
                    306,
                    0
                ],
                "published": "2025-03-15T17:09:04Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    17,
                    9,
                    4,
                    5,
                    74,
                    0
                ],
                "title": "PredicateFix: Repairing Static Analysis Alerts with Bridging Predicates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PredicateFix: Repairing Static Analysis Alerts with Bridging Predicates"
                },
                "summary": "Fixing static analysis alerts in source code with Large Language Models\n(LLMs) is becoming increasingly popular. However, LLMs often hallucinate and\nperform poorly for complex and less common alerts. Retrieval-augmented\ngeneration (RAG) aims to solve this problem by providing the model with a\nrelevant example, but existing approaches face the challenge of unsatisfactory\nquality of such examples.\n  To address this challenge, we utilize the predicates in the analysis rule,\nwhich serve as a bridge between the alert and relevant code snippets within a\nclean code corpus, called key examples. Based on this insight, we propose an\nalgorithm to retrieve key examples for an alert automatically, and build\nPredicateFix as a RAG pipeline to fix alerts from two static code analyzers:\nCodeQL and GoInsight. Evaluation with multiple LLMs shows that PredicateFix\nincreases the number of correct repairs by 27.1% ~ 69.3%, significantly\noutperforming other baseline RAG approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixing static analysis alerts in source code with Large Language Models\n(LLMs) is becoming increasingly popular. However, LLMs often hallucinate and\nperform poorly for complex and less common alerts. Retrieval-augmented\ngeneration (RAG) aims to solve this problem by providing the model with a\nrelevant example, but existing approaches face the challenge of unsatisfactory\nquality of such examples.\n  To address this challenge, we utilize the predicates in the analysis rule,\nwhich serve as a bridge between the alert and relevant code snippets within a\nclean code corpus, called key examples. Based on this insight, we propose an\nalgorithm to retrieve key examples for an alert automatically, and build\nPredicateFix as a RAG pipeline to fix alerts from two static code analyzers:\nCodeQL and GoInsight. Evaluation with multiple LLMs shows that PredicateFix\nincreases the number of correct repairs by 27.1% ~ 69.3%, significantly\noutperforming other baseline RAG approaches."
                },
                "authors": [
                    {
                        "name": "Yuan-An Xiao"
                    },
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Junwei Zhou"
                    },
                    {
                        "name": "Shengyu Cheng"
                    },
                    {
                        "name": "Yingfei Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yingfei Xiong"
                },
                "author": "Yingfei Xiong",
                "arxiv_doi": "10.1145/3744916.3773159",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3773159",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.12205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 5 figures; accepted for ICSE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01341v2",
                "updated": "2025-11-02T11:22:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    11,
                    22,
                    10,
                    6,
                    306,
                    0
                ],
                "published": "2025-02-03T13:34:51Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    34,
                    51,
                    0,
                    34,
                    0
                ],
                "title": "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Document Understanding"
                },
                "summary": "Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), lack inductive bias\nto constrain visual features within the linguistic structure of the LLM's\nembedding space, making them data-hungry and prone to cross-modal misalignment.\nIn this work, we propose a novel vision-text alignment method, AlignVLM, that\nmaps visual features to a weighted average of LLM text embeddings. Our approach\nleverages the linguistic priors encoded by the LLM to ensure that visual\nfeatures are mapped to regions of the space that the LLM can effectively\ninterpret. AlignVLM is particularly effective for document understanding tasks,\nwhere visual and textual modalities are highly correlated. Our extensive\nexperiments show that AlignVLM achieves state-of-the-art performance compared\nto prior alignment methods, with larger gains on document understanding tasks\nand under low-resource setups. We provide further analysis demonstrating its\nefficiency and robustness to noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), lack inductive bias\nto constrain visual features within the linguistic structure of the LLM's\nembedding space, making them data-hungry and prone to cross-modal misalignment.\nIn this work, we propose a novel vision-text alignment method, AlignVLM, that\nmaps visual features to a weighted average of LLM text embeddings. Our approach\nleverages the linguistic priors encoded by the LLM to ensure that visual\nfeatures are mapped to regions of the space that the LLM can effectively\ninterpret. AlignVLM is particularly effective for document understanding tasks,\nwhere visual and textual modalities are highly correlated. Our extensive\nexperiments show that AlignVLM achieves state-of-the-art performance compared\nto prior alignment methods, with larger gains on document understanding tasks\nand under low-resource setups. We provide further analysis demonstrating its\nefficiency and robustness to noise."
                },
                "authors": [
                    {
                        "name": "Ahmed Masry"
                    },
                    {
                        "name": "Juan A. Rodriguez"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Aarash Feizi"
                    },
                    {
                        "name": "Akshay Kalkunte Suresh"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Xiangru Jian"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Marco Pedersoli"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Enamul Hoque"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Issam H. Laradji"
                    },
                    {
                        "name": "David Vazquez"
                    },
                    {
                        "name": "Perouz Taslakian"
                    },
                    {
                        "name": "Spandana Gella"
                    },
                    {
                        "name": "Sai Rajeswar"
                    }
                ],
                "author_detail": {
                    "name": "Sai Rajeswar"
                },
                "author": "Sai Rajeswar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19739v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19739v3",
                "updated": "2025-11-02T09:40:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    40,
                    36,
                    6,
                    306,
                    0
                ],
                "published": "2025-03-25T15:04:53Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    4,
                    53,
                    1,
                    84,
                    0
                ],
                "title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via\n  Frequency-Decoupled Alignment and Degradation-Robust Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via\n  Frequency-Decoupled Alignment and Degradation-Robust Fusion"
                },
                "summary": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs .Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs .Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE"
                },
                "authors": [
                    {
                        "name": "Pihai Sun"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Youyu Chen"
                    },
                    {
                        "name": "Wenbo Zhao"
                    },
                    {
                        "name": "Kui Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianming Liu"
                },
                "author": "Xianming Liu",
                "arxiv_comment": "[IROS 2025, camera ready version]: 8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19739v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19739v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04772v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04772v5",
                "updated": "2025-11-02T08:57:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    8,
                    57,
                    42,
                    6,
                    306,
                    0
                ],
                "published": "2024-06-07T09:17:33Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    9,
                    17,
                    33,
                    4,
                    159,
                    0
                ],
                "title": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning"
                },
                "summary": "Recent rehearsal-free continual learning (CL) methods guided by prompts\nachieve strong performance on vision tasks with non-stationary data but remain\nresource-intensive, hindering real-world edge deployment. We introduce\nresource-efficient prompting (REP), which improves the computational and memory\nefficiency of prompt-based rehearsal-free continual learning methods while\nminimizing accuracy trade-offs. Our approach employs swift prompt selection to\nrefine input data using a carefully provisioned model and introduces adaptive\ntoken merging (AToM) and adaptive layer dropping (ALD) for efficient prompt\nupdates. AToM and ALD selectively skip data and model layers while preserving\ntask-specific features during the learning of new tasks. Extensive experiments\non multiple image classification datasets demonstrate REP's superior resource\nefficiency over state-of-the-art rehearsal-free CL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent rehearsal-free continual learning (CL) methods guided by prompts\nachieve strong performance on vision tasks with non-stationary data but remain\nresource-intensive, hindering real-world edge deployment. We introduce\nresource-efficient prompting (REP), which improves the computational and memory\nefficiency of prompt-based rehearsal-free continual learning methods while\nminimizing accuracy trade-offs. Our approach employs swift prompt selection to\nrefine input data using a carefully provisioned model and introduces adaptive\ntoken merging (AToM) and adaptive layer dropping (ALD) for efficient prompt\nupdates. AToM and ALD selectively skip data and model layers while preserving\ntask-specific features during the learning of new tasks. Extensive experiments\non multiple image classification datasets demonstrate REP's superior resource\nefficiency over state-of-the-art rehearsal-free CL methods."
                },
                "authors": [
                    {
                        "name": "Sungho Jeon"
                    },
                    {
                        "name": "Xinyue Ma"
                    },
                    {
                        "name": "Kwang In Kim"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Myeongjae Jeon"
                },
                "author": "Myeongjae Jeon",
                "arxiv_comment": "accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04772v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04772v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16613v3",
                "updated": "2025-11-02T08:42:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    8,
                    42,
                    30,
                    6,
                    306,
                    0
                ],
                "published": "2025-04-23T10:59:03Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    59,
                    3,
                    2,
                    113,
                    0
                ],
                "title": "UAV-Mounted IRS (UMI) in the Presence of Hovering Fluctuations: 3D\n  Pattern Characterization and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-Mounted IRS (UMI) in the Presence of Hovering Fluctuations: 3D\n  Pattern Characterization and Performance Analysis"
                },
                "summary": "This paper investigates unmanned aerial vehicle (UAV)-mounted intelligent\nreflecting surfaces (IRS) to leverage the benefits of this technology for\nfuture communication networks, such as 6G. Key advantages include enhanced\nspectral and energy efficiency, expanded network coverage, and flexible\ndeployment. One of the main challenges in employing UAV-mounted IRS (UMI)\ntechnology is the random fluctuations of hovering UAVs. Focusing on this\nchallenge, this paper explores the capabilities of UMI with passive/active\nelements affected by UAV fluctuations in both horizontal and vertical angles,\nconsidering the three-dimensional (3D) radiation pattern of the IRS. The\nrelationship between UAV fluctuations and IRS pattern is investigated by taking\ninto account the random angular vibrations of UAVs. A tractable and closed-form\ndistribution function for the IRS pattern is derived, using linear\napproximation and by dividing it into several sectors. In addition, closed-form\nexpressions for outage probability (OP) are obtained using central limit\ntheorem (CLT) and Gamma approximation. The theoretical expressions are\nvalidated through Monte Carlo simulations. The findings indicate that the\nrandom fluctuations of hovering UAVs have a notable impact on the performance\nof UMI systems. To avoid link interruptions due to UAV instability, IRS should\nutilize fewer elements, even though this leads to a decrease in directivity. As\na result, unlike terrestrial IRS, incorporating more elements into aerial IRS\nsystems does not necessarily improve performance due to the fluctuations in\nUAV. Numerical results show that the OP can be minimized by selecting the\noptimal number of IRS elements and using active elements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates unmanned aerial vehicle (UAV)-mounted intelligent\nreflecting surfaces (IRS) to leverage the benefits of this technology for\nfuture communication networks, such as 6G. Key advantages include enhanced\nspectral and energy efficiency, expanded network coverage, and flexible\ndeployment. One of the main challenges in employing UAV-mounted IRS (UMI)\ntechnology is the random fluctuations of hovering UAVs. Focusing on this\nchallenge, this paper explores the capabilities of UMI with passive/active\nelements affected by UAV fluctuations in both horizontal and vertical angles,\nconsidering the three-dimensional (3D) radiation pattern of the IRS. The\nrelationship between UAV fluctuations and IRS pattern is investigated by taking\ninto account the random angular vibrations of UAVs. A tractable and closed-form\ndistribution function for the IRS pattern is derived, using linear\napproximation and by dividing it into several sectors. In addition, closed-form\nexpressions for outage probability (OP) are obtained using central limit\ntheorem (CLT) and Gamma approximation. The theoretical expressions are\nvalidated through Monte Carlo simulations. The findings indicate that the\nrandom fluctuations of hovering UAVs have a notable impact on the performance\nof UMI systems. To avoid link interruptions due to UAV instability, IRS should\nutilize fewer elements, even though this leads to a decrease in directivity. As\na result, unlike terrestrial IRS, incorporating more elements into aerial IRS\nsystems does not necessarily improve performance due to the fluctuations in\nUAV. Numerical results show that the OP can be minimized by selecting the\noptimal number of IRS elements and using active elements."
                },
                "authors": [
                    {
                        "name": "Mohammad Javad Zakavi"
                    },
                    {
                        "name": "Mahtab Mirmohseni"
                    },
                    {
                        "name": "Farid Ashtiani"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05286v2",
                "updated": "2025-11-02T08:05:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    8,
                    5,
                    33,
                    6,
                    306,
                    0
                ],
                "published": "2025-05-08T14:28:47Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    28,
                    47,
                    3,
                    128,
                    0
                ],
                "title": "HEXGEN-FLOW: Optimizing LLM Inference Request Scheduling for Agentic\n  Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXGEN-FLOW: Optimizing LLM Inference Request Scheduling for Agentic\n  Text-to-SQL"
                },
                "summary": "Recent advancements in leveraging the agentic paradigm of large language\nmodels (LLMs) have substantially improved Text-to-SQL capabilities, empowering\nusers without specialized database knowledge to intuitively query databases.\nHowever, deploying agentic LLM-based Text-to-SQL systems in production presents\nsignificant challenges, stemming from their inherently multi-stage\ncomputational dependencies, strict latency requirements, and the complexity of\ndeployment across heterogeneous GPUs widely existing in enterprise clusters.\nMeanwhile, existing LLM serving frameworks are primarily designed for\nindependent inference tasks, resulting in suboptimal performance and frequent\nservice-level objective (SLO) violations in Text-to-SQL workloads. In this\npaper, we introduce HEXGEN-FLOW, a novel framework designed explicitly to\nschedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on\nheterogeneous GPU clusters serving multi-tenant Text-to-SQL requests.\nHEXGEN-FLOW introduces a hierarchical scheduling approach that combines global\nworkload-balanced task dispatching with an adaptive local priority queue,\nguided by a systematic analysis of agentic Text-to-SQL workflows. Additionally,\nwe propose a lightweight simulation-based method for tuning critical scheduling\nhyperparameters, further enhancing robustness and adaptability. Our evaluation\non realistic Text-to-SQL benchmarks demonstrates that HEXGEN-FLOW significantly\noutperforms state-of-the-art LLM serving frameworks. Across all traces,\nHEXGEN-FLOW reduces P95 tail latency by $1.42{\\sim}1.56\\times$ and increases\nthroughput by $1.49{\\sim}1.81\\times$, demonstrating robust improvements under\ndiverse workloads. Our code is available at\nhttps://github.com/Relaxed-System-Lab/Hexgen-Flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in leveraging the agentic paradigm of large language\nmodels (LLMs) have substantially improved Text-to-SQL capabilities, empowering\nusers without specialized database knowledge to intuitively query databases.\nHowever, deploying agentic LLM-based Text-to-SQL systems in production presents\nsignificant challenges, stemming from their inherently multi-stage\ncomputational dependencies, strict latency requirements, and the complexity of\ndeployment across heterogeneous GPUs widely existing in enterprise clusters.\nMeanwhile, existing LLM serving frameworks are primarily designed for\nindependent inference tasks, resulting in suboptimal performance and frequent\nservice-level objective (SLO) violations in Text-to-SQL workloads. In this\npaper, we introduce HEXGEN-FLOW, a novel framework designed explicitly to\nschedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on\nheterogeneous GPU clusters serving multi-tenant Text-to-SQL requests.\nHEXGEN-FLOW introduces a hierarchical scheduling approach that combines global\nworkload-balanced task dispatching with an adaptive local priority queue,\nguided by a systematic analysis of agentic Text-to-SQL workflows. Additionally,\nwe propose a lightweight simulation-based method for tuning critical scheduling\nhyperparameters, further enhancing robustness and adaptability. Our evaluation\non realistic Text-to-SQL benchmarks demonstrates that HEXGEN-FLOW significantly\noutperforms state-of-the-art LLM serving frameworks. Across all traces,\nHEXGEN-FLOW reduces P95 tail latency by $1.42{\\sim}1.56\\times$ and increases\nthroughput by $1.49{\\sim}1.81\\times$, demonstrating robust improvements under\ndiverse workloads. Our code is available at\nhttps://github.com/Relaxed-System-Lab/Hexgen-Flow."
                },
                "authors": [
                    {
                        "name": "You Peng"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Wenqi Jiang"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01258v2",
                "updated": "2025-11-02T07:54:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    7,
                    54,
                    31,
                    6,
                    306,
                    0
                ],
                "published": "2025-09-25T05:26:20Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    5,
                    26,
                    20,
                    3,
                    268,
                    0
                ],
                "title": "Measuring Algorithmic Partisanship via Zero-Shot Classification and Its\n  Implications on Political Discourse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Algorithmic Partisanship via Zero-Shot Classification and Its\n  Implications on Political Discourse"
                },
                "summary": "Amidst the rapid normalization of generative artificial intelligence (GAI),\nintelligent systems have come to dominate political discourse across\ninformation media. However, internalized political biases stemming from\ntraining data skews, human prejudice, and algorithmic flaws continue to plague\nthis novel technology. This study employs a zero-shot classification approach\nto evaluate algorithmic political partisanship through a methodical combination\nof ideological alignment, topicality, response sentiment, and objectivity. A\ntotal of 1800 model responses across six mainstream large language models\n(LLMs) were individually input into four distinct fine-tuned classification\nalgorithms, each responsible for computing one of the aforementioned metrics.\nThe results show an amplified liberal-authoritarian alignment across the six\nLLMs evaluated, with notable instances of reasoning supersessions and canned\nrefusals. The study subsequently highlights the psychological influences\nunderpinning human-computer interactions and how intrinsic biases can permeate\npublic discourse. The resulting distortion of the political landscape can\nultimately manifest as conformity or polarization, depending on the region's\npre-existing socio-political structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amidst the rapid normalization of generative artificial intelligence (GAI),\nintelligent systems have come to dominate political discourse across\ninformation media. However, internalized political biases stemming from\ntraining data skews, human prejudice, and algorithmic flaws continue to plague\nthis novel technology. This study employs a zero-shot classification approach\nto evaluate algorithmic political partisanship through a methodical combination\nof ideological alignment, topicality, response sentiment, and objectivity. A\ntotal of 1800 model responses across six mainstream large language models\n(LLMs) were individually input into four distinct fine-tuned classification\nalgorithms, each responsible for computing one of the aforementioned metrics.\nThe results show an amplified liberal-authoritarian alignment across the six\nLLMs evaluated, with notable instances of reasoning supersessions and canned\nrefusals. The study subsequently highlights the psychological influences\nunderpinning human-computer interactions and how intrinsic biases can permeate\npublic discourse. The resulting distortion of the political landscape can\nultimately manifest as conformity or polarization, depending on the region's\npre-existing socio-political structures."
                },
                "authors": [
                    {
                        "name": "Nathan Junzi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Junzi Chen"
                },
                "author": "Nathan Junzi Chen",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]