[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v1",
                "updated": "2025-08-01T07:03:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v2",
                "updated": "2025-08-01T03:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    43,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v2",
                "updated": "2025-08-01T03:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    37,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v2",
                "updated": "2025-07-31T21:00:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    0,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v2",
                "updated": "2025-07-31T07:35:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    35,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23292v1",
                "updated": "2025-07-31T07:10:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:10:39Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy"
                },
                "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."
                },
                "authors": [
                    {
                        "name": "RJ Skerry-Ryan"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Soroosh Mariooryad"
                    },
                    {
                        "name": "David Kao"
                    },
                    {
                        "name": "Daisy Stanton"
                    },
                    {
                        "name": "Eric Battenberg"
                    },
                    {
                        "name": "Matt Shannon"
                    },
                    {
                        "name": "Ron J. Weiss"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Tom Bagby"
                    }
                ],
                "author_detail": {
                    "name": "Tom Bagby"
                },
                "author": "Tom Bagby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v1",
                "updated": "2025-07-29T04:21:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v2",
                "updated": "2025-07-28T20:44:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    20,
                    44,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20030v1",
                "updated": "2025-07-26T18:20:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T18:20:25Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression"
                },
                "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."
                },
                "authors": [
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Mu Sheng"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v2",
                "updated": "2025-07-26T15:25:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    25,
                    22,
                    5,
                    207,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v3",
                "updated": "2025-07-26T15:13:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    13,
                    56,
                    5,
                    207,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages; Accepted at the 42nd International Conference on Machine\n  Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v2",
                "updated": "2025-07-26T13:33:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    13,
                    33,
                    6,
                    5,
                    207,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v1",
                "updated": "2025-07-26T10:34:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19823v1",
                "updated": "2025-07-26T06:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T06:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs"
                },
                "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."
                },
                "authors": [
                    {
                        "name": "Dongquan Yang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaotian Yu"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pl Andrs Papp"
                    },
                    {
                        "name": "Toni Bhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16784v1",
                "updated": "2025-07-22T17:30:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:30:04Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"
                },
                "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use."
                },
                "authors": [
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Nathaniel Morgan"
                    },
                    {
                        "name": "Tina Li"
                    },
                    {
                        "name": "Derek Zhao"
                    },
                    {
                        "name": "Ai Vy Ngo"
                    },
                    {
                        "name": "Philip Schroeder"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Jack O'Brien"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Research preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16768v1",
                "updated": "2025-07-22T17:13:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:13:47Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding"
                },
                "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."
                },
                "authors": [
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10131v3",
                "updated": "2025-07-22T16:49:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    49,
                    24,
                    1,
                    203,
                    0
                ],
                "published": "2022-12-20T09:58:39Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    9,
                    58,
                    39,
                    1,
                    354,
                    0
                ],
                "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms"
                },
                "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative."
                },
                "authors": [
                    {
                        "name": "Serhii Ivanenko"
                    },
                    {
                        "name": "Vasyl Lanko"
                    },
                    {
                        "name": "Rudi Horn"
                    },
                    {
                        "name": "Vojin Jovanovic"
                    },
                    {
                        "name": "Rodrigo Bruno"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Bruno"
                },
                "author": "Rodrigo Bruno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16243v1",
                "updated": "2025-07-22T05:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T05:34:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Genus Zero Kashiwara-Vergne Solutions from Braids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genus Zero Kashiwara-Vergne Solutions from Braids"
                },
                "summary": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator."
                },
                "authors": [
                    {
                        "name": "Zsuzsanna Dancso"
                    },
                    {
                        "name": "Iva Halacheva"
                    },
                    {
                        "name": "Guillaume Laplante-Anfossi"
                    },
                    {
                        "name": "Marcy Robertson"
                    },
                    {
                        "name": "Chandan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Singh"
                },
                "author": "Chandan Singh",
                "arxiv_comment": "comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M60, 17B, 55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v1",
                "updated": "2025-07-22T04:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v2",
                "updated": "2025-07-21T19:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    5,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_doi": "10.1109/RTSS62706.2024.00036",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RTSS62706.2024.00036",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent",
                "arxiv_journal_ref": "IEEE Real-Time Systems Symposium (RTSS), 2024, pp. 336-348",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v2",
                "updated": "2025-07-19T17:46:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    46,
                    19,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "We are withdrawing the submission in order to thoroughly revise the\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17772v1",
                "updated": "2025-07-19T17:02:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T17:02:15Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "title": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments"
                },
                "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Ahmad Alhonainy"
                    },
                    {
                        "name": "Praveen Rao"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Rao"
                },
                "arxiv_affiliation": "University of Missouri, USA",
                "author": "Praveen Rao",
                "arxiv_comment": "Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v3",
                "updated": "2025-07-19T07:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    7,
                    41,
                    3,
                    5,
                    200,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v2",
                "updated": "2025-07-19T03:40:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    3,
                    40,
                    40,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Added discussion and comparison with SpecPrefill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17771v1",
                "updated": "2025-07-19T00:57:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T00:57:54Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration"
                },
                "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms."
                },
                "authors": [
                    {
                        "name": "Dmitri Lyalikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitri Lyalikov"
                },
                "author": "Dmitri Lyalikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13961v1",
                "updated": "2025-07-18T14:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "Secretive Hotplug Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secretive Hotplug Coded Caching"
                },
                "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v2",
                "updated": "2025-07-18T13:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    29,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_journal_ref": "Proceedings of the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25), September 22--26, 2025, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v3",
                "updated": "2025-07-18T01:49:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    49,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v2",
                "updated": "2025-07-18T01:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    36,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v1",
                "updated": "2025-07-17T23:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Fernando Bermdez-Medina"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Lezhi Li"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Victoria MnchJuan Haladjian"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Raunak Sinha"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Mehrdad Farajtbar"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Emily Zhang"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "David Gera"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Chen Wu"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Shang-Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14204v1",
                "updated": "2025-07-14T19:09:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:09:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache."
                },
                "authors": [
                    {
                        "name": "Dachuan Shi"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Xiangchi Yuan"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Haoran You"
                    },
                    {
                        "name": "Sixu Li"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.00819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00819v1",
                "updated": "2025-08-01T17:56:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    56,
                    7,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T17:56:07Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    56,
                    7,
                    4,
                    213,
                    0
                ],
                "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models"
                },
                "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation."
                },
                "authors": [
                    {
                        "name": "Jinsong Li"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02198v2",
                "updated": "2025-08-01T17:54:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    54,
                    2,
                    4,
                    213,
                    0
                ],
                "published": "2025-03-04T02:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    2,
                    18,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "FalconGym: A Photorealistic Simulation Framework for Zero-Shot\n  Sim-to-Real Vision-Based Quadrotor Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconGym: A Photorealistic Simulation Framework for Zero-Shot\n  Sim-to-Real Vision-Based Quadrotor Navigation"
                },
                "summary": "We present a novel framework demonstrating zero-shot sim-to-real transfer of\nvisual control policies learned in a Neural Radiance Field (NeRF) environment\nfor quadrotors to fly through racing gates. Robust transfer from simulation to\nreal flight poses a major challenge, as standard simulators often lack\nsufficient visual fidelity. To address this, we construct a photorealistic\nsimulation environment of quadrotor racing tracks, called FalconGym, which\nprovides effectively unlimited synthetic images for training. Within FalconGym,\nwe develop a pipelined approach for crossing gates that combines (i) a Neural\nPose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor\nposes from single-frame RGB images and IMU data, and (ii) a\nself-attention-based multi-modal controller that adaptively integrates visual\nfeatures and pose estimation. This multi-modal design compensates for\nperception noise and intermittent gate visibility. We train this controller\npurely in FalconGym with imitation learning and deploy the resulting policy to\nreal hardware with no additional fine-tuning. Simulation experiments on three\ndistinct tracks (circle, U-turn and figure-8) demonstrate that our controller\noutperforms a vision-only state-of-the-art baseline in both success rate and\ngate-crossing accuracy. In 30 live hardware flights spanning three tracks and\n120 gates, our controller achieves a 95.8% success rate and an average error of\njust 10 cm when flying through 38 cm-radius gates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel framework demonstrating zero-shot sim-to-real transfer of\nvisual control policies learned in a Neural Radiance Field (NeRF) environment\nfor quadrotors to fly through racing gates. Robust transfer from simulation to\nreal flight poses a major challenge, as standard simulators often lack\nsufficient visual fidelity. To address this, we construct a photorealistic\nsimulation environment of quadrotor racing tracks, called FalconGym, which\nprovides effectively unlimited synthetic images for training. Within FalconGym,\nwe develop a pipelined approach for crossing gates that combines (i) a Neural\nPose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor\nposes from single-frame RGB images and IMU data, and (ii) a\nself-attention-based multi-modal controller that adaptively integrates visual\nfeatures and pose estimation. This multi-modal design compensates for\nperception noise and intermittent gate visibility. We train this controller\npurely in FalconGym with imitation learning and deploy the resulting policy to\nreal hardware with no additional fine-tuning. Simulation experiments on three\ndistinct tracks (circle, U-turn and figure-8) demonstrate that our controller\noutperforms a vision-only state-of-the-art baseline in both success rate and\ngate-crossing accuracy. In 30 live hardware flights spanning three tracks and\n120 gates, our controller achieves a 95.8% success rate and an average error of\njust 10 cm when flying through 38 cm-radius gates."
                },
                "authors": [
                    {
                        "name": "Yan Miao"
                    },
                    {
                        "name": "Will Shen"
                    },
                    {
                        "name": "Sayan Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Sayan Mitra"
                },
                "author": "Sayan Mitra",
                "arxiv_comment": "Accepted in IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14676v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14676v2",
                "updated": "2025-08-01T17:45:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    45,
                    12,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-20T17:58:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    58,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "A JWST View of the Overmassive Black Hole in NGC 4486B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A JWST View of the Overmassive Black Hole in NGC 4486B"
                },
                "summary": "We present a new stellar dynamical measurement of the supermassive black hole\n(SMBH) in the compact elliptical galaxy NGC 4486B, based on integral field\nspectroscopy with JWST/NIRSpec. The two-dimensional kinematic maps reveal a\nresolved double nucleus and a velocity dispersion peak offset from the\nphotometric center. Utilizing two independent methods-Schwarzschild\norbit-superposition and Jeans Anisotropic Modeling-we tightly constrain the\nblack hole mass by fitting the full line-of-sight velocity distribution. Our\naxisymmetric Schwarzschild models yield a best-fit black hole mass of $M_{BH} =\n3.6^{+0.7}_{-0.7} \\times 10^8 \\, M_{\\odot}$, slightly lower but significantly\nmore precise than previous estimates. However, since our models do not account\nfor the non-equilibrium nature of the double nucleus, this value may represent\na lower limit. Across all tested dynamical models, the inferred $M_{BH}/M_*$\nratio ranges from ~ 4-13%, providing robust evidence for an overmassive SMBH in\nNGC 4486B. Combined with the galaxy's location deep within the Virgo Cluster,\nour results support the interpretation that NGC 4486B is the tidally stripped\nremnant core of a formerly massive galaxy. As the JWST/NIRSpec field of view is\ninsufficient to constrain the dark matter halo, we incorporate archival\nground-based long-slit kinematics extending to 5 arcsec. While this provides\nsome leverage on the dark matter content, the constraints remain relatively\nweak. We place only an upper limit on the dark matter fraction, with\n$M_{DM}/M_{*} < 0.5$ within 1 kpc-well beyond the effective radius. The\ninferred black hole mass remains unchanged with or without a dark matter halo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new stellar dynamical measurement of the supermassive black hole\n(SMBH) in the compact elliptical galaxy NGC 4486B, based on integral field\nspectroscopy with JWST/NIRSpec. The two-dimensional kinematic maps reveal a\nresolved double nucleus and a velocity dispersion peak offset from the\nphotometric center. Utilizing two independent methods-Schwarzschild\norbit-superposition and Jeans Anisotropic Modeling-we tightly constrain the\nblack hole mass by fitting the full line-of-sight velocity distribution. Our\naxisymmetric Schwarzschild models yield a best-fit black hole mass of $M_{BH} =\n3.6^{+0.7}_{-0.7} \\times 10^8 \\, M_{\\odot}$, slightly lower but significantly\nmore precise than previous estimates. However, since our models do not account\nfor the non-equilibrium nature of the double nucleus, this value may represent\na lower limit. Across all tested dynamical models, the inferred $M_{BH}/M_*$\nratio ranges from ~ 4-13%, providing robust evidence for an overmassive SMBH in\nNGC 4486B. Combined with the galaxy's location deep within the Virgo Cluster,\nour results support the interpretation that NGC 4486B is the tidally stripped\nremnant core of a formerly massive galaxy. As the JWST/NIRSpec field of view is\ninsufficient to constrain the dark matter halo, we incorporate archival\nground-based long-slit kinematics extending to 5 arcsec. While this provides\nsome leverage on the dark matter content, the constraints remain relatively\nweak. We place only an upper limit on the dark matter fraction, with\n$M_{DM}/M_{*} < 0.5$ within 1 kpc-well beyond the effective radius. The\ninferred black hole mass remains unchanged with or without a dark matter halo."
                },
                "authors": [
                    {
                        "name": "Behzad Tahmasebzadeh"
                    },
                    {
                        "name": "Matthew A. Taylor"
                    },
                    {
                        "name": "Monica Valluri"
                    },
                    {
                        "name": "Haruka Yoshino"
                    },
                    {
                        "name": "Eugene Vasiliev"
                    },
                    {
                        "name": "Michael J. Drinkwater"
                    },
                    {
                        "name": "Solveig Thompson"
                    },
                    {
                        "name": "Kristen Dage"
                    },
                    {
                        "name": "Patrick Ct"
                    },
                    {
                        "name": "Laura Ferrarese"
                    },
                    {
                        "name": "Tatsuya Akiba"
                    },
                    {
                        "name": "Vivienne Baldassare"
                    },
                    {
                        "name": "Misty C. Bentz"
                    },
                    {
                        "name": "John P. Blakeslee"
                    },
                    {
                        "name": "Holger Baumgardt"
                    },
                    {
                        "name": "Youkyung Ko"
                    },
                    {
                        "name": "Chengze Liu"
                    },
                    {
                        "name": "Ann-Marie Madigan"
                    },
                    {
                        "name": "Eric W. Peng"
                    },
                    {
                        "name": "Joel Roediger"
                    },
                    {
                        "name": "Kaixiang Wang"
                    },
                    {
                        "name": "Tyrone E. Woods"
                    }
                ],
                "author_detail": {
                    "name": "Tyrone E. Woods"
                },
                "author": "Tyrone E. Woods",
                "arxiv_comment": "12 pages, 5 figures, 2 tables; accepted by ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14676v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14676v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07938v2",
                "updated": "2025-08-01T17:41:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    41,
                    23,
                    4,
                    213,
                    0
                ],
                "published": "2025-06-09T16:57:54Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    16,
                    57,
                    54,
                    0,
                    160,
                    0
                ],
                "title": "New Insights into the T Tauri Binary Separation Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Insights into the T Tauri Binary Separation Distribution"
                },
                "summary": "For three decades, adaptive optic surveys have revealed an excess of T Tauri\nbinaries across a = 10-100 au in nearby star-forming regions compared to the\nfield population of main-sequence (MS) stars. Such an excess requires that most\nstars are born in dense clusters and subjected to significant dynamical\nprocessing that disrupts such binaries across intermediate separations.\nHowever, we demonstrate that the apparent excess is due to an observational\nselection bias. Close binaries within a < 100 au clear out their dusty\ncircumstellar disks on faster timescales compared to wide binaries and single\nstars. A magnitude-limited sample is therefore biased toward close binaries\nthat have preferentially cleared out their obscuring disks. We re-examine the\nseparation distribution of pre-MS binaries in low-density Taurus, moderately\ndense Upper Scorpius, and the extremely dense Orion Nebula Cluster (ONC). By\nlimiting the samples to primary spectral type / mass instead of magnitude, the\nartificial excess across a = 10-100 au disappears in all three environments.\nAcross wider separations a = 100-4,000 au, Taurus exhibits an excess of\ncompanions (mostly tertiaries), the ONC displays a deficit, and Upper Scorpius\nmatches the field MS population. The field derives from an amalgam of all three\nenvironments, where Upper Scorpius corresponds to the average birth environment\nof solar-type stars. The total binary fraction within a < 10,000 au in Taurus\nis only 52% +/- 7%, substantially lower than the 100% inferred from the biased\nobservations and only slightly higher than the field MS value of 45%. N-body\ninteractions preferentially disrupt outer tertiaries with only marginal\ndynamical processing of the inner binaries, especially those within a < 100 au.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For three decades, adaptive optic surveys have revealed an excess of T Tauri\nbinaries across a = 10-100 au in nearby star-forming regions compared to the\nfield population of main-sequence (MS) stars. Such an excess requires that most\nstars are born in dense clusters and subjected to significant dynamical\nprocessing that disrupts such binaries across intermediate separations.\nHowever, we demonstrate that the apparent excess is due to an observational\nselection bias. Close binaries within a < 100 au clear out their dusty\ncircumstellar disks on faster timescales compared to wide binaries and single\nstars. A magnitude-limited sample is therefore biased toward close binaries\nthat have preferentially cleared out their obscuring disks. We re-examine the\nseparation distribution of pre-MS binaries in low-density Taurus, moderately\ndense Upper Scorpius, and the extremely dense Orion Nebula Cluster (ONC). By\nlimiting the samples to primary spectral type / mass instead of magnitude, the\nartificial excess across a = 10-100 au disappears in all three environments.\nAcross wider separations a = 100-4,000 au, Taurus exhibits an excess of\ncompanions (mostly tertiaries), the ONC displays a deficit, and Upper Scorpius\nmatches the field MS population. The field derives from an amalgam of all three\nenvironments, where Upper Scorpius corresponds to the average birth environment\nof solar-type stars. The total binary fraction within a < 10,000 au in Taurus\nis only 52% +/- 7%, substantially lower than the 100% inferred from the biased\nobservations and only slightly higher than the field MS value of 45%. N-body\ninteractions preferentially disrupt outer tertiaries with only marginal\ndynamical processing of the inner binaries, especially those within a < 100 au."
                },
                "authors": [
                    {
                        "name": "Caleb Eastlund"
                    },
                    {
                        "name": "Maxwell Moe"
                    },
                    {
                        "name": "Kaitlin M. Kratter"
                    },
                    {
                        "name": "Marina Kounkel"
                    }
                ],
                "author_detail": {
                    "name": "Marina Kounkel"
                },
                "author": "Marina Kounkel",
                "arxiv_comment": "Accepted by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00806v1",
                "updated": "2025-08-01T17:39:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    39,
                    25,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T17:39:25Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    39,
                    25,
                    4,
                    213,
                    0
                ],
                "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory\n  Management"
                },
                "summary": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline."
                },
                "authors": [
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Zhuohong Deng"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Hongzi Zhu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00804v1",
                "updated": "2025-08-01T17:37:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    37,
                    19,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T17:37:19Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    37,
                    19,
                    4,
                    213,
                    0
                ],
                "title": "Online Fine-Tuning of Carbon Emission Predictions using Real-Time\n  Recurrent Learning for State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Fine-Tuning of Carbon Emission Predictions using Real-Time\n  Recurrent Learning for State Space Models"
                },
                "summary": "This paper introduces a new approach for fine-tuning the predictions of\nstructured state space models (SSMs) at inference time using real-time\nrecurrent learning. While SSMs are known for their efficiency and long-range\nmodeling capabilities, they are typically trained offline and remain static\nduring deployment. Our method enables online adaptation by continuously\nupdating model parameters in response to incoming data. We evaluate our\napproach for linear-recurrent-unit SSMs using a small carbon emission dataset\ncollected from embedded automotive hardware. Experimental results show that our\nmethod consistently reduces prediction error online during inference,\ndemonstrating its potential for dynamic, resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new approach for fine-tuning the predictions of\nstructured state space models (SSMs) at inference time using real-time\nrecurrent learning. While SSMs are known for their efficiency and long-range\nmodeling capabilities, they are typically trained offline and remain static\nduring deployment. Our method enables online adaptation by continuously\nupdating model parameters in response to incoming data. We evaluate our\napproach for linear-recurrent-unit SSMs using a small carbon emission dataset\ncollected from embedded automotive hardware. Experimental results show that our\nmethod consistently reduces prediction error online during inference,\ndemonstrating its potential for dynamic, resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Julian Lemmel"
                    },
                    {
                        "name": "Manuel Kranzl"
                    },
                    {
                        "name": "Adam Lamine"
                    },
                    {
                        "name": "Philipp Neubauer"
                    },
                    {
                        "name": "Radu Grosu"
                    },
                    {
                        "name": "Sophie Neubauer"
                    }
                ],
                "author_detail": {
                    "name": "Sophie Neubauer"
                },
                "author": "Sophie Neubauer",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18365v3",
                "updated": "2025-08-01T17:19:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    19,
                    56,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-24T12:46:30Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    30,
                    3,
                    205,
                    0
                ],
                "title": "RecPS: Privacy Risk Scoring for Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecPS: Privacy Risk Scoring for Recommender Systems"
                },
                "summary": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning."
                },
                "authors": [
                    {
                        "name": "Jiajie He"
                    },
                    {
                        "name": "Yuechun Gu"
                    },
                    {
                        "name": "Keke Chen"
                    }
                ],
                "author_detail": {
                    "name": "Keke Chen"
                },
                "author": "Keke Chen",
                "arxiv_comment": "Accepted by ACM RecSys 2025; to appear",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21046v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21046v3",
                "updated": "2025-08-01T17:17:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    17,
                    9,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-28T17:59:05Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    5,
                    0,
                    209,
                    0
                ],
                "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks."
                },
                "authors": [
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Xuan Qi"
                    },
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Yixiong Fang"
                    },
                    {
                        "name": "Qiwen Zhao"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zhenhailong Wang"
                    },
                    {
                        "name": "Minda Hu"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "51 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21046v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21046v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21688v2",
                "updated": "2025-08-01T17:16:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    16,
                    8,
                    4,
                    213,
                    0
                ],
                "published": "2025-04-30T14:23:50Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    23,
                    50,
                    2,
                    120,
                    0
                ],
                "title": "Assessing Racial Disparities in Healthcare Expenditures via Mediator\n  Distribution Shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Racial Disparities in Healthcare Expenditures via Mediator\n  Distribution Shifts"
                },
                "summary": "Racial disparities in healthcare expenditures are well-documented, yet the\nunderlying drivers remain complex and require further investigation. This study\ndevelops a framework for decomposing such disparities through shifts in the\ndistributions of mediating variables, rather than treating race itself as a\nmanipulable exposure. We define disparities as differences in\ncovariate-adjusted outcome distributions across racial groups, and decompose\nthe total disparity into two components: one attributable to differences in\nmediator distributions, and another residual component that would remain even\nafter equalizing these distributions. Using data from the Medical Expenditures\nPanel Survey, we examine the extent to which expenditure disparities would\npersist or be reduced if mediators such as socioeconomic status, insurance\naccess, health behaviors, or health status were equalized across racial groups.\nTo ensure valid inference, we derive asymptotically linear estimators based on\ninfluence-function techniques and flexible machine learning tools, including\nsuper learners and a two-part model designed for the zero-inflated,\nright-skewed nature of expenditure data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Racial disparities in healthcare expenditures are well-documented, yet the\nunderlying drivers remain complex and require further investigation. This study\ndevelops a framework for decomposing such disparities through shifts in the\ndistributions of mediating variables, rather than treating race itself as a\nmanipulable exposure. We define disparities as differences in\ncovariate-adjusted outcome distributions across racial groups, and decompose\nthe total disparity into two components: one attributable to differences in\nmediator distributions, and another residual component that would remain even\nafter equalizing these distributions. Using data from the Medical Expenditures\nPanel Survey, we examine the extent to which expenditure disparities would\npersist or be reduced if mediators such as socioeconomic status, insurance\naccess, health behaviors, or health status were equalized across racial groups.\nTo ensure valid inference, we derive asymptotically linear estimators based on\ninfluence-function techniques and flexible machine learning tools, including\nsuper learners and a two-part model designed for the zero-inflated,\nright-skewed nature of expenditure data."
                },
                "authors": [
                    {
                        "name": "Xiaxian Ou"
                    },
                    {
                        "name": "Xinwei He"
                    },
                    {
                        "name": "David Benkeser"
                    },
                    {
                        "name": "Razieh Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Razieh Nabi"
                },
                "author": "Razieh Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00788v1",
                "updated": "2025-08-01T17:11:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    11,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T17:11:42Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    11,
                    42,
                    4,
                    213,
                    0
                ],
                "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun\n  Handling in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun\n  Handling in Large Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research."
                },
                "authors": [
                    {
                        "name": "Xushuo Tang"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Yongrui Gu"
                    },
                    {
                        "name": "Wenke Yang"
                    },
                    {
                        "name": "Mingchen Ju"
                    },
                    {
                        "name": "Xin Cao"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02039v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02039v3",
                "updated": "2025-08-01T17:05:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    5,
                    21,
                    4,
                    213,
                    0
                ],
                "published": "2025-01-03T14:35:32Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    35,
                    32,
                    4,
                    3,
                    0
                ],
                "title": "An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Fan Bu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Siyi Wang"
                    },
                    {
                        "name": "Ziyao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyao Liu"
                },
                "author": "Ziyao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02039v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02039v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00768v1",
                "updated": "2025-08-01T16:43:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    43,
                    45,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:43:45Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    43,
                    45,
                    4,
                    213,
                    0
                ],
                "title": "Evaluating Angle and Amplitude Encoding Strategies for Variational\n  Quantum Machine Learning: their impact on model's accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Angle and Amplitude Encoding Strategies for Variational\n  Quantum Machine Learning: their impact on model's accuracy"
                },
                "summary": "Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models."
                },
                "authors": [
                    {
                        "name": "Antonio Tudisco"
                    },
                    {
                        "name": "Andrea Marchesin"
                    },
                    {
                        "name": "Maurizio Zamboni"
                    },
                    {
                        "name": "Mariagrazia Graziano"
                    },
                    {
                        "name": "Giovanna Turvani"
                    }
                ],
                "author_detail": {
                    "name": "Giovanna Turvani"
                },
                "author": "Giovanna Turvani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00762v1",
                "updated": "2025-08-01T16:38:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    38,
                    18,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:38:18Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    38,
                    18,
                    4,
                    213,
                    0
                ],
                "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A\n  Zero-Shot Approach using LLM-Driven Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A\n  Zero-Shot Approach using LLM-Driven Code Generation"
                },
                "summary": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category."
                },
                "authors": [
                    {
                        "name": "Atakan Site"
                    },
                    {
                        "name": "Emre Hakan Erdemir"
                    },
                    {
                        "name": "Glen Eryiit"
                    }
                ],
                "author_detail": {
                    "name": "Glen Eryiit"
                },
                "author": "Glen Eryiit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03351v2",
                "updated": "2025-08-01T16:37:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    37,
                    59,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-06T09:19:16Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    19,
                    16,
                    1,
                    126,
                    0
                ],
                "title": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar"
                },
                "summary": "Reconstructing a high-quality, animatable 3D human avatar with expressive\nfacial and hand motions from a single image has gained significant attention\ndue to its broad application potential. 3D human avatar reconstruction\ntypically requires multi-view or monocular videos and training on individual\nIDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's\nexpressiveness, these methods often focus on body motion but struggle with\nfacial expressions. To address these challenges, we first introduce an\nexpressive human model (EHM) to enhance facial expression capabilities and\ndevelop an accurate tracking method. Based on this template model, we propose\nGUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar\nreconstruction. We leverage inverse texture mapping and projection sampling\ntechniques to infer Ubody (upper-body) Gaussians from a single image. The\nrendered images are refined through a neural refiner. Experimental results\ndemonstrate that GUAVA significantly outperforms previous methods in rendering\nquality and offers significant speed improvements, with reconstruction times in\nthe sub-second range (0.1s), and supports real-time animation and rendering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing a high-quality, animatable 3D human avatar with expressive\nfacial and hand motions from a single image has gained significant attention\ndue to its broad application potential. 3D human avatar reconstruction\ntypically requires multi-view or monocular videos and training on individual\nIDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's\nexpressiveness, these methods often focus on body motion but struggle with\nfacial expressions. To address these challenges, we first introduce an\nexpressive human model (EHM) to enhance facial expression capabilities and\ndevelop an accurate tracking method. Based on this template model, we propose\nGUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar\nreconstruction. We leverage inverse texture mapping and projection sampling\ntechniques to infer Ubody (upper-body) Gaussians from a single image. The\nrendered images are refined through a neural refiner. Experimental results\ndemonstrate that GUAVA significantly outperforms previous methods in rendering\nquality and offers significant speed improvements, with reconstruction times in\nthe sub-second range (0.1s), and supports real-time animation and rendering."
                },
                "authors": [
                    {
                        "name": "Dongbin Zhang"
                    },
                    {
                        "name": "Yunfei Liu"
                    },
                    {
                        "name": "Lijian Lin"
                    },
                    {
                        "name": "Ye Zhu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Minghan Qin"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Haoqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoqian Wang"
                },
                "author": "Haoqian Wang",
                "arxiv_comment": "Accepted to ICCV 2025, Project page:\n  https://eastbeanzhang.github.io/GUAVA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17217v2",
                "updated": "2025-08-01T16:37:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    37,
                    16,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-22T18:46:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    18,
                    46,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs"
                },
                "summary": "Large Language Models (LLMs) often exhibit gender bias, resulting in unequal\ntreatment of male and female subjects across different contexts. To address\nthis issue, we propose a novel data generation framework that fosters\nexploratory thinking in LLMs. Our approach prompts models to generate story\npairs featuring male and female protagonists in structurally identical, morally\nambiguous scenarios, then elicits and compares their moral judgments. When\ninconsistencies arise, the model is guided to produce balanced, gender-neutral\njudgments. These story-judgment pairs are used to fine-tune or optimize the\nmodels via Direct Preference Optimization (DPO). Experimental results show that\nour method significantly reduces gender bias while preserving or even enhancing\ngeneral model capabilities. We will release the code and generated data. We\nrelease the code and generated data at:\nhttps://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit gender bias, resulting in unequal\ntreatment of male and female subjects across different contexts. To address\nthis issue, we propose a novel data generation framework that fosters\nexploratory thinking in LLMs. Our approach prompts models to generate story\npairs featuring male and female protagonists in structurally identical, morally\nambiguous scenarios, then elicits and compares their moral judgments. When\ninconsistencies arise, the model is guided to produce balanced, gender-neutral\njudgments. These story-judgment pairs are used to fine-tune or optimize the\nmodels via Direct Preference Optimization (DPO). Experimental results show that\nour method significantly reduces gender bias while preserving or even enhancing\ngeneral model capabilities. We will release the code and generated data. We\nrelease the code and generated data at:\nhttps://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main."
                },
                "authors": [
                    {
                        "name": "Kangda Wei"
                    },
                    {
                        "name": "Hasnat Md Abdullah"
                    },
                    {
                        "name": "Ruihong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruihong Huang"
                },
                "author": "Ruihong Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00760v1",
                "updated": "2025-08-01T16:34:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    34,
                    57,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:34:57Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    34,
                    57,
                    4,
                    213,
                    0
                ],
                "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese\n  Hate Speech Detection under Cloaking Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese\n  Hate Speech Detection under Cloaking Perturbations"
                },
                "summary": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches."
                },
                "authors": [
                    {
                        "name": "Qiyao Xue"
                    },
                    {
                        "name": "Yuchen Dou"
                    },
                    {
                        "name": "Ryan Shi"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00756v1",
                "updated": "2025-08-01T16:32:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    32,
                    48,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:32:48Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    32,
                    48,
                    4,
                    213,
                    0
                ],
                "title": "LeakyCLIP: Extracting Training Data from CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeakyCLIP: Extracting Training Data from CLIP"
                },
                "summary": "Understanding the memorization and privacy leakage risks in Contrastive\nLanguage--Image Pretraining (CLIP) is critical for ensuring the security of\nmultimodal models. Recent studies have demonstrated the feasibility of\nextracting sensitive training examples from diffusion models, with conditional\ndiffusion models exhibiting a stronger tendency to memorize and leak\ninformation. In this work, we investigate data memorization and extraction\nrisks in CLIP through the lens of CLIP inversion, a process that aims to\nreconstruct training images from text prompts. To this end, we introduce\n\\textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality,\nsemantically accurate image reconstruction from CLIP embeddings. We identify\nthree key challenges in CLIP inversion: 1) non-robust features, 2) limited\nvisual semantics in text embeddings, and 3) low reconstruction fidelity. To\naddress these challenges, LeakyCLIP employs 1) adversarial fine-tuning to\nenhance optimization smoothness, 2) linear transformation-based embedding\nalignment, and 3) Stable Diffusion-based refinement to improve fidelity.\nEmpirical results demonstrate the superiority of LeakyCLIP, achieving over 358%\nimprovement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared\nto baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive\nleakage risk, showing that training data membership can even be successfully\ninferred from the metrics of low-fidelity reconstructions. Our work introduces\na practical method for CLIP inversion while offering novel insights into the\nnature and scope of privacy risks in multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the memorization and privacy leakage risks in Contrastive\nLanguage--Image Pretraining (CLIP) is critical for ensuring the security of\nmultimodal models. Recent studies have demonstrated the feasibility of\nextracting sensitive training examples from diffusion models, with conditional\ndiffusion models exhibiting a stronger tendency to memorize and leak\ninformation. In this work, we investigate data memorization and extraction\nrisks in CLIP through the lens of CLIP inversion, a process that aims to\nreconstruct training images from text prompts. To this end, we introduce\n\\textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality,\nsemantically accurate image reconstruction from CLIP embeddings. We identify\nthree key challenges in CLIP inversion: 1) non-robust features, 2) limited\nvisual semantics in text embeddings, and 3) low reconstruction fidelity. To\naddress these challenges, LeakyCLIP employs 1) adversarial fine-tuning to\nenhance optimization smoothness, 2) linear transformation-based embedding\nalignment, and 3) Stable Diffusion-based refinement to improve fidelity.\nEmpirical results demonstrate the superiority of LeakyCLIP, achieving over 358%\nimprovement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared\nto baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive\nleakage risk, showing that training data membership can even be successfully\ninferred from the metrics of low-fidelity reconstructions. Our work introduces\na practical method for CLIP inversion while offering novel insights into the\nnature and scope of privacy risks in multimodal models."
                },
                "authors": [
                    {
                        "name": "Yunhao Chen"
                    },
                    {
                        "name": "Shujie Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xingjun Ma"
                },
                "author": "Xingjun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09751v2",
                "updated": "2025-08-01T16:30:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    30,
                    2,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-13T19:05:43Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    19,
                    5,
                    43,
                    6,
                    194,
                    0
                ],
                "title": "Sound and Complete Neurosymbolic Reasoning with LLM-Grounded\n  Interpretations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound and Complete Neurosymbolic Reasoning with LLM-Grounded\n  Interpretations"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneurosymbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneurosymbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties."
                },
                "authors": [
                    {
                        "name": "Bradley P. Allen"
                    },
                    {
                        "name": "Prateek Chhikara"
                    },
                    {
                        "name": "Thomas Macaulay Ferguson"
                    },
                    {
                        "name": "Filip Ilievski"
                    },
                    {
                        "name": "Paul Groth"
                    }
                ],
                "author_detail": {
                    "name": "Paul Groth"
                },
                "author": "Paul Groth",
                "arxiv_comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00751v1",
                "updated": "2025-08-01T16:28:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    28,
                    18,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:28:18Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    28,
                    18,
                    4,
                    213,
                    0
                ],
                "title": "Harnessing the Power of Interleaving and Counterfactual Evaluation for\n  Airbnb Search Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Power of Interleaving and Counterfactual Evaluation for\n  Airbnb Search Ranking"
                },
                "summary": "Evaluation plays a crucial role in the development of ranking algorithms on\nsearch and recommender systems. It enables online platforms to create\nuser-friendly features that drive commercial success in a steady and effective\nmanner. The online environment is particularly conducive to applying causal\ninference techniques, such as randomized controlled experiments (known as A/B\ntest), which are often more challenging to implement in fields like medicine\nand public policy. However, businesses face unique challenges when it comes to\neffective A/B test. Specifically, achieving sufficient statistical power for\nconversion-based metrics can be time-consuming, especially for significant\npurchases like booking accommodations. While offline evaluations are quicker\nand more cost-effective, they often lack accuracy and are inadequate for\nselecting candidates for A/B test. To address these challenges, we developed\ninterleaving and counterfactual evaluation methods to facilitate rapid online\nassessments for identifying the most promising candidates for A/B tests. Our\napproach not only increased the sensitivity of experiments by a factor of up to\n100 (depending on the approach and metrics) compared to traditional A/B testing\nbut also streamlined the experimental process. The practical insights gained\nfrom usage in production can also benefit organizations with similar interests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation plays a crucial role in the development of ranking algorithms on\nsearch and recommender systems. It enables online platforms to create\nuser-friendly features that drive commercial success in a steady and effective\nmanner. The online environment is particularly conducive to applying causal\ninference techniques, such as randomized controlled experiments (known as A/B\ntest), which are often more challenging to implement in fields like medicine\nand public policy. However, businesses face unique challenges when it comes to\neffective A/B test. Specifically, achieving sufficient statistical power for\nconversion-based metrics can be time-consuming, especially for significant\npurchases like booking accommodations. While offline evaluations are quicker\nand more cost-effective, they often lack accuracy and are inadequate for\nselecting candidates for A/B test. To address these challenges, we developed\ninterleaving and counterfactual evaluation methods to facilitate rapid online\nassessments for identifying the most promising candidates for A/B tests. Our\napproach not only increased the sensitivity of experiments by a factor of up to\n100 (depending on the approach and metrics) compared to traditional A/B testing\nbut also streamlined the experimental process. The practical insights gained\nfrom usage in production can also benefit organizations with similar interests."
                },
                "authors": [
                    {
                        "name": "Qing Zhang"
                    },
                    {
                        "name": "Alex Deng"
                    },
                    {
                        "name": "Michelle Du"
                    },
                    {
                        "name": "Huiji Gao"
                    },
                    {
                        "name": "Liwei He"
                    },
                    {
                        "name": "Sanjeev Katariya"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Katariya"
                },
                "author": "Sanjeev Katariya",
                "arxiv_doi": "10.1145/3711896.3737232",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711896.3737232",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.00751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages",
                "arxiv_journal_ref": "Proceedings of the 31st ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining V.2 (KDD 2025)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15867v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15867v3",
                "updated": "2025-08-01T16:25:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    25,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2024-11-24T15:06:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    15,
                    6,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "PanoLlama: Generating Endless and Coherent Panoramas with\n  Next-Token-Prediction LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PanoLlama: Generating Endless and Coherent Panoramas with\n  Next-Token-Prediction LLMs"
                },
                "summary": "Panoramic Image Generation (PIG) aims to create coherent images of arbitrary\nlengths. Most existing methods fall in the joint diffusion paradigm, but their\ncomplex and heuristic crop connection designs often limit their ability to\nachieve multilevel coherence. By deconstructing this challenge into its core\ncomponents, we find it naturally aligns with next-token prediction, leading us\nto adopt an autoregressive (AR) paradigm for PIG modeling. However, existing\nvisual AR (VAR) models are limited to fixed-size generation, lacking the\ncapability to produce panoramic images. In this paper, we propose PanoLlama, a\nnovel framework that achieves endless and coherent panorama generation with the\nautoregressive paradigm. Our approach develops a training-free strategy that\nutilizes token redirection to overcome the size limitations of existing VAR\nmodels, enabling next-crop prediction in both horizontal and vertical\ndirections. This refreshes the PIG pipeline while achieving SOTA performance in\ncoherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally,\nPanoLlama supports applications other PIG methods cannot achieve, including\nmask-free layout control, multi-scale and multi-guidance synthesis. To\nfacilitate standardized evaluation, we also establish a dataset with 1,000\nprompts spanning 100+ themes, providing a new testing benchmark for PIG\nresearch. The code is available at https://github.com/0606zt/PanoLlama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panoramic Image Generation (PIG) aims to create coherent images of arbitrary\nlengths. Most existing methods fall in the joint diffusion paradigm, but their\ncomplex and heuristic crop connection designs often limit their ability to\nachieve multilevel coherence. By deconstructing this challenge into its core\ncomponents, we find it naturally aligns with next-token prediction, leading us\nto adopt an autoregressive (AR) paradigm for PIG modeling. However, existing\nvisual AR (VAR) models are limited to fixed-size generation, lacking the\ncapability to produce panoramic images. In this paper, we propose PanoLlama, a\nnovel framework that achieves endless and coherent panorama generation with the\nautoregressive paradigm. Our approach develops a training-free strategy that\nutilizes token redirection to overcome the size limitations of existing VAR\nmodels, enabling next-crop prediction in both horizontal and vertical\ndirections. This refreshes the PIG pipeline while achieving SOTA performance in\ncoherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally,\nPanoLlama supports applications other PIG methods cannot achieve, including\nmask-free layout control, multi-scale and multi-guidance synthesis. To\nfacilitate standardized evaluation, we also establish a dataset with 1,000\nprompts spanning 100+ themes, providing a new testing benchmark for PIG\nresearch. The code is available at https://github.com/0606zt/PanoLlama."
                },
                "authors": [
                    {
                        "name": "Teng Zhou"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Yongchuan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yongchuan Tang"
                },
                "author": "Yongchuan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15867v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00746v1",
                "updated": "2025-08-01T16:21:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    21,
                    11,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:21:11Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    21,
                    11,
                    4,
                    213,
                    0
                ],
                "title": "GECO: Geometrically Consistent Embedding with Lightspeed Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GECO: Geometrically Consistent Embedding with Lightspeed Inference"
                },
                "summary": "Recent advances in feature learning have shown that self-supervised vision\nfoundation models can capture semantic correspondences but often lack awareness\nof underlying 3D geometry. GECO addresses this gap by producing geometrically\ncoherent features that semantically distinguish parts based on geometry (e.g.,\nleft/right eyes, front/back legs). We propose a training framework based on\noptimal transport, enabling supervision beyond keypoints, even under occlusions\nand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%\nfaster than prior methods, while achieving state-of-the-art performance on\nPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.\nFinally, we show that PCK alone is insufficient to capture geometric quality\nand introduce new metrics and insights for more geometry-aware feature\nlearning. Link to project page:\nhttps://reginehartwig.github.io/publications/geco/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in feature learning have shown that self-supervised vision\nfoundation models can capture semantic correspondences but often lack awareness\nof underlying 3D geometry. GECO addresses this gap by producing geometrically\ncoherent features that semantically distinguish parts based on geometry (e.g.,\nleft/right eyes, front/back legs). We propose a training framework based on\noptimal transport, enabling supervision beyond keypoints, even under occlusions\nand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%\nfaster than prior methods, while achieving state-of-the-art performance on\nPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.\nFinally, we show that PCK alone is insufficient to capture geometric quality\nand introduce new metrics and insights for more geometry-aware feature\nlearning. Link to project page:\nhttps://reginehartwig.github.io/publications/geco/"
                },
                "authors": [
                    {
                        "name": "Regine Hartwig"
                    },
                    {
                        "name": "Dominik Muhle"
                    },
                    {
                        "name": "Riccardo Marin"
                    },
                    {
                        "name": "Daniel Cremers"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Cremers"
                },
                "author": "Daniel Cremers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00743v1",
                "updated": "2025-08-01T16:18:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    18,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:18:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    18,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Agentic large language models improve retrieval-based radiology question\n  answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic large language models improve retrieval-based radiology question\n  answering"
                },
                "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility."
                },
                "authors": [
                    {
                        "name": "Sebastian Wind"
                    },
                    {
                        "name": "Jeta Sopa"
                    },
                    {
                        "name": "Daniel Truhn"
                    },
                    {
                        "name": "Mahshad Lotfinia"
                    },
                    {
                        "name": "Tri-Thien Nguyen"
                    },
                    {
                        "name": "Keno Bressem"
                    },
                    {
                        "name": "Lisa Adams"
                    },
                    {
                        "name": "Mirabela Rusu"
                    },
                    {
                        "name": "Harald Kstler"
                    },
                    {
                        "name": "Gerhard Wellein"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Soroosh Tayebi Arasteh"
                    }
                ],
                "author_detail": {
                    "name": "Soroosh Tayebi Arasteh"
                },
                "author": "Soroosh Tayebi Arasteh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22675v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22675v3",
                "updated": "2025-08-01T16:17:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    17,
                    57,
                    4,
                    213,
                    0
                ],
                "published": "2025-03-28T17:59:03Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    17,
                    59,
                    3,
                    4,
                    87,
                    0
                ],
                "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation"
                },
                "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation."
                },
                "authors": [
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yuning Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yuning Jiang"
                },
                "author": "Yuning Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22675v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22675v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00741v1",
                "updated": "2025-08-01T16:12:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    12,
                    23,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:12:23Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    12,
                    23,
                    4,
                    213,
                    0
                ],
                "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data\n  Leveraging Declarative Facts in Earlier Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data\n  Leveraging Declarative Facts in Earlier Training Data"
                },
                "summary": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety."
                },
                "authors": [
                    {
                        "name": "Sohaib Imran"
                    },
                    {
                        "name": "Rob Lamb"
                    },
                    {
                        "name": "Peter M. Atkinson"
                    }
                ],
                "author_detail": {
                    "name": "Peter M. Atkinson"
                },
                "author": "Peter M. Atkinson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00737v1",
                "updated": "2025-08-01T16:08:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    8,
                    5,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:08:05Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    8,
                    5,
                    4,
                    213,
                    0
                ],
                "title": "How LLMs are Shaping the Future of Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How LLMs are Shaping the Future of Virtual Reality"
                },
                "summary": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems."
                },
                "authors": [
                    {
                        "name": "Seda zkaya"
                    },
                    {
                        "name": "Santiago Berrezueta-Guzman"
                    },
                    {
                        "name": "Stefan Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wagner"
                },
                "author": "Stefan Wagner",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00736v1",
                "updated": "2025-08-01T16:06:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    6,
                    13,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:06:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    6,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "A normalizing flow approach for the inference of star cluster properties\n  from unresolved broadband photometry I: Comparison to spectral energy\n  distribution fitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A normalizing flow approach for the inference of star cluster properties\n  from unresolved broadband photometry I: Comparison to spectral energy\n  distribution fitting"
                },
                "summary": "Estimating properties of star clusters from unresolved broadband photometry\nis a challenging problem that is classically tackled by spectral energy\ndistribution (SED) fitting methods that are based on simple stellar population\n(SSP) models. However, because of their exponential scaling, grid-based methods\nsuffer from computational limitations. In addition, stochastic latent variables\nin the model can make the computation of the likelihood function intractable.\nThese limitations can be overcome by modern generative deep learning methods\nthat offer flexible and powerful tools for modeling high-dimensional posterior\ndistributions and fast inference from learned data. We present a normalizing\nflow approach for the inference of cluster age, mass, and reddening from Hubble\nSpace Telescope (HST) broadband photometry. In particular, we explore our\nnetwork's behavior on an inference problem that has been analyzed in previous\nworks. We used the SED modeling code CIGALE to create a dataset of synthetic\nphotometric observations for $5 \\times 10^6$ mock star clusters. Subsequently,\nthis data set was used to train a coupling-based flow in the form of a\nconditional invertible neural network (cINN) to predict posterior probability\ndistributions for cluster age, mass, and reddening from photometric\nobservations. We predicted cluster parameters for the 'Physics at High Angular\nresolution in Nearby GalaxieS' (PHANGS) Data Release 3 catalog. To evaluate the\ncapabilities of the network, we compared our results to the publicly available\nPHANGS estimates and found that the estimates agree reasonably well. We\ndemonstrate that normalizing flow methods can be a viable tool for the\ninference of cluster parameters, and argue that this approach is especially\nuseful when latent variables make the computation of the likelihood intractable\nand in scenarios that require efficient density estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating properties of star clusters from unresolved broadband photometry\nis a challenging problem that is classically tackled by spectral energy\ndistribution (SED) fitting methods that are based on simple stellar population\n(SSP) models. However, because of their exponential scaling, grid-based methods\nsuffer from computational limitations. In addition, stochastic latent variables\nin the model can make the computation of the likelihood function intractable.\nThese limitations can be overcome by modern generative deep learning methods\nthat offer flexible and powerful tools for modeling high-dimensional posterior\ndistributions and fast inference from learned data. We present a normalizing\nflow approach for the inference of cluster age, mass, and reddening from Hubble\nSpace Telescope (HST) broadband photometry. In particular, we explore our\nnetwork's behavior on an inference problem that has been analyzed in previous\nworks. We used the SED modeling code CIGALE to create a dataset of synthetic\nphotometric observations for $5 \\times 10^6$ mock star clusters. Subsequently,\nthis data set was used to train a coupling-based flow in the form of a\nconditional invertible neural network (cINN) to predict posterior probability\ndistributions for cluster age, mass, and reddening from photometric\nobservations. We predicted cluster parameters for the 'Physics at High Angular\nresolution in Nearby GalaxieS' (PHANGS) Data Release 3 catalog. To evaluate the\ncapabilities of the network, we compared our results to the publicly available\nPHANGS estimates and found that the estimates agree reasonably well. We\ndemonstrate that normalizing flow methods can be a viable tool for the\ninference of cluster parameters, and argue that this approach is especially\nuseful when latent variables make the computation of the likelihood intractable\nand in scenarios that require efficient density estimation."
                },
                "authors": [
                    {
                        "name": "Daniel Walter"
                    },
                    {
                        "name": "Victor F. Ksoll"
                    },
                    {
                        "name": "Ralf S. Klessen"
                    },
                    {
                        "name": "Mederic Boquien"
                    },
                    {
                        "name": "Aida Wofford"
                    },
                    {
                        "name": "Francesco Belfiore"
                    },
                    {
                        "name": "Daniel A. Dale"
                    },
                    {
                        "name": "Kathryn Grasha"
                    },
                    {
                        "name": "David A. Thilker"
                    },
                    {
                        "name": "Leonardo Ubeda"
                    },
                    {
                        "name": "Thomas G. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Thomas G. Williams"
                },
                "author": "Thomas G. Williams",
                "arxiv_comment": "13 pages, 10 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00733v1",
                "updated": "2025-08-01T16:03:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    3,
                    57,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:03:57Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    3,
                    57,
                    4,
                    213,
                    0
                ],
                "title": "AudioGen-Omni: A Unified Multimodal Diffusion Transformer for\n  Video-Synchronized Audio, Speech, and Song Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioGen-Omni: A Unified Multimodal Diffusion Transformer for\n  Video-Synchronized Audio, Speech, and Song Generation"
                },
                "summary": "We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality."
                },
                "authors": [
                    {
                        "name": "Le Wang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Feng Deng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Di Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Di Zhang"
                },
                "author": "Di Zhang",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13943v2",
                "updated": "2025-08-01T15:56:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    56,
                    30,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-20T05:22:17Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    5,
                    22,
                    17,
                    1,
                    140,
                    0
                ],
                "title": "From Press to Pixels: Evolving Urdu Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Press to Pixels: Evolving Urdu Text Recognition"
                },
                "summary": "This paper introduces an end-to-end pipeline for Optical Character\nRecognition (OCR) on Urdu newspapers, addressing challenges posed by complex\nmulti-column layouts, low-resolution scans, and the stylistic variability of\nthe Nastaliq script. Our system comprises four modules: (1) article\nsegmentation, (2) image super-resolution, (3) column segmentation, and (4) text\nrecognition. We fine-tune YOLOv11x for segmentation, achieving 0.963 precision\nfor articles and 0.970 for columns. A SwinIR-based super-resolution model\nboosts LLM text recognition accuracy by 25-70%. We also introduce the Urdu\nNewspaper Benchmark (UNB), a manually annotated dataset for Urdu OCR. Using UNB\nand the OpenITI corpus, we compare traditional CNN+RNN-based OCR models with\nmodern LLMs. Gemini-2.5-Pro achieves the best performance with a WER of 0.133.\nWe further analyze LLM outputs via insertion, deletion, and substitution error\nbreakdowns, as well as character-level confusion analysis. Finally, we show\nthat fine-tuning on just 500 samples yields a 6.13% WER improvement,\nhighlighting the adaptability of LLMs for Urdu OCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an end-to-end pipeline for Optical Character\nRecognition (OCR) on Urdu newspapers, addressing challenges posed by complex\nmulti-column layouts, low-resolution scans, and the stylistic variability of\nthe Nastaliq script. Our system comprises four modules: (1) article\nsegmentation, (2) image super-resolution, (3) column segmentation, and (4) text\nrecognition. We fine-tune YOLOv11x for segmentation, achieving 0.963 precision\nfor articles and 0.970 for columns. A SwinIR-based super-resolution model\nboosts LLM text recognition accuracy by 25-70%. We also introduce the Urdu\nNewspaper Benchmark (UNB), a manually annotated dataset for Urdu OCR. Using UNB\nand the OpenITI corpus, we compare traditional CNN+RNN-based OCR models with\nmodern LLMs. Gemini-2.5-Pro achieves the best performance with a WER of 0.133.\nWe further analyze LLM outputs via insertion, deletion, and substitution error\nbreakdowns, as well as character-level confusion analysis. Finally, we show\nthat fine-tuning on just 500 samples yields a 6.13% WER improvement,\nhighlighting the adaptability of LLMs for Urdu OCR."
                },
                "authors": [
                    {
                        "name": "Samee Arif"
                    },
                    {
                        "name": "Sualeha Farid"
                    }
                ],
                "author_detail": {
                    "name": "Sualeha Farid"
                },
                "author": "Sualeha Farid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02748v2",
                "updated": "2025-08-01T15:45:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    45,
                    13,
                    4,
                    213,
                    0
                ],
                "published": "2025-01-06T03:52:16Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    3,
                    52,
                    16,
                    0,
                    6,
                    0
                ],
                "title": "Probing the Merger Rates of Supermassive Black Holes and Galaxies with\n  Gravitational Waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Merger Rates of Supermassive Black Holes and Galaxies with\n  Gravitational Waves"
                },
                "summary": "The mergers of galaxies and supermassive black holes (SMBHs) are key drivers\nof galaxy evolution, contributing to the growth of both galaxies and their\ncentral black holes. Current and upcoming gravitational wave (GW) detectors --\nPulsar Timing Arrays (PTAs), LISA, Taiji, and Tianqin -- offer unique access to\nthese processes by observing GW signals from SMBH binaries. We present a\nframework to infer galaxy and SMBH merger rates by combining mock LISA\ndetections of SMBH mergers with PTA constraints on the stochastic GW\nbackground, while incorporating observational uncertainties in stellar mass\nfunctions and $M_\\bullet$-$M_*$ relations. We find that the number of\nLISA-detected events and their joint distribution in mass and redshift are key\nto constraining merger rates -- datasets with around forty events yield results\nconsistent with galaxy pair observations, whereas limited event counts lead to\nbiases at high redshift. Including PTA data further reduces parameter\nuncertainties. Our method also effectively constrains the delay time between\ngalaxy and SMBH mergers, with longer delays suppressing high-redshift SMBH\nmerger rates and shifting mass growth from mergers to accretion. According to\nour mock analysis, the models with delay times longer than $0.5\\text{Gyr}$\n($0.8\\text{Gyr}$), accretion becomes the primary driver of SMBH mass growth\nbeyond $z \\sim 6$ ($4$). In contrast, the SMBH occupation fraction at $z>3$\nremains poorly constrained due to its degeneracies with delay time and the\ngalaxy merger rate. These findings highlight both the promise and limitations\nof using GW observations to probe the coevolution of galaxies and SMBHs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mergers of galaxies and supermassive black holes (SMBHs) are key drivers\nof galaxy evolution, contributing to the growth of both galaxies and their\ncentral black holes. Current and upcoming gravitational wave (GW) detectors --\nPulsar Timing Arrays (PTAs), LISA, Taiji, and Tianqin -- offer unique access to\nthese processes by observing GW signals from SMBH binaries. We present a\nframework to infer galaxy and SMBH merger rates by combining mock LISA\ndetections of SMBH mergers with PTA constraints on the stochastic GW\nbackground, while incorporating observational uncertainties in stellar mass\nfunctions and $M_\\bullet$-$M_*$ relations. We find that the number of\nLISA-detected events and their joint distribution in mass and redshift are key\nto constraining merger rates -- datasets with around forty events yield results\nconsistent with galaxy pair observations, whereas limited event counts lead to\nbiases at high redshift. Including PTA data further reduces parameter\nuncertainties. Our method also effectively constrains the delay time between\ngalaxy and SMBH mergers, with longer delays suppressing high-redshift SMBH\nmerger rates and shifting mass growth from mergers to accretion. According to\nour mock analysis, the models with delay times longer than $0.5\\text{Gyr}$\n($0.8\\text{Gyr}$), accretion becomes the primary driver of SMBH mass growth\nbeyond $z \\sim 6$ ($4$). In contrast, the SMBH occupation fraction at $z>3$\nremains poorly constrained due to its degeneracies with delay time and the\ngalaxy merger rate. These findings highlight both the promise and limitations\nof using GW observations to probe the coevolution of galaxies and SMBHs."
                },
                "authors": [
                    {
                        "name": "Yun Fang"
                    },
                    {
                        "name": "Rong-Gen Cai"
                    }
                ],
                "author_detail": {
                    "name": "Rong-Gen Cai"
                },
                "author": "Rong-Gen Cai",
                "arxiv_comment": "accepted by MNRAS, 16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00719v1",
                "updated": "2025-08-01T15:38:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    38,
                    21,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:38:21Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    38,
                    21,
                    4,
                    213,
                    0
                ],
                "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA"
                },
                "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Shiqi Fan"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Siwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Siwei Liu"
                },
                "author": "Siwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08441v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08441v3",
                "updated": "2025-08-01T15:28:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    28,
                    51,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-12T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    17,
                    2,
                    43,
                    0
                ],
                "title": "Better Embeddings with Coupled Adam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Embeddings with Coupled Adam"
                },
                "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets."
                },
                "authors": [
                    {
                        "name": "Felix Stollenwerk"
                    },
                    {
                        "name": "Tobias Stollenwerk"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Stollenwerk"
                },
                "author": "Tobias Stollenwerk",
                "arxiv_comment": "ACL 2025 (Main), see https://aclanthology.org/2025.acl-long.1321/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08441v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08441v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13900v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13900v3",
                "updated": "2025-08-01T15:27:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    27,
                    59,
                    4,
                    213,
                    0
                ],
                "published": "2024-07-18T21:06:39Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    21,
                    6,
                    39,
                    3,
                    200,
                    0
                ],
                "title": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools"
                },
                "summary": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software -- leading to new frontiers in software\nengineering (SE). The advanced capabilities of generative AI tools to support\nsoftware development tasks have led to a rise in their adoption within software\ndevelopment workflows. However, little is known about how AI tools perceive\nevidence-based beliefs and practices verified by research findings. To this\nend, we conduct a preliminary evaluation conceptually replicating prior work to\nexplore the \"beliefs\" of generative AI tools used to support software\ndevelopment tasks. We investigate 17 evidence-based claims posited by empirical\nSE research across five generative AI tools. Our findings show that generative\nAI tools have ambiguous beliefs regarding research claims and lack credible\nevidence to support responses. Based on our results, we provide implications\nfor practitioners integrating generative AI-based systems into development\ncontexts and shed light on future research directions to enhance the\nreliability and trustworthiness of generative AI -- aiming to increase\nawareness and adoption of evidence-based SE research findings in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software -- leading to new frontiers in software\nengineering (SE). The advanced capabilities of generative AI tools to support\nsoftware development tasks have led to a rise in their adoption within software\ndevelopment workflows. However, little is known about how AI tools perceive\nevidence-based beliefs and practices verified by research findings. To this\nend, we conduct a preliminary evaluation conceptually replicating prior work to\nexplore the \"beliefs\" of generative AI tools used to support software\ndevelopment tasks. We investigate 17 evidence-based claims posited by empirical\nSE research across five generative AI tools. Our findings show that generative\nAI tools have ambiguous beliefs regarding research claims and lack credible\nevidence to support responses. Based on our results, we provide implications\nfor practitioners integrating generative AI-based systems into development\ncontexts and shed light on future research directions to enhance the\nreliability and trustworthiness of generative AI -- aiming to increase\nawareness and adoption of evidence-based SE research findings in practice."
                },
                "authors": [
                    {
                        "name": "Chris Brown"
                    },
                    {
                        "name": "Jason Cusati"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cusati"
                },
                "author": "Jason Cusati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13900v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13900v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00709v1",
                "updated": "2025-08-01T15:23:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    23,
                    20,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:23:20Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    23,
                    20,
                    4,
                    213,
                    0
                ],
                "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian\n  Common Law System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian\n  Common Law System"
                },
                "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Balaramamahanthi Deepak Patnaik"
                    },
                    {
                        "name": "Shivam Mishra"
                    },
                    {
                        "name": "Ajay Varghese Thomas"
                    },
                    {
                        "name": "Noel Shallum"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00705v1",
                "updated": "2025-08-01T15:20:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    20,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:20:24Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    20,
                    24,
                    4,
                    213,
                    0
                ],
                "title": "An Online Data Analysis Framework for Accelerator-Based Physics\n  Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Online Data Analysis Framework for Accelerator-Based Physics\n  Experiments"
                },
                "summary": "A robust and flexible architecture capable of providing real-time analysis on\ndiagnostic data in experimental physics is of crucial importance to physics\nexperiments. In this paper, we present such an online framework, used in June\n2025 as part of the HRMT-68 experiment, performed at the HiRadMat facility at\nCERN, using the Super Proton Synchrotron (SPS) beam line. HRMT-68 was a\nfixed-target laboratory astrophysics experiment aiming to identify plasma\ninstabilities generated by a relativistic electron-positron beam during\ntraversal of an argon plasma. This framework was essential for experimental\ndata acquisition and analysis, and can be adapted for a broad range of\nexperiments with a variety of experimental diagnostics. The framework's modular\nand customizable design enabled us to rapidly observe and extract emergent\nfeatures from a diverse range of diagnostic data. Simultaneously, it allowed\nfor both the introduction of new diagnostic devices and the modification of our\nanalysis as features of interest were identified. As a result, we were able to\neffectively diagnose equipment malfunction, and infer the beam's response to\nvarying bunch duration, beam intensity, and the plasma state without resorting\nto offline analysis, at which time adjustment or improvement would have been\nimpossible. We present the features of this agile framework, whose codebase we\nhave made publicly available, which can be adapted for future experiments with\nminimal modification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A robust and flexible architecture capable of providing real-time analysis on\ndiagnostic data in experimental physics is of crucial importance to physics\nexperiments. In this paper, we present such an online framework, used in June\n2025 as part of the HRMT-68 experiment, performed at the HiRadMat facility at\nCERN, using the Super Proton Synchrotron (SPS) beam line. HRMT-68 was a\nfixed-target laboratory astrophysics experiment aiming to identify plasma\ninstabilities generated by a relativistic electron-positron beam during\ntraversal of an argon plasma. This framework was essential for experimental\ndata acquisition and analysis, and can be adapted for a broad range of\nexperiments with a variety of experimental diagnostics. The framework's modular\nand customizable design enabled us to rapidly observe and extract emergent\nfeatures from a diverse range of diagnostic data. Simultaneously, it allowed\nfor both the introduction of new diagnostic devices and the modification of our\nanalysis as features of interest were identified. As a result, we were able to\neffectively diagnose equipment malfunction, and infer the beam's response to\nvarying bunch duration, beam intensity, and the plasma state without resorting\nto offline analysis, at which time adjustment or improvement would have been\nimpossible. We present the features of this agile framework, whose codebase we\nhave made publicly available, which can be adapted for future experiments with\nminimal modification."
                },
                "authors": [
                    {
                        "name": "Hayden Ramm"
                    },
                    {
                        "name": "Pascal Simon"
                    },
                    {
                        "name": "Paraskevi Alexaki"
                    },
                    {
                        "name": "Christopher Arran"
                    },
                    {
                        "name": "Robert Bingham"
                    },
                    {
                        "name": "Alice Goillot"
                    },
                    {
                        "name": "Jon Tomas Gudmundsson"
                    },
                    {
                        "name": "Jonathan Halliday"
                    },
                    {
                        "name": "Bryn Lloyd"
                    },
                    {
                        "name": "Eva Los"
                    },
                    {
                        "name": "Vasiliki Stergiou"
                    },
                    {
                        "name": "Sifei Zhang"
                    },
                    {
                        "name": "Gianluca Gregori"
                    },
                    {
                        "name": "Nikolaos Charitonidis"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Charitonidis"
                },
                "author": "Nikolaos Charitonidis",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07699v2",
                "updated": "2025-08-01T15:19:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    19,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-11T16:51:25Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    51,
                    25,
                    1,
                    42,
                    0
                ],
                "title": "Sharp Anti-Concentration Inequalities for Extremum Statistics via\n  Copulas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Anti-Concentration Inequalities for Extremum Statistics via\n  Copulas"
                },
                "summary": "We derive sharp upper and lower bounds for the pointwise concentration\nfunction of the maximum statistic of $d$ identically distributed real-valued\nrandom variables. Our first main result places no restrictions either on the\ncommon marginal law of the samples or on the copula describing their joint\ndistribution. We show that, in general, strictly sublinear dependence of the\nconcentration function on the dimension $d$ is not possible. We then introduce\na new class of copulas, namely those with a convex diagonal section, and\ndemonstrate that restricting to this class yields a sharper upper bound on the\nconcentration function. This allows us to establish several new\ndimension-independent and poly-logarithmic-in-$d$ anti-concentration\ninequalities for a variety of marginal distributions under mild dependence\nassumptions. Our theory improves upon the best known results in certain special\ncases. Applications to high-dimensional statistical inference are presented,\nincluding a specific example pertaining to Gaussian mixture approximations for\nfactor models, for which our main results lead to superior distributional\nguarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We derive sharp upper and lower bounds for the pointwise concentration\nfunction of the maximum statistic of $d$ identically distributed real-valued\nrandom variables. Our first main result places no restrictions either on the\ncommon marginal law of the samples or on the copula describing their joint\ndistribution. We show that, in general, strictly sublinear dependence of the\nconcentration function on the dimension $d$ is not possible. We then introduce\na new class of copulas, namely those with a convex diagonal section, and\ndemonstrate that restricting to this class yields a sharper upper bound on the\nconcentration function. This allows us to establish several new\ndimension-independent and poly-logarithmic-in-$d$ anti-concentration\ninequalities for a variety of marginal distributions under mild dependence\nassumptions. Our theory improves upon the best known results in certain special\ncases. Applications to high-dimensional statistical inference are presented,\nincluding a specific example pertaining to Gaussian mixture approximations for\nfactor models, for which our main results lead to superior distributional\nguarantees."
                },
                "authors": [
                    {
                        "name": "Matias D. Cattaneo"
                    },
                    {
                        "name": "Ricardo P. Masini"
                    },
                    {
                        "name": "William G. Underwood"
                    }
                ],
                "author_detail": {
                    "name": "William G. Underwood"
                },
                "author": "William G. Underwood",
                "arxiv_comment": "24 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60E15 (Primary) 62H05, 62G32 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00700v1",
                "updated": "2025-08-01T15:17:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    17,
                    34,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:17:34Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    17,
                    34,
                    4,
                    213,
                    0
                ],
                "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written\n  Code?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written\n  Code?"
                },
                "summary": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation."
                },
                "authors": [
                    {
                        "name": "Alfred Santa Molison"
                    },
                    {
                        "name": "Marcia Moraes"
                    },
                    {
                        "name": "Glaucia Melo"
                    },
                    {
                        "name": "Fabio Santos"
                    },
                    {
                        "name": "Wesley K. G. Assuncao"
                    }
                ],
                "author_detail": {
                    "name": "Wesley K. G. Assuncao"
                },
                "author": "Wesley K. G. Assuncao",
                "arxiv_comment": "Accepted ESEM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00696v1",
                "updated": "2025-08-01T15:12:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    12,
                    58,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:12:58Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    12,
                    58,
                    4,
                    213,
                    0
                ],
                "title": "Online Rolling Controlled Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Rolling Controlled Sequential Monte Carlo"
                },
                "summary": "We introduce methodology for real-time inference in general-state-space\nhidden Markov models. Specifically, we extend recent advances in controlled\nsequential Monte Carlo (CSMC) methods-originally proposed for offline\nsmoothing-to the online setting via a rolling window mechanism. Our novel\nonline rolling controlled sequential Monte Carlo (ORCSMC) algorithm employs two\nparticle systems to simultaneously estimate twisting functions and perform\nfiltering, ensuring real-time adaptivity to new observations while maintaining\nbounded computational cost. Numerical results on linear-Gaussian, stochastic\nvolatility, and neuroscience models demonstrate improved estimation accuracy\nand robustness in higher dimensions, compared to standard particle filtering\napproaches. The method offers a statistically efficient and practical solution\nfor sequential and real-time inference in complex latent variable models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce methodology for real-time inference in general-state-space\nhidden Markov models. Specifically, we extend recent advances in controlled\nsequential Monte Carlo (CSMC) methods-originally proposed for offline\nsmoothing-to the online setting via a rolling window mechanism. Our novel\nonline rolling controlled sequential Monte Carlo (ORCSMC) algorithm employs two\nparticle systems to simultaneously estimate twisting functions and perform\nfiltering, ensuring real-time adaptivity to new observations while maintaining\nbounded computational cost. Numerical results on linear-Gaussian, stochastic\nvolatility, and neuroscience models demonstrate improved estimation accuracy\nand robustness in higher dimensions, compared to standard particle filtering\napproaches. The method offers a statistically efficient and practical solution\nfor sequential and real-time inference in complex latent variable models."
                },
                "authors": [
                    {
                        "name": "Liwen Xue"
                    },
                    {
                        "name": "Axel Finke"
                    },
                    {
                        "name": "Adam M. Johansen"
                    }
                ],
                "author_detail": {
                    "name": "Adam M. Johansen"
                },
                "author": "Adam M. Johansen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02724v2",
                "updated": "2025-08-01T15:04:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    4,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-03T15:41:04Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    41,
                    4,
                    3,
                    184,
                    0
                ],
                "title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein\n  Interaction Prediction Across Organisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein\n  Interaction Prediction Across Organisms"
                },
                "summary": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data."
                },
                "authors": [
                    {
                        "name": "Shiyi Liu"
                    },
                    {
                        "name": "Buwen Liang"
                    },
                    {
                        "name": "Yuetong Fang"
                    },
                    {
                        "name": "Zixuan Jiang"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00680v1",
                "updated": "2025-08-01T14:49:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    49,
                    50,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:49:50Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    49,
                    50,
                    4,
                    213,
                    0
                ],
                "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Call Claude: Can LLMs Detect Changes of Writing Style?"
                },
                "summary": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported."
                },
                "authors": [
                    {
                        "name": "Johannes Rmisch"
                    },
                    {
                        "name": "Svetlana Gorovaia"
                    },
                    {
                        "name": "Mariia Halchynska"
                    },
                    {
                        "name": "Gleb Schmidt"
                    },
                    {
                        "name": "Ivan P. Yamshchikov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan P. Yamshchikov"
                },
                "author": "Ivan P. Yamshchikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00673v1",
                "updated": "2025-08-01T14:46:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    46,
                    57,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:46:57Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    46,
                    57,
                    4,
                    213,
                    0
                ],
                "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of\n  Culture in Persian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MELAC: Massive Evaluation of Large Language Models with Alignment of\n  Culture in Persian Language"
                },
                "summary": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field."
                },
                "authors": [
                    {
                        "name": "Farhan Farsi"
                    },
                    {
                        "name": "Farnaz Aghababaloo"
                    },
                    {
                        "name": "Shahriar Shariati Motlagh"
                    },
                    {
                        "name": "Parsa Ghofrani"
                    },
                    {
                        "name": "MohammadAli SadraeiJavaheri"
                    },
                    {
                        "name": "Shayan Bali"
                    },
                    {
                        "name": "Amirhossein Shabani"
                    },
                    {
                        "name": "Farbod Bijary"
                    },
                    {
                        "name": "Ghazal Zamaninejad"
                    },
                    {
                        "name": "AmirMohammad Salehoof"
                    },
                    {
                        "name": "Saeedeh Momtazi"
                    }
                ],
                "author_detail": {
                    "name": "Saeedeh Momtazi"
                },
                "author": "Saeedeh Momtazi",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00669v1",
                "updated": "2025-08-01T14:41:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    41,
                    31,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:41:31Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    41,
                    31,
                    4,
                    213,
                    0
                ],
                "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement\n  Techniques and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement\n  Techniques and Applications"
                },
                "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI."
                },
                "authors": [
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Zizhan Ma"
                    },
                    {
                        "name": "Meidan Ding"
                    },
                    {
                        "name": "Shiyi Zheng"
                    },
                    {
                        "name": "Shengyuan Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Wenting Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Linlin Shen"
                    },
                    {
                        "name": "Yixuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Yuan"
                },
                "author": "Yixuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00667v1",
                "updated": "2025-08-01T14:40:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    40,
                    45,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:40:45Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    40,
                    45,
                    4,
                    213,
                    0
                ],
                "title": "Dependence of halo properties on central-satellite magnitude gaps\n  through weak lensing measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dependence of halo properties on central-satellite magnitude gaps\n  through weak lensing measurements"
                },
                "summary": "The magnitude gap between the central and satellite galaxies encodes\ninformation about the mass accretion history of a dark matter halo, and serves\nas a useful observational probe for the mass distribution in a halo. In this\nwork, we perform the first weak lensing test of the connections between the\nmagnitude gap and the halo profile. We measure the halo profiles of isolated\ncentral galaxies (ICGs) selected primarily from the SDSS Main Galaxy Sample.\nHalo mass and concentration are inferred by fitting stacked lensing profiles in\nbins of central luminosity, $L_\\mathrm{c}$, and the central-satellite magnitude\ngap, $L_\\mathrm{gap}$. We detect dependence on the magnitude gap in both halo\nproperties. The dependence is the strongest in the ICG luminosity range of\n$10^{10.3}<L_\\mathrm{c}[h^{-2}L_\\odot]\\leq 10^{10.7}$, where halos with smaller\ngaps have higher masses and lower concentrations. When $10^{10.7}\n<L_c[h^{-2}L_\\odot] \\leq 10^{11.1}$, however, no significant gap dependence is\ndetected. In the range of $10^{9.9}<L_\\mathrm{c}[h^{-2}L_\\odot] \\leq\n10^{10.3}$, a disordering of the gap dependence is marginally observable. We\ncompare the observational results with predictions by two lightcone catalogs\nbuilt from the Illustris TNG300 and the Millennium simulations. The gap\ndependence in the two mock samples show overall consistency with observations,\nbut neither matches them in all $L_\\mathrm{c}$ bins to a quantitative level. We\nalso compare the significance of the gap dependence on halo mass and\nconcentration and find that our measurement prefers gap dependence in both\nparameters, while the halo mass dependence is preferred over the concentration\nif only one of the two dependencies is allowed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The magnitude gap between the central and satellite galaxies encodes\ninformation about the mass accretion history of a dark matter halo, and serves\nas a useful observational probe for the mass distribution in a halo. In this\nwork, we perform the first weak lensing test of the connections between the\nmagnitude gap and the halo profile. We measure the halo profiles of isolated\ncentral galaxies (ICGs) selected primarily from the SDSS Main Galaxy Sample.\nHalo mass and concentration are inferred by fitting stacked lensing profiles in\nbins of central luminosity, $L_\\mathrm{c}$, and the central-satellite magnitude\ngap, $L_\\mathrm{gap}$. We detect dependence on the magnitude gap in both halo\nproperties. The dependence is the strongest in the ICG luminosity range of\n$10^{10.3}<L_\\mathrm{c}[h^{-2}L_\\odot]\\leq 10^{10.7}$, where halos with smaller\ngaps have higher masses and lower concentrations. When $10^{10.7}\n<L_c[h^{-2}L_\\odot] \\leq 10^{11.1}$, however, no significant gap dependence is\ndetected. In the range of $10^{9.9}<L_\\mathrm{c}[h^{-2}L_\\odot] \\leq\n10^{10.3}$, a disordering of the gap dependence is marginally observable. We\ncompare the observational results with predictions by two lightcone catalogs\nbuilt from the Illustris TNG300 and the Millennium simulations. The gap\ndependence in the two mock samples show overall consistency with observations,\nbut neither matches them in all $L_\\mathrm{c}$ bins to a quantitative level. We\nalso compare the significance of the gap dependence on halo mass and\nconcentration and find that our measurement prefers gap dependence in both\nparameters, while the halo mass dependence is preferred over the concentration\nif only one of the two dependencies is allowed."
                },
                "authors": [
                    {
                        "name": "Mingtao Yang"
                    },
                    {
                        "name": "Jiaxin Han"
                    },
                    {
                        "name": "Wenting Wang"
                    },
                    {
                        "name": "Hekun Li"
                    },
                    {
                        "name": "Cong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shuai Feng"
                    },
                    {
                        "name": "Shiyin Shen"
                    },
                    {
                        "name": "Zhenjie Liu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    },
                    {
                        "name": "Yi Lu"
                    },
                    {
                        "name": "Surhud More"
                    }
                ],
                "author_detail": {
                    "name": "Surhud More"
                },
                "author": "Surhud More",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08395v2",
                "updated": "2025-08-01T14:26:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    26,
                    57,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-12T13:37:03Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    37,
                    3,
                    2,
                    43,
                    0
                ],
                "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance"
                },
                "summary": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them."
                },
                "authors": [
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21696v2",
                "updated": "2025-08-01T14:25:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    25,
                    34,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-29T11:20:03Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    11,
                    20,
                    3,
                    1,
                    210,
                    0
                ],
                "title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN"
                },
                "summary": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations."
                },
                "authors": [
                    {
                        "name": "Abdelaziz Salama"
                    },
                    {
                        "name": "Zeinab Nezami"
                    },
                    {
                        "name": "Mohammed M. H. Qazzaz"
                    },
                    {
                        "name": "Maryam Hafeez"
                    },
                    {
                        "name": "Syed Ali Raza Zaidi"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ali Raza Zaidi"
                },
                "author": "Syed Ali Raza Zaidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00658v1",
                "updated": "2025-08-01T14:22:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    22,
                    51,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:22:51Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    22,
                    51,
                    4,
                    213,
                    0
                ],
                "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for\n  Causal Time Series Inference across Frequencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for\n  Causal Time Series Inference across Frequencies"
                },
                "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available."
                },
                "authors": [
                    {
                        "name": "Chakattrai Sookkongwaree"
                    },
                    {
                        "name": "Tattep Lakmuang"
                    },
                    {
                        "name": "Chainarong Amornbunchornvej"
                    }
                ],
                "author_detail": {
                    "name": "Chainarong Amornbunchornvej"
                },
                "author": "Chainarong Amornbunchornvej",
                "arxiv_comment": "First draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00642v1",
                "updated": "2025-08-01T13:56:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    56,
                    39,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:56:39Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    56,
                    39,
                    4,
                    213,
                    0
                ],
                "title": "Prototype Development and Calibration of the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prototype Development and Calibration of the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The space-based CUbesat Solar Polarimeter (CUSP) mission aims to measure the\nlinear polarization of solar flares in the hard X-ray band by means of a\nCompton scattering polarimeter. CUSP will allow to study the magnetic\nreconnection and particle acceleration in the flaring magnetic structures of\nour star with its unprecedented sensitivity to solar flare polarization. CUSP\nis a project in the framework of the Alcor Program of the Italian Space Agency\naimed to develop new CubeSat missions. It has been proposed as a constellation\nof a two Cubesat mission to monitor the Sun for Space Weather, and will proceed\nwith a single-satellite asset in its baseline implementation.\n  In the frame of CUSP's Phase B study, that started in December 2024 for a\n1-year period, we present the development status of this dual-phase\npolarimeter. Preliminary laboratory results using two chains of acquisition\nwill be discussed. The first chain of acquisition, based on the Hamamatsu R7600\nmulti-anode photomultiplier tubes coupled to plastic scintillator bars and read\nout by the MAROC-3A ASIC, is used to detect the Compton scattering of incoming\nphotons. On the other hand, GAGG crystals coupled to avalanche photo-diodes\nwith a readout based on the SKIROC-2A ASIC are used to absorb the scattered\nphotons. By reconstructing the azimuthal scattering direction for many incoming\nphotons, one can infer the linear polarization degree and angle of the source.\nWe will discuss the calibration results obtained with our prototype detector by\nusing well-known radioactive isotopes, allowing us to assess the performances\nof our detector over the full 25-100 keV energy range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The space-based CUbesat Solar Polarimeter (CUSP) mission aims to measure the\nlinear polarization of solar flares in the hard X-ray band by means of a\nCompton scattering polarimeter. CUSP will allow to study the magnetic\nreconnection and particle acceleration in the flaring magnetic structures of\nour star with its unprecedented sensitivity to solar flare polarization. CUSP\nis a project in the framework of the Alcor Program of the Italian Space Agency\naimed to develop new CubeSat missions. It has been proposed as a constellation\nof a two Cubesat mission to monitor the Sun for Space Weather, and will proceed\nwith a single-satellite asset in its baseline implementation.\n  In the frame of CUSP's Phase B study, that started in December 2024 for a\n1-year period, we present the development status of this dual-phase\npolarimeter. Preliminary laboratory results using two chains of acquisition\nwill be discussed. The first chain of acquisition, based on the Hamamatsu R7600\nmulti-anode photomultiplier tubes coupled to plastic scintillator bars and read\nout by the MAROC-3A ASIC, is used to detect the Compton scattering of incoming\nphotons. On the other hand, GAGG crystals coupled to avalanche photo-diodes\nwith a readout based on the SKIROC-2A ASIC are used to absorb the scattered\nphotons. By reconstructing the azimuthal scattering direction for many incoming\nphotons, one can infer the linear polarization degree and angle of the source.\nWe will discuss the calibration results obtained with our prototype detector by\nusing well-known radioactive isotopes, allowing us to assess the performances\nof our detector over the full 25-100 keV energy range."
                },
                "authors": [
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "7 pages, 4 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00636v1",
                "updated": "2025-08-01T13:51:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    51,
                    25,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:51:25Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    51,
                    25,
                    4,
                    213,
                    0
                ],
                "title": "FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning\n  with Major Malicious Clients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning\n  with Major Malicious Clients"
                },
                "summary": "Federated learning is a distributed training framework vulnerable to\nByzantine attacks, particularly when over 50% of clients are malicious or when\ndatasets are highly non-independent and identically distributed (non-IID).\nAdditionally, most existing defense mechanisms are designed for specific attack\ntypes (e.g., gradient similarity-based schemes can only defend against outlier\nmodel poisoning), limiting their effectiveness. In response, we propose\nFedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the\naforementioned issues by leveraging the high sensitivity of membership\ninference to model bias. By requiring clients to include an additional\nmini-batch of server-specified data in their training, FedGuard can identify\nand exclude poisoned models, as their confidence in the mini-batch will drop\nsignificantly. Our comprehensive evaluation unequivocally shows that, under\nthree highly non-IID datasets, with 90% of clients being Byzantine and seven\ndifferent types of Byzantine attacks occurring in each round, FedGuard\nsignificantly outperforms existing robust federated learning schemes in\nmitigating various types of Byzantine attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning is a distributed training framework vulnerable to\nByzantine attacks, particularly when over 50% of clients are malicious or when\ndatasets are highly non-independent and identically distributed (non-IID).\nAdditionally, most existing defense mechanisms are designed for specific attack\ntypes (e.g., gradient similarity-based schemes can only defend against outlier\nmodel poisoning), limiting their effectiveness. In response, we propose\nFedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the\naforementioned issues by leveraging the high sensitivity of membership\ninference to model bias. By requiring clients to include an additional\nmini-batch of server-specified data in their training, FedGuard can identify\nand exclude poisoned models, as their confidence in the mini-batch will drop\nsignificantly. Our comprehensive evaluation unequivocally shows that, under\nthree highly non-IID datasets, with 90% of clients being Byzantine and seven\ndifferent types of Byzantine attacks occurring in each round, FedGuard\nsignificantly outperforms existing robust federated learning schemes in\nmitigating various types of Byzantine attacks."
                },
                "authors": [
                    {
                        "name": "Haocheng Jiang"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Jixin Zhang"
                    },
                    {
                        "name": "Willy Susilo"
                    },
                    {
                        "name": "Mingwu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingwu Zhang"
                },
                "author": "Mingwu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00632v1",
                "updated": "2025-08-01T13:45:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    45,
                    13,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:45:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    45,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings"
                },
                "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches."
                },
                "authors": [
                    {
                        "name": "Alexia Jolicoeur-Martineau"
                    }
                ],
                "author_detail": {
                    "name": "Alexia Jolicoeur-Martineau"
                },
                "author": "Alexia Jolicoeur-Martineau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00630v1",
                "updated": "2025-08-01T13:41:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    41,
                    58,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:41:58Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    41,
                    58,
                    4,
                    213,
                    0
                ],
                "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCeT: Behavioral Model Correctness Evaluation using Large Language\n  Models"
                },
                "summary": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram."
                },
                "authors": [
                    {
                        "name": "Khaled Ahmed"
                    },
                    {
                        "name": "Jialing Song"
                    },
                    {
                        "name": "Boqi Chen"
                    },
                    {
                        "name": "Ou Wei"
                    },
                    {
                        "name": "Bingzhou Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bingzhou Zheng"
                },
                "author": "Bingzhou Zheng",
                "arxiv_comment": "MODELS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15268v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15268v9",
                "updated": "2025-08-01T13:40:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    1,
                    4,
                    213,
                    0
                ],
                "published": "2025-04-21T17:52:36Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    52,
                    36,
                    0,
                    111,
                    0
                ],
                "title": "Beyond Correlation: Positive Definite Dependence Measures for Robust\n  Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Correlation: Positive Definite Dependence Measures for Robust\n  Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios"
                },
                "summary": "We live in a multivariate world, and effective modeling of financial\nportfolios, including their construction, allocation, forecasting, and risk\nanalysis, simply is not possible without explicitly modeling the dependence\nstructure of their assets. Dependence structure can drive portfolio results\nmore than the combined effects of other parameters in investment and risk\nmodels, but the literature provides relatively little to define the\nfinite-sample distributions of dependence measures in useable and useful ways\nunder challenging, real-world financial data conditions. Yet this is exactly\nwhat is needed to make valid inferences about their estimates, and to use these\ninferences for essential purposes such as hypothesis testing, dynamic\nmonitoring, realistic and granular scenario and reverse scenario analyses, and\nmitigating the effects of correlation breakdowns during market upheavals. This\nwork develops a new and straightforward method, Nonparametric Angles-based\nCorrelation (NAbC), for defining the finite-sample distributions of any\ndependence measure whose matrix of pairwise associations is positive definite\n(e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). NAbC\nremains valid under marginal asset distributions with notably different and\nvarying degrees of serial correlation, non-stationarity, heavy-tailedness, and\nasymmetry. NAbC provides p-values and confidence intervals at both the matrix\nlevel and the pairwise cell level, for both one and two-sample tests, with\nanalytical consistency across levels. Finally, NAbC maintains validity even\nwhen selected cells in the matrix are frozen, thus enabling flexible, granular,\nand realistic scenarios and stress tests. NAbC stands alone in providing all of\nthese capabilities simultaneously, and should prove to be a very useful means\nby which we can better understand and manage financial portfolios in our\nmultivariate world",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We live in a multivariate world, and effective modeling of financial\nportfolios, including their construction, allocation, forecasting, and risk\nanalysis, simply is not possible without explicitly modeling the dependence\nstructure of their assets. Dependence structure can drive portfolio results\nmore than the combined effects of other parameters in investment and risk\nmodels, but the literature provides relatively little to define the\nfinite-sample distributions of dependence measures in useable and useful ways\nunder challenging, real-world financial data conditions. Yet this is exactly\nwhat is needed to make valid inferences about their estimates, and to use these\ninferences for essential purposes such as hypothesis testing, dynamic\nmonitoring, realistic and granular scenario and reverse scenario analyses, and\nmitigating the effects of correlation breakdowns during market upheavals. This\nwork develops a new and straightforward method, Nonparametric Angles-based\nCorrelation (NAbC), for defining the finite-sample distributions of any\ndependence measure whose matrix of pairwise associations is positive definite\n(e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). NAbC\nremains valid under marginal asset distributions with notably different and\nvarying degrees of serial correlation, non-stationarity, heavy-tailedness, and\nasymmetry. NAbC provides p-values and confidence intervals at both the matrix\nlevel and the pairwise cell level, for both one and two-sample tests, with\nanalytical consistency across levels. Finally, NAbC maintains validity even\nwhen selected cells in the matrix are frozen, thus enabling flexible, granular,\nand realistic scenarios and stress tests. NAbC stands alone in providing all of\nthese capabilities simultaneously, and should prove to be a very useful means\nby which we can better understand and manage financial portfolios in our\nmultivariate world"
                },
                "authors": [
                    {
                        "name": "JD Opdyke"
                    }
                ],
                "author_detail": {
                    "name": "JD Opdyke"
                },
                "author": "JD Opdyke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15268v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15268v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-07, 62E20, 62F10, 62F12, 60E05, 60G70, 91B30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00626v1",
                "updated": "2025-08-01T13:38:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    38,
                    38,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:38:38Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    38,
                    38,
                    4,
                    213,
                    0
                ],
                "title": "Deep Learning-Based Rate-Adaptive CSI Feedback for Wideband XL-MIMO\n  Systems in the Near-Field Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Rate-Adaptive CSI Feedback for Wideband XL-MIMO\n  Systems in the Near-Field Domain"
                },
                "summary": "Accurate and efficient channel state information (CSI) feedback is crucial\nfor unlocking the substantial spectral efficiency gains of extremely\nlarge-scale MIMO (XL-MIMO) systems in future 6G networks. However, the\ncombination of near-field spherical wave propagation and frequency-dependent\nbeam split effects in wideband scenarios poses significant challenges for CSI\nrepresentation and compression. This paper proposes WideNLNet-CA, a\nrate-adaptive deep learning framework designed to enable efficient CSI feedback\nin wideband near-field XL-MIMO systems. WideNLNet-CA introduces a lightweight\nencoder-decoder architecture with multi-stage downsampling and upsampling,\nincorporating computationally efficient residual blocks to capture complex\nmulti-scale channel features with reduced overhead. A novel compression ratio\nadaptive module with feature importance estimation is introduced to dynamically\nmodulate feature selection based on target compression ratios, enabling\nflexible adaptation across a wide range of feedback rates using a single model.\nEvaluation results demonstrate that WideNLNet-CA consistently outperforms\nexisting compressive sensing and deep learning-based works across various\ncompression ratios and bandwidths, while maintaining fast inference and low\nmodel storage requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and efficient channel state information (CSI) feedback is crucial\nfor unlocking the substantial spectral efficiency gains of extremely\nlarge-scale MIMO (XL-MIMO) systems in future 6G networks. However, the\ncombination of near-field spherical wave propagation and frequency-dependent\nbeam split effects in wideband scenarios poses significant challenges for CSI\nrepresentation and compression. This paper proposes WideNLNet-CA, a\nrate-adaptive deep learning framework designed to enable efficient CSI feedback\nin wideband near-field XL-MIMO systems. WideNLNet-CA introduces a lightweight\nencoder-decoder architecture with multi-stage downsampling and upsampling,\nincorporating computationally efficient residual blocks to capture complex\nmulti-scale channel features with reduced overhead. A novel compression ratio\nadaptive module with feature importance estimation is introduced to dynamically\nmodulate feature selection based on target compression ratios, enabling\nflexible adaptation across a wide range of feedback rates using a single model.\nEvaluation results demonstrate that WideNLNet-CA consistently outperforms\nexisting compressive sensing and deep learning-based works across various\ncompression ratios and bandwidths, while maintaining fast inference and low\nmodel storage requirements."
                },
                "authors": [
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    }
                ],
                "author_detail": {
                    "name": "Rahim Tafazolli"
                },
                "author": "Rahim Tafazolli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00619v1",
                "updated": "2025-08-01T13:28:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    28,
                    1,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:28:01Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    28,
                    1,
                    4,
                    213,
                    0
                ],
                "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language\n  Models"
                },
                "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors."
                },
                "authors": [
                    {
                        "name": "Shantanu Thorat"
                    },
                    {
                        "name": "Andrew Caines"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Caines"
                },
                "author": "Andrew Caines",
                "arxiv_comment": "MPhil in Advanced Computer Science thesis for University of Cambridge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00617v1",
                "updated": "2025-08-01T13:25:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    59,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:59Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    59,
                    4,
                    213,
                    0
                ],
                "title": "Constructive Disintegration and Conditional Modes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructive Disintegration and Conditional Modes"
                },
                "summary": "Conditioning, the central operation in Bayesian statistics, is formalised by\nthe notion of disintegration of measures. However, due to the implicit nature\nof their definition, constructing disintegrations is often difficult. A\nfolklore result in machine learning conflates the construction of a\ndisintegration with the restriction of probability density functions onto the\nsubset of events that are consistent with a given observation. We provide a\ncomprehensive set of mathematical tools which can be used to construct\ndisintegrations and apply these to find densities of disintegrations on\ndifferentiable manifolds. Using our results, we provide a disturbingly simple\nexample in which the restricted density and the disintegration density\ndrastically disagree. Motivated by applications in approximate Bayesian\ninference and Bayesian inverse problems, we further study the modes of\ndisintegrations. We show that the recently introduced notion of a \"conditional\nmode\" does not coincide in general with the modes of the conditional measure\nobtained through disintegration, but rather the modes of the restricted\nmeasure. We also discuss the implications of the discrepancy between the two\nmeasures in practice, advocating for the utility of both approaches depending\non the modelling context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditioning, the central operation in Bayesian statistics, is formalised by\nthe notion of disintegration of measures. However, due to the implicit nature\nof their definition, constructing disintegrations is often difficult. A\nfolklore result in machine learning conflates the construction of a\ndisintegration with the restriction of probability density functions onto the\nsubset of events that are consistent with a given observation. We provide a\ncomprehensive set of mathematical tools which can be used to construct\ndisintegrations and apply these to find densities of disintegrations on\ndifferentiable manifolds. Using our results, we provide a disturbingly simple\nexample in which the restricted density and the disintegration density\ndrastically disagree. Motivated by applications in approximate Bayesian\ninference and Bayesian inverse problems, we further study the modes of\ndisintegrations. We show that the recently introduced notion of a \"conditional\nmode\" does not coincide in general with the modes of the conditional measure\nobtained through disintegration, but rather the modes of the restricted\nmeasure. We also discuss the implications of the discrepancy between the two\nmeasures in practice, advocating for the utility of both approaches depending\non the modelling context."
                },
                "authors": [
                    {
                        "name": "Nathal Da Costa"
                    },
                    {
                        "name": "Marvin Pfrtner"
                    },
                    {
                        "name": "Jon Cockayne"
                    }
                ],
                "author_detail": {
                    "name": "Jon Cockayne"
                },
                "author": "Jon Cockayne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00614v1",
                "updated": "2025-08-01T13:23:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    23,
                    21,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:23:21Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    23,
                    21,
                    4,
                    213,
                    0
                ],
                "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will\n  you care?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will\n  you care?"
                },
                "summary": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions."
                },
                "authors": [
                    {
                        "name": "Lennart Meincke"
                    },
                    {
                        "name": "Ethan Mollick"
                    },
                    {
                        "name": "Lilach Mollick"
                    },
                    {
                        "name": "Dan Shapiro"
                    }
                ],
                "author_detail": {
                    "name": "Dan Shapiro"
                },
                "author": "Dan Shapiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12927v2",
                "updated": "2025-08-01T13:19:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    19,
                    46,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-18T15:09:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "SEFL: Enhancing Educational Assignment Feedback with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEFL: Enhancing Educational Assignment Feedback with LLM Agents"
                },
                "summary": "Providing high-quality feedback to student assignments is crucial for student\nsuccess, but it is constrained by time and costs. In this work, we introduce\nSynthetic Educational Feedback Loops (SEFL), a synthetic data framework\ndesigned to generate data that resembles immediate, on-demand feedback at scale\nwithout relying on extensive, real-world student assignments. To get this type\nof data, two large language models (LLMs) operate in teacher-student roles to\nsimulate assignment completion and formative feedback, generating synthetic\npairs of student work and corresponding critiques and actionable improvements\nfrom a teacher. With this data, we fine-tune smaller, more computationally\nefficient LLMs on these synthetic pairs, enabling them to replicate key\nfeatures of high-quality, goal-oriented feedback. Unlike personalized tutoring\napproaches that offer multi-turn, individualized instruction, SEFL specifically\nfocuses on replicating the teacher-student assignment feedback loop in higher\neducation. Through comprehensive evaluations with four LLM judges and three\nhuman experts, we demonstrate that SEFL-tuned models outperform both their\nnon-tuned counterparts in feedback quality and an existing baseline. The\npotential for societal impact is reinforced by extensive qualitative comments\nby ratings by human stakeholders -- both students and higher education\ninstructors. All in all, SEFL has substantial potential to transform feedback\nprocesses for higher education and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing high-quality feedback to student assignments is crucial for student\nsuccess, but it is constrained by time and costs. In this work, we introduce\nSynthetic Educational Feedback Loops (SEFL), a synthetic data framework\ndesigned to generate data that resembles immediate, on-demand feedback at scale\nwithout relying on extensive, real-world student assignments. To get this type\nof data, two large language models (LLMs) operate in teacher-student roles to\nsimulate assignment completion and formative feedback, generating synthetic\npairs of student work and corresponding critiques and actionable improvements\nfrom a teacher. With this data, we fine-tune smaller, more computationally\nefficient LLMs on these synthetic pairs, enabling them to replicate key\nfeatures of high-quality, goal-oriented feedback. Unlike personalized tutoring\napproaches that offer multi-turn, individualized instruction, SEFL specifically\nfocuses on replicating the teacher-student assignment feedback loop in higher\neducation. Through comprehensive evaluations with four LLM judges and three\nhuman experts, we demonstrate that SEFL-tuned models outperform both their\nnon-tuned counterparts in feedback quality and an existing baseline. The\npotential for societal impact is reinforced by extensive qualitative comments\nby ratings by human stakeholders -- both students and higher education\ninstructors. All in all, SEFL has substantial potential to transform feedback\nprocesses for higher education and beyond."
                },
                "authors": [
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Amalie Pernille Dilling"
                    },
                    {
                        "name": "Lon Gondelman"
                    },
                    {
                        "name": "Niels Erik Ruan Lyngdorf"
                    },
                    {
                        "name": "Euan D. Lindsay"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00611v1",
                "updated": "2025-08-01T13:14:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    14,
                    41,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:14:41Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    14,
                    41,
                    4,
                    213,
                    0
                ],
                "title": "Rapid cosmological inference with the two-loop matter power spectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid cosmological inference with the two-loop matter power spectrum"
                },
                "summary": "We compute the two-loop effective field theory (EFT) power spectrum of dark\nmatter density fluctuations in $\\Lambda$CDM using the recently proposed COBRA\nmethod (Bakx. et al, 2025). With COBRA, we are able to evaluate the two-loop\nmatter power spectrum in $\\sim 1$ millisecond at $ \\sim 0.1 \\%$ precision on\none CPU for arbitrary redshifts and on scales where perturbation theory\napplies. As an application, we use the nonlinear matter power spectrum from the\nDark Sky simulation to assess the performance of the two-loop EFT power\nspectrum compared to the one-loop EFT power spectrum at $z=0$. We find that,\nfor volumes typical for Stage IV galaxy surveys, $V = 25 \\,(\\text{Gpc}/h)^3$,\nthe two-loop EFT can provide unbiased cosmological constraints on\n$\\Omega_m,H_0$ and $A_s$ using scales up to $k_\\text{max}=0.26\\, h/\\text{Mpc}$,\nthereby outperforming the constraints from the one-loop EFT\n($k_\\text{max}=0.11\\, h/\\text{Mpc}$). The Figure of Merit on these three\nparameters increases by a factor $\\sim 1.9$ and the one-dimensional\nmarginalized constraints improve by $\\sim35\\%$ for $\\Omega_m$, $\\sim20\\%$ for\n$H_0$ and $\\sim 15\\%$ for $A_s$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute the two-loop effective field theory (EFT) power spectrum of dark\nmatter density fluctuations in $\\Lambda$CDM using the recently proposed COBRA\nmethod (Bakx. et al, 2025). With COBRA, we are able to evaluate the two-loop\nmatter power spectrum in $\\sim 1$ millisecond at $ \\sim 0.1 \\%$ precision on\none CPU for arbitrary redshifts and on scales where perturbation theory\napplies. As an application, we use the nonlinear matter power spectrum from the\nDark Sky simulation to assess the performance of the two-loop EFT power\nspectrum compared to the one-loop EFT power spectrum at $z=0$. We find that,\nfor volumes typical for Stage IV galaxy surveys, $V = 25 \\,(\\text{Gpc}/h)^3$,\nthe two-loop EFT can provide unbiased cosmological constraints on\n$\\Omega_m,H_0$ and $A_s$ using scales up to $k_\\text{max}=0.26\\, h/\\text{Mpc}$,\nthereby outperforming the constraints from the one-loop EFT\n($k_\\text{max}=0.11\\, h/\\text{Mpc}$). The Figure of Merit on these three\nparameters increases by a factor $\\sim 1.9$ and the one-dimensional\nmarginalized constraints improve by $\\sim35\\%$ for $\\Omega_m$, $\\sim20\\%$ for\n$H_0$ and $\\sim 15\\%$ for $A_s$."
                },
                "authors": [
                    {
                        "name": "Thomas Bakx"
                    },
                    {
                        "name": "Henrique Rubira"
                    },
                    {
                        "name": "Nora Elisa Chisari"
                    },
                    {
                        "name": "Zvonimir Vlah"
                    }
                ],
                "author_detail": {
                    "name": "Zvonimir Vlah"
                },
                "author": "Zvonimir Vlah",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00604v1",
                "updated": "2025-08-01T13:07:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    7,
                    16,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:07:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    7,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "Composable OS Kernel Architectures for Autonomous Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composable OS Kernel Architectures for Autonomous Intelligence"
                },
                "summary": "As intelligent systems permeate edge devices, cloud infrastructure, and\nembedded real-time environments, this research proposes a new OS kernel\narchitecture for intelligent systems, transforming kernels from static resource\nmanagers to adaptive, AI-integrated platforms. Key contributions include: (1)\ntreating Loadable Kernel Modules (LKMs) as AI-oriented computation units for\nfast sensory and cognitive processing in kernel space; (2) expanding the Linux\nkernel into an AI-native environment with built-in deep learning inference,\nfloating-point acceleration, and real-time adaptive scheduling for efficient ML\nworkloads; and (3) introducing a Neurosymbolic kernel design leveraging\nCategory Theory and Homotopy Type Theory to unify symbolic reasoning and\ndifferentiable logic within OS internals. Together, these approaches enable\noperating systems to proactively anticipate and adapt to the cognitive needs of\nautonomous intelligent applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As intelligent systems permeate edge devices, cloud infrastructure, and\nembedded real-time environments, this research proposes a new OS kernel\narchitecture for intelligent systems, transforming kernels from static resource\nmanagers to adaptive, AI-integrated platforms. Key contributions include: (1)\ntreating Loadable Kernel Modules (LKMs) as AI-oriented computation units for\nfast sensory and cognitive processing in kernel space; (2) expanding the Linux\nkernel into an AI-native environment with built-in deep learning inference,\nfloating-point acceleration, and real-time adaptive scheduling for efficient ML\nworkloads; and (3) introducing a Neurosymbolic kernel design leveraging\nCategory Theory and Homotopy Type Theory to unify symbolic reasoning and\ndifferentiable logic within OS internals. Together, these approaches enable\noperating systems to proactively anticipate and adapt to the cognitive needs of\nautonomous intelligent applications."
                },
                "authors": [
                    {
                        "name": "Rajpreet Singh"
                    },
                    {
                        "name": "Vidhi Kothari"
                    }
                ],
                "author_detail": {
                    "name": "Vidhi Kothari"
                },
                "author": "Vidhi Kothari",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21053v2",
                "updated": "2025-08-01T13:04:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    4,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-28T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    57,
                    0,
                    209,
                    0
                ],
                "title": "Flow Matching Policy Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow Matching Policy Gradients"
                },
                "summary": "Flow-based generative models, including diffusion models, excel at modeling\ncontinuous distributions in high-dimensional spaces. In this work, we introduce\nFlow Policy Optimization (FPO), a simple on-policy reinforcement learning\nalgorithm that brings flow matching into the policy gradient framework. FPO\ncasts policy optimization as maximizing an advantage-weighted ratio computed\nfrom the conditional flow matching loss, in a manner compatible with the\npopular PPO-clip framework. It sidesteps the need for exact likelihood\ncomputation while preserving the generative capabilities of flow-based models.\nUnlike prior approaches for diffusion-based reinforcement learning that bind\ntraining to a specific sampling method, FPO is agnostic to the choice of\ndiffusion or flow integration at both training and inference time. We show that\nFPO can train diffusion-style policies from scratch in a variety of continuous\ncontrol tasks. We find that flow-based models can capture multimodal action\ndistributions and achieve higher performance than Gaussian policies,\nparticularly in under-conditioned settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-based generative models, including diffusion models, excel at modeling\ncontinuous distributions in high-dimensional spaces. In this work, we introduce\nFlow Policy Optimization (FPO), a simple on-policy reinforcement learning\nalgorithm that brings flow matching into the policy gradient framework. FPO\ncasts policy optimization as maximizing an advantage-weighted ratio computed\nfrom the conditional flow matching loss, in a manner compatible with the\npopular PPO-clip framework. It sidesteps the need for exact likelihood\ncomputation while preserving the generative capabilities of flow-based models.\nUnlike prior approaches for diffusion-based reinforcement learning that bind\ntraining to a specific sampling method, FPO is agnostic to the choice of\ndiffusion or flow integration at both training and inference time. We show that\nFPO can train diffusion-style policies from scratch in a variety of continuous\ncontrol tasks. We find that flow-based models can capture multimodal action\ndistributions and achieve higher performance than Gaussian policies,\nparticularly in under-conditioned settings."
                },
                "authors": [
                    {
                        "name": "David McAllister"
                    },
                    {
                        "name": "Songwei Ge"
                    },
                    {
                        "name": "Brent Yi"
                    },
                    {
                        "name": "Chung Min Kim"
                    },
                    {
                        "name": "Ethan Weber"
                    },
                    {
                        "name": "Hongsuk Choi"
                    },
                    {
                        "name": "Haiwen Feng"
                    },
                    {
                        "name": "Angjoo Kanazawa"
                    }
                ],
                "author_detail": {
                    "name": "Angjoo Kanazawa"
                },
                "author": "Angjoo Kanazawa",
                "arxiv_comment": "See our blog post at https://flowreinforce.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00602v1",
                "updated": "2025-08-01T13:04:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    4,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:04:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    4,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection\n  and Leakage Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection\n  and Leakage Attacks"
                },
                "summary": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard."
                },
                "authors": [
                    {
                        "name": "Francesco Panebianco"
                    },
                    {
                        "name": "Stefano Bonfanti"
                    },
                    {
                        "name": "Francesco Trov"
                    },
                    {
                        "name": "Michele Carminati"
                    }
                ],
                "author_detail": {
                    "name": "Michele Carminati"
                },
                "author": "Michele Carminati",
                "arxiv_comment": "22 pages, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15066v2",
                "updated": "2025-08-01T13:03:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    3,
                    5,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-20T18:02:50Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    18,
                    2,
                    50,
                    6,
                    201,
                    0
                ],
                "title": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback"
                },
                "summary": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area."
                },
                "authors": [
                    {
                        "name": "Yiyuan Yang"
                    },
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Kai Ying"
                    },
                    {
                        "name": "Zhiguang Wang"
                    },
                    {
                        "name": "Tom Bamford"
                    },
                    {
                        "name": "Svitlana Vyetrenko"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "Under review. 19 pages, 8 figures, 12 tables. Code and dataset are\n  publicly available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00600v1",
                "updated": "2025-08-01T12:58:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    58,
                    34,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:58:34Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    58,
                    34,
                    4,
                    213,
                    0
                ],
                "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large\n  Language Models"
                },
                "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines."
                },
                "authors": [
                    {
                        "name": "Mingruo Yuan"
                    },
                    {
                        "name": "Shuyi Zhang"
                    },
                    {
                        "name": "Ben Kao"
                    }
                ],
                "author_detail": {
                    "name": "Ben Kao"
                },
                "author": "Ben Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23276v2",
                "updated": "2025-08-01T12:49:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    49,
                    36,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T06:32:06Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    6,
                    32,
                    6,
                    3,
                    212,
                    0
                ],
                "title": "How Far Are AI Scientists from Changing the World?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are AI Scientists from Changing the World?"
                },
                "summary": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be."
                },
                "authors": [
                    {
                        "name": "Qiujie Xie"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Fuchen Shen"
                    },
                    {
                        "name": "Shulin Huang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Jiahui Zhou"
                    },
                    {
                        "name": "Zilan Mao"
                    },
                    {
                        "name": "Zijie Yang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00588v1",
                "updated": "2025-08-01T12:41:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    41,
                    40,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:41:40Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    41,
                    40,
                    4,
                    213,
                    0
                ],
                "title": "Output-recurrent gated state space model for multiphase flows modeling\n  and uncertainty quantification of exhaust vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-recurrent gated state space model for multiphase flows modeling\n  and uncertainty quantification of exhaust vehicles"
                },
                "summary": "This paper presents an Output-Recurrent Gated State Space Model (OR-GSSM) for\ncomplex multiphase flows modeling and uncertainty quantification of exhaust\nvehicles during motion. By establishing the state-space formulation of the\ngas-liquid Navier-Stokes equations applying semigroup theory and Galerkin\nprojection, explicitly characterizing the dynamic coupling evolution between\nthe velocity, pressure, and volume fraction fields. A novel Gated State Space\nTransition (GSST) unit is designed to learn parameterized transition and input\nmatrices with adaptive timescales, enhancing physical interpretability and\ncomputational efficiency. The output recursion mechanism aligns with the\nnumerical solution characteristics of state-space equations, mitigating\nlong-term error accumulation and addressing training-inference pattern mismatch\nissues inherent in teacher forcing and scheduled sampling. Validations on the\nunderwater cone-head and water-exit hemisphere-head vehicles demonstrate that:\nOR-GSSM outperforms OR-ConvLSTM and OR-ConvGRU baselines in accuracy and\ncomputational efficiency through its physics-informed adaptive state-space unit\ndesign and parallel matrix operations; The output recursion mechanism ensures\nmore stable training, better generalization, and higher prediction accuracy\nthan teacher forcing and scheduled sampling; OR-GSSM accurately captures the\ngas-phase expansion, gas-liquid mixing formation, backflow jet generation,\nbubble shedding, and entire water-exit process, etc, showcasing outstanding\nmodeling capability; Its uncertainty quantification effectively characterizes\nflow features and uncertainty distributions, validating prediction reliability.\nThe proposed method resolves the accuracy-real-time trade-off in traditional\ncomputational fluid dynamics, advancing machine learning for multiphase flow\nmodeling and uncertainty quantification in exhaust vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an Output-Recurrent Gated State Space Model (OR-GSSM) for\ncomplex multiphase flows modeling and uncertainty quantification of exhaust\nvehicles during motion. By establishing the state-space formulation of the\ngas-liquid Navier-Stokes equations applying semigroup theory and Galerkin\nprojection, explicitly characterizing the dynamic coupling evolution between\nthe velocity, pressure, and volume fraction fields. A novel Gated State Space\nTransition (GSST) unit is designed to learn parameterized transition and input\nmatrices with adaptive timescales, enhancing physical interpretability and\ncomputational efficiency. The output recursion mechanism aligns with the\nnumerical solution characteristics of state-space equations, mitigating\nlong-term error accumulation and addressing training-inference pattern mismatch\nissues inherent in teacher forcing and scheduled sampling. Validations on the\nunderwater cone-head and water-exit hemisphere-head vehicles demonstrate that:\nOR-GSSM outperforms OR-ConvLSTM and OR-ConvGRU baselines in accuracy and\ncomputational efficiency through its physics-informed adaptive state-space unit\ndesign and parallel matrix operations; The output recursion mechanism ensures\nmore stable training, better generalization, and higher prediction accuracy\nthan teacher forcing and scheduled sampling; OR-GSSM accurately captures the\ngas-phase expansion, gas-liquid mixing formation, backflow jet generation,\nbubble shedding, and entire water-exit process, etc, showcasing outstanding\nmodeling capability; Its uncertainty quantification effectively characterizes\nflow features and uncertainty distributions, validating prediction reliability.\nThe proposed method resolves the accuracy-real-time trade-off in traditional\ncomputational fluid dynamics, advancing machine learning for multiphase flow\nmodeling and uncertainty quantification in exhaust vehicles."
                },
                "authors": [
                    {
                        "name": "Ruilin Chen"
                    },
                    {
                        "name": "Ming Fang"
                    },
                    {
                        "name": "Guihui Ma"
                    }
                ],
                "author_detail": {
                    "name": "Guihui Ma"
                },
                "author": "Guihui Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00581v1",
                "updated": "2025-08-01T12:24:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    24,
                    49,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:24:49Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    24,
                    49,
                    4,
                    213,
                    0
                ],
                "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated\n  Pre-Consultation Questionnaire Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated\n  Pre-Consultation Questionnaire Generation"
                },
                "summary": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection."
                },
                "authors": [
                    {
                        "name": "Ruiqing Ding"
                    },
                    {
                        "name": "Qianfang Sun"
                    },
                    {
                        "name": "Yongkang Leng"
                    },
                    {
                        "name": "Hui Yin"
                    },
                    {
                        "name": "Xiaojian Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojian Li"
                },
                "author": "Xiaojian Li",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14983v2",
                "updated": "2025-08-01T12:24:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    24,
                    1,
                    4,
                    213,
                    0
                ],
                "published": "2024-11-22T14:43:28Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    43,
                    28,
                    4,
                    327,
                    0
                ],
                "title": "Large sample scaling analysis of the Zig-Zag algorithm for Bayesian\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large sample scaling analysis of the Zig-Zag algorithm for Bayesian\n  inference"
                },
                "summary": "Piecewise deterministic Markov processes provide scalable methods for\nsampling from the posterior distributions in big data settings by admitting\nprincipled sub-sampling strategies that do not bias the output. An important\nexample is the Zig-Zag process of [Ann. Stats. 47 (2019) 1288 - 1320] where\nclever sub-sampling has been shown to produce an essentially independent sample\nat a cost that does not scale with the size of the data. However, sub-sampling\nalso leads to slower convergence and poor mixing of the process, a behaviour\nwhich questions the promised scalability of the algorithm. We provide a large\nsample scaling analysis of the Zig-Zag process and its sub-sampling versions in\nsettings of parametric Bayesian inference. In the transient phase of the\nalgorithm, we show that the Zig-Zag trajectories are well approximated by the\nsolution to a system of ODEs. These ODEs possess a drift in the direction of\ndecreasing KL-divergence between the assumed model and the true distribution\nand are explicitly characterized in the paper. In the stationary phase, we give\nweak convergence results for different versions of the Zig-Zag process. Based\non our results, we estimate that for large data sets of size n, using suitable\ncontrol variates with sub-sampling in Zig-Zag, the algorithm costs O(1) to\nobtain an essentially independent sample; a computational speed-up of O(n) over\nthe canonical version of Zig-Zag and other traditional MCMC methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piecewise deterministic Markov processes provide scalable methods for\nsampling from the posterior distributions in big data settings by admitting\nprincipled sub-sampling strategies that do not bias the output. An important\nexample is the Zig-Zag process of [Ann. Stats. 47 (2019) 1288 - 1320] where\nclever sub-sampling has been shown to produce an essentially independent sample\nat a cost that does not scale with the size of the data. However, sub-sampling\nalso leads to slower convergence and poor mixing of the process, a behaviour\nwhich questions the promised scalability of the algorithm. We provide a large\nsample scaling analysis of the Zig-Zag process and its sub-sampling versions in\nsettings of parametric Bayesian inference. In the transient phase of the\nalgorithm, we show that the Zig-Zag trajectories are well approximated by the\nsolution to a system of ODEs. These ODEs possess a drift in the direction of\ndecreasing KL-divergence between the assumed model and the true distribution\nand are explicitly characterized in the paper. In the stationary phase, we give\nweak convergence results for different versions of the Zig-Zag process. Based\non our results, we estimate that for large data sets of size n, using suitable\ncontrol variates with sub-sampling in Zig-Zag, the algorithm costs O(1) to\nobtain an essentially independent sample; a computational speed-up of O(n) over\nthe canonical version of Zig-Zag and other traditional MCMC methods"
                },
                "authors": [
                    {
                        "name": "Sanket Agrawal"
                    },
                    {
                        "name": "Joris Bierkens"
                    },
                    {
                        "name": "Gareth O. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Gareth O. Roberts"
                },
                "author": "Gareth O. Roberts",
                "arxiv_comment": "50 pages, 7 figues, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08, 60F05, 62F15, 65C05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00580v1",
                "updated": "2025-08-01T12:23:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    23,
                    29,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:23:29Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    23,
                    29,
                    4,
                    213,
                    0
                ],
                "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on\n  Planetary Rovers Using RGB, Depth, and Thermal Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on\n  Planetary Rovers Using RGB, Depth, and Thermal Imagery"
                },
                "summary": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics."
                },
                "authors": [
                    {
                        "name": "Raul Castilla-Arquillo"
                    },
                    {
                        "name": "Carlos Perez-del-Pulgar"
                    },
                    {
                        "name": "Levin Gerdes"
                    },
                    {
                        "name": "Alfonso Garcia-Cerezo"
                    },
                    {
                        "name": "Miguel A. Olivares-Mendez"
                    }
                ],
                "author_detail": {
                    "name": "Miguel A. Olivares-Mendez"
                },
                "author": "Miguel A. Olivares-Mendez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00579v1",
                "updated": "2025-08-01T12:22:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    22,
                    53,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:22:53Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    22,
                    53,
                    4,
                    213,
                    0
                ],
                "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval"
                },
                "summary": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents."
                },
                "authors": [
                    {
                        "name": "Ziyu Gong"
                    },
                    {
                        "name": "Yihua Huang"
                    },
                    {
                        "name": "Chengcheng Mai"
                    }
                ],
                "author_detail": {
                    "name": "Chengcheng Mai"
                },
                "author": "Chengcheng Mai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12201v2",
                "updated": "2025-08-01T12:20:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    20,
                    2,
                    4,
                    213,
                    0
                ],
                "published": "2024-12-15T03:08:28Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    8,
                    28,
                    6,
                    350,
                    0
                ],
                "title": "Embracing Large Language Models in Traffic Flow Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embracing Large Language Models in Traffic Flow Forecasting"
                },
                "summary": "Traffic flow forecasting aims to predict future traffic flows based on the\nhistorical traffic conditions and the road network. It is an important problem\nin intelligent transportation systems, with a plethora of methods been\nproposed. Existing efforts mainly focus on capturing and utilizing\nspatio-temporal dependencies to predict future traffic flows. Though promising,\nthey fall short in adapting to test-time environmental changes of traffic\nconditions. To tackle this challenge, we propose to introduce large language\nmodels (LLMs) to help traffic flow forecasting and design a novel method named\nLarge Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two\nbranches, capturing different spatio-temporal relations using graph and\nhypergraph structures respectively. The two branches are first pre-trained\nindividually, and during test-time, they yield different predictions. Based on\nthese predictions, a large language model is used to select the most likely\nresult. Then, a ranking loss is applied as the learning objective to enhance\nthe prediction ability of the two branches. Extensive experiments on several\ndatasets demonstrate the effectiveness of the proposed LEAF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic flow forecasting aims to predict future traffic flows based on the\nhistorical traffic conditions and the road network. It is an important problem\nin intelligent transportation systems, with a plethora of methods been\nproposed. Existing efforts mainly focus on capturing and utilizing\nspatio-temporal dependencies to predict future traffic flows. Though promising,\nthey fall short in adapting to test-time environmental changes of traffic\nconditions. To tackle this challenge, we propose to introduce large language\nmodels (LLMs) to help traffic flow forecasting and design a novel method named\nLarge Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two\nbranches, capturing different spatio-temporal relations using graph and\nhypergraph structures respectively. The two branches are first pre-trained\nindividually, and during test-time, they yield different predictions. Based on\nthese predictions, a large language model is used to select the most likely\nresult. Then, a ranking loss is applied as the learning objective to enhance\nthe prediction ability of the two branches. Extensive experiments on several\ndatasets demonstrate the effectiveness of the proposed LEAF."
                },
                "authors": [
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Haomin Wen"
                    },
                    {
                        "name": "Zhiping Xiao"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00574v1",
                "updated": "2025-08-01T12:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    17,
                    35,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:17:35Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    17,
                    35,
                    4,
                    213,
                    0
                ],
                "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via\n  Synthetic Continuous Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via\n  Synthetic Continuous Chain-of-Thought"
                },
                "summary": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off."
                },
                "authors": [
                    {
                        "name": "Jianwei Wang"
                    },
                    {
                        "name": "Ziming Wu"
                    },
                    {
                        "name": "Fuming Lai"
                    },
                    {
                        "name": "Shaobing Lian"
                    },
                    {
                        "name": "Ziqian Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Ziqian Zeng"
                },
                "author": "Ziqian Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05161v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05161v4",
                "updated": "2025-08-01T12:11:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    11,
                    18,
                    4,
                    213,
                    0
                ],
                "published": "2022-06-10T14:59:21Z",
                "published_parsed": [
                    2022,
                    6,
                    10,
                    14,
                    59,
                    21,
                    4,
                    161,
                    0
                ],
                "title": "Approximating optimal SMC proposal distributions in individual-based\n  epidemic models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating optimal SMC proposal distributions in individual-based\n  epidemic models"
                },
                "summary": "Many epidemic models are naturally defined as individual-based models: where\nwe track the state of each individual within a susceptible population.\nInference for individual-based models is challenging due to the\nhigh-dimensional state-space of such models, which increases exponentially with\npopulation size. We consider sequential Monte Carlo algorithms for inference\nfor individual-based epidemic models where we make direct observations of the\nstate of a sample of individuals. Standard implementations, such as the\nbootstrap filter or the auxiliary particle filter are inefficient due to\nmismatch between the proposal distribution of the state and future\nobservations. We develop new efficient proposal distributions that take account\nof future observations, leveraging the properties that (i) we can analytically\ncalculate the optimal proposal distribution for a single individual given\nfuture observations and the future infection rate of that individual; and (ii)\nthe dynamics of individuals are independent if we condition on their infection\nrates. Thus we construct estimates of the future infection rate for each\nindividual, and then use an independent proposal for the state of each\nindividual given this estimate. Empirical results show order of magnitude\nimprovement in efficiency of the sequential Monte Carlo sampler for both SIS\nand SEIR models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many epidemic models are naturally defined as individual-based models: where\nwe track the state of each individual within a susceptible population.\nInference for individual-based models is challenging due to the\nhigh-dimensional state-space of such models, which increases exponentially with\npopulation size. We consider sequential Monte Carlo algorithms for inference\nfor individual-based epidemic models where we make direct observations of the\nstate of a sample of individuals. Standard implementations, such as the\nbootstrap filter or the auxiliary particle filter are inefficient due to\nmismatch between the proposal distribution of the state and future\nobservations. We develop new efficient proposal distributions that take account\nof future observations, leveraging the properties that (i) we can analytically\ncalculate the optimal proposal distribution for a single individual given\nfuture observations and the future infection rate of that individual; and (ii)\nthe dynamics of individuals are independent if we condition on their infection\nrates. Thus we construct estimates of the future infection rate for each\nindividual, and then use an independent proposal for the state of each\nindividual given this estimate. Empirical results show order of magnitude\nimprovement in efficiency of the sequential Monte Carlo sampler for both SIS\nand SEIR models."
                },
                "authors": [
                    {
                        "name": "Lorenzo Rimella"
                    },
                    {
                        "name": "Christopher Jewell"
                    },
                    {
                        "name": "Paul Fearnhead"
                    }
                ],
                "author_detail": {
                    "name": "Paul Fearnhead"
                },
                "author": "Paul Fearnhead",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2206.05161v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05161v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00570v1",
                "updated": "2025-08-01T12:11:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    11,
                    10,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:11:10Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    11,
                    10,
                    4,
                    213,
                    0
                ],
                "title": "Session-Based Recommendation with Validated and Enriched LLM Intents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Session-Based Recommendation with Validated and Enriched LLM Intents"
                },
                "summary": "Session-based recommendation (SBR) aims to predict the next item for an\nanonymous user in a timely manner. However, SBR suffers from data sparsity due\nto the short and anonymous nature of sessions. Recently, an emerging line of\nwork has explored inferring the underlying user intents of a session using\nlarge language models (LLMs), with the generated intents serving as auxiliary\ntraining signals to enhance SBR models. Despite its promise, this approach\nfaces three key challenges: validating intent quality, incorporating\nsession-level multi-intents, and complementing inevitable LLM failure cases. In\nthis paper, we propose VELI4SBR, a two-stage framework that leverages Validated\nand Enriched LLM-generated Intents for SBR. In the first stage, we generate\nhigh-quality intents using a predict-and-correct loop that validates the\ninformativeness of LLM-generated intents with a global intent pool to constrain\nthe LLM's output space and reduce hallucination. In the second stage, we\nenhance the SBR model using the generated intents through a lightweight\nmulti-intent prediction and fusion mechanism. Furthermore, we introduce a\ntraining strategy that compensates for LLM failures by inferring intents from\ninter-session behavioral similarities. Extensive experiments show that VELI4SBR\noutperforms state-of-the-art baselines while improving explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Session-based recommendation (SBR) aims to predict the next item for an\nanonymous user in a timely manner. However, SBR suffers from data sparsity due\nto the short and anonymous nature of sessions. Recently, an emerging line of\nwork has explored inferring the underlying user intents of a session using\nlarge language models (LLMs), with the generated intents serving as auxiliary\ntraining signals to enhance SBR models. Despite its promise, this approach\nfaces three key challenges: validating intent quality, incorporating\nsession-level multi-intents, and complementing inevitable LLM failure cases. In\nthis paper, we propose VELI4SBR, a two-stage framework that leverages Validated\nand Enriched LLM-generated Intents for SBR. In the first stage, we generate\nhigh-quality intents using a predict-and-correct loop that validates the\ninformativeness of LLM-generated intents with a global intent pool to constrain\nthe LLM's output space and reduce hallucination. In the second stage, we\nenhance the SBR model using the generated intents through a lightweight\nmulti-intent prediction and fusion mechanism. Furthermore, we introduce a\ntraining strategy that compensates for LLM failures by inferring intents from\ninter-session behavioral similarities. Extensive experiments show that VELI4SBR\noutperforms state-of-the-art baselines while improving explainability."
                },
                "authors": [
                    {
                        "name": "Gyuseok Lee"
                    },
                    {
                        "name": "Yaokun Liu"
                    },
                    {
                        "name": "Yifan Liu"
                    },
                    {
                        "name": "Susik Yoon"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "SeongKu Kang"
                    }
                ],
                "author_detail": {
                    "name": "SeongKu Kang"
                },
                "author": "SeongKu Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00554v1",
                "updated": "2025-08-01T11:48:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    48,
                    13,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T11:48:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    48,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest\n  Mechanism"
                },
                "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Li Zhao"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Zuoyou Jiang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Yuxiao Bai"
                    },
                    {
                        "name": "Mengting Chen"
                    },
                    {
                        "name": "Xinyang Wang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Zuo Bai"
                    }
                ],
                "author_detail": {
                    "name": "Zuo Bai"
                },
                "author": "Zuo Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00553v1",
                "updated": "2025-08-01T11:48:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    48,
                    11,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T11:48:11Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    48,
                    11,
                    4,
                    213,
                    0
                ],
                "title": "HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention\n  in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention\n  in Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) encode images into lengthy sequences of visual\ntokens, leading to excessive computational overhead and limited inference\nefficiency. While prior efforts prune or merge tokens to address this issue,\nthey often rely on special tokens (e.g., CLS) or require task-specific\ntraining, hindering scalability across architectures. In this paper, we propose\nHiPrune, a training-free and model-agnostic token Pruning framework that\nexploits the Hierarchical attention structure within vision encoders. We\nidentify that middle layers attend to object-centric regions, while deep layers\ncapture global contextual features. Based on this observation, HiPrune selects\nthree types of informative tokens: (1) Anchor tokens with high attention in\nobject-centric layers, (2) Buffer tokens adjacent to anchors for spatial\ncontinuity, and (3) Register tokens with strong attention in deep layers for\nglobal summarization. Our method requires no retraining and integrates\nseamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,\nLLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art\npruning performance, preserving up to 99.3% task accuracy with only 33.3%\ntokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it\nreduces inference FLOPs and latency by up to 9$\\times$, showcasing strong\ngeneralization across models and tasks. Code is available at\nhttps://github.com/Danielement321/HiPrune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) encode images into lengthy sequences of visual\ntokens, leading to excessive computational overhead and limited inference\nefficiency. While prior efforts prune or merge tokens to address this issue,\nthey often rely on special tokens (e.g., CLS) or require task-specific\ntraining, hindering scalability across architectures. In this paper, we propose\nHiPrune, a training-free and model-agnostic token Pruning framework that\nexploits the Hierarchical attention structure within vision encoders. We\nidentify that middle layers attend to object-centric regions, while deep layers\ncapture global contextual features. Based on this observation, HiPrune selects\nthree types of informative tokens: (1) Anchor tokens with high attention in\nobject-centric layers, (2) Buffer tokens adjacent to anchors for spatial\ncontinuity, and (3) Register tokens with strong attention in deep layers for\nglobal summarization. Our method requires no retraining and integrates\nseamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,\nLLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art\npruning performance, preserving up to 99.3% task accuracy with only 33.3%\ntokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it\nreduces inference FLOPs and latency by up to 9$\\times$, showcasing strong\ngeneralization across models and tasks. Code is available at\nhttps://github.com/Danielement321/HiPrune."
                },
                "authors": [
                    {
                        "name": "Jizhihui Liu"
                    },
                    {
                        "name": "Feiyi Du"
                    },
                    {
                        "name": "Guangdao Zhu"
                    },
                    {
                        "name": "Niu Lian"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Bin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Bin Chen"
                },
                "author": "Bin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00552v1",
                "updated": "2025-08-01T11:47:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    47,
                    36,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T11:47:36Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    47,
                    36,
                    4,
                    213,
                    0
                ],
                "title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable\n  Adversarial Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable\n  Adversarial Purification"
                },
                "summary": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification."
                },
                "authors": [
                    {
                        "name": "Chihan Huang"
                    },
                    {
                        "name": "Belal Alsinglawi"
                    },
                    {
                        "name": "Islam Al-qudah"
                    }
                ],
                "author_detail": {
                    "name": "Islam Al-qudah"
                },
                "author": "Islam Al-qudah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00548v1",
                "updated": "2025-08-01T11:43:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    43,
                    30,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T11:43:30Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    43,
                    30,
                    4,
                    213,
                    0
                ],
                "title": "Video Color Grading via Look-Up Table Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Color Grading via Look-Up Table Generation"
                },
                "summary": "Different from color correction and transfer, color grading involves\nadjusting colors for artistic or storytelling purposes in a video, which is\nused to establish a specific look or mood. However, due to the complexity of\nthe process and the need for specialized editing skills, video color grading\nremains primarily the domain of professional colorists. In this paper, we\npresent a reference-based video color grading framework. Our key idea is\nexplicitly generating a look-up table (LUT) for color attribute alignment\nbetween reference scenes and input video via a diffusion model. As a training\nobjective, we enforce that high-level features of the reference scenes like\nlook, mood, and emotion should be similar to that of the input video. Our\nLUT-based approach allows for color grading without any loss of structural\ndetails in the whole video frames as well as achieving fast inference. We\nfurther build a pipeline to incorporate a user-preference via text prompts for\nlow-level feature enhancement such as contrast and brightness, etc.\nExperimental results, including extensive user studies, demonstrate the\neffectiveness of our approach for video color grading. Codes are publicly\navailable at https://github.com/seunghyuns98/VideoColorGrading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different from color correction and transfer, color grading involves\nadjusting colors for artistic or storytelling purposes in a video, which is\nused to establish a specific look or mood. However, due to the complexity of\nthe process and the need for specialized editing skills, video color grading\nremains primarily the domain of professional colorists. In this paper, we\npresent a reference-based video color grading framework. Our key idea is\nexplicitly generating a look-up table (LUT) for color attribute alignment\nbetween reference scenes and input video via a diffusion model. As a training\nobjective, we enforce that high-level features of the reference scenes like\nlook, mood, and emotion should be similar to that of the input video. Our\nLUT-based approach allows for color grading without any loss of structural\ndetails in the whole video frames as well as achieving fast inference. We\nfurther build a pipeline to incorporate a user-preference via text prompts for\nlow-level feature enhancement such as contrast and brightness, etc.\nExperimental results, including extensive user studies, demonstrate the\neffectiveness of our approach for video color grading. Codes are publicly\navailable at https://github.com/seunghyuns98/VideoColorGrading."
                },
                "authors": [
                    {
                        "name": "Seunghyun Shin"
                    },
                    {
                        "name": "Dongmin Shin"
                    },
                    {
                        "name": "Jisu Shin"
                    },
                    {
                        "name": "Hae-Gon Jeon"
                    },
                    {
                        "name": "Joon-Young Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joon-Young Lee"
                },
                "author": "Joon-Young Lee",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00546v1",
                "updated": "2025-08-01T11:39:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    39,
                    32,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T11:39:32Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    39,
                    32,
                    4,
                    213,
                    0
                ],
                "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval"
                },
                "summary": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%."
                },
                "authors": [
                    {
                        "name": "Wenchao Gu"
                    },
                    {
                        "name": "Zongyi Lyu"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00542v1",
                "updated": "2025-08-01T11:30:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    30,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T11:30:24Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    30,
                    24,
                    4,
                    213,
                    0
                ],
                "title": "Assessing (im)balance in signed brain networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing (im)balance in signed brain networks"
                },
                "summary": "Many complex systems - be they financial, natural or social - are composed by\nunits - such as stocks, neurons or agents - whose joint activity can be\nrepresented as a multivariate time series. An issue of both practical and\ntheoretical importance concerns the possibility of inferring the presence of a\nstatic relationships between any two units solely from their dynamic state. The\npresent contribution aims at providing an answer within the frame of\ntraditional hypothesis testing. Briefly speaking, our suggestion is that of\nlinking any two units if behaving in a sufficiently similar way. To achieve\nsuch a goal, we project a multivariate time series onto a signed graph, by i)\ncomparing the empirical properties of the former with those expected under a\nsuitable benchmark and ii) linking any two units with a positive (negative)\nedge in case the corresponding series share a significantly large number of\nconcordant (discordant) values. To define our benchmarks, we adopt an\ninformation-theoretic approach that is rooted into the constrained maximisation\nof Shannon entropy, a procedure inducing an ensemble of multivariate time\nseries that preserves some of the empirical properties on average while\nrandomising everything else. We showcase the possible applications of our\nmethod by addressing one of the most timely issues in the domain of\nneurosciences, i.e. that of determining if brain networks are frustrated or not\n- and, in case, to what extent. As our results suggest, this is indeed the\ncase, the structure of the negative subgraph being more prone to inter-subject\nvariability than the complementary, positive subgraph. At the mesoscopic level,\ninstead, the minimisation of the Bayesian Information Criterion instantiated\nwith the Signed Stochastic Block Model reveals that brain areas gather into\nmodules aligning with the statistical variant of the Relaxed Balance Theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many complex systems - be they financial, natural or social - are composed by\nunits - such as stocks, neurons or agents - whose joint activity can be\nrepresented as a multivariate time series. An issue of both practical and\ntheoretical importance concerns the possibility of inferring the presence of a\nstatic relationships between any two units solely from their dynamic state. The\npresent contribution aims at providing an answer within the frame of\ntraditional hypothesis testing. Briefly speaking, our suggestion is that of\nlinking any two units if behaving in a sufficiently similar way. To achieve\nsuch a goal, we project a multivariate time series onto a signed graph, by i)\ncomparing the empirical properties of the former with those expected under a\nsuitable benchmark and ii) linking any two units with a positive (negative)\nedge in case the corresponding series share a significantly large number of\nconcordant (discordant) values. To define our benchmarks, we adopt an\ninformation-theoretic approach that is rooted into the constrained maximisation\nof Shannon entropy, a procedure inducing an ensemble of multivariate time\nseries that preserves some of the empirical properties on average while\nrandomising everything else. We showcase the possible applications of our\nmethod by addressing one of the most timely issues in the domain of\nneurosciences, i.e. that of determining if brain networks are frustrated or not\n- and, in case, to what extent. As our results suggest, this is indeed the\ncase, the structure of the negative subgraph being more prone to inter-subject\nvariability than the complementary, positive subgraph. At the mesoscopic level,\ninstead, the minimisation of the Bayesian Information Criterion instantiated\nwith the Signed Stochastic Block Model reveals that brain areas gather into\nmodules aligning with the statistical variant of the Relaxed Balance Theory."
                },
                "authors": [
                    {
                        "name": "Marzio Di Vece"
                    },
                    {
                        "name": "Emanuele Agrimi"
                    },
                    {
                        "name": "Samuele Tatullo"
                    },
                    {
                        "name": "Tommaso Gili"
                    },
                    {
                        "name": "Miguel Ibez-Berganza"
                    },
                    {
                        "name": "Tiziano Squartini"
                    }
                ],
                "author_detail": {
                    "name": "Tiziano Squartini"
                },
                "author": "Tiziano Squartini",
                "arxiv_comment": "41 pages, 17 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00528v1",
                "updated": "2025-08-01T11:09:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    9,
                    13,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T11:09:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    9,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "EPANet: Efficient Path Aggregation Network for Underwater Fish Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPANet: Efficient Path Aggregation Network for Underwater Fish Detection"
                },
                "summary": "Underwater fish detection (UFD) remains a challenging task in computer vision\ndue to low object resolution, significant background interference, and high\nvisual similarity between targets and surroundings. Existing approaches\nprimarily focus on local feature enhancement or incorporate complex attention\nmechanisms to highlight small objects, often at the cost of increased model\ncomplexity and reduced efficiency. To address these limitations, we propose an\nefficient path aggregation network (EPANet), which leverages complementary\nfeature integration to achieve accurate and lightweight UFD. EPANet consists of\ntwo key components: an efficient path aggregation feature pyramid network\n(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP\nbottleneck). The EPA-FPN introduces long-range skip connections across\ndisparate scales to improve semantic-spatial complementarity, while cross-layer\nfusion paths are adopted to enhance feature integration efficiency. The MS-DDSP\nbottleneck extends the conventional bottleneck structure by introducing\nfiner-grained feature division and diverse convolutional operations, thereby\nincreasing local feature diversity and representation capacity. Extensive\nexperiments on benchmark UFD datasets demonstrate that EPANet outperforms\nstate-of-the-art methods in terms of detection accuracy and inference speed,\nwhile maintaining comparable or even lower parameter complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater fish detection (UFD) remains a challenging task in computer vision\ndue to low object resolution, significant background interference, and high\nvisual similarity between targets and surroundings. Existing approaches\nprimarily focus on local feature enhancement or incorporate complex attention\nmechanisms to highlight small objects, often at the cost of increased model\ncomplexity and reduced efficiency. To address these limitations, we propose an\nefficient path aggregation network (EPANet), which leverages complementary\nfeature integration to achieve accurate and lightweight UFD. EPANet consists of\ntwo key components: an efficient path aggregation feature pyramid network\n(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP\nbottleneck). The EPA-FPN introduces long-range skip connections across\ndisparate scales to improve semantic-spatial complementarity, while cross-layer\nfusion paths are adopted to enhance feature integration efficiency. The MS-DDSP\nbottleneck extends the conventional bottleneck structure by introducing\nfiner-grained feature division and diverse convolutional operations, thereby\nincreasing local feature diversity and representation capacity. Extensive\nexperiments on benchmark UFD datasets demonstrate that EPANet outperforms\nstate-of-the-art methods in terms of detection accuracy and inference speed,\nwhile maintaining comparable or even lower parameter complexity."
                },
                "authors": [
                    {
                        "name": "Jinsong Yang"
                    },
                    {
                        "name": "Zeyuan Hu"
                    },
                    {
                        "name": "Yichen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yichen Li"
                },
                "author": "Yichen Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1407.0064v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1407.0064v5",
                "updated": "2025-08-01T10:59:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    59,
                    17,
                    4,
                    213,
                    0
                ],
                "published": "2014-06-30T21:33:11Z",
                "published_parsed": [
                    2014,
                    6,
                    30,
                    21,
                    33,
                    11,
                    0,
                    181,
                    0
                ],
                "title": "Zero & $N$-inflated overdispersed binomial models for sum-constrained\n  Poisson count processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero & $N$-inflated overdispersed binomial models for sum-constrained\n  Poisson count processes"
                },
                "summary": "A frequent challenge encountered with compositional ecological data is how to\ninterpret and model data with a high proportion of zeros and $N$'s. Such data\nfrequently occur in ecological applications where counts of species are\ncollected until a pre-specified total imposed (typically) by sampling cost is\nreached. In the bivariate count (two-species) setting we focus on in this\narticle, zero-inflation of one species will result in $N$-inflation of the\nother. This can lead to species absence being attributed to an unsuitable\nhabitat as opposed to missingness by chance. Similarly, an excess of $N$'s will\nlead to misleading inferences about habitat preference and abundance estimates.\nOur contribution is to identify that two independent zero-inflated Poisson\nprocesses subject to a sum constraint provide a novel biologically-motivated\ngenerating mechanism for the occurrence of binomial count data exhibiting zero\nand $N$-inflation. We identify an extension to the model to capture additional\noverdispersion within the data resulting in a novel zero and $N$-inflated\nbeta-binomial model. We consider two motivating datasets, one involving a\npesticide treatment for an invasive species, and a second involving the\nabundance of two plant species. We demonstrate that incorporation of covariates\nin each case enable learning about sources of zero and $N$-inflation as well as\nabundance. We show that the models result in improved understanding of\nunderlying biological processes as well as improved predictive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A frequent challenge encountered with compositional ecological data is how to\ninterpret and model data with a high proportion of zeros and $N$'s. Such data\nfrequently occur in ecological applications where counts of species are\ncollected until a pre-specified total imposed (typically) by sampling cost is\nreached. In the bivariate count (two-species) setting we focus on in this\narticle, zero-inflation of one species will result in $N$-inflation of the\nother. This can lead to species absence being attributed to an unsuitable\nhabitat as opposed to missingness by chance. Similarly, an excess of $N$'s will\nlead to misleading inferences about habitat preference and abundance estimates.\nOur contribution is to identify that two independent zero-inflated Poisson\nprocesses subject to a sum constraint provide a novel biologically-motivated\ngenerating mechanism for the occurrence of binomial count data exhibiting zero\nand $N$-inflation. We identify an extension to the model to capture additional\noverdispersion within the data resulting in a novel zero and $N$-inflated\nbeta-binomial model. We consider two motivating datasets, one involving a\npesticide treatment for an invasive species, and a second involving the\nabundance of two plant species. We demonstrate that incorporation of covariates\nin each case enable learning about sources of zero and $N$-inflation as well as\nabundance. We show that the models result in improved understanding of\nunderlying biological processes as well as improved predictive performance."
                },
                "authors": [
                    {
                        "name": "James Sweeney"
                    },
                    {
                        "name": "John Haslett"
                    },
                    {
                        "name": "Dipankar Bandyopadhyay"
                    },
                    {
                        "name": "Michael Fop"
                    },
                    {
                        "name": "Andrew C. Parnell"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C. Parnell"
                },
                "author": "Andrew C. Parnell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1407.0064v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1407.0064v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19870v3",
                "updated": "2025-08-01T10:57:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    57,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-26T11:55:44Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    55,
                    44,
                    0,
                    146,
                    0
                ],
                "title": "A pipeline for searching and fitting instrumental glitches in LISA data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A pipeline for searching and fitting instrumental glitches in LISA data"
                },
                "summary": "Instrumental artefacts, such as glitches, can significantly compromise the\nscientific output of LISA. Our methodology employs advanced Bayesian\ntechniques, including Reversible Jump Markov Chain Monte Carlo and parallel\ntempering to find and characterize glitches and astrophysical signals. The\nrobustness of the pipeline is demonstrated through its ability to\nsimultaneously handle diverse glitch morphologies and it is validated with a\n'Spritz'-type data set from the LISA Data Challenge. Our approach enables\naccurate inference on Massive Black Hole Binaries, while simultaneously\ncharacterizing both instrumental artefacts and noise. These results present a\nsignificant development in strategies for differentiating between instrumental\nnoise and astrophysical signals, which will ultimately improve the accuracy and\nreliability of source population analyses with LISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instrumental artefacts, such as glitches, can significantly compromise the\nscientific output of LISA. Our methodology employs advanced Bayesian\ntechniques, including Reversible Jump Markov Chain Monte Carlo and parallel\ntempering to find and characterize glitches and astrophysical signals. The\nrobustness of the pipeline is demonstrated through its ability to\nsimultaneously handle diverse glitch morphologies and it is validated with a\n'Spritz'-type data set from the LISA Data Challenge. Our approach enables\naccurate inference on Massive Black Hole Binaries, while simultaneously\ncharacterizing both instrumental artefacts and noise. These results present a\nsignificant development in strategies for differentiating between instrumental\nnoise and astrophysical signals, which will ultimately improve the accuracy and\nreliability of source population analyses with LISA."
                },
                "authors": [
                    {
                        "name": "Martina Muratore"
                    },
                    {
                        "name": "Jonathan Gair"
                    },
                    {
                        "name": "Olaf Hartwig"
                    },
                    {
                        "name": "Michael L. Katz"
                    },
                    {
                        "name": "Alexandre Toubiana"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Toubiana"
                },
                "author": "Alexandre Toubiana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19693v2",
                "updated": "2025-08-01T10:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    53,
                    31,
                    4,
                    213,
                    0
                ],
                "published": "2025-03-25T14:18:21Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    18,
                    21,
                    1,
                    84,
                    0
                ],
                "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation"
                },
                "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance"
                },
                "authors": [
                    {
                        "name": "Itay Nakash"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Elad Hoffer"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15170v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15170v3",
                "updated": "2025-08-01T10:42:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    42,
                    39,
                    4,
                    213,
                    0
                ],
                "published": "2025-06-18T06:33:19Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    33,
                    19,
                    2,
                    169,
                    0
                ],
                "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in\n  Jailbreak Attacks and Defenses within LLM Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in\n  Jailbreak Attacks and Defenses within LLM Ecosystem"
                },
                "summary": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs."
                },
                "authors": [
                    {
                        "name": "Yanxu Mao"
                    },
                    {
                        "name": "Tiehan Cui"
                    },
                    {
                        "name": "Peipei Liu"
                    },
                    {
                        "name": "Datao You"
                    },
                    {
                        "name": "Hongsong Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hongsong Zhu"
                },
                "author": "Hongsong Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15170v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15170v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.04095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.04095v3",
                "updated": "2025-08-01T10:37:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    37,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2023-05-06T16:47:52Z",
                "published_parsed": [
                    2023,
                    5,
                    6,
                    16,
                    47,
                    52,
                    5,
                    126,
                    0
                ],
                "title": "Gradient Leakage Defense with Key-Lock Module for Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Leakage Defense with Key-Lock Module for Federated Learning"
                },
                "summary": "Federated Learning (FL) is a widely adopted privacy-preserving machine\nlearning approach where private data remains local, enabling secure\ncomputations and the exchange of local model gradients between local clients\nand third-party parameter servers. However, recent findings reveal that privacy\nmay be compromised and sensitive information potentially recovered from shared\ngradients. In this study, we offer detailed analysis and a novel perspective on\nunderstanding the gradient leakage problem. These theoretical works lead to a\nnew gradient leakage defense technique that secures arbitrary model\narchitectures using a private key-lock module. Only the locked gradient is\ntransmitted to the parameter server for global model aggregation. Our proposed\nlearning method is resistant to gradient leakage attacks, and the key-lock\nmodule is designed and trained to ensure that, without the private information\nof the key-lock module: a) reconstructing private training data from the shared\ngradient is infeasible; and b) the global model's inference performance is\nsignificantly compromised. We discuss the theoretical underpinnings of why\ngradients can leak private information and provide theoretical proof of our\nmethod's effectiveness. We conducted extensive empirical evaluations with many\nmodels on several popular benchmarks, demonstrating the robustness of our\nproposed approach in both maintaining model performance and defending against\ngradient leakage attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a widely adopted privacy-preserving machine\nlearning approach where private data remains local, enabling secure\ncomputations and the exchange of local model gradients between local clients\nand third-party parameter servers. However, recent findings reveal that privacy\nmay be compromised and sensitive information potentially recovered from shared\ngradients. In this study, we offer detailed analysis and a novel perspective on\nunderstanding the gradient leakage problem. These theoretical works lead to a\nnew gradient leakage defense technique that secures arbitrary model\narchitectures using a private key-lock module. Only the locked gradient is\ntransmitted to the parameter server for global model aggregation. Our proposed\nlearning method is resistant to gradient leakage attacks, and the key-lock\nmodule is designed and trained to ensure that, without the private information\nof the key-lock module: a) reconstructing private training data from the shared\ngradient is infeasible; and b) the global model's inference performance is\nsignificantly compromised. We discuss the theoretical underpinnings of why\ngradients can leak private information and provide theoretical proof of our\nmethod's effectiveness. We conducted extensive empirical evaluations with many\nmodels on several popular benchmarks, demonstrating the robustness of our\nproposed approach in both maintaining model performance and defending against\ngradient leakage attacks."
                },
                "authors": [
                    {
                        "name": "Hanchi Ren"
                    },
                    {
                        "name": "Jingjing Deng"
                    },
                    {
                        "name": "Xianghua Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xianghua Xie"
                },
                "author": "Xianghua Xie",
                "arxiv_comment": "The source code can be found at https://github.com/Rand2AI/FedKL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.04095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.04095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00507v1",
                "updated": "2025-08-01T10:36:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    36,
                    39,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T10:36:39Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    36,
                    39,
                    4,
                    213,
                    0
                ],
                "title": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration\n  for Text-Attributed Graph Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration\n  for Text-Attributed Graph Anomaly Detection"
                },
                "summary": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD."
                },
                "authors": [
                    {
                        "name": "Yiming Xu"
                    },
                    {
                        "name": "Jiarun Chen"
                    },
                    {
                        "name": "Zhen Peng"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Lan Ma"
                    },
                    {
                        "name": "Bin Shi"
                    },
                    {
                        "name": "Bo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dong"
                },
                "author": "Bo Dong",
                "arxiv_comment": "Accepted by ACM Multimedia 2025 (MM '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18348v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18348v3",
                "updated": "2025-08-01T10:35:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    35,
                    50,
                    4,
                    213,
                    0
                ],
                "published": "2025-06-23T07:12:08Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    12,
                    8,
                    0,
                    174,
                    0
                ],
                "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team"
                },
                "summary": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research."
                },
                "authors": [
                    {
                        "name": "Weilun Yu"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Yonggui Huang"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Honggang Qi"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Xiaoli Diao"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18348v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18348v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08753v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08753v3",
                "updated": "2025-08-01T10:25:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    25,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2024-03-13T17:51:47Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    51,
                    47,
                    2,
                    73,
                    0
                ],
                "title": "Invalid proxies and volatility changes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invalid proxies and volatility changes"
                },
                "summary": "When in proxy-SVARs the covariance matrix of VAR disturbances is subject to\nexogenous, permanent breaks that cause IRFs to change across volatility\nregimes, even strong, exogenous external instruments yield inconsistent\nestimates of the dynamic causal effects. However, if these volatility shifts\nare properly incorporated into the analysis through (testable) \"stability\nrestrictions\", we demonstrate that the target IRFs are point-identified and can\nbe estimated consistently under a necessary and sufficient rank condition. If\nthe shifts in volatility are sufficiently informative, standard asymptotic\ninference remains valid even with (i) local-to-zero covariance between the\nproxies and the instrumented structural shocks, and (ii) potential failures of\ninstrument exogeneity. Intuitively, shifts in volatility act similarly to\nstrong instruments that are correlated with both the target and non-target\nshocks. We illustrate the effectiveness of our approach by revisiting a seminal\nfiscal proxy-SVAR for the US economy. We detect a sharp change in the size of\nthe tax multiplier when the narrative tax instrument is complemented with the\ndecline in unconditional volatility observed during the transition from the\nGreat Inflation to the Great Moderation. The narrative tax instrument\ncontributes to identify the tax shock in both regimes, although our empirical\nanalysis raises concerns about its \"statistical\" validity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When in proxy-SVARs the covariance matrix of VAR disturbances is subject to\nexogenous, permanent breaks that cause IRFs to change across volatility\nregimes, even strong, exogenous external instruments yield inconsistent\nestimates of the dynamic causal effects. However, if these volatility shifts\nare properly incorporated into the analysis through (testable) \"stability\nrestrictions\", we demonstrate that the target IRFs are point-identified and can\nbe estimated consistently under a necessary and sufficient rank condition. If\nthe shifts in volatility are sufficiently informative, standard asymptotic\ninference remains valid even with (i) local-to-zero covariance between the\nproxies and the instrumented structural shocks, and (ii) potential failures of\ninstrument exogeneity. Intuitively, shifts in volatility act similarly to\nstrong instruments that are correlated with both the target and non-target\nshocks. We illustrate the effectiveness of our approach by revisiting a seminal\nfiscal proxy-SVAR for the US economy. We detect a sharp change in the size of\nthe tax multiplier when the narrative tax instrument is complemented with the\ndecline in unconditional volatility observed during the transition from the\nGreat Inflation to the Great Moderation. The narrative tax instrument\ncontributes to identify the tax shock in both regimes, although our empirical\nanalysis raises concerns about its \"statistical\" validity."
                },
                "authors": [
                    {
                        "name": "Giovanni Angelini"
                    },
                    {
                        "name": "Luca Fanelli"
                    },
                    {
                        "name": "Luca Neri"
                    }
                ],
                "author_detail": {
                    "name": "Luca Neri"
                },
                "author": "Luca Neri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08753v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08753v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00500v1",
                "updated": "2025-08-01T10:24:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    24,
                    47,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T10:24:47Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    24,
                    47,
                    4,
                    213,
                    0
                ],
                "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via\n  Probabilistic Model Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via\n  Probabilistic Model Checking"
                },
                "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Chris M. Poskitt"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Jiali Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jiali Wei"
                },
                "author": "Jiali Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17407v2",
                "updated": "2025-08-01T10:09:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    9,
                    29,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-24T18:36:15Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    36,
                    15,
                    0,
                    55,
                    0
                ],
                "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning"
                },
                "summary": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Jiwoo Hong"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "ACL 2025 (ORAL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22462v2",
                "updated": "2025-08-01T10:07:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    7,
                    37,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T08:08:48Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    8,
                    48,
                    2,
                    211,
                    0
                ],
                "title": "IFEvalCode: Controlled Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFEvalCode: Controlled Code Generation"
                },
                "summary": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions."
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Guanglin Niu"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19651v2",
                "updated": "2025-08-01T10:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    7,
                    8,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-27T00:49:44Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    0,
                    49,
                    44,
                    3,
                    58,
                    0
                ],
                "title": "Unlocking Multi-Modal Potentials for Link Prediction on Dynamic\n  Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Multi-Modal Potentials for Link Prediction on Dynamic\n  Text-Attributed Graphs"
                },
                "summary": "Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that\ncaptures evolving temporal events (edges) alongside rich textual attributes.\nExisting studies can be broadly categorized into TGNN-driven and LLM-driven\napproaches, both of which encode textual attributes and temporal structures for\nDyTAG representation. We observe that DyTAGs inherently comprise three distinct\nmodalities: temporal, textual, and structural, often exhibiting completely\ndisjoint distributions. However, the first two modalities are largely\noverlooked by existing studies, leading to suboptimal performance. To address\nthis, we propose MoMent, a multi-modal model that explicitly models,\nintegrates, and aligns each modality to learn node representations for link\nprediction. Given the disjoint nature of the original modality distributions,\nwe first construct modality-specific features and encode them using individual\nencoders to capture correlations across temporal patterns, semantic context,\nand local structures. Each encoder generates modality-specific tokens, which\nare then fused into comprehensive node representations with a theoretical\nguarantee. To avoid disjoint subspaces of these heterogeneous modalities, we\npropose a dual-domain alignment loss that first aligns their distributions\nglobally and then fine-tunes coherence at the instance level. This enhances\ncoherent representations from temporal, textual, and structural views.\nExtensive experiments across seven datasets show that MoMent achieves up to\n17.28% accuracy improvement and up to 31x speed-up against eight baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that\ncaptures evolving temporal events (edges) alongside rich textual attributes.\nExisting studies can be broadly categorized into TGNN-driven and LLM-driven\napproaches, both of which encode textual attributes and temporal structures for\nDyTAG representation. We observe that DyTAGs inherently comprise three distinct\nmodalities: temporal, textual, and structural, often exhibiting completely\ndisjoint distributions. However, the first two modalities are largely\noverlooked by existing studies, leading to suboptimal performance. To address\nthis, we propose MoMent, a multi-modal model that explicitly models,\nintegrates, and aligns each modality to learn node representations for link\nprediction. Given the disjoint nature of the original modality distributions,\nwe first construct modality-specific features and encode them using individual\nencoders to capture correlations across temporal patterns, semantic context,\nand local structures. Each encoder generates modality-specific tokens, which\nare then fused into comprehensive node representations with a theoretical\nguarantee. To avoid disjoint subspaces of these heterogeneous modalities, we\npropose a dual-domain alignment loss that first aligns their distributions\nglobally and then fine-tunes coherence at the instance level. This enhances\ncoherent representations from temporal, textual, and structural views.\nExtensive experiments across seven datasets show that MoMent achieves up to\n17.28% accuracy improvement and up to 31x speed-up against eight baselines."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Xu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Xiwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiwei Xu"
                },
                "author": "Xiwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00489v1",
                "updated": "2025-08-01T10:06:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    6,
                    38,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T10:06:38Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    6,
                    38,
                    4,
                    213,
                    0
                ],
                "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Missing Parts: Augmenting Fact Verification with Half-Truth\n  Detection"
                },
                "summary": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Jincheng Wang"
                    },
                    {
                        "name": "Anthony K. H. Tung"
                    }
                ],
                "author_detail": {
                    "name": "Anthony K. H. Tung"
                },
                "author": "Anthony K. H. Tung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03703v2",
                "updated": "2025-08-01T09:56:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    56,
                    35,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-04T16:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    16,
                    38,
                    9,
                    4,
                    185,
                    0
                ],
                "title": "Sign Spotting Disambiguation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Spotting Disambiguation using Large Language Models"
                },
                "summary": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting."
                },
                "authors": [
                    {
                        "name": "JianHe Low"
                    },
                    {
                        "name": "Ozge Mercanoglu Sincan"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_comment": "Accepted in the international conference on Intelligent Virtual\n  Agents (IVA Adjunct)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00478v1",
                "updated": "2025-08-01T09:53:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    53,
                    6,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:53:06Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    53,
                    6,
                    4,
                    213,
                    0
                ],
                "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy\n  Optimization"
                },
                "summary": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use."
                },
                "authors": [
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Lu Lin"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Biplab Sikdar"
                    }
                ],
                "author_detail": {
                    "name": "Biplab Sikdar"
                },
                "author": "Biplab Sikdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A10, 91A43, 68T01, 94A60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; I.2.6; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23689v2",
                "updated": "2025-08-01T09:52:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    52,
                    43,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T16:06:45Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    6,
                    45,
                    3,
                    212,
                    0
                ],
                "title": "Probing graph topology from local quantum measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing graph topology from local quantum measurements"
                },
                "summary": "We show that global properties of an unknown quantum network, such as the\naverage degree, hub density, and the number of closed paths of fixed length,\ncan be inferred from strictly local quantum measurements. In particular, we\ndemonstrate that a malicious agent with access to only a small subset of nodes\ncan initialize quantum states locally and, through repeated short-time\nmeasurements, extract sensitive structural information about the entire\nnetwork. The intrusion strategy is inspired by extreme learning and quantum\nreservoir computing and combines short-time quantum evolution with a\nnon-iterative linear readout with trainable weights. These results suggest new\nstrategies for intrusion detection and structural diagnostics in future quantum\nInternet infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that global properties of an unknown quantum network, such as the\naverage degree, hub density, and the number of closed paths of fixed length,\ncan be inferred from strictly local quantum measurements. In particular, we\ndemonstrate that a malicious agent with access to only a small subset of nodes\ncan initialize quantum states locally and, through repeated short-time\nmeasurements, extract sensitive structural information about the entire\nnetwork. The intrusion strategy is inspired by extreme learning and quantum\nreservoir computing and combines short-time quantum evolution with a\nnon-iterative linear readout with trainable weights. These results suggest new\nstrategies for intrusion detection and structural diagnostics in future quantum\nInternet infrastructures."
                },
                "authors": [
                    {
                        "name": "F. Romeo"
                    },
                    {
                        "name": "J. Settino"
                    }
                ],
                "author_detail": {
                    "name": "J. Settino"
                },
                "author": "J. Settino",
                "arxiv_comment": "6 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00468v1",
                "updated": "2025-08-01T09:43:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    43,
                    12,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:43:12Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    43,
                    12,
                    4,
                    213,
                    0
                ],
                "title": "Inference of maximum parsimony phylogenetic trees with model-based\n  classical and quantum methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of maximum parsimony phylogenetic trees with model-based\n  classical and quantum methods"
                },
                "summary": "The maximum parsimony phylogenetic tree reconstruction problem is NP-hard,\npresenting a computational bottleneck for classical computing and motivating\nthe exploration of emerging paradigms like quantum computing. To this end, we\ndesign three optimization models compatible with both classical and quantum\nsolvers. Our method directly searches the complete solution space of all\npossible tree topologies and ancestral states, thereby avoiding the potential\nbiases associated with pre-constructing candidate internal nodes. Among these\nmodels, the branch-based model drastically reduces the number of variables and\nexplicit constraints through a specific variable definition, providing a novel\nmodeling approach effective not only for phylogenetic tree building but also\nfor other tree problems. The correctness of this model is validated with a\nclassical solver, which obtains solutions that are generally better than those\nfrom heuristics on the GAPDH gene dataset. Moreover, our quantum simulations\nsuccessfully find the exact optimal solutions for small-scale instances with\nrapid convergence, highlighting the potential of quantum computing to offer a\nnew avenue for solving these intractable problems in evolutionary biology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum parsimony phylogenetic tree reconstruction problem is NP-hard,\npresenting a computational bottleneck for classical computing and motivating\nthe exploration of emerging paradigms like quantum computing. To this end, we\ndesign three optimization models compatible with both classical and quantum\nsolvers. Our method directly searches the complete solution space of all\npossible tree topologies and ancestral states, thereby avoiding the potential\nbiases associated with pre-constructing candidate internal nodes. Among these\nmodels, the branch-based model drastically reduces the number of variables and\nexplicit constraints through a specific variable definition, providing a novel\nmodeling approach effective not only for phylogenetic tree building but also\nfor other tree problems. The correctness of this model is validated with a\nclassical solver, which obtains solutions that are generally better than those\nfrom heuristics on the GAPDH gene dataset. Moreover, our quantum simulations\nsuccessfully find the exact optimal solutions for small-scale instances with\nrapid convergence, highlighting the potential of quantum computing to offer a\nnew avenue for solving these intractable problems in evolutionary biology."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jun-Han Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Han Huang"
                },
                "author": "Jun-Han Huang",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02962v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02962v4",
                "updated": "2025-08-01T09:41:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    41,
                    5,
                    4,
                    213,
                    0
                ],
                "published": "2025-06-30T09:02:45Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    2,
                    45,
                    0,
                    181,
                    0
                ],
                "title": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while LLMs remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have aimed to enhance models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to reliance on single-query mode. In this paper, we\npropose RAG-R1, a novel training framework designed to enable LLMs to\nadaptively leverage internal and external knowledge during the reasoning\nprocess. We further expand the generation and retrieval processes within the\nframework from single-query mode to multi-query parallelism, with the aim of\nreducing inference time and enhancing the model's capabilities. Extensive\nexperiments on seven question-answering benchmarks demonstrate that our method\noutperforms the strongest baseline by up to 13.2% and decreases inference time\nby 11.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while LLMs remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have aimed to enhance models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to reliance on single-query mode. In this paper, we\npropose RAG-R1, a novel training framework designed to enable LLMs to\nadaptively leverage internal and external knowledge during the reasoning\nprocess. We further expand the generation and retrieval processes within the\nframework from single-query mode to multi-query parallelism, with the aim of\nreducing inference time and enhancing the model's capabilities. Extensive\nexperiments on seven question-answering benchmarks demonstrate that our method\noutperforms the strongest baseline by up to 13.2% and decreases inference time\nby 11.1%."
                },
                "authors": [
                    {
                        "name": "Zhiwen Tan"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Qintong Wu"
                    },
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02962v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02962v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00459v1",
                "updated": "2025-08-01T09:31:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    31,
                    48,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:31:48Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    31,
                    48,
                    4,
                    213,
                    0
                ],
                "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Machines: Mathematical Reasoning in the Age of LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended."
                },
                "authors": [
                    {
                        "name": "Andrea Asperti"
                    },
                    {
                        "name": "Alberto Naibo"
                    },
                    {
                        "name": "Claudio Sacerdoti Coen"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Sacerdoti Coen"
                },
                "author": "Claudio Sacerdoti Coen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00454v1",
                "updated": "2025-08-01T09:26:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    26,
                    1,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:26:01Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    26,
                    1,
                    4,
                    213,
                    0
                ],
                "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges"
                },
                "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness."
                },
                "authors": [
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Yunfeng Wang"
                    },
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Keyan Ding"
                    }
                ],
                "author_detail": {
                    "name": "Keyan Ding"
                },
                "author": "Keyan Ding",
                "arxiv_comment": "15 pages, 2 pages, under review at AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00450v1",
                "updated": "2025-08-01T09:10:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    10,
                    56,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:10:56Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    10,
                    56,
                    4,
                    213,
                    0
                ],
                "title": "When Relevance Meets Novelty: Dual-Stable Periodic Optimization for\n  Exploratory Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Relevance Meets Novelty: Dual-Stable Periodic Optimization for\n  Exploratory Recommendation"
                },
                "summary": "Traditional recommendation systems tend to trap users in strong feedback\nloops by excessively pushing content aligned with their historical preferences,\nthereby limiting exploration opportunities and causing content fatigue.\nAlthough large language models (LLMs) demonstrate potential with their diverse\ncontent generation capabilities, existing LLM-enhanced dual-model frameworks\nface two major limitations: first, they overlook long-term preferences driven\nby group identity, leading to biased interest modeling; second, they suffer\nfrom static optimization flaws, as a one-time alignment process fails to\nleverage incremental user data for closed-loop optimization. To address these\nchallenges, we propose the Co-Evolutionary Alignment (CoEA) method. For\ninterest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)\nmodule, jointly modeling long-term group identity and short-term individual\ninterests through parallel processing of behavioral sequences. For static\noptimization limitations, we design a Periodic Collaborative Optimization (PCO)\nmechanism. This mechanism regularly conducts preference verification on\nincremental data using the Relevance LLM, then guides the Novelty LLM to\nperform fine-tuning based on the verification results, and subsequently feeds\nback the output of the incrementally fine-tuned Novelty LLM to the Relevance\nLLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.\nExtensive online and offline experiments verify the effectiveness of the CoEA\nmodel in exploratory recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommendation systems tend to trap users in strong feedback\nloops by excessively pushing content aligned with their historical preferences,\nthereby limiting exploration opportunities and causing content fatigue.\nAlthough large language models (LLMs) demonstrate potential with their diverse\ncontent generation capabilities, existing LLM-enhanced dual-model frameworks\nface two major limitations: first, they overlook long-term preferences driven\nby group identity, leading to biased interest modeling; second, they suffer\nfrom static optimization flaws, as a one-time alignment process fails to\nleverage incremental user data for closed-loop optimization. To address these\nchallenges, we propose the Co-Evolutionary Alignment (CoEA) method. For\ninterest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)\nmodule, jointly modeling long-term group identity and short-term individual\ninterests through parallel processing of behavioral sequences. For static\noptimization limitations, we design a Periodic Collaborative Optimization (PCO)\nmechanism. This mechanism regularly conducts preference verification on\nincremental data using the Relevance LLM, then guides the Novelty LLM to\nperform fine-tuning based on the verification results, and subsequently feeds\nback the output of the incrementally fine-tuned Novelty LLM to the Relevance\nLLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.\nExtensive online and offline experiments verify the effectiveness of the CoEA\nmodel in exploratory recommendation."
                },
                "authors": [
                    {
                        "name": "Hongxiang Lin"
                    },
                    {
                        "name": "Hao Guo"
                    },
                    {
                        "name": "Zeshun Li"
                    },
                    {
                        "name": "Erpeng Xue"
                    },
                    {
                        "name": "Yongqian He"
                    },
                    {
                        "name": "Xiangyu Hou"
                    },
                    {
                        "name": "Zhaoyu Hu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Sheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Chen"
                },
                "author": "Sheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00447v1",
                "updated": "2025-08-01T09:08:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    8,
                    10,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:08:10Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    8,
                    10,
                    4,
                    213,
                    0
                ],
                "title": "CLIPTime: Time-Aware Multimodal Representation Learning from Images and\n  Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIPTime: Time-Aware Multimodal Representation Learning from Images and\n  Text"
                },
                "summary": "Understanding the temporal dynamics of biological growth is critical across\ndiverse fields such as microbiology, agriculture, and biodegradation research.\nAlthough vision-language models like Contrastive Language Image Pretraining\n(CLIP) have shown strong capabilities in joint visual-textual reasoning, their\neffectiveness in capturing temporal progression remains limited. To address\nthis, we propose CLIPTime, a multimodal, multitask framework designed to\npredict both the developmental stage and the corresponding timestamp of fungal\ngrowth from image and text inputs. Built upon the CLIP architecture, our model\nlearns joint visual-textual embeddings and enables time-aware inference without\nrequiring explicit temporal input during testing. To facilitate training and\nevaluation, we introduce a synthetic fungal growth dataset annotated with\naligned timestamps and categorical stage labels. CLIPTime jointly performs\nclassification and regression, predicting discrete growth stages alongside\ncontinuous timestamps. We also propose custom evaluation metrics, including\ntemporal accuracy and regression error, to assess the precision of time-aware\npredictions. Experimental results demonstrate that CLIPTime effectively models\nbiological progression and produces interpretable, temporally grounded outputs,\nhighlighting the potential of vision-language models in real-world biological\nmonitoring applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the temporal dynamics of biological growth is critical across\ndiverse fields such as microbiology, agriculture, and biodegradation research.\nAlthough vision-language models like Contrastive Language Image Pretraining\n(CLIP) have shown strong capabilities in joint visual-textual reasoning, their\neffectiveness in capturing temporal progression remains limited. To address\nthis, we propose CLIPTime, a multimodal, multitask framework designed to\npredict both the developmental stage and the corresponding timestamp of fungal\ngrowth from image and text inputs. Built upon the CLIP architecture, our model\nlearns joint visual-textual embeddings and enables time-aware inference without\nrequiring explicit temporal input during testing. To facilitate training and\nevaluation, we introduce a synthetic fungal growth dataset annotated with\naligned timestamps and categorical stage labels. CLIPTime jointly performs\nclassification and regression, predicting discrete growth stages alongside\ncontinuous timestamps. We also propose custom evaluation metrics, including\ntemporal accuracy and regression error, to assess the precision of time-aware\npredictions. Experimental results demonstrate that CLIPTime effectively models\nbiological progression and produces interpretable, temporally grounded outputs,\nhighlighting the potential of vision-language models in real-world biological\nmonitoring applications."
                },
                "authors": [
                    {
                        "name": "Anju Rani"
                    },
                    {
                        "name": "Daniel Ortiz-Arroyo"
                    },
                    {
                        "name": "Petar Durdevic"
                    }
                ],
                "author_detail": {
                    "name": "Petar Durdevic"
                },
                "author": "Petar Durdevic",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.00806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00806v1",
                "updated": "2025-08-01T17:39:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    39,
                    25,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T17:39:25Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    39,
                    25,
                    4,
                    213,
                    0
                ],
                "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory\n  Management"
                },
                "summary": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline."
                },
                "authors": [
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Zhuohong Deng"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Hongzi Zhu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00804v1",
                "updated": "2025-08-01T17:37:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    37,
                    19,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T17:37:19Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    37,
                    19,
                    4,
                    213,
                    0
                ],
                "title": "Online Fine-Tuning of Carbon Emission Predictions using Real-Time\n  Recurrent Learning for State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Fine-Tuning of Carbon Emission Predictions using Real-Time\n  Recurrent Learning for State Space Models"
                },
                "summary": "This paper introduces a new approach for fine-tuning the predictions of\nstructured state space models (SSMs) at inference time using real-time\nrecurrent learning. While SSMs are known for their efficiency and long-range\nmodeling capabilities, they are typically trained offline and remain static\nduring deployment. Our method enables online adaptation by continuously\nupdating model parameters in response to incoming data. We evaluate our\napproach for linear-recurrent-unit SSMs using a small carbon emission dataset\ncollected from embedded automotive hardware. Experimental results show that our\nmethod consistently reduces prediction error online during inference,\ndemonstrating its potential for dynamic, resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new approach for fine-tuning the predictions of\nstructured state space models (SSMs) at inference time using real-time\nrecurrent learning. While SSMs are known for their efficiency and long-range\nmodeling capabilities, they are typically trained offline and remain static\nduring deployment. Our method enables online adaptation by continuously\nupdating model parameters in response to incoming data. We evaluate our\napproach for linear-recurrent-unit SSMs using a small carbon emission dataset\ncollected from embedded automotive hardware. Experimental results show that our\nmethod consistently reduces prediction error online during inference,\ndemonstrating its potential for dynamic, resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Julian Lemmel"
                    },
                    {
                        "name": "Manuel Kranzl"
                    },
                    {
                        "name": "Adam Lamine"
                    },
                    {
                        "name": "Philipp Neubauer"
                    },
                    {
                        "name": "Radu Grosu"
                    },
                    {
                        "name": "Sophie Neubauer"
                    }
                ],
                "author_detail": {
                    "name": "Sophie Neubauer"
                },
                "author": "Sophie Neubauer",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15126v2",
                "updated": "2025-08-01T17:31:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    31,
                    33,
                    4,
                    213,
                    0
                ],
                "published": "2024-09-23T15:32:46Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    32,
                    46,
                    0,
                    267,
                    0
                ],
                "title": "UTrace: Poisoning Forensics for Private Collaborative Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UTrace: Poisoning Forensics for Private Collaborative Learning"
                },
                "summary": "Privacy-preserving machine learning (PPML) enables multiple data owners to\ncontribute their data privately to a set of servers that run a secure\nmulti-party computation (MPC) protocol to train a joint ML model. In these\nprotocols, the input data remains private throughout the training process, and\nonly the resulting model is made available. While this approach benefits\nprivacy, it also exacerbates the risks of data poisoning, where compromised\ndata owners induce undesirable model behavior by contributing malicious\ndatasets. Existing MPC mechanisms can mitigate certain poisoning attacks, but\nthese measures are not exhaustive. To complement existing poisoning defenses,\nwe introduce UTrace: a framework for User-level Traceback of poisoning attacks\nin PPML. Utrace computes user responsibility scores using gradient similarity\nmetrics aggregated across the most relevant samples in an owner's dataset.\nUTrace is effective at low poisoning rates and is resilient to poisoning\nattacks distributed across multiple data owners, unlike existing\nunlearning-based methods. We introduce methods for checkpointing gradients with\nlow storage overhead, enabling traceback in the absence of data owners at\ndeployment time. We also design several optimizations that reduce traceback\ntime and communication in MPC. We provide a comprehensive evaluation of UTrace\nacross four datasets from three data modalities (vision, text, and malware) and\nshow its effectiveness against 10 poisoning attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-preserving machine learning (PPML) enables multiple data owners to\ncontribute their data privately to a set of servers that run a secure\nmulti-party computation (MPC) protocol to train a joint ML model. In these\nprotocols, the input data remains private throughout the training process, and\nonly the resulting model is made available. While this approach benefits\nprivacy, it also exacerbates the risks of data poisoning, where compromised\ndata owners induce undesirable model behavior by contributing malicious\ndatasets. Existing MPC mechanisms can mitigate certain poisoning attacks, but\nthese measures are not exhaustive. To complement existing poisoning defenses,\nwe introduce UTrace: a framework for User-level Traceback of poisoning attacks\nin PPML. Utrace computes user responsibility scores using gradient similarity\nmetrics aggregated across the most relevant samples in an owner's dataset.\nUTrace is effective at low poisoning rates and is resilient to poisoning\nattacks distributed across multiple data owners, unlike existing\nunlearning-based methods. We introduce methods for checkpointing gradients with\nlow storage overhead, enabling traceback in the absence of data owners at\ndeployment time. We also design several optimizations that reduce traceback\ntime and communication in MPC. We provide a comprehensive evaluation of UTrace\nacross four datasets from three data modalities (vision, text, and malware) and\nshow its effectiveness against 10 poisoning attacks."
                },
                "authors": [
                    {
                        "name": "Evan Rose"
                    },
                    {
                        "name": "Hidde Lycklama"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Anwar Hithnawi"
                    },
                    {
                        "name": "Alina Oprea"
                    }
                ],
                "author_detail": {
                    "name": "Alina Oprea"
                },
                "author": "Alina Oprea",
                "arxiv_comment": "28 pages, 10 figures; update ack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00800v1",
                "updated": "2025-08-01T17:30:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    30,
                    3,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T17:30:03Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    30,
                    3,
                    4,
                    213,
                    0
                ],
                "title": "Multibeam High Throughput Satellite: Hardware Foundation, Resource\n  Allocation, and Precoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multibeam High Throughput Satellite: Hardware Foundation, Resource\n  Allocation, and Precoding"
                },
                "summary": "With its wide coverage and uninterrupted service, satellite communication is\na critical technology for next-generation 6G communications. High throughput\nsatellite (HTS) systems, utilizing multipoint beam and frequency multiplexing\ntechniques, enable satellite communication capacity of up to Tbps to meet the\ngrowing traffic demand. Therefore, it is imperative to review\nthe-state-of-the-art of multibeam HTS systems and identify their associated\nchallenges and perspectives. Firstly, we summarize the multibeam HTS hardware\nfoundations, including ground station systems, on-board payloads, and user\nterminals. Subsequently, we review the flexible on-board radio resource\nallocation approaches of bandwidth, power, time slot, and joint allocation\nschemes of HTS systems to optimize resource utilization and cater to\nnon-uniform service demand. Additionally, we survey multibeam precoding methods\nfor the HTS system to achieve full-frequency reuse and interference\ncancellation, which are classified according to different deployments such as\nsingle gateway precoding, multiple gateway precoding, on-board precoding, and\nhybrid on-board/on-ground precoding. Finally, we disscuss the challenges\nrelated to Q/V band link outage, time and frequency synchronization of\ngateways, the accuracy of channel state information (CSI), payload light-weight\ndevelopment, and the application of deep learning (DL). Research on these\ntopics will contribute to enhancing the performance of HTS systems and finally\ndelivering high-speed data to areas underserved by terrestrial networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With its wide coverage and uninterrupted service, satellite communication is\na critical technology for next-generation 6G communications. High throughput\nsatellite (HTS) systems, utilizing multipoint beam and frequency multiplexing\ntechniques, enable satellite communication capacity of up to Tbps to meet the\ngrowing traffic demand. Therefore, it is imperative to review\nthe-state-of-the-art of multibeam HTS systems and identify their associated\nchallenges and perspectives. Firstly, we summarize the multibeam HTS hardware\nfoundations, including ground station systems, on-board payloads, and user\nterminals. Subsequently, we review the flexible on-board radio resource\nallocation approaches of bandwidth, power, time slot, and joint allocation\nschemes of HTS systems to optimize resource utilization and cater to\nnon-uniform service demand. Additionally, we survey multibeam precoding methods\nfor the HTS system to achieve full-frequency reuse and interference\ncancellation, which are classified according to different deployments such as\nsingle gateway precoding, multiple gateway precoding, on-board precoding, and\nhybrid on-board/on-ground precoding. Finally, we disscuss the challenges\nrelated to Q/V band link outage, time and frequency synchronization of\ngateways, the accuracy of channel state information (CSI), payload light-weight\ndevelopment, and the application of deep learning (DL). Research on these\ntopics will contribute to enhancing the performance of HTS systems and finally\ndelivering high-speed data to areas underserved by terrestrial networks."
                },
                "authors": [
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Wen-Xuan Long"
                    },
                    {
                        "name": "Bing-Qian Wang"
                    },
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Rui-Jin Sun"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Gan Zheng"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "38 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00793v1",
                "updated": "2025-08-01T17:21:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    21,
                    26,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T17:21:26Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    21,
                    26,
                    4,
                    213,
                    0
                ],
                "title": "Entanglement Management in Space-Based Quantum Information Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement Management in Space-Based Quantum Information Networks"
                },
                "summary": "With the evolution of quantum computing, quantum sensing and secure quantum\ncommunication protocols, the demand for global development of Quantum\nInformation Networks (QIN) has become crucial. Satellites play an indispensable\nrole in enabling connectivity across vast distances, transcending terrestrial\nlimitations. In this article, we explore various ways in which satellites may\nbe involved in the deployment of these novel networks from their integration\ninto the network architecture to the challenges they face.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the evolution of quantum computing, quantum sensing and secure quantum\ncommunication protocols, the demand for global development of Quantum\nInformation Networks (QIN) has become crucial. Satellites play an indispensable\nrole in enabling connectivity across vast distances, transcending terrestrial\nlimitations. In this article, we explore various ways in which satellites may\nbe involved in the deployment of these novel networks from their integration\ninto the network architecture to the challenges they face."
                },
                "authors": [
                    {
                        "name": "Luca Paccard"
                    },
                    {
                        "name": "Agathe Blaise"
                    },
                    {
                        "name": "Fabrice Arnal"
                    },
                    {
                        "name": "Laurent de Forges de Parny"
                    }
                ],
                "author_detail": {
                    "name": "Laurent de Forges de Parny"
                },
                "author": "Laurent de Forges de Parny",
                "arxiv_doi": "10.1117/12.3075233",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1117/12.3075233",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.00793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages",
                "arxiv_journal_ref": "Proceedings Volume 13699, International Conference on Space Optics\n  - ICSO 2024; 136994P (2025)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18365v3",
                "updated": "2025-08-01T17:19:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    19,
                    56,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-24T12:46:30Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    46,
                    30,
                    3,
                    205,
                    0
                ],
                "title": "RecPS: Privacy Risk Scoring for Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecPS: Privacy Risk Scoring for Recommender Systems"
                },
                "summary": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning."
                },
                "authors": [
                    {
                        "name": "Jiajie He"
                    },
                    {
                        "name": "Yuechun Gu"
                    },
                    {
                        "name": "Keke Chen"
                    }
                ],
                "author_detail": {
                    "name": "Keke Chen"
                },
                "author": "Keke Chen",
                "arxiv_comment": "Accepted by ACM RecSys 2025; to appear",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21046v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21046v3",
                "updated": "2025-08-01T17:17:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    17,
                    9,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-28T17:59:05Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    5,
                    0,
                    209,
                    0
                ],
                "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks."
                },
                "authors": [
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Xuan Qi"
                    },
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Yixiong Fang"
                    },
                    {
                        "name": "Qiwen Zhao"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zhenhailong Wang"
                    },
                    {
                        "name": "Minda Hu"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "51 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21046v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21046v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00788v1",
                "updated": "2025-08-01T17:11:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    11,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T17:11:42Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    11,
                    42,
                    4,
                    213,
                    0
                ],
                "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun\n  Handling in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun\n  Handling in Large Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research."
                },
                "authors": [
                    {
                        "name": "Xushuo Tang"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Yongrui Gu"
                    },
                    {
                        "name": "Wenke Yang"
                    },
                    {
                        "name": "Mingchen Ju"
                    },
                    {
                        "name": "Xin Cao"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19673v2",
                "updated": "2025-08-01T17:06:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    6,
                    27,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-25T20:54:04Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    20,
                    54,
                    4,
                    4,
                    206,
                    0
                ],
                "title": "SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions"
                },
                "summary": "Accurate pain assessment in patients with limited ability to communicate,\nsuch as older adults with dementia, represents a critical healthcare challenge.\nRobust automated systems of pain detection may facilitate such assessments.\nExisting pain detection datasets, however, suffer from limited ethnic/racial\ndiversity, privacy constraints, and underrepresentation of older adults who are\nthe primary target population for clinical deployment. We present SynPAIN, a\nlarge-scale synthetic dataset containing 10,710 facial expression images (5,355\nneutral/expressive pairs) across five ethnicities/races, two age groups (young:\n20-35, old: 75+), and two genders. Using commercial generative AI tools, we\ncreated demographically balanced synthetic identities with clinically\nmeaningful pain expressions. Our validation demonstrates that synthetic pain\nexpressions exhibit expected pain patterns, scoring significantly higher than\nneutral and non-pain expressions using clinically validated pain assessment\ntools based on facial action unit analysis. We experimentally demonstrate\nSynPAIN's utility in identifying algorithmic bias in existing pain detection\nmodels. Through comprehensive bias evaluation, we reveal substantial\nperformance disparities across demographic characteristics. These performance\ndisparities were previously undetectable with smaller, less diverse datasets.\nFurthermore, we demonstrate that age-matched synthetic data augmentation\nimproves pain detection performance on real clinical data, achieving a 7.0%\nimprovement in average precision. SynPAIN addresses critical gaps in pain\nassessment research by providing the first publicly available, demographically\ndiverse synthetic dataset specifically designed for older adult pain detection,\nwhile establishing a framework for measuring and mitigating algorithmic bias.\nThe dataset is available at https://doi.org/10.5683/SP3/WCXMAP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate pain assessment in patients with limited ability to communicate,\nsuch as older adults with dementia, represents a critical healthcare challenge.\nRobust automated systems of pain detection may facilitate such assessments.\nExisting pain detection datasets, however, suffer from limited ethnic/racial\ndiversity, privacy constraints, and underrepresentation of older adults who are\nthe primary target population for clinical deployment. We present SynPAIN, a\nlarge-scale synthetic dataset containing 10,710 facial expression images (5,355\nneutral/expressive pairs) across five ethnicities/races, two age groups (young:\n20-35, old: 75+), and two genders. Using commercial generative AI tools, we\ncreated demographically balanced synthetic identities with clinically\nmeaningful pain expressions. Our validation demonstrates that synthetic pain\nexpressions exhibit expected pain patterns, scoring significantly higher than\nneutral and non-pain expressions using clinically validated pain assessment\ntools based on facial action unit analysis. We experimentally demonstrate\nSynPAIN's utility in identifying algorithmic bias in existing pain detection\nmodels. Through comprehensive bias evaluation, we reveal substantial\nperformance disparities across demographic characteristics. These performance\ndisparities were previously undetectable with smaller, less diverse datasets.\nFurthermore, we demonstrate that age-matched synthetic data augmentation\nimproves pain detection performance on real clinical data, achieving a 7.0%\nimprovement in average precision. SynPAIN addresses critical gaps in pain\nassessment research by providing the first publicly available, demographically\ndiverse synthetic dataset specifically designed for older adult pain detection,\nwhile establishing a framework for measuring and mitigating algorithmic bias.\nThe dataset is available at https://doi.org/10.5683/SP3/WCXMAP"
                },
                "authors": [
                    {
                        "name": "Babak Taati"
                    },
                    {
                        "name": "Muhammad Muzammil"
                    },
                    {
                        "name": "Yasamin Zarghami"
                    },
                    {
                        "name": "Abhishek Moturu"
                    },
                    {
                        "name": "Amirhossein Kazerouni"
                    },
                    {
                        "name": "Hailey Reimer"
                    },
                    {
                        "name": "Alex Mihailidis"
                    },
                    {
                        "name": "Thomas Hadjistavropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hadjistavropoulos"
                },
                "author": "Thomas Hadjistavropoulos",
                "arxiv_comment": "10 pages, 4 figures, submitted to IEEE JBHI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02039v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02039v3",
                "updated": "2025-08-01T17:05:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    5,
                    21,
                    4,
                    213,
                    0
                ],
                "published": "2025-01-03T14:35:32Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    35,
                    32,
                    4,
                    3,
                    0
                ],
                "title": "An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Fan Bu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Siyi Wang"
                    },
                    {
                        "name": "Ziyao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyao Liu"
                },
                "author": "Ziyao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02039v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02039v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00762v1",
                "updated": "2025-08-01T16:38:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    38,
                    18,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:38:18Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    38,
                    18,
                    4,
                    213,
                    0
                ],
                "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A\n  Zero-Shot Approach using LLM-Driven Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A\n  Zero-Shot Approach using LLM-Driven Code Generation"
                },
                "summary": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category."
                },
                "authors": [
                    {
                        "name": "Atakan Site"
                    },
                    {
                        "name": "Emre Hakan Erdemir"
                    },
                    {
                        "name": "Glen Eryiit"
                    }
                ],
                "author_detail": {
                    "name": "Glen Eryiit"
                },
                "author": "Glen Eryiit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17217v2",
                "updated": "2025-08-01T16:37:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    37,
                    16,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-22T18:46:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    18,
                    46,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs"
                },
                "summary": "Large Language Models (LLMs) often exhibit gender bias, resulting in unequal\ntreatment of male and female subjects across different contexts. To address\nthis issue, we propose a novel data generation framework that fosters\nexploratory thinking in LLMs. Our approach prompts models to generate story\npairs featuring male and female protagonists in structurally identical, morally\nambiguous scenarios, then elicits and compares their moral judgments. When\ninconsistencies arise, the model is guided to produce balanced, gender-neutral\njudgments. These story-judgment pairs are used to fine-tune or optimize the\nmodels via Direct Preference Optimization (DPO). Experimental results show that\nour method significantly reduces gender bias while preserving or even enhancing\ngeneral model capabilities. We will release the code and generated data. We\nrelease the code and generated data at:\nhttps://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit gender bias, resulting in unequal\ntreatment of male and female subjects across different contexts. To address\nthis issue, we propose a novel data generation framework that fosters\nexploratory thinking in LLMs. Our approach prompts models to generate story\npairs featuring male and female protagonists in structurally identical, morally\nambiguous scenarios, then elicits and compares their moral judgments. When\ninconsistencies arise, the model is guided to produce balanced, gender-neutral\njudgments. These story-judgment pairs are used to fine-tune or optimize the\nmodels via Direct Preference Optimization (DPO). Experimental results show that\nour method significantly reduces gender bias while preserving or even enhancing\ngeneral model capabilities. We will release the code and generated data. We\nrelease the code and generated data at:\nhttps://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main."
                },
                "authors": [
                    {
                        "name": "Kangda Wei"
                    },
                    {
                        "name": "Hasnat Md Abdullah"
                    },
                    {
                        "name": "Ruihong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruihong Huang"
                },
                "author": "Ruihong Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00760v1",
                "updated": "2025-08-01T16:34:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    34,
                    57,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:34:57Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    34,
                    57,
                    4,
                    213,
                    0
                ],
                "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese\n  Hate Speech Detection under Cloaking Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese\n  Hate Speech Detection under Cloaking Perturbations"
                },
                "summary": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches."
                },
                "authors": [
                    {
                        "name": "Qiyao Xue"
                    },
                    {
                        "name": "Yuchen Dou"
                    },
                    {
                        "name": "Ryan Shi"
                    },
                    {
                        "name": "Xiang Lorraine Li"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09751v2",
                "updated": "2025-08-01T16:30:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    30,
                    2,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-13T19:05:43Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    19,
                    5,
                    43,
                    6,
                    194,
                    0
                ],
                "title": "Sound and Complete Neurosymbolic Reasoning with LLM-Grounded\n  Interpretations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound and Complete Neurosymbolic Reasoning with LLM-Grounded\n  Interpretations"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneurosymbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneurosymbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties."
                },
                "authors": [
                    {
                        "name": "Bradley P. Allen"
                    },
                    {
                        "name": "Prateek Chhikara"
                    },
                    {
                        "name": "Thomas Macaulay Ferguson"
                    },
                    {
                        "name": "Filip Ilievski"
                    },
                    {
                        "name": "Paul Groth"
                    }
                ],
                "author_detail": {
                    "name": "Paul Groth"
                },
                "author": "Paul Groth",
                "arxiv_comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15867v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15867v3",
                "updated": "2025-08-01T16:25:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    25,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2024-11-24T15:06:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    15,
                    6,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "PanoLlama: Generating Endless and Coherent Panoramas with\n  Next-Token-Prediction LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PanoLlama: Generating Endless and Coherent Panoramas with\n  Next-Token-Prediction LLMs"
                },
                "summary": "Panoramic Image Generation (PIG) aims to create coherent images of arbitrary\nlengths. Most existing methods fall in the joint diffusion paradigm, but their\ncomplex and heuristic crop connection designs often limit their ability to\nachieve multilevel coherence. By deconstructing this challenge into its core\ncomponents, we find it naturally aligns with next-token prediction, leading us\nto adopt an autoregressive (AR) paradigm for PIG modeling. However, existing\nvisual AR (VAR) models are limited to fixed-size generation, lacking the\ncapability to produce panoramic images. In this paper, we propose PanoLlama, a\nnovel framework that achieves endless and coherent panorama generation with the\nautoregressive paradigm. Our approach develops a training-free strategy that\nutilizes token redirection to overcome the size limitations of existing VAR\nmodels, enabling next-crop prediction in both horizontal and vertical\ndirections. This refreshes the PIG pipeline while achieving SOTA performance in\ncoherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally,\nPanoLlama supports applications other PIG methods cannot achieve, including\nmask-free layout control, multi-scale and multi-guidance synthesis. To\nfacilitate standardized evaluation, we also establish a dataset with 1,000\nprompts spanning 100+ themes, providing a new testing benchmark for PIG\nresearch. The code is available at https://github.com/0606zt/PanoLlama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panoramic Image Generation (PIG) aims to create coherent images of arbitrary\nlengths. Most existing methods fall in the joint diffusion paradigm, but their\ncomplex and heuristic crop connection designs often limit their ability to\nachieve multilevel coherence. By deconstructing this challenge into its core\ncomponents, we find it naturally aligns with next-token prediction, leading us\nto adopt an autoregressive (AR) paradigm for PIG modeling. However, existing\nvisual AR (VAR) models are limited to fixed-size generation, lacking the\ncapability to produce panoramic images. In this paper, we propose PanoLlama, a\nnovel framework that achieves endless and coherent panorama generation with the\nautoregressive paradigm. Our approach develops a training-free strategy that\nutilizes token redirection to overcome the size limitations of existing VAR\nmodels, enabling next-crop prediction in both horizontal and vertical\ndirections. This refreshes the PIG pipeline while achieving SOTA performance in\ncoherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally,\nPanoLlama supports applications other PIG methods cannot achieve, including\nmask-free layout control, multi-scale and multi-guidance synthesis. To\nfacilitate standardized evaluation, we also establish a dataset with 1,000\nprompts spanning 100+ themes, providing a new testing benchmark for PIG\nresearch. The code is available at https://github.com/0606zt/PanoLlama."
                },
                "authors": [
                    {
                        "name": "Teng Zhou"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Yongchuan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yongchuan Tang"
                },
                "author": "Yongchuan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15867v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00743v1",
                "updated": "2025-08-01T16:18:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    18,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:18:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    18,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Agentic large language models improve retrieval-based radiology question\n  answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic large language models improve retrieval-based radiology question\n  answering"
                },
                "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility."
                },
                "authors": [
                    {
                        "name": "Sebastian Wind"
                    },
                    {
                        "name": "Jeta Sopa"
                    },
                    {
                        "name": "Daniel Truhn"
                    },
                    {
                        "name": "Mahshad Lotfinia"
                    },
                    {
                        "name": "Tri-Thien Nguyen"
                    },
                    {
                        "name": "Keno Bressem"
                    },
                    {
                        "name": "Lisa Adams"
                    },
                    {
                        "name": "Mirabela Rusu"
                    },
                    {
                        "name": "Harald Kstler"
                    },
                    {
                        "name": "Gerhard Wellein"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Soroosh Tayebi Arasteh"
                    }
                ],
                "author_detail": {
                    "name": "Soroosh Tayebi Arasteh"
                },
                "author": "Soroosh Tayebi Arasteh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00741v1",
                "updated": "2025-08-01T16:12:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    12,
                    23,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:12:23Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    12,
                    23,
                    4,
                    213,
                    0
                ],
                "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data\n  Leveraging Declarative Facts in Earlier Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data\n  Leveraging Declarative Facts in Earlier Training Data"
                },
                "summary": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety."
                },
                "authors": [
                    {
                        "name": "Sohaib Imran"
                    },
                    {
                        "name": "Rob Lamb"
                    },
                    {
                        "name": "Peter M. Atkinson"
                    }
                ],
                "author_detail": {
                    "name": "Peter M. Atkinson"
                },
                "author": "Peter M. Atkinson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00737v1",
                "updated": "2025-08-01T16:08:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    8,
                    5,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T16:08:05Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    8,
                    5,
                    4,
                    213,
                    0
                ],
                "title": "How LLMs are Shaping the Future of Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How LLMs are Shaping the Future of Virtual Reality"
                },
                "summary": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems."
                },
                "authors": [
                    {
                        "name": "Seda zkaya"
                    },
                    {
                        "name": "Santiago Berrezueta-Guzman"
                    },
                    {
                        "name": "Stefan Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wagner"
                },
                "author": "Stefan Wagner",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05824v2",
                "updated": "2025-08-01T16:00:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    0,
                    18,
                    4,
                    213,
                    0
                ],
                "published": "2024-11-05T08:01:16Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    1,
                    16,
                    1,
                    310,
                    0
                ],
                "title": "Navigating Distribution Shifts in Medical Image Analysis: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating Distribution Shifts in Medical Image Analysis: A Survey"
                },
                "summary": "Medical Image Analysis (MedIA) has become indispensable in modern healthcare,\nenhancing clinical diagnostics and personalized treatment. Despite the\nremarkable advancements supported by deep learning (DL) technologies, their\npractical deployment faces challenges due to distribution shifts, where models\ntrained on specific datasets underperform across others from varying hospitals,\nregions, or patient populations. To navigate this issue, researchers have been\nactively developing strategies to increase the adaptability and robustness of\nDL models, enabling their effective use in unfamiliar and diverse environments.\nThis paper systematically reviews approaches that apply DL techniques to MedIA\nsystems affected by distribution shifts. Unlike traditional categorizations\nbased on technical specifications, our approach is grounded in the real-world\noperational constraints faced by healthcare institutions. Specifically, we\ncategorize the existing body of work into Joint Training, Federated Learning,\nFine-tuning, and Domain Generalization, with each method tailored to distinct\nscenarios caused by Data Accessibility, Privacy Concerns, and Collaborative\nProtocols. This perspective equips researchers with a nuanced understanding of\nhow DL can be strategically deployed to address distribution shifts in MedIA,\nensuring diverse and robust medical applications. By delving deeper into these\ntopics, we highlight potential pathways for future research that not only\naddress existing limitations but also push the boundaries of deployable MedIA\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Image Analysis (MedIA) has become indispensable in modern healthcare,\nenhancing clinical diagnostics and personalized treatment. Despite the\nremarkable advancements supported by deep learning (DL) technologies, their\npractical deployment faces challenges due to distribution shifts, where models\ntrained on specific datasets underperform across others from varying hospitals,\nregions, or patient populations. To navigate this issue, researchers have been\nactively developing strategies to increase the adaptability and robustness of\nDL models, enabling their effective use in unfamiliar and diverse environments.\nThis paper systematically reviews approaches that apply DL techniques to MedIA\nsystems affected by distribution shifts. Unlike traditional categorizations\nbased on technical specifications, our approach is grounded in the real-world\noperational constraints faced by healthcare institutions. Specifically, we\ncategorize the existing body of work into Joint Training, Federated Learning,\nFine-tuning, and Domain Generalization, with each method tailored to distinct\nscenarios caused by Data Accessibility, Privacy Concerns, and Collaborative\nProtocols. This perspective equips researchers with a nuanced understanding of\nhow DL can be strategically deployed to address distribution shifts in MedIA,\nensuring diverse and robust medical applications. By delving deeper into these\ntopics, we highlight potential pathways for future research that not only\naddress existing limitations but also push the boundaries of deployable MedIA\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Zixian Su"
                    },
                    {
                        "name": "Jingwei Guo"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Frans Coenen"
                    },
                    {
                        "name": "Kaizhu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaizhu Huang"
                },
                "author": "Kaizhu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13943v2",
                "updated": "2025-08-01T15:56:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    56,
                    30,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-20T05:22:17Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    5,
                    22,
                    17,
                    1,
                    140,
                    0
                ],
                "title": "From Press to Pixels: Evolving Urdu Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Press to Pixels: Evolving Urdu Text Recognition"
                },
                "summary": "This paper introduces an end-to-end pipeline for Optical Character\nRecognition (OCR) on Urdu newspapers, addressing challenges posed by complex\nmulti-column layouts, low-resolution scans, and the stylistic variability of\nthe Nastaliq script. Our system comprises four modules: (1) article\nsegmentation, (2) image super-resolution, (3) column segmentation, and (4) text\nrecognition. We fine-tune YOLOv11x for segmentation, achieving 0.963 precision\nfor articles and 0.970 for columns. A SwinIR-based super-resolution model\nboosts LLM text recognition accuracy by 25-70%. We also introduce the Urdu\nNewspaper Benchmark (UNB), a manually annotated dataset for Urdu OCR. Using UNB\nand the OpenITI corpus, we compare traditional CNN+RNN-based OCR models with\nmodern LLMs. Gemini-2.5-Pro achieves the best performance with a WER of 0.133.\nWe further analyze LLM outputs via insertion, deletion, and substitution error\nbreakdowns, as well as character-level confusion analysis. Finally, we show\nthat fine-tuning on just 500 samples yields a 6.13% WER improvement,\nhighlighting the adaptability of LLMs for Urdu OCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an end-to-end pipeline for Optical Character\nRecognition (OCR) on Urdu newspapers, addressing challenges posed by complex\nmulti-column layouts, low-resolution scans, and the stylistic variability of\nthe Nastaliq script. Our system comprises four modules: (1) article\nsegmentation, (2) image super-resolution, (3) column segmentation, and (4) text\nrecognition. We fine-tune YOLOv11x for segmentation, achieving 0.963 precision\nfor articles and 0.970 for columns. A SwinIR-based super-resolution model\nboosts LLM text recognition accuracy by 25-70%. We also introduce the Urdu\nNewspaper Benchmark (UNB), a manually annotated dataset for Urdu OCR. Using UNB\nand the OpenITI corpus, we compare traditional CNN+RNN-based OCR models with\nmodern LLMs. Gemini-2.5-Pro achieves the best performance with a WER of 0.133.\nWe further analyze LLM outputs via insertion, deletion, and substitution error\nbreakdowns, as well as character-level confusion analysis. Finally, we show\nthat fine-tuning on just 500 samples yields a 6.13% WER improvement,\nhighlighting the adaptability of LLMs for Urdu OCR."
                },
                "authors": [
                    {
                        "name": "Samee Arif"
                    },
                    {
                        "name": "Sualeha Farid"
                    }
                ],
                "author_detail": {
                    "name": "Sualeha Farid"
                },
                "author": "Sualeha Farid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00723v1",
                "updated": "2025-08-01T15:44:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    44,
                    25,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:44:25Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    44,
                    25,
                    4,
                    213,
                    0
                ],
                "title": "Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors\n  Impacting AI Adoption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors\n  Impacting AI Adoption"
                },
                "summary": "Growing excitement around deploying AI across various domains calls for a\ncareful assessment of how human decision-makers interact with AI-powered\nsystems. In particular, it is essential to understand when decision-makers\nvoluntarily choose to consult AI tools, which we term decision-maker adoption.\nWe interviewed experts across four domains -- medicine, law, journalism, and\nthe public sector -- to explore current AI use cases and perceptions of\nadoption. From these interviews, we identify key factors that shape\ndecision-maker adoption of AI tools: the decision-maker's background,\nperceptions of the AI, consequences for the decision-maker, and perceived\nimplications for other stakeholders. We translate these factors into an AI\nadoption sheet to analyze how decision-makers approach adoption choices through\ncomparative, cross-domain case studies, highlighting how our factors help\nexplain inter-domain differences in adoption. Our findings offer practical\nguidance for supporting the responsible and context-aware deployment of AI by\nbetter accounting for the decision-maker's perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing excitement around deploying AI across various domains calls for a\ncareful assessment of how human decision-makers interact with AI-powered\nsystems. In particular, it is essential to understand when decision-makers\nvoluntarily choose to consult AI tools, which we term decision-maker adoption.\nWe interviewed experts across four domains -- medicine, law, journalism, and\nthe public sector -- to explore current AI use cases and perceptions of\nadoption. From these interviews, we identify key factors that shape\ndecision-maker adoption of AI tools: the decision-maker's background,\nperceptions of the AI, consequences for the decision-maker, and perceived\nimplications for other stakeholders. We translate these factors into an AI\nadoption sheet to analyze how decision-makers approach adoption choices through\ncomparative, cross-domain case studies, highlighting how our factors help\nexplain inter-domain differences in adoption. Our findings offer practical\nguidance for supporting the responsible and context-aware deployment of AI by\nbetter accounting for the decision-maker's perspective."
                },
                "authors": [
                    {
                        "name": "Rebecca Yu"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Hoda Heidari"
                    }
                ],
                "author_detail": {
                    "name": "Hoda Heidari"
                },
                "author": "Hoda Heidari",
                "arxiv_comment": "To be published in Proceedings of the Eighth AAAI/ACM Conference on\n  AI, Ethics, and Society (AIES-25). 10 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00719v1",
                "updated": "2025-08-01T15:38:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    38,
                    21,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:38:21Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    38,
                    21,
                    4,
                    213,
                    0
                ],
                "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA"
                },
                "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Shiqi Fan"
                    },
                    {
                        "name": "Mengzhu Wang"
                    },
                    {
                        "name": "Siwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Siwei Liu"
                },
                "author": "Siwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08441v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08441v3",
                "updated": "2025-08-01T15:28:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    28,
                    51,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-12T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    17,
                    2,
                    43,
                    0
                ],
                "title": "Better Embeddings with Coupled Adam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Embeddings with Coupled Adam"
                },
                "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets."
                },
                "authors": [
                    {
                        "name": "Felix Stollenwerk"
                    },
                    {
                        "name": "Tobias Stollenwerk"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Stollenwerk"
                },
                "author": "Tobias Stollenwerk",
                "arxiv_comment": "ACL 2025 (Main), see https://aclanthology.org/2025.acl-long.1321/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08441v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08441v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13900v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13900v3",
                "updated": "2025-08-01T15:27:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    27,
                    59,
                    4,
                    213,
                    0
                ],
                "published": "2024-07-18T21:06:39Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    21,
                    6,
                    39,
                    3,
                    200,
                    0
                ],
                "title": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools"
                },
                "summary": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software -- leading to new frontiers in software\nengineering (SE). The advanced capabilities of generative AI tools to support\nsoftware development tasks have led to a rise in their adoption within software\ndevelopment workflows. However, little is known about how AI tools perceive\nevidence-based beliefs and practices verified by research findings. To this\nend, we conduct a preliminary evaluation conceptually replicating prior work to\nexplore the \"beliefs\" of generative AI tools used to support software\ndevelopment tasks. We investigate 17 evidence-based claims posited by empirical\nSE research across five generative AI tools. Our findings show that generative\nAI tools have ambiguous beliefs regarding research claims and lack credible\nevidence to support responses. Based on our results, we provide implications\nfor practitioners integrating generative AI-based systems into development\ncontexts and shed light on future research directions to enhance the\nreliability and trustworthiness of generative AI -- aiming to increase\nawareness and adoption of evidence-based SE research findings in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software -- leading to new frontiers in software\nengineering (SE). The advanced capabilities of generative AI tools to support\nsoftware development tasks have led to a rise in their adoption within software\ndevelopment workflows. However, little is known about how AI tools perceive\nevidence-based beliefs and practices verified by research findings. To this\nend, we conduct a preliminary evaluation conceptually replicating prior work to\nexplore the \"beliefs\" of generative AI tools used to support software\ndevelopment tasks. We investigate 17 evidence-based claims posited by empirical\nSE research across five generative AI tools. Our findings show that generative\nAI tools have ambiguous beliefs regarding research claims and lack credible\nevidence to support responses. Based on our results, we provide implications\nfor practitioners integrating generative AI-based systems into development\ncontexts and shed light on future research directions to enhance the\nreliability and trustworthiness of generative AI -- aiming to increase\nawareness and adoption of evidence-based SE research findings in practice."
                },
                "authors": [
                    {
                        "name": "Chris Brown"
                    },
                    {
                        "name": "Jason Cusati"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cusati"
                },
                "author": "Jason Cusati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13900v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13900v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00709v1",
                "updated": "2025-08-01T15:23:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    23,
                    20,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:23:20Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    23,
                    20,
                    4,
                    213,
                    0
                ],
                "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian\n  Common Law System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian\n  Common Law System"
                },
                "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Balaramamahanthi Deepak Patnaik"
                    },
                    {
                        "name": "Shivam Mishra"
                    },
                    {
                        "name": "Ajay Varghese Thomas"
                    },
                    {
                        "name": "Noel Shallum"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00700v1",
                "updated": "2025-08-01T15:17:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    17,
                    34,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:17:34Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    17,
                    34,
                    4,
                    213,
                    0
                ],
                "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written\n  Code?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written\n  Code?"
                },
                "summary": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation."
                },
                "authors": [
                    {
                        "name": "Alfred Santa Molison"
                    },
                    {
                        "name": "Marcia Moraes"
                    },
                    {
                        "name": "Glaucia Melo"
                    },
                    {
                        "name": "Fabio Santos"
                    },
                    {
                        "name": "Wesley K. G. Assuncao"
                    }
                ],
                "author_detail": {
                    "name": "Wesley K. G. Assuncao"
                },
                "author": "Wesley K. G. Assuncao",
                "arxiv_comment": "Accepted ESEM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00697v1",
                "updated": "2025-08-01T15:14:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    14,
                    39,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T15:14:39Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    14,
                    39,
                    4,
                    213,
                    0
                ],
                "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation"
                },
                "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies."
                },
                "authors": [
                    {
                        "name": "Yiming Wu"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Zhenghao Chen"
                    },
                    {
                        "name": "Jianxin Pang"
                    },
                    {
                        "name": "Dong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Xu"
                },
                "author": "Dong Xu",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19861v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19861v2",
                "updated": "2025-08-01T15:03:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    3,
                    13,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-26T08:36:16Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    8,
                    36,
                    16,
                    5,
                    207,
                    0
                ],
                "title": "Quantum-Informed Machine Learning for Chaotic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Informed Machine Learning for Chaotic Systems"
                },
                "summary": "Learning the behaviour of chaotic systems remains challenging due to\ninstability in long-term predictions and difficulties in accurately capturing\ninvariant statistical properties. While quantum machine learning offers a\npromising route to efficiently capture physical properties from\nhigh-dimensional data, its practical deployment is hindered by current hardware\nnoise and limited scalability. Here, we introduce a quantum-informed machine\nlearning framework for learning partial differential equations, with an\napplication focus on chaotic systems. A quantum circuit Born machine is\nemployed to learn the invariant properties of chaotic dynamical systems,\nachieving substantial memory efficiency by representing these complex physical\nstatistics with a compact set of trainable circuit parameters. This approach\nreduces the data storage requirement by over two orders of magnitude compared\nto the raw simulation data. The resulting statistical quantum-informed prior is\nthen incorporated into a Koopman-based auto-regressive model to address issues\nsuch as gradient vanishing or explosion, while maintaining long-term\nstatistical fidelity. The framework is evaluated on three representative\nsystems: the Kuramoto-Sivashinsky equation, two-dimensional Kolmogorov flow and\nturbulent channel flow. In all cases, the quantum-informed model achieves\nsuperior performance compared to its classical counterparts without quantum\npriors. This hybrid architecture offers a practical route for learning\ndynamical systems using near-term quantum hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the behaviour of chaotic systems remains challenging due to\ninstability in long-term predictions and difficulties in accurately capturing\ninvariant statistical properties. While quantum machine learning offers a\npromising route to efficiently capture physical properties from\nhigh-dimensional data, its practical deployment is hindered by current hardware\nnoise and limited scalability. Here, we introduce a quantum-informed machine\nlearning framework for learning partial differential equations, with an\napplication focus on chaotic systems. A quantum circuit Born machine is\nemployed to learn the invariant properties of chaotic dynamical systems,\nachieving substantial memory efficiency by representing these complex physical\nstatistics with a compact set of trainable circuit parameters. This approach\nreduces the data storage requirement by over two orders of magnitude compared\nto the raw simulation data. The resulting statistical quantum-informed prior is\nthen incorporated into a Koopman-based auto-regressive model to address issues\nsuch as gradient vanishing or explosion, while maintaining long-term\nstatistical fidelity. The framework is evaluated on three representative\nsystems: the Kuramoto-Sivashinsky equation, two-dimensional Kolmogorov flow and\nturbulent channel flow. In all cases, the quantum-informed model achieves\nsuperior performance compared to its classical counterparts without quantum\npriors. This hybrid architecture offers a practical route for learning\ndynamical systems using near-term quantum hardware."
                },
                "authors": [
                    {
                        "name": "Maida Wang"
                    },
                    {
                        "name": "Xiao Xue"
                    },
                    {
                        "name": "Peter V. Coveney"
                    }
                ],
                "author_detail": {
                    "name": "Peter V. Coveney"
                },
                "author": "Peter V. Coveney",
                "arxiv_comment": "33 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19861v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19861v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00680v1",
                "updated": "2025-08-01T14:49:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    49,
                    50,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:49:50Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    49,
                    50,
                    4,
                    213,
                    0
                ],
                "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Call Claude: Can LLMs Detect Changes of Writing Style?"
                },
                "summary": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported."
                },
                "authors": [
                    {
                        "name": "Johannes Rmisch"
                    },
                    {
                        "name": "Svetlana Gorovaia"
                    },
                    {
                        "name": "Mariia Halchynska"
                    },
                    {
                        "name": "Gleb Schmidt"
                    },
                    {
                        "name": "Ivan P. Yamshchikov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan P. Yamshchikov"
                },
                "author": "Ivan P. Yamshchikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00673v1",
                "updated": "2025-08-01T14:46:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    46,
                    57,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:46:57Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    46,
                    57,
                    4,
                    213,
                    0
                ],
                "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of\n  Culture in Persian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MELAC: Massive Evaluation of Large Language Models with Alignment of\n  Culture in Persian Language"
                },
                "summary": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field."
                },
                "authors": [
                    {
                        "name": "Farhan Farsi"
                    },
                    {
                        "name": "Farnaz Aghababaloo"
                    },
                    {
                        "name": "Shahriar Shariati Motlagh"
                    },
                    {
                        "name": "Parsa Ghofrani"
                    },
                    {
                        "name": "MohammadAli SadraeiJavaheri"
                    },
                    {
                        "name": "Shayan Bali"
                    },
                    {
                        "name": "Amirhossein Shabani"
                    },
                    {
                        "name": "Farbod Bijary"
                    },
                    {
                        "name": "Ghazal Zamaninejad"
                    },
                    {
                        "name": "AmirMohammad Salehoof"
                    },
                    {
                        "name": "Saeedeh Momtazi"
                    }
                ],
                "author_detail": {
                    "name": "Saeedeh Momtazi"
                },
                "author": "Saeedeh Momtazi",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00669v1",
                "updated": "2025-08-01T14:41:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    41,
                    31,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:41:31Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    41,
                    31,
                    4,
                    213,
                    0
                ],
                "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement\n  Techniques and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement\n  Techniques and Applications"
                },
                "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI."
                },
                "authors": [
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Zizhan Ma"
                    },
                    {
                        "name": "Meidan Ding"
                    },
                    {
                        "name": "Shiyi Zheng"
                    },
                    {
                        "name": "Shengyuan Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Wenting Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Linlin Shen"
                    },
                    {
                        "name": "Yixuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Yuan"
                },
                "author": "Yixuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08395v2",
                "updated": "2025-08-01T14:26:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    26,
                    57,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-12T13:37:03Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    37,
                    3,
                    2,
                    43,
                    0
                ],
                "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance"
                },
                "summary": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them."
                },
                "authors": [
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21696v2",
                "updated": "2025-08-01T14:25:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    25,
                    34,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-29T11:20:03Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    11,
                    20,
                    3,
                    1,
                    210,
                    0
                ],
                "title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN"
                },
                "summary": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations."
                },
                "authors": [
                    {
                        "name": "Abdelaziz Salama"
                    },
                    {
                        "name": "Zeinab Nezami"
                    },
                    {
                        "name": "Mohammed M. H. Qazzaz"
                    },
                    {
                        "name": "Maryam Hafeez"
                    },
                    {
                        "name": "Syed Ali Raza Zaidi"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ali Raza Zaidi"
                },
                "author": "Syed Ali Raza Zaidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15173v2",
                "updated": "2025-08-01T14:16:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    16,
                    18,
                    4,
                    213,
                    0
                ],
                "published": "2024-11-16T12:29:59Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    12,
                    29,
                    59,
                    5,
                    321,
                    0
                ],
                "title": "Un-mixing Test-time Adaptation under Heterogeneous Data Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Un-mixing Test-time Adaptation under Heterogeneous Data Streams"
                },
                "summary": "Deploying deep models in real-world scenarios remains challenging due to\nsignificant performance drops under distribution shifts between training and\ndeployment environments. Test-Time Adaptation (TTA) has recently emerged as a\npromising solution, enabling on-the-fly model adaptation without access to\nsource data. However, its effectiveness degrades significantly in the presence\nof complex, mixed distribution shifts - common in practical settings - where\nmultiple latent domains coexist. Adapting under such intrinsic heterogeneity,\nespecially in unlabeled and online conditions, remains an open and\nunderexplored challenge. In this paper, we study TTA under mixed distribution\nshifts and move beyond conventional homogeneous adaptation paradigms. By\nrevisiting TTA from a frequency-domain perspective, we observe that\ndistribution heterogeneity often manifests in Fourier space - for instance,\nhigh-frequency components tend to carry domain-specific variations. This\nmotivates us to perform domain-aware separation using high-frequency texture\ncues, making diverse shift patterns more tractable. To this end, we propose\nFreDA, a novel Frequency-based Decentralized Adaptation framework that\ndecomposes globally heterogeneous data into locally homogeneous components in\nthe frequency domain. It further employs decentralized learning and\naugmentation strategies to robustly adapt under complex, evolving shifts.\nExtensive experiments across various environments (corrupted, natural, and\nmedical) demonstrate the superiority of our proposed framework over the\nstate-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying deep models in real-world scenarios remains challenging due to\nsignificant performance drops under distribution shifts between training and\ndeployment environments. Test-Time Adaptation (TTA) has recently emerged as a\npromising solution, enabling on-the-fly model adaptation without access to\nsource data. However, its effectiveness degrades significantly in the presence\nof complex, mixed distribution shifts - common in practical settings - where\nmultiple latent domains coexist. Adapting under such intrinsic heterogeneity,\nespecially in unlabeled and online conditions, remains an open and\nunderexplored challenge. In this paper, we study TTA under mixed distribution\nshifts and move beyond conventional homogeneous adaptation paradigms. By\nrevisiting TTA from a frequency-domain perspective, we observe that\ndistribution heterogeneity often manifests in Fourier space - for instance,\nhigh-frequency components tend to carry domain-specific variations. This\nmotivates us to perform domain-aware separation using high-frequency texture\ncues, making diverse shift patterns more tractable. To this end, we propose\nFreDA, a novel Frequency-based Decentralized Adaptation framework that\ndecomposes globally heterogeneous data into locally homogeneous components in\nthe frequency domain. It further employs decentralized learning and\naugmentation strategies to robustly adapt under complex, evolving shifts.\nExtensive experiments across various environments (corrupted, natural, and\nmedical) demonstrate the superiority of our proposed framework over the\nstate-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Zixian Su"
                    },
                    {
                        "name": "Jingwei Guo"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Kaizhu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaizhu Huang"
                },
                "author": "Kaizhu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00651v1",
                "updated": "2025-08-01T14:12:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    12,
                    43,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:12:43Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    12,
                    43,
                    4,
                    213,
                    0
                ],
                "title": "Review: Adaptive Radiation Therapy for Head and Neck Cancer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review: Adaptive Radiation Therapy for Head and Neck Cancer"
                },
                "summary": "The future of ART in head and neck cancer is just beginning. Novel\ntechnologies have pushed the boundary of what is possible in terms of\ntechniques to identify biomarkers for adaptation as well as innovative devices\nspecialized to respond to these adaptations, sometimes in real-time. Important\ninterdisciplinary steps must be taken moving forward to ensure the safe\ndeployment of these new techniques, such as rigorous quality assurance\nevaluations from medical physicists, clinical trials from physicians, and\ncomprehensive testing from vendors prior to release. In summary, we aimed not\nto provide a single correct answer for the optimal implementation of ART in the\nera of imaging biomarkers, but to encourage the field to collaborate and bring\neach idea discussed here together to overcome current barriers and deliver the\nbest treatment possible to the patient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The future of ART in head and neck cancer is just beginning. Novel\ntechnologies have pushed the boundary of what is possible in terms of\ntechniques to identify biomarkers for adaptation as well as innovative devices\nspecialized to respond to these adaptations, sometimes in real-time. Important\ninterdisciplinary steps must be taken moving forward to ensure the safe\ndeployment of these new techniques, such as rigorous quality assurance\nevaluations from medical physicists, clinical trials from physicians, and\ncomprehensive testing from vendors prior to release. In summary, we aimed not\nto provide a single correct answer for the optimal implementation of ART in the\nera of imaging biomarkers, but to encourage the field to collaborate and bring\neach idea discussed here together to overcome current barriers and deliver the\nbest treatment possible to the patient."
                },
                "authors": [
                    {
                        "name": "Lucas McCullum"
                    },
                    {
                        "name": "Sonali J. Joshi"
                    },
                    {
                        "name": "Brandon M. Godinich"
                    },
                    {
                        "name": "Parshawn Gerafian"
                    },
                    {
                        "name": "Rishabh Gaur"
                    },
                    {
                        "name": "Qusai Alakayleh"
                    },
                    {
                        "name": "Ergys Subashi"
                    },
                    {
                        "name": "Renjie He"
                    },
                    {
                        "name": "Samuel L. Mulder"
                    },
                    {
                        "name": "Zaphanlene Kaffey"
                    },
                    {
                        "name": "Grace Murley"
                    },
                    {
                        "name": "Natalie A. West"
                    },
                    {
                        "name": "Saleh Ramezani"
                    },
                    {
                        "name": "Cem Dede"
                    },
                    {
                        "name": "Laia Humbert-Vidan"
                    },
                    {
                        "name": "Clifton D. Fuller"
                    }
                ],
                "author_detail": {
                    "name": "Clifton D. Fuller"
                },
                "author": "Clifton D. Fuller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00632v1",
                "updated": "2025-08-01T13:45:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    45,
                    13,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:45:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    45,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings"
                },
                "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches."
                },
                "authors": [
                    {
                        "name": "Alexia Jolicoeur-Martineau"
                    }
                ],
                "author_detail": {
                    "name": "Alexia Jolicoeur-Martineau"
                },
                "author": "Alexia Jolicoeur-Martineau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00630v1",
                "updated": "2025-08-01T13:41:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    41,
                    58,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:41:58Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    41,
                    58,
                    4,
                    213,
                    0
                ],
                "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCeT: Behavioral Model Correctness Evaluation using Large Language\n  Models"
                },
                "summary": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram."
                },
                "authors": [
                    {
                        "name": "Khaled Ahmed"
                    },
                    {
                        "name": "Jialing Song"
                    },
                    {
                        "name": "Boqi Chen"
                    },
                    {
                        "name": "Ou Wei"
                    },
                    {
                        "name": "Bingzhou Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bingzhou Zheng"
                },
                "author": "Bingzhou Zheng",
                "arxiv_comment": "MODELS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00619v1",
                "updated": "2025-08-01T13:28:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    28,
                    1,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:28:01Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    28,
                    1,
                    4,
                    213,
                    0
                ],
                "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language\n  Models"
                },
                "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors."
                },
                "authors": [
                    {
                        "name": "Shantanu Thorat"
                    },
                    {
                        "name": "Andrew Caines"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Caines"
                },
                "author": "Andrew Caines",
                "arxiv_comment": "MPhil in Advanced Computer Science thesis for University of Cambridge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00615v1",
                "updated": "2025-08-01T13:25:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    4,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:04Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    4,
                    4,
                    213,
                    0
                ],
                "title": "Similarity-Based Self-Construct Graph Model for Predicting Patient\n  Criticalness Using Graph Neural Networks and EHR Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-Based Self-Construct Graph Model for Predicting Patient\n  Criticalness Using Graph Neural Networks and EHR Data"
                },
                "summary": "Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment."
                },
                "authors": [
                    {
                        "name": "Mukesh Kumar Sahu"
                    },
                    {
                        "name": "Pinki Roy"
                    }
                ],
                "author_detail": {
                    "name": "Pinki Roy"
                },
                "author": "Pinki Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00614v1",
                "updated": "2025-08-01T13:23:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    23,
                    21,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:23:21Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    23,
                    21,
                    4,
                    213,
                    0
                ],
                "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will\n  you care?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will\n  you care?"
                },
                "summary": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions."
                },
                "authors": [
                    {
                        "name": "Lennart Meincke"
                    },
                    {
                        "name": "Ethan Mollick"
                    },
                    {
                        "name": "Lilach Mollick"
                    },
                    {
                        "name": "Dan Shapiro"
                    }
                ],
                "author_detail": {
                    "name": "Dan Shapiro"
                },
                "author": "Dan Shapiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12927v2",
                "updated": "2025-08-01T13:19:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    19,
                    46,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-18T15:09:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "SEFL: Enhancing Educational Assignment Feedback with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEFL: Enhancing Educational Assignment Feedback with LLM Agents"
                },
                "summary": "Providing high-quality feedback to student assignments is crucial for student\nsuccess, but it is constrained by time and costs. In this work, we introduce\nSynthetic Educational Feedback Loops (SEFL), a synthetic data framework\ndesigned to generate data that resembles immediate, on-demand feedback at scale\nwithout relying on extensive, real-world student assignments. To get this type\nof data, two large language models (LLMs) operate in teacher-student roles to\nsimulate assignment completion and formative feedback, generating synthetic\npairs of student work and corresponding critiques and actionable improvements\nfrom a teacher. With this data, we fine-tune smaller, more computationally\nefficient LLMs on these synthetic pairs, enabling them to replicate key\nfeatures of high-quality, goal-oriented feedback. Unlike personalized tutoring\napproaches that offer multi-turn, individualized instruction, SEFL specifically\nfocuses on replicating the teacher-student assignment feedback loop in higher\neducation. Through comprehensive evaluations with four LLM judges and three\nhuman experts, we demonstrate that SEFL-tuned models outperform both their\nnon-tuned counterparts in feedback quality and an existing baseline. The\npotential for societal impact is reinforced by extensive qualitative comments\nby ratings by human stakeholders -- both students and higher education\ninstructors. All in all, SEFL has substantial potential to transform feedback\nprocesses for higher education and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing high-quality feedback to student assignments is crucial for student\nsuccess, but it is constrained by time and costs. In this work, we introduce\nSynthetic Educational Feedback Loops (SEFL), a synthetic data framework\ndesigned to generate data that resembles immediate, on-demand feedback at scale\nwithout relying on extensive, real-world student assignments. To get this type\nof data, two large language models (LLMs) operate in teacher-student roles to\nsimulate assignment completion and formative feedback, generating synthetic\npairs of student work and corresponding critiques and actionable improvements\nfrom a teacher. With this data, we fine-tune smaller, more computationally\nefficient LLMs on these synthetic pairs, enabling them to replicate key\nfeatures of high-quality, goal-oriented feedback. Unlike personalized tutoring\napproaches that offer multi-turn, individualized instruction, SEFL specifically\nfocuses on replicating the teacher-student assignment feedback loop in higher\neducation. Through comprehensive evaluations with four LLM judges and three\nhuman experts, we demonstrate that SEFL-tuned models outperform both their\nnon-tuned counterparts in feedback quality and an existing baseline. The\npotential for societal impact is reinforced by extensive qualitative comments\nby ratings by human stakeholders -- both students and higher education\ninstructors. All in all, SEFL has substantial potential to transform feedback\nprocesses for higher education and beyond."
                },
                "authors": [
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Amalie Pernille Dilling"
                    },
                    {
                        "name": "Lon Gondelman"
                    },
                    {
                        "name": "Niels Erik Ruan Lyngdorf"
                    },
                    {
                        "name": "Euan D. Lindsay"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00602v1",
                "updated": "2025-08-01T13:04:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    4,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:04:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    4,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection\n  and Leakage Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection\n  and Leakage Attacks"
                },
                "summary": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard."
                },
                "authors": [
                    {
                        "name": "Francesco Panebianco"
                    },
                    {
                        "name": "Stefano Bonfanti"
                    },
                    {
                        "name": "Francesco Trov"
                    },
                    {
                        "name": "Michele Carminati"
                    }
                ],
                "author_detail": {
                    "name": "Michele Carminati"
                },
                "author": "Michele Carminati",
                "arxiv_comment": "22 pages, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15066v2",
                "updated": "2025-08-01T13:03:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    3,
                    5,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-20T18:02:50Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    18,
                    2,
                    50,
                    6,
                    201,
                    0
                ],
                "title": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback"
                },
                "summary": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area."
                },
                "authors": [
                    {
                        "name": "Yiyuan Yang"
                    },
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Kai Ying"
                    },
                    {
                        "name": "Zhiguang Wang"
                    },
                    {
                        "name": "Tom Bamford"
                    },
                    {
                        "name": "Svitlana Vyetrenko"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "Under review. 19 pages, 8 figures, 12 tables. Code and dataset are\n  publicly available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00600v1",
                "updated": "2025-08-01T12:58:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    58,
                    34,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:58:34Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    58,
                    34,
                    4,
                    213,
                    0
                ],
                "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large\n  Language Models"
                },
                "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines."
                },
                "authors": [
                    {
                        "name": "Mingruo Yuan"
                    },
                    {
                        "name": "Shuyi Zhang"
                    },
                    {
                        "name": "Ben Kao"
                    }
                ],
                "author_detail": {
                    "name": "Ben Kao"
                },
                "author": "Ben Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23276v2",
                "updated": "2025-08-01T12:49:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    49,
                    36,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T06:32:06Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    6,
                    32,
                    6,
                    3,
                    212,
                    0
                ],
                "title": "How Far Are AI Scientists from Changing the World?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are AI Scientists from Changing the World?"
                },
                "summary": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be."
                },
                "authors": [
                    {
                        "name": "Qiujie Xie"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Fuchen Shen"
                    },
                    {
                        "name": "Shulin Huang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Jiahui Zhou"
                    },
                    {
                        "name": "Zilan Mao"
                    },
                    {
                        "name": "Zijie Yang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00581v1",
                "updated": "2025-08-01T12:24:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    24,
                    49,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:24:49Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    24,
                    49,
                    4,
                    213,
                    0
                ],
                "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated\n  Pre-Consultation Questionnaire Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated\n  Pre-Consultation Questionnaire Generation"
                },
                "summary": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection."
                },
                "authors": [
                    {
                        "name": "Ruiqing Ding"
                    },
                    {
                        "name": "Qianfang Sun"
                    },
                    {
                        "name": "Yongkang Leng"
                    },
                    {
                        "name": "Hui Yin"
                    },
                    {
                        "name": "Xiaojian Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojian Li"
                },
                "author": "Xiaojian Li",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00580v1",
                "updated": "2025-08-01T12:23:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    23,
                    29,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:23:29Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    23,
                    29,
                    4,
                    213,
                    0
                ],
                "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on\n  Planetary Rovers Using RGB, Depth, and Thermal Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on\n  Planetary Rovers Using RGB, Depth, and Thermal Imagery"
                },
                "summary": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics."
                },
                "authors": [
                    {
                        "name": "Raul Castilla-Arquillo"
                    },
                    {
                        "name": "Carlos Perez-del-Pulgar"
                    },
                    {
                        "name": "Levin Gerdes"
                    },
                    {
                        "name": "Alfonso Garcia-Cerezo"
                    },
                    {
                        "name": "Miguel A. Olivares-Mendez"
                    }
                ],
                "author_detail": {
                    "name": "Miguel A. Olivares-Mendez"
                },
                "author": "Miguel A. Olivares-Mendez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00579v1",
                "updated": "2025-08-01T12:22:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    22,
                    53,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:22:53Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    22,
                    53,
                    4,
                    213,
                    0
                ],
                "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for\n  Document Question-Answering with Hierarchical Index and Multi-Granularity\n  Retrieval"
                },
                "summary": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents."
                },
                "authors": [
                    {
                        "name": "Ziyu Gong"
                    },
                    {
                        "name": "Yihua Huang"
                    },
                    {
                        "name": "Chengcheng Mai"
                    }
                ],
                "author_detail": {
                    "name": "Chengcheng Mai"
                },
                "author": "Chengcheng Mai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12201v2",
                "updated": "2025-08-01T12:20:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    20,
                    2,
                    4,
                    213,
                    0
                ],
                "published": "2024-12-15T03:08:28Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    8,
                    28,
                    6,
                    350,
                    0
                ],
                "title": "Embracing Large Language Models in Traffic Flow Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embracing Large Language Models in Traffic Flow Forecasting"
                },
                "summary": "Traffic flow forecasting aims to predict future traffic flows based on the\nhistorical traffic conditions and the road network. It is an important problem\nin intelligent transportation systems, with a plethora of methods been\nproposed. Existing efforts mainly focus on capturing and utilizing\nspatio-temporal dependencies to predict future traffic flows. Though promising,\nthey fall short in adapting to test-time environmental changes of traffic\nconditions. To tackle this challenge, we propose to introduce large language\nmodels (LLMs) to help traffic flow forecasting and design a novel method named\nLarge Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two\nbranches, capturing different spatio-temporal relations using graph and\nhypergraph structures respectively. The two branches are first pre-trained\nindividually, and during test-time, they yield different predictions. Based on\nthese predictions, a large language model is used to select the most likely\nresult. Then, a ranking loss is applied as the learning objective to enhance\nthe prediction ability of the two branches. Extensive experiments on several\ndatasets demonstrate the effectiveness of the proposed LEAF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic flow forecasting aims to predict future traffic flows based on the\nhistorical traffic conditions and the road network. It is an important problem\nin intelligent transportation systems, with a plethora of methods been\nproposed. Existing efforts mainly focus on capturing and utilizing\nspatio-temporal dependencies to predict future traffic flows. Though promising,\nthey fall short in adapting to test-time environmental changes of traffic\nconditions. To tackle this challenge, we propose to introduce large language\nmodels (LLMs) to help traffic flow forecasting and design a novel method named\nLarge Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two\nbranches, capturing different spatio-temporal relations using graph and\nhypergraph structures respectively. The two branches are first pre-trained\nindividually, and during test-time, they yield different predictions. Based on\nthese predictions, a large language model is used to select the most likely\nresult. Then, a ranking loss is applied as the learning objective to enhance\nthe prediction ability of the two branches. Extensive experiments on several\ndatasets demonstrate the effectiveness of the proposed LEAF."
                },
                "authors": [
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Haomin Wen"
                    },
                    {
                        "name": "Zhiping Xiao"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00576v1",
                "updated": "2025-08-01T12:19:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    19,
                    18,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:19:18Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    19,
                    18,
                    4,
                    213,
                    0
                ],
                "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal\n  Interactions in Multimodal AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal\n  Interactions in Multimodal AI Models"
                },
                "summary": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models."
                },
                "authors": [
                    {
                        "name": "Zhanliang Wang"
                    },
                    {
                        "name": "Kai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Wang"
                },
                "author": "Kai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00574v1",
                "updated": "2025-08-01T12:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    17,
                    35,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:17:35Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    17,
                    35,
                    4,
                    213,
                    0
                ],
                "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via\n  Synthetic Continuous Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via\n  Synthetic Continuous Chain-of-Thought"
                },
                "summary": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off."
                },
                "authors": [
                    {
                        "name": "Jianwei Wang"
                    },
                    {
                        "name": "Ziming Wu"
                    },
                    {
                        "name": "Fuming Lai"
                    },
                    {
                        "name": "Shaobing Lian"
                    },
                    {
                        "name": "Ziqian Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Ziqian Zeng"
                },
                "author": "Ziqian Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00570v1",
                "updated": "2025-08-01T12:11:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    11,
                    10,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T12:11:10Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    12,
                    11,
                    10,
                    4,
                    213,
                    0
                ],
                "title": "Session-Based Recommendation with Validated and Enriched LLM Intents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Session-Based Recommendation with Validated and Enriched LLM Intents"
                },
                "summary": "Session-based recommendation (SBR) aims to predict the next item for an\nanonymous user in a timely manner. However, SBR suffers from data sparsity due\nto the short and anonymous nature of sessions. Recently, an emerging line of\nwork has explored inferring the underlying user intents of a session using\nlarge language models (LLMs), with the generated intents serving as auxiliary\ntraining signals to enhance SBR models. Despite its promise, this approach\nfaces three key challenges: validating intent quality, incorporating\nsession-level multi-intents, and complementing inevitable LLM failure cases. In\nthis paper, we propose VELI4SBR, a two-stage framework that leverages Validated\nand Enriched LLM-generated Intents for SBR. In the first stage, we generate\nhigh-quality intents using a predict-and-correct loop that validates the\ninformativeness of LLM-generated intents with a global intent pool to constrain\nthe LLM's output space and reduce hallucination. In the second stage, we\nenhance the SBR model using the generated intents through a lightweight\nmulti-intent prediction and fusion mechanism. Furthermore, we introduce a\ntraining strategy that compensates for LLM failures by inferring intents from\ninter-session behavioral similarities. Extensive experiments show that VELI4SBR\noutperforms state-of-the-art baselines while improving explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Session-based recommendation (SBR) aims to predict the next item for an\nanonymous user in a timely manner. However, SBR suffers from data sparsity due\nto the short and anonymous nature of sessions. Recently, an emerging line of\nwork has explored inferring the underlying user intents of a session using\nlarge language models (LLMs), with the generated intents serving as auxiliary\ntraining signals to enhance SBR models. Despite its promise, this approach\nfaces three key challenges: validating intent quality, incorporating\nsession-level multi-intents, and complementing inevitable LLM failure cases. In\nthis paper, we propose VELI4SBR, a two-stage framework that leverages Validated\nand Enriched LLM-generated Intents for SBR. In the first stage, we generate\nhigh-quality intents using a predict-and-correct loop that validates the\ninformativeness of LLM-generated intents with a global intent pool to constrain\nthe LLM's output space and reduce hallucination. In the second stage, we\nenhance the SBR model using the generated intents through a lightweight\nmulti-intent prediction and fusion mechanism. Furthermore, we introduce a\ntraining strategy that compensates for LLM failures by inferring intents from\ninter-session behavioral similarities. Extensive experiments show that VELI4SBR\noutperforms state-of-the-art baselines while improving explainability."
                },
                "authors": [
                    {
                        "name": "Gyuseok Lee"
                    },
                    {
                        "name": "Yaokun Liu"
                    },
                    {
                        "name": "Yifan Liu"
                    },
                    {
                        "name": "Susik Yoon"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "SeongKu Kang"
                    }
                ],
                "author_detail": {
                    "name": "SeongKu Kang"
                },
                "author": "SeongKu Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00554v1",
                "updated": "2025-08-01T11:48:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    48,
                    13,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T11:48:13Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    48,
                    13,
                    4,
                    213,
                    0
                ],
                "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest\n  Mechanism"
                },
                "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Li Zhao"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Zuoyou Jiang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Yuxiao Bai"
                    },
                    {
                        "name": "Mengting Chen"
                    },
                    {
                        "name": "Xinyang Wang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Zuo Bai"
                    }
                ],
                "author_detail": {
                    "name": "Zuo Bai"
                },
                "author": "Zuo Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00552v1",
                "updated": "2025-08-01T11:47:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    47,
                    36,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T11:47:36Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    11,
                    47,
                    36,
                    4,
                    213,
                    0
                ],
                "title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable\n  Adversarial Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable\n  Adversarial Purification"
                },
                "summary": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification."
                },
                "authors": [
                    {
                        "name": "Chihan Huang"
                    },
                    {
                        "name": "Belal Alsinglawi"
                    },
                    {
                        "name": "Islam Al-qudah"
                    }
                ],
                "author_detail": {
                    "name": "Islam Al-qudah"
                },
                "author": "Islam Al-qudah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19693v2",
                "updated": "2025-08-01T10:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    53,
                    31,
                    4,
                    213,
                    0
                ],
                "published": "2025-03-25T14:18:21Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    14,
                    18,
                    21,
                    1,
                    84,
                    0
                ],
                "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation"
                },
                "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance"
                },
                "authors": [
                    {
                        "name": "Itay Nakash"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Elad Hoffer"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15170v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15170v3",
                "updated": "2025-08-01T10:42:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    42,
                    39,
                    4,
                    213,
                    0
                ],
                "published": "2025-06-18T06:33:19Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    33,
                    19,
                    2,
                    169,
                    0
                ],
                "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in\n  Jailbreak Attacks and Defenses within LLM Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in\n  Jailbreak Attacks and Defenses within LLM Ecosystem"
                },
                "summary": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs."
                },
                "authors": [
                    {
                        "name": "Yanxu Mao"
                    },
                    {
                        "name": "Tiehan Cui"
                    },
                    {
                        "name": "Peipei Liu"
                    },
                    {
                        "name": "Datao You"
                    },
                    {
                        "name": "Hongsong Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hongsong Zhu"
                },
                "author": "Hongsong Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15170v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15170v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00507v1",
                "updated": "2025-08-01T10:36:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    36,
                    39,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T10:36:39Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    36,
                    39,
                    4,
                    213,
                    0
                ],
                "title": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration\n  for Text-Attributed Graph Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration\n  for Text-Attributed Graph Anomaly Detection"
                },
                "summary": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD."
                },
                "authors": [
                    {
                        "name": "Yiming Xu"
                    },
                    {
                        "name": "Jiarun Chen"
                    },
                    {
                        "name": "Zhen Peng"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Lan Ma"
                    },
                    {
                        "name": "Bin Shi"
                    },
                    {
                        "name": "Bo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dong"
                },
                "author": "Bo Dong",
                "arxiv_comment": "Accepted by ACM Multimedia 2025 (MM '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18348v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18348v3",
                "updated": "2025-08-01T10:35:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    35,
                    50,
                    4,
                    213,
                    0
                ],
                "published": "2025-06-23T07:12:08Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    12,
                    8,
                    0,
                    174,
                    0
                ],
                "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team"
                },
                "summary": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research."
                },
                "authors": [
                    {
                        "name": "Weilun Yu"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Yonggui Huang"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Honggang Qi"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Xiaoli Diao"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18348v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18348v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23643v2",
                "updated": "2025-08-01T10:34:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    34,
                    9,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T15:22:23Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    22,
                    23,
                    3,
                    212,
                    0
                ],
                "title": "FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free\n  Training Framework for Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free\n  Training Framework for Spiking Neural Networks"
                },
                "summary": "Spiking Neural Networks (SNNs) offer a biologically plausible framework for\nenergy-efficient neuromorphic computing. However, it is a challenge to train\nSNNs due to their non-differentiability, efficiently. Existing gradient\napproximation approaches frequently sacrifice accuracy and face deployment\nlimitations on edge devices due to the substantial computational requirements\nof backpropagation. To address these challenges, we propose a Forward-Forward\n(FF) based gradient approximation-free training framework for Spiking Neural\nNetworks, which treats spiking activations as black-box modules, thereby\neliminating the need for gradient approximation while significantly reducing\ncomputational complexity. Furthermore, we introduce a class-aware complexity\nadaptation mechanism that dynamically optimizes the loss function based on\ninter-class difficulty metrics, enabling efficient allocation of network\nresources across different categories. Experimental results demonstrate that\nour proposed training framework achieves test accuracies of 99.58%, 92.13%, and\n75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,\nsurpassing all existing FF-based SNN approaches. Additionally, our proposed\nmethod exhibits significant advantages in terms of memory access and\ncomputational power consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) offer a biologically plausible framework for\nenergy-efficient neuromorphic computing. However, it is a challenge to train\nSNNs due to their non-differentiability, efficiently. Existing gradient\napproximation approaches frequently sacrifice accuracy and face deployment\nlimitations on edge devices due to the substantial computational requirements\nof backpropagation. To address these challenges, we propose a Forward-Forward\n(FF) based gradient approximation-free training framework for Spiking Neural\nNetworks, which treats spiking activations as black-box modules, thereby\neliminating the need for gradient approximation while significantly reducing\ncomputational complexity. Furthermore, we introduce a class-aware complexity\nadaptation mechanism that dynamically optimizes the loss function based on\ninter-class difficulty metrics, enabling efficient allocation of network\nresources across different categories. Experimental results demonstrate that\nour proposed training framework achieves test accuracies of 99.58%, 92.13%, and\n75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,\nsurpassing all existing FF-based SNN approaches. Additionally, our proposed\nmethod exhibits significant advantages in terms of memory access and\ncomputational power consumption."
                },
                "authors": [
                    {
                        "name": "Changqing Xu"
                    },
                    {
                        "name": "Ziqiang Yang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Xinfang Liao"
                    },
                    {
                        "name": "Guiqi Mo"
                    },
                    {
                        "name": "Hao Zeng"
                    },
                    {
                        "name": "Yintang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yintang Yang"
                },
                "author": "Yintang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00500v1",
                "updated": "2025-08-01T10:24:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    24,
                    47,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T10:24:47Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    24,
                    47,
                    4,
                    213,
                    0
                ],
                "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via\n  Probabilistic Model Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via\n  Probabilistic Model Checking"
                },
                "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Chris M. Poskitt"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Jiali Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jiali Wei"
                },
                "author": "Jiali Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17407v2",
                "updated": "2025-08-01T10:09:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    9,
                    29,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-24T18:36:15Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    36,
                    15,
                    0,
                    55,
                    0
                ],
                "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning"
                },
                "summary": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Jiwoo Hong"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "ACL 2025 (ORAL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22462v2",
                "updated": "2025-08-01T10:07:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    7,
                    37,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T08:08:48Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    8,
                    48,
                    2,
                    211,
                    0
                ],
                "title": "IFEvalCode: Controlled Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFEvalCode: Controlled Code Generation"
                },
                "summary": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions."
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Guanglin Niu"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19651v2",
                "updated": "2025-08-01T10:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    7,
                    8,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-27T00:49:44Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    0,
                    49,
                    44,
                    3,
                    58,
                    0
                ],
                "title": "Unlocking Multi-Modal Potentials for Link Prediction on Dynamic\n  Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Multi-Modal Potentials for Link Prediction on Dynamic\n  Text-Attributed Graphs"
                },
                "summary": "Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that\ncaptures evolving temporal events (edges) alongside rich textual attributes.\nExisting studies can be broadly categorized into TGNN-driven and LLM-driven\napproaches, both of which encode textual attributes and temporal structures for\nDyTAG representation. We observe that DyTAGs inherently comprise three distinct\nmodalities: temporal, textual, and structural, often exhibiting completely\ndisjoint distributions. However, the first two modalities are largely\noverlooked by existing studies, leading to suboptimal performance. To address\nthis, we propose MoMent, a multi-modal model that explicitly models,\nintegrates, and aligns each modality to learn node representations for link\nprediction. Given the disjoint nature of the original modality distributions,\nwe first construct modality-specific features and encode them using individual\nencoders to capture correlations across temporal patterns, semantic context,\nand local structures. Each encoder generates modality-specific tokens, which\nare then fused into comprehensive node representations with a theoretical\nguarantee. To avoid disjoint subspaces of these heterogeneous modalities, we\npropose a dual-domain alignment loss that first aligns their distributions\nglobally and then fine-tunes coherence at the instance level. This enhances\ncoherent representations from temporal, textual, and structural views.\nExtensive experiments across seven datasets show that MoMent achieves up to\n17.28% accuracy improvement and up to 31x speed-up against eight baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that\ncaptures evolving temporal events (edges) alongside rich textual attributes.\nExisting studies can be broadly categorized into TGNN-driven and LLM-driven\napproaches, both of which encode textual attributes and temporal structures for\nDyTAG representation. We observe that DyTAGs inherently comprise three distinct\nmodalities: temporal, textual, and structural, often exhibiting completely\ndisjoint distributions. However, the first two modalities are largely\noverlooked by existing studies, leading to suboptimal performance. To address\nthis, we propose MoMent, a multi-modal model that explicitly models,\nintegrates, and aligns each modality to learn node representations for link\nprediction. Given the disjoint nature of the original modality distributions,\nwe first construct modality-specific features and encode them using individual\nencoders to capture correlations across temporal patterns, semantic context,\nand local structures. Each encoder generates modality-specific tokens, which\nare then fused into comprehensive node representations with a theoretical\nguarantee. To avoid disjoint subspaces of these heterogeneous modalities, we\npropose a dual-domain alignment loss that first aligns their distributions\nglobally and then fine-tunes coherence at the instance level. This enhances\ncoherent representations from temporal, textual, and structural views.\nExtensive experiments across seven datasets show that MoMent achieves up to\n17.28% accuracy improvement and up to 31x speed-up against eight baselines."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Xu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Xiwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiwei Xu"
                },
                "author": "Xiwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14614v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14614v4",
                "updated": "2025-08-01T10:00:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    0,
                    8,
                    4,
                    213,
                    0
                ],
                "published": "2025-04-20T13:44:39Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    13,
                    44,
                    39,
                    6,
                    110,
                    0
                ],
                "title": "Measurement-device-independent quantum key distribution with asymmetric\n  sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement-device-independent quantum key distribution with asymmetric\n  sources"
                },
                "summary": "Measurement-device-independent quantum key distribution (MDI-QKD), which\neliminates all the attacks from the eavesdropper to the measurement party, has\nbeen one of the most promising technology for the implementation of end-to-end\nquantum networks. In practice, the asymmetry of both sources and channels is\ngenerally inevitable. Therefore, we propose a theory to analyze the performance\nwhen any two MDI users in networks communicates using asymmetric sources in\ndistinct single or multiple temporal modes. As a specific application, we model\nto obtain the key rate of MDI-QKD with weak coherent pulse source and\nspontaneous parametric down-conversion source, and compare the performance to\nthe cases with symmetric (i.e. identical) sources. The result demonstrates that\nthe actual performance does not degrade due to the asymmetry of sources. In\ncontrary, it maintains at a good level over the entire distance we study. This\nwork provides a theoretical basis for analyzing and optimizing MDI-QKD networks\nwith asymmetric sources, and thus paving the way for the practical deployment\nof completely asymmetric MDI-QKD networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement-device-independent quantum key distribution (MDI-QKD), which\neliminates all the attacks from the eavesdropper to the measurement party, has\nbeen one of the most promising technology for the implementation of end-to-end\nquantum networks. In practice, the asymmetry of both sources and channels is\ngenerally inevitable. Therefore, we propose a theory to analyze the performance\nwhen any two MDI users in networks communicates using asymmetric sources in\ndistinct single or multiple temporal modes. As a specific application, we model\nto obtain the key rate of MDI-QKD with weak coherent pulse source and\nspontaneous parametric down-conversion source, and compare the performance to\nthe cases with symmetric (i.e. identical) sources. The result demonstrates that\nthe actual performance does not degrade due to the asymmetry of sources. In\ncontrary, it maintains at a good level over the entire distance we study. This\nwork provides a theoretical basis for analyzing and optimizing MDI-QKD networks\nwith asymmetric sources, and thus paving the way for the practical deployment\nof completely asymmetric MDI-QKD networks."
                },
                "authors": [
                    {
                        "name": "Jia-Ju Deng"
                    },
                    {
                        "name": "Feng-Yu Lu"
                    },
                    {
                        "name": "Zhen-Qiu Zhong"
                    },
                    {
                        "name": "Xiao-Hai Zhan"
                    },
                    {
                        "name": "Zhen-Qiang Yin"
                    },
                    {
                        "name": "Shuang Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "De-Yong He"
                    },
                    {
                        "name": "Guang-Can Guo"
                    },
                    {
                        "name": "Zheng-Fu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zheng-Fu Han"
                },
                "arxiv_affiliation": "Hefei National Laboratory, University of Science and Technology of China, Hefei, China",
                "author": "Zheng-Fu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14614v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14614v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03703v2",
                "updated": "2025-08-01T09:56:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    56,
                    35,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-04T16:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    16,
                    38,
                    9,
                    4,
                    185,
                    0
                ],
                "title": "Sign Spotting Disambiguation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Spotting Disambiguation using Large Language Models"
                },
                "summary": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting."
                },
                "authors": [
                    {
                        "name": "JianHe Low"
                    },
                    {
                        "name": "Ozge Mercanoglu Sincan"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_comment": "Accepted in the international conference on Intelligent Virtual\n  Agents (IVA Adjunct)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00478v1",
                "updated": "2025-08-01T09:53:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    53,
                    6,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:53:06Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    53,
                    6,
                    4,
                    213,
                    0
                ],
                "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy\n  Optimization"
                },
                "summary": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use."
                },
                "authors": [
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Lu Lin"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Biplab Sikdar"
                    }
                ],
                "author_detail": {
                    "name": "Biplab Sikdar"
                },
                "author": "Biplab Sikdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A10, 91A43, 68T01, 94A60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; I.2.6; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02962v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02962v4",
                "updated": "2025-08-01T09:41:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    41,
                    5,
                    4,
                    213,
                    0
                ],
                "published": "2025-06-30T09:02:45Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    2,
                    45,
                    0,
                    181,
                    0
                ],
                "title": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while LLMs remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have aimed to enhance models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to reliance on single-query mode. In this paper, we\npropose RAG-R1, a novel training framework designed to enable LLMs to\nadaptively leverage internal and external knowledge during the reasoning\nprocess. We further expand the generation and retrieval processes within the\nframework from single-query mode to multi-query parallelism, with the aim of\nreducing inference time and enhancing the model's capabilities. Extensive\nexperiments on seven question-answering benchmarks demonstrate that our method\noutperforms the strongest baseline by up to 13.2% and decreases inference time\nby 11.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while LLMs remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have aimed to enhance models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to reliance on single-query mode. In this paper, we\npropose RAG-R1, a novel training framework designed to enable LLMs to\nadaptively leverage internal and external knowledge during the reasoning\nprocess. We further expand the generation and retrieval processes within the\nframework from single-query mode to multi-query parallelism, with the aim of\nreducing inference time and enhancing the model's capabilities. Extensive\nexperiments on seven question-answering benchmarks demonstrate that our method\noutperforms the strongest baseline by up to 13.2% and decreases inference time\nby 11.1%."
                },
                "authors": [
                    {
                        "name": "Zhiwen Tan"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Qintong Wu"
                    },
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02962v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02962v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00459v1",
                "updated": "2025-08-01T09:31:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    31,
                    48,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:31:48Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    31,
                    48,
                    4,
                    213,
                    0
                ],
                "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Machines: Mathematical Reasoning in the Age of LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended."
                },
                "authors": [
                    {
                        "name": "Andrea Asperti"
                    },
                    {
                        "name": "Alberto Naibo"
                    },
                    {
                        "name": "Claudio Sacerdoti Coen"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Sacerdoti Coen"
                },
                "author": "Claudio Sacerdoti Coen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00454v1",
                "updated": "2025-08-01T09:26:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    26,
                    1,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:26:01Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    26,
                    1,
                    4,
                    213,
                    0
                ],
                "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges"
                },
                "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness."
                },
                "authors": [
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Yunfeng Wang"
                    },
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Keyan Ding"
                    }
                ],
                "author_detail": {
                    "name": "Keyan Ding"
                },
                "author": "Keyan Ding",
                "arxiv_comment": "15 pages, 2 pages, under review at AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00450v1",
                "updated": "2025-08-01T09:10:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    10,
                    56,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T09:10:56Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    10,
                    56,
                    4,
                    213,
                    0
                ],
                "title": "When Relevance Meets Novelty: Dual-Stable Periodic Optimization for\n  Exploratory Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Relevance Meets Novelty: Dual-Stable Periodic Optimization for\n  Exploratory Recommendation"
                },
                "summary": "Traditional recommendation systems tend to trap users in strong feedback\nloops by excessively pushing content aligned with their historical preferences,\nthereby limiting exploration opportunities and causing content fatigue.\nAlthough large language models (LLMs) demonstrate potential with their diverse\ncontent generation capabilities, existing LLM-enhanced dual-model frameworks\nface two major limitations: first, they overlook long-term preferences driven\nby group identity, leading to biased interest modeling; second, they suffer\nfrom static optimization flaws, as a one-time alignment process fails to\nleverage incremental user data for closed-loop optimization. To address these\nchallenges, we propose the Co-Evolutionary Alignment (CoEA) method. For\ninterest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)\nmodule, jointly modeling long-term group identity and short-term individual\ninterests through parallel processing of behavioral sequences. For static\noptimization limitations, we design a Periodic Collaborative Optimization (PCO)\nmechanism. This mechanism regularly conducts preference verification on\nincremental data using the Relevance LLM, then guides the Novelty LLM to\nperform fine-tuning based on the verification results, and subsequently feeds\nback the output of the incrementally fine-tuned Novelty LLM to the Relevance\nLLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.\nExtensive online and offline experiments verify the effectiveness of the CoEA\nmodel in exploratory recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommendation systems tend to trap users in strong feedback\nloops by excessively pushing content aligned with their historical preferences,\nthereby limiting exploration opportunities and causing content fatigue.\nAlthough large language models (LLMs) demonstrate potential with their diverse\ncontent generation capabilities, existing LLM-enhanced dual-model frameworks\nface two major limitations: first, they overlook long-term preferences driven\nby group identity, leading to biased interest modeling; second, they suffer\nfrom static optimization flaws, as a one-time alignment process fails to\nleverage incremental user data for closed-loop optimization. To address these\nchallenges, we propose the Co-Evolutionary Alignment (CoEA) method. For\ninterest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)\nmodule, jointly modeling long-term group identity and short-term individual\ninterests through parallel processing of behavioral sequences. For static\noptimization limitations, we design a Periodic Collaborative Optimization (PCO)\nmechanism. This mechanism regularly conducts preference verification on\nincremental data using the Relevance LLM, then guides the Novelty LLM to\nperform fine-tuning based on the verification results, and subsequently feeds\nback the output of the incrementally fine-tuned Novelty LLM to the Relevance\nLLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.\nExtensive online and offline experiments verify the effectiveness of the CoEA\nmodel in exploratory recommendation."
                },
                "authors": [
                    {
                        "name": "Hongxiang Lin"
                    },
                    {
                        "name": "Hao Guo"
                    },
                    {
                        "name": "Zeshun Li"
                    },
                    {
                        "name": "Erpeng Xue"
                    },
                    {
                        "name": "Yongqian He"
                    },
                    {
                        "name": "Xiangyu Hou"
                    },
                    {
                        "name": "Zhaoyu Hu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Sheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Chen"
                },
                "author": "Sheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16974v2",
                "updated": "2025-08-01T09:04:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    4,
                    46,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-22T19:25:10Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    19,
                    25,
                    10,
                    1,
                    203,
                    0
                ],
                "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs\n  in the Agricultural Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs\n  in the Agricultural Domain"
                },
                "summary": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Publicly available general-purpose Large Language Models\n(LLMs) typically offer generic agriculture advisories, lacking precision in\nlocal and multilingual contexts. Our study addresses this limitation by\ngenerating multilingual (English, Hindi, Punjabi) synthetic datasets from\nagriculture-specific documents from India and fine-tuning LLMs for the task of\nquestion answering (QA). Evaluation on human-created datasets demonstrates\nsignificant improvements in factuality, relevance, and agricultural consensus\nfor the fine-tuned LLMs compared to the baseline counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Publicly available general-purpose Large Language Models\n(LLMs) typically offer generic agriculture advisories, lacking precision in\nlocal and multilingual contexts. Our study addresses this limitation by\ngenerating multilingual (English, Hindi, Punjabi) synthetic datasets from\nagriculture-specific documents from India and fine-tuning LLMs for the task of\nquestion answering (QA). Evaluation on human-created datasets demonstrates\nsignificant improvements in factuality, relevance, and agricultural consensus\nfor the fine-tuned LLMs compared to the baseline counterparts."
                },
                "authors": [
                    {
                        "name": "Rishemjit Kaur"
                    },
                    {
                        "name": "Arshdeep Singh Bhankhar"
                    },
                    {
                        "name": "Jashanpreet Singh Salh"
                    },
                    {
                        "name": "Sudhir Rajput"
                    },
                    {
                        "name": "Vidhi"
                    },
                    {
                        "name": "Kashish Mahendra"
                    },
                    {
                        "name": "Bhavika Berwal"
                    },
                    {
                        "name": "Ritesh Kumar"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    }
                ],
                "author_detail": {
                    "name": "Surangika Ranathunga"
                },
                "author": "Surangika Ranathunga",
                "arxiv_comment": "16 pages, 9 tables, Appendix A-L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00440v1",
                "updated": "2025-08-01T08:57:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    57,
                    2,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:57:02Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    57,
                    2,
                    4,
                    213,
                    0
                ],
                "title": "Reducing the gap between general purpose data and aerial images in\n  concentrated solar power plants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the gap between general purpose data and aerial images in\n  concentrated solar power plants"
                },
                "summary": "In the context of Concentrated Solar Power (CSP) plants, aerial images\ncaptured by drones present a unique set of challenges. Unlike urban or natural\nlandscapes commonly found in existing datasets, solar fields contain highly\nreflective surfaces, and domain-specific elements that are uncommon in\ntraditional computer vision benchmarks. As a result, machine learning models\ntrained on generic datasets struggle to generalize to this setting without\nextensive retraining and large volumes of annotated data. However, collecting\nand labeling such data is costly and time-consuming, making it impractical for\nrapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of\nAerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By\ngenerating synthetic data that closely mimic real-world conditions, our\nobjective is to facilitate pretraining of models before deployment,\nsignificantly reducing the need for extensive manual labeling. Our main\ncontributions are threefold: (1) we introduce AerialCSP, a high-quality\nsynthetic dataset for aerial inspection of CSP plants, providing annotated data\nfor object detection and image segmentation; (2) we benchmark multiple models\non AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we\ndemonstrate that pretraining on AerialCSP significantly improves real-world\nfault detection, particularly for rare and small defects, reducing the need for\nextensive manual labeling. AerialCSP is made publicly available at\nhttps://mpcutino.github.io/aerialcsp/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of Concentrated Solar Power (CSP) plants, aerial images\ncaptured by drones present a unique set of challenges. Unlike urban or natural\nlandscapes commonly found in existing datasets, solar fields contain highly\nreflective surfaces, and domain-specific elements that are uncommon in\ntraditional computer vision benchmarks. As a result, machine learning models\ntrained on generic datasets struggle to generalize to this setting without\nextensive retraining and large volumes of annotated data. However, collecting\nand labeling such data is costly and time-consuming, making it impractical for\nrapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of\nAerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By\ngenerating synthetic data that closely mimic real-world conditions, our\nobjective is to facilitate pretraining of models before deployment,\nsignificantly reducing the need for extensive manual labeling. Our main\ncontributions are threefold: (1) we introduce AerialCSP, a high-quality\nsynthetic dataset for aerial inspection of CSP plants, providing annotated data\nfor object detection and image segmentation; (2) we benchmark multiple models\non AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we\ndemonstrate that pretraining on AerialCSP significantly improves real-world\nfault detection, particularly for rare and small defects, reducing the need for\nextensive manual labeling. AerialCSP is made publicly available at\nhttps://mpcutino.github.io/aerialcsp/."
                },
                "authors": [
                    {
                        "name": "M. A. Prez-Cutio"
                    },
                    {
                        "name": "J. Valverde"
                    },
                    {
                        "name": "J. Capitn"
                    },
                    {
                        "name": "J. M. Daz-Bez"
                    }
                ],
                "author_detail": {
                    "name": "J. M. Daz-Bez"
                },
                "author": "J. M. Daz-Bez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00429v1",
                "updated": "2025-08-01T08:37:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    37,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:37:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    37,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network"
                },
                "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning."
                },
                "authors": [
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "17 pages, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00422v1",
                "updated": "2025-08-01T08:24:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    24,
                    14,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:24:14Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    24,
                    14,
                    4,
                    213,
                    0
                ],
                "title": "Automated Type Annotation in Python Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Type Annotation in Python Using Large Language Models"
                },
                "summary": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby"
                },
                "authors": [
                    {
                        "name": "Varun Bharti"
                    },
                    {
                        "name": "Shashwat Jha"
                    },
                    {
                        "name": "Dhruv Kumar"
                    },
                    {
                        "name": "Pankaj Jalote"
                    }
                ],
                "author_detail": {
                    "name": "Pankaj Jalote"
                },
                "author": "Pankaj Jalote",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23859v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23859v3",
                "updated": "2025-08-01T08:21:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    21,
                    27,
                    4,
                    213,
                    0
                ],
                "published": "2025-03-31T09:06:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    6,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "radio-llava: Advancing Vision-Language Models for Radio Astronomical\n  Source Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "radio-llava: Advancing Vision-Language Models for Radio Astronomical\n  Source Analysis"
                },
                "summary": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform vision-only classifiers and exhibit ~20% drop\non general multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on multimodal\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform vision-only classifiers and exhibit ~20% drop\non general multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on multimodal\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting."
                },
                "authors": [
                    {
                        "name": "S. Riggi"
                    },
                    {
                        "name": "T. Cecconello"
                    },
                    {
                        "name": "A. Pilzer"
                    },
                    {
                        "name": "S. Palazzo"
                    },
                    {
                        "name": "N. Gupta"
                    },
                    {
                        "name": "A. M. Hopkins"
                    },
                    {
                        "name": "C. Trigilio"
                    },
                    {
                        "name": "G. Umana"
                    }
                ],
                "author_detail": {
                    "name": "G. Umana"
                },
                "author": "G. Umana",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23859v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23859v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00419v1",
                "updated": "2025-08-01T08:15:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    15,
                    15,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:15:15Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    15,
                    15,
                    4,
                    213,
                    0
                ],
                "title": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised\n  LLMs and SMT Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised\n  LLMs and SMT Solvers"
                },
                "summary": "Loop invariants are essential for proving the correctness of programs with\nloops. Developing loop invariants is challenging, and fully automatic synthesis\ncannot be guaranteed for arbitrary programs. Some approaches have been proposed\nto synthesize loop invariants using symbolic techniques and more recently using\nneural approaches. These approaches are able to correctly synthesize loop\ninvariants only for subsets of standard benchmarks. In this work, we\ninvestigate whether modern, reasoning-optimized large language models can do\nbetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled\ngenerate-and-check pipeline with the Z3 SMT solver, using solver\ncounterexamples to iteratively guide invariant refinement. We use Code2Inv\nbenchmark, which provides C programs along with their formal preconditions and\npostconditions. On this benchmark of 133 tasks, our framework achieves 100%\ncoverage (133 out of 133), outperforming the previous best of 107 out of 133,\nwhile requiring only 1-2 model proposals per instance and 14-55 seconds of\nwall-clock time. These results demonstrate that LLMs possess latent logical\nreasoning capabilities which can help automate loop invariant synthesis. While\nour experiments target C-specific programs, this approach should be\ngeneralizable to other imperative languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loop invariants are essential for proving the correctness of programs with\nloops. Developing loop invariants is challenging, and fully automatic synthesis\ncannot be guaranteed for arbitrary programs. Some approaches have been proposed\nto synthesize loop invariants using symbolic techniques and more recently using\nneural approaches. These approaches are able to correctly synthesize loop\ninvariants only for subsets of standard benchmarks. In this work, we\ninvestigate whether modern, reasoning-optimized large language models can do\nbetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled\ngenerate-and-check pipeline with the Z3 SMT solver, using solver\ncounterexamples to iteratively guide invariant refinement. We use Code2Inv\nbenchmark, which provides C programs along with their formal preconditions and\npostconditions. On this benchmark of 133 tasks, our framework achieves 100%\ncoverage (133 out of 133), outperforming the previous best of 107 out of 133,\nwhile requiring only 1-2 model proposals per instance and 14-55 seconds of\nwall-clock time. These results demonstrate that LLMs possess latent logical\nreasoning capabilities which can help automate loop invariant synthesis. While\nour experiments target C-specific programs, this approach should be\ngeneralizable to other imperative languages."
                },
                "authors": [
                    {
                        "name": "Varun Bharti"
                    },
                    {
                        "name": "Shashwat Jha"
                    },
                    {
                        "name": "Dhruv Kumar"
                    },
                    {
                        "name": "Pankaj Jalote"
                    }
                ],
                "author_detail": {
                    "name": "Pankaj Jalote"
                },
                "author": "Pankaj Jalote",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00410v1",
                "updated": "2025-08-01T08:09:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    9,
                    14,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:09:14Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    9,
                    14,
                    4,
                    213,
                    0
                ],
                "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language\n  Model Reasoning via Contrastive Agreement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-Reward: Self-supervised Reinforcement Learning for Large Language\n  Model Reasoning via Contrastive Agreement"
                },
                "summary": "Although reinforcement learning with verifiable rewards (RLVR) shows promise\nin improving the reasoning ability of large language models (LLMs), the scaling\nup dilemma remains due to the reliance on human annotated labels especially for\ncomplex tasks. Recent alternatives that explore various self-reward signals\nexhibit the eliciting potential of LLM reasoning, but suffer from the\nnon-negligible collapse issue. Inspired by the success of self-supervised\nlearning, we propose \\textit{Co-Reward}, a novel RL framework that leverages\ncontrastive agreement across semantically analogical questions as a reward\nbasis. Specifically, we construct a similar question for each training sample\n(without labels) and synthesize their individual surrogate labels through a\nsimple rollout voting, and then the reward is constructed by cross-referring\nthe labels of each question pair to enforce the internal reasoning consistency\nacross analogical inputs. Intuitively, such a self-supervised reward-shaping\nmechanism increases the difficulty of learning collapse into a trivial\nsolution, and promotes stable reasoning elicitation and improvement through\nexpanding the input sample variants. Empirically, Co-Reward achieves superior\nperformance compared to other self-reward baselines on multiple reasoning\nbenchmarks and LLM series, and reaches or even surpasses ground-truth (GT)\nlabeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward\non Llama-3.2-3B-Instruct. Our code is publicly available at\nhttps://github.com/tmlr-group/Co-Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although reinforcement learning with verifiable rewards (RLVR) shows promise\nin improving the reasoning ability of large language models (LLMs), the scaling\nup dilemma remains due to the reliance on human annotated labels especially for\ncomplex tasks. Recent alternatives that explore various self-reward signals\nexhibit the eliciting potential of LLM reasoning, but suffer from the\nnon-negligible collapse issue. Inspired by the success of self-supervised\nlearning, we propose \\textit{Co-Reward}, a novel RL framework that leverages\ncontrastive agreement across semantically analogical questions as a reward\nbasis. Specifically, we construct a similar question for each training sample\n(without labels) and synthesize their individual surrogate labels through a\nsimple rollout voting, and then the reward is constructed by cross-referring\nthe labels of each question pair to enforce the internal reasoning consistency\nacross analogical inputs. Intuitively, such a self-supervised reward-shaping\nmechanism increases the difficulty of learning collapse into a trivial\nsolution, and promotes stable reasoning elicitation and improvement through\nexpanding the input sample variants. Empirically, Co-Reward achieves superior\nperformance compared to other self-reward baselines on multiple reasoning\nbenchmarks and LLM series, and reaches or even surpasses ground-truth (GT)\nlabeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward\non Llama-3.2-3B-Instruct. Our code is publicly available at\nhttps://github.com/tmlr-group/Co-Reward."
                },
                "authors": [
                    {
                        "name": "Zizhuo Zhang"
                    },
                    {
                        "name": "Jianing Zhu"
                    },
                    {
                        "name": "Xinmu Ge"
                    },
                    {
                        "name": "Zihua Zhao"
                    },
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Jiangchao Yao"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00408v1",
                "updated": "2025-08-01T08:08:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    8,
                    26,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:08:26Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    8,
                    26,
                    4,
                    213,
                    0
                ],
                "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Unit Test Generation from Real-World Functions"
                },
                "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%)."
                },
                "authors": [
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Qianru Zhang"
                    },
                    {
                        "name": "Mingzhe Du"
                    },
                    {
                        "name": "See-Kiong Ng"
                    }
                ],
                "author_detail": {
                    "name": "See-Kiong Ng"
                },
                "author": "See-Kiong Ng",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07556v2",
                "updated": "2025-08-01T07:38:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    38,
                    59,
                    4,
                    213,
                    0
                ],
                "published": "2025-03-10T17:25:24Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    25,
                    24,
                    0,
                    69,
                    0
                ],
                "title": "Novice Developers' Perspectives on Adopting LLMs for Software\n  Development: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novice Developers' Perspectives on Adopting LLMs for Software\n  Development: A Systematic Literature Review"
                },
                "summary": "Following the rise of large language models (LLMs), many studies have emerged\nin recent years focusing on exploring the adoption of LLM-based tools for\nsoftware development by novice developers: computer science/software\nengineering students and early-career industry developers with two years or\nless of professional experience. These studies have sought to understand the\nperspectives of novice developers on using these tools, a critical aspect of\nthe successful adoption of LLMs in software engineering. To systematically\ncollect and summarise these studies, we conducted a systematic literature\nreview (SLR) following the guidelines by Kitchenham et al. on 80 primary\nstudies published between April 2022 and June 2025 to answer four research\nquestions (RQs). In answering RQ1, we categorised the study motivations and\nmethodological approaches. In RQ2, we identified the software development tasks\nfor which novice developers use LLMs. In RQ3, we categorised the advantages,\nchallenges, and recommendations discussed in the studies. Finally, we discuss\nthe study limitations and future research needs suggested in the primary\nstudies in answering RQ4. Throughout the paper, we also indicate directions for\nfuture work and implications for software engineering researchers, educators,\nand developers. Our research artifacts are publicly available at\nhttps://github.com/Samuellucas97/SupplementaryInfoPackage-SLR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the rise of large language models (LLMs), many studies have emerged\nin recent years focusing on exploring the adoption of LLM-based tools for\nsoftware development by novice developers: computer science/software\nengineering students and early-career industry developers with two years or\nless of professional experience. These studies have sought to understand the\nperspectives of novice developers on using these tools, a critical aspect of\nthe successful adoption of LLMs in software engineering. To systematically\ncollect and summarise these studies, we conducted a systematic literature\nreview (SLR) following the guidelines by Kitchenham et al. on 80 primary\nstudies published between April 2022 and June 2025 to answer four research\nquestions (RQs). In answering RQ1, we categorised the study motivations and\nmethodological approaches. In RQ2, we identified the software development tasks\nfor which novice developers use LLMs. In RQ3, we categorised the advantages,\nchallenges, and recommendations discussed in the studies. Finally, we discuss\nthe study limitations and future research needs suggested in the primary\nstudies in answering RQ4. Throughout the paper, we also indicate directions for\nfuture work and implications for software engineering researchers, educators,\nand developers. Our research artifacts are publicly available at\nhttps://github.com/Samuellucas97/SupplementaryInfoPackage-SLR."
                },
                "authors": [
                    {
                        "name": "Samuel Ferino"
                    },
                    {
                        "name": "Rashina Hoda"
                    },
                    {
                        "name": "John Grundy"
                    },
                    {
                        "name": "Christoph Treude"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Treude"
                },
                "author": "Christoph Treude",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00385v1",
                "updated": "2025-08-01T07:26:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    26,
                    39,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T07:26:39Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    26,
                    39,
                    4,
                    213,
                    0
                ],
                "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness"
                },
                "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Xuangliang Zhang"
                    },
                    {
                        "name": "Keyan Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Yang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Deng"
                },
                "author": "Yang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00384v1",
                "updated": "2025-08-01T07:25:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    25,
                    59,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T07:25:59Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    25,
                    59,
                    4,
                    213,
                    0
                ],
                "title": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator"
                },
                "summary": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles."
                },
                "authors": [
                    {
                        "name": "Juanwu Lu"
                    },
                    {
                        "name": "Rohit Gupta"
                    },
                    {
                        "name": "Ahmadreza Moradipari"
                    },
                    {
                        "name": "Kyungtae Han"
                    },
                    {
                        "name": "Ruqi Zhang"
                    },
                    {
                        "name": "Ziran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ziran Wang"
                },
                "author": "Ziran Wang",
                "arxiv_comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. Source Code: https://github.com/juanwulu/niva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00381v1",
                "updated": "2025-08-01T07:19:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    19,
                    23,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T07:19:23Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    19,
                    23,
                    4,
                    213,
                    0
                ],
                "title": "Advancing Welding Defect Detection in Maritime Operations via\n  Adapt-WeldNet and Defect Detection Interpretability Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Welding Defect Detection in Maritime Operations via\n  Adapt-WeldNet and Defect Detection Interpretability Analysis"
                },
                "summary": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments."
                },
                "authors": [
                    {
                        "name": "Kamal Basha S"
                    },
                    {
                        "name": "Athira Nambiar"
                    }
                ],
                "author_detail": {
                    "name": "Athira Nambiar"
                },
                "author": "Athira Nambiar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v1",
                "updated": "2025-08-01T07:03:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13205v2",
                "updated": "2025-08-01T06:53:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    53,
                    55,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-19T14:56:24Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    56,
                    24,
                    0,
                    139,
                    0
                ],
                "title": "Quantum Knowledge Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Knowledge Distillation for Large Language Models"
                },
                "summary": "As foundational tools in natural language processing, Large Language Models\n(LLMs) have immense parameter scales, which makes deployment and inference\nincreasingly prohibitive, especially in resource-constrained devices.\nTherefore, knowledge distillation for LLMs, i.e., compressing the LLM to a\nsmaller model, is meaningful. With strong parameter representation capacity,\nquantum computing is regarded as a promising solution. Here, we propose a\nQuantum knowledge Distillation model for LLMs (QD-LLM) that leverages\nvariational quantum circuits to learn from LLMs. In classical simulation,\nQD-LLM outperforms several mainstream distillation methods on multiple text\nclassification tasks in terms of both accuracy and efficiency using only 11\nqubits. The results reveal an interesting phenomenon that the simulation of\nquantum student models may be regarded as a new class of quantum-inspired\nclassical algorithms. Remarkably, we deploy the obtained circuits on the Baihua\nsuperconducting quantum processor via the Quafu platform to assess practical\nfeasibility. The model maintains stable inference performance despite hardware\nconstraints such as decoherence and finite sampling. In summary, QD-LLM marks a\nfoundational step in connecting quantum computing with LLMs, demonstrating the\nfeasibility of quantum-native approaches that aim to compress and deploy models\nof increasingly larger scales. The code of this article has been open-sourced\nat https://github.com/Lilingxiao-bupt/QD-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As foundational tools in natural language processing, Large Language Models\n(LLMs) have immense parameter scales, which makes deployment and inference\nincreasingly prohibitive, especially in resource-constrained devices.\nTherefore, knowledge distillation for LLMs, i.e., compressing the LLM to a\nsmaller model, is meaningful. With strong parameter representation capacity,\nquantum computing is regarded as a promising solution. Here, we propose a\nQuantum knowledge Distillation model for LLMs (QD-LLM) that leverages\nvariational quantum circuits to learn from LLMs. In classical simulation,\nQD-LLM outperforms several mainstream distillation methods on multiple text\nclassification tasks in terms of both accuracy and efficiency using only 11\nqubits. The results reveal an interesting phenomenon that the simulation of\nquantum student models may be regarded as a new class of quantum-inspired\nclassical algorithms. Remarkably, we deploy the obtained circuits on the Baihua\nsuperconducting quantum processor via the Quafu platform to assess practical\nfeasibility. The model maintains stable inference performance despite hardware\nconstraints such as decoherence and finite sampling. In summary, QD-LLM marks a\nfoundational step in connecting quantum computing with LLMs, demonstrating the\nfeasibility of quantum-native approaches that aim to compress and deploy models\nof increasingly larger scales. The code of this article has been open-sourced\nat https://github.com/Lilingxiao-bupt/QD-LLM."
                },
                "authors": [
                    {
                        "name": "Lingxiao Li"
                    },
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Jiacheng Fan"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Sujuan Qin"
                    },
                    {
                        "name": "Qiaoyan Wen"
                    },
                    {
                        "name": "Fei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Fei Gao"
                },
                "author": "Fei Gao",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22062v3",
                "updated": "2025-08-01T06:40:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    40,
                    13,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-29T17:59:58Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    17,
                    59,
                    58,
                    1,
                    210,
                    0
                ],
                "title": "Meta CLIP 2: A Worldwide Scaling Recipe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta CLIP 2: A Worldwide Scaling Recipe"
                },
                "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval."
                },
                "authors": [
                    {
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Ching-Feng Yeh"
                    },
                    {
                        "name": "Kehan Lyu"
                    },
                    {
                        "name": "Ramya Raghavendra"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Lifei Huang"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Zhuang Liu"
                    },
                    {
                        "name": "Saining Xie"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Shang-Wen Li"
                    },
                    {
                        "name": "Hu Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hu Xu"
                },
                "author": "Hu Xu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23628v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23628v3",
                "updated": "2025-08-01T06:39:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    39,
                    45,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-29T16:34:58Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    34,
                    58,
                    3,
                    149,
                    0
                ],
                "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic\n  Schema Induction from Web-Scale Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic\n  Schema Induction from Web-Scale Corpora"
                },
                "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 92\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 92\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models."
                },
                "authors": [
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Hongyu Luo"
                    },
                    {
                        "name": "Yauwai Yim"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Feng Qin"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Xi Peng"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "Huiwen Yang"
                    },
                    {
                        "name": "Leijie Wu"
                    },
                    {
                        "name": "Yi Ji"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23628v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23628v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08332v2",
                "updated": "2025-08-01T06:36:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    36,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-06-10T01:38:57Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    1,
                    38,
                    57,
                    1,
                    161,
                    0
                ],
                "title": "ORFS-agent: Tool-Using Agents for Chip Design Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORFS-agent: Tool-Using Agents for Chip Design Optimization"
                },
                "summary": "Machine learning has been widely used to optimize complex engineering\nworkflows across numerous domains. In the context of integrated circuit design,\nmodern flows (e.g., going from a register-transfer level netlist to physical\nlayouts) involve extensive configuration via thousands of parameters, and small\nchanges to these parameters can have large downstream impacts on desired\noutcomes - namely design performance, power, and area. Recent advances in Large\nLanguage Models (LLMs) offer new opportunities for learning and reasoning\nwithin such high-dimensional optimization tasks. In this work, we introduce\nORFS-agent, an LLM-based iterative optimization agent that automates parameter\ntuning in an open-source hardware design flow. ORFS-agent adaptively explores\nparameter configurations, demonstrating clear improvements over standard\nBayesian optimization approaches in terms of resource efficiency and final\ndesign metrics. Our empirical evaluations on two different technology nodes and\na range of circuit benchmarks indicate that ORFS-agent can improve both routed\nwirelength and effective clock period by over 13%, all while using 40% fewer\noptimization iterations. Moreover, by following natural language objectives to\ntrade off certain metrics for others, ORFS-agent demonstrates a flexible and\ninterpretable framework for multi-objective optimization. Crucially, RFS-agent\nis modular and model-agnostic, and can be plugged in to any frontier LLM\nwithout any further fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has been widely used to optimize complex engineering\nworkflows across numerous domains. In the context of integrated circuit design,\nmodern flows (e.g., going from a register-transfer level netlist to physical\nlayouts) involve extensive configuration via thousands of parameters, and small\nchanges to these parameters can have large downstream impacts on desired\noutcomes - namely design performance, power, and area. Recent advances in Large\nLanguage Models (LLMs) offer new opportunities for learning and reasoning\nwithin such high-dimensional optimization tasks. In this work, we introduce\nORFS-agent, an LLM-based iterative optimization agent that automates parameter\ntuning in an open-source hardware design flow. ORFS-agent adaptively explores\nparameter configurations, demonstrating clear improvements over standard\nBayesian optimization approaches in terms of resource efficiency and final\ndesign metrics. Our empirical evaluations on two different technology nodes and\na range of circuit benchmarks indicate that ORFS-agent can improve both routed\nwirelength and effective clock period by over 13%, all while using 40% fewer\noptimization iterations. Moreover, by following natural language objectives to\ntrade off certain metrics for others, ORFS-agent demonstrates a flexible and\ninterpretable framework for multi-objective optimization. Crucially, RFS-agent\nis modular and model-agnostic, and can be plugged in to any frontier LLM\nwithout any further fine-tuning."
                },
                "authors": [
                    {
                        "name": "Amur Ghose"
                    },
                    {
                        "name": "Andrew B. Kahng"
                    },
                    {
                        "name": "Sayak Kundu"
                    },
                    {
                        "name": "Zhiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wang"
                },
                "author": "Zhiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16604v2",
                "updated": "2025-08-01T06:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    19,
                    26,
                    4,
                    213,
                    0
                ],
                "published": "2025-04-23T10:32:45Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    32,
                    45,
                    2,
                    113,
                    0
                ],
                "title": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to\n  Challenge Conspiracy Theories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to\n  Challenge Conspiracy Theories"
                },
                "summary": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic."
                },
                "authors": [
                    {
                        "name": "Mareike Lisker"
                    },
                    {
                        "name": "Christina Gottschalk"
                    },
                    {
                        "name": "Helena Mihaljevi"
                    }
                ],
                "author_detail": {
                    "name": "Helena Mihaljevi"
                },
                "author": "Helena Mihaljevi",
                "arxiv_comment": "16 pages, Association for Computational Linguistics, Proceedings of\n  the 9th Workshop on Online Abuse and Harms (WOAH 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00344v1",
                "updated": "2025-08-01T06:17:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    17,
                    11,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T06:17:11Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    17,
                    11,
                    4,
                    213,
                    0
                ],
                "title": "PilotRL: Training Language Model Agents via Global Planning-Guided\n  Progressive Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PilotRL: Training Language Model Agents via Global Planning-Guided\n  Progressive Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Huang Leng"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10284v3",
                "updated": "2025-08-01T06:12:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    12,
                    35,
                    4,
                    213,
                    0
                ],
                "published": "2025-04-14T14:52:28Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    52,
                    28,
                    0,
                    104,
                    0
                ],
                "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\n  Evaluation Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\n  Evaluation Protocol"
                },
                "summary": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table."
                },
                "authors": [
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiefu Ou"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15055v2",
                "updated": "2025-08-01T05:56:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    5,
                    56,
                    21,
                    4,
                    213,
                    0
                ],
                "published": "2025-05-21T03:24:11Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    3,
                    24,
                    11,
                    2,
                    141,
                    0
                ],
                "title": "Lost in Benchmarks? Rethinking Large Language Model Benchmarking with\n  Item Response Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Benchmarks? Rethinking Large Language Model Benchmarking with\n  Item Response Theory"
                },
                "summary": "The evaluation of large language models (LLMs) via benchmarks is widespread,\nyet inconsistencies between different leaderboards and poor separability among\ntop models raise concerns about their ability to accurately reflect authentic\nmodel capabilities. This paper provides a critical analysis of benchmark\neffectiveness, examining mainstream prominent LLM benchmarks using results from\ndiverse models. We first propose Pseudo-Siamese Network for Item Response\nTheory (PSN-IRT), an enhanced Item Response Theory framework that incorporates\na rich set of item parameters within an IRT-grounded architecture. PSN-IRT can\nbe utilized for accurate and reliable estimations of item characteristics and\nmodel abilities. Based on PSN-IRT, we conduct extensive analysis on 11 LLM\nbenchmarks comprising 41,871 items, revealing significant and varied\nshortcomings in their measurement quality. Furthermore, we demonstrate that\nleveraging PSN-IRT is able to construct smaller benchmarks while maintaining\nstronger alignment with human preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of large language models (LLMs) via benchmarks is widespread,\nyet inconsistencies between different leaderboards and poor separability among\ntop models raise concerns about their ability to accurately reflect authentic\nmodel capabilities. This paper provides a critical analysis of benchmark\neffectiveness, examining mainstream prominent LLM benchmarks using results from\ndiverse models. We first propose Pseudo-Siamese Network for Item Response\nTheory (PSN-IRT), an enhanced Item Response Theory framework that incorporates\na rich set of item parameters within an IRT-grounded architecture. PSN-IRT can\nbe utilized for accurate and reliable estimations of item characteristics and\nmodel abilities. Based on PSN-IRT, we conduct extensive analysis on 11 LLM\nbenchmarks comprising 41,871 items, revealing significant and varied\nshortcomings in their measurement quality. Furthermore, we demonstrate that\nleveraging PSN-IRT is able to construct smaller benchmarks while maintaining\nstronger alignment with human preference."
                },
                "authors": [
                    {
                        "name": "Hongli Zhou"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Ziqing Zhao"
                    },
                    {
                        "name": "Lvyuan Han"
                    },
                    {
                        "name": "Huicheng Wang"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Wei Bao"
                    },
                    {
                        "name": "Jian Dong"
                    },
                    {
                        "name": "Bing Xu"
                    },
                    {
                        "name": "Conghui Zhu"
                    },
                    {
                        "name": "Hailong Cao"
                    },
                    {
                        "name": "Tiejun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Zhao"
                },
                "author": "Tiejun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12312v2",
                "updated": "2025-08-01T05:43:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    5,
                    43,
                    41,
                    4,
                    213,
                    0
                ],
                "published": "2025-04-09T09:54:03Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    54,
                    3,
                    2,
                    99,
                    0
                ],
                "title": "Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large\n  Language Models with Logic Programming-based Test Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large\n  Language Models with Logic Programming-based Test Oracles"
                },
                "summary": "Large Language Models (LLMs) have achieved significant progress in language\nunderstanding and reasoning. Evaluating and analyzing their logical reasoning\nabilities has therefore become essential. However, existing datasets and\nbenchmarks are often limited to overly simplistic, unnatural, or contextually\nconstrained examples. In response to the growing demand, we introduce\nSmartyPat-Bench, a challenging, naturally expressed, and systematically labeled\nbenchmark derived from real-world high-quality Reddit posts containing subtle\nlogical fallacies. Unlike existing datasets and benchmarks, it provides more\ndetailed annotations of logical fallacies and features more diverse data. To\nfurther scale up the study and address the limitations of manual data\ncollection and labeling - such as fallacy-type imbalance and labor-intensive\nannotation - we introduce SmartyPat, an automated framework powered by logic\nprogramming-based oracles. SmartyPat utilizes Prolog rules to systematically\ngenerate logically fallacious statements, which are then refined into fluent\nnatural-language sentences by LLMs, ensuring precise fallacy representation.\nExtensive evaluation demonstrates that SmartyPat produces fallacies comparable\nin subtlety and quality to human-generated content and significantly\noutperforms baseline methods. Finally, experiments reveal nuanced insights into\nLLM capabilities, highlighting that while excessive reasoning steps hinder\nfallacy detection accuracy, structured reasoning enhances fallacy\ncategorization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant progress in language\nunderstanding and reasoning. Evaluating and analyzing their logical reasoning\nabilities has therefore become essential. However, existing datasets and\nbenchmarks are often limited to overly simplistic, unnatural, or contextually\nconstrained examples. In response to the growing demand, we introduce\nSmartyPat-Bench, a challenging, naturally expressed, and systematically labeled\nbenchmark derived from real-world high-quality Reddit posts containing subtle\nlogical fallacies. Unlike existing datasets and benchmarks, it provides more\ndetailed annotations of logical fallacies and features more diverse data. To\nfurther scale up the study and address the limitations of manual data\ncollection and labeling - such as fallacy-type imbalance and labor-intensive\nannotation - we introduce SmartyPat, an automated framework powered by logic\nprogramming-based oracles. SmartyPat utilizes Prolog rules to systematically\ngenerate logically fallacious statements, which are then refined into fluent\nnatural-language sentences by LLMs, ensuring precise fallacy representation.\nExtensive evaluation demonstrates that SmartyPat produces fallacies comparable\nin subtlety and quality to human-generated content and significantly\noutperforms baseline methods. Finally, experiments reveal nuanced insights into\nLLM capabilities, highlighting that while excessive reasoning steps hinder\nfallacy detection accuracy, structured reasoning enhances fallacy\ncategorization performance."
                },
                "authors": [
                    {
                        "name": "Zihao Xu"
                    },
                    {
                        "name": "Junchen Ding"
                    },
                    {
                        "name": "Yiling Lou"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Dong Gong"
                    },
                    {
                        "name": "Yuekang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuekang Li"
                },
                "author": "Yuekang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00328v1",
                "updated": "2025-08-01T05:21:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    5,
                    21,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T05:21:42Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    5,
                    21,
                    42,
                    4,
                    213,
                    0
                ],
                "title": "From Patient Burdens to User Agency: Designing for Real-Time Protection\n  Support in Online Health Consultations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Patient Burdens to User Agency: Designing for Real-Time Protection\n  Support in Online Health Consultations"
                },
                "summary": "Online medical consultation platforms, while convenient, are undermined by\nsignificant privacy risks that erode user trust. We first conducted in-depth\nsemi-structured interviews with 12 users to understand their perceptions of\nsecurity and privacy landscapes on online medical consultation platforms, as\nwell as their practices, challenges and expectation. Our analysis reveals a\ncritical disconnect between users' desires for anonymity and control, and\nplatform realities that offload the responsibility of ``privacy labor''. To\nbridge this gap, we present SafeShare, an interaction technique that leverages\nlocalized LLM to redact consultations in real-time. SafeShare balances utility\nand privacy through selectively anonymize private information. A technical\nevaluation of SafeShare's core PII detection module on 3 dataset demonstrates\nhigh efficacy, achieving 89.64\\% accuracy with Qwen3-4B on IMCS21 dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online medical consultation platforms, while convenient, are undermined by\nsignificant privacy risks that erode user trust. We first conducted in-depth\nsemi-structured interviews with 12 users to understand their perceptions of\nsecurity and privacy landscapes on online medical consultation platforms, as\nwell as their practices, challenges and expectation. Our analysis reveals a\ncritical disconnect between users' desires for anonymity and control, and\nplatform realities that offload the responsibility of ``privacy labor''. To\nbridge this gap, we present SafeShare, an interaction technique that leverages\nlocalized LLM to redact consultations in real-time. SafeShare balances utility\nand privacy through selectively anonymize private information. A technical\nevaluation of SafeShare's core PII detection module on 3 dataset demonstrates\nhigh efficacy, achieving 89.64\\% accuracy with Qwen3-4B on IMCS21 dataset."
                },
                "authors": [
                    {
                        "name": "Shuning Zhang"
                    },
                    {
                        "name": "Ying Ma"
                    },
                    {
                        "name": "Yongquan `Owen' Hu"
                    },
                    {
                        "name": "Ting Dang"
                    },
                    {
                        "name": "Hong Jia"
                    },
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Hewu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hewu Li"
                },
                "author": "Hewu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00321v1",
                "updated": "2025-08-01T05:11:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    5,
                    11,
                    29,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T05:11:29Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    5,
                    11,
                    29,
                    4,
                    213,
                    0
                ],
                "title": "Evaluating the Efficacy of Large Language Models for Generating\n  Fine-Grained Visual Privacy Policies in Homes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Efficacy of Large Language Models for Generating\n  Fine-Grained Visual Privacy Policies in Homes"
                },
                "summary": "The proliferation of visual sensors in smart home environments, particularly\nthrough wearable devices like smart glasses, introduces profound privacy\nchallenges. Existing privacy controls are often static and coarse-grained,\nfailing to accommodate the dynamic and socially nuanced nature of home\nenvironments. This paper investigates the viability of using Large Language\nModels (LLMs) as the core of a dynamic and adaptive privacy policy engine. We\npropose a conceptual framework where visual data is classified using a\nmulti-dimensional schema that considers data sensitivity, spatial context, and\nsocial presence. An LLM then reasons over this contextual information to\nenforce fine-grained privacy rules, such as selective object obfuscation, in\nreal-time. Through a comparative evaluation of state-of-the-art Vision Language\nModels (including GPT-4o and the Qwen-VL series) in simulated home settings ,\nour findings show the feasibility of this approach. The LLM-based engine\nachieved a top machine-evaluated appropriateness score of 3.99 out of 5, and\nthe policies generated by the models received a top human-evaluated score of\n4.00 out of 5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of visual sensors in smart home environments, particularly\nthrough wearable devices like smart glasses, introduces profound privacy\nchallenges. Existing privacy controls are often static and coarse-grained,\nfailing to accommodate the dynamic and socially nuanced nature of home\nenvironments. This paper investigates the viability of using Large Language\nModels (LLMs) as the core of a dynamic and adaptive privacy policy engine. We\npropose a conceptual framework where visual data is classified using a\nmulti-dimensional schema that considers data sensitivity, spatial context, and\nsocial presence. An LLM then reasons over this contextual information to\nenforce fine-grained privacy rules, such as selective object obfuscation, in\nreal-time. Through a comparative evaluation of state-of-the-art Vision Language\nModels (including GPT-4o and the Qwen-VL series) in simulated home settings ,\nour findings show the feasibility of this approach. The LLM-based engine\nachieved a top machine-evaluated appropriateness score of 3.99 out of 5, and\nthe policies generated by the models received a top human-evaluated score of\n4.00 out of 5."
                },
                "authors": [
                    {
                        "name": "Shuning Zhang"
                    },
                    {
                        "name": "Ying Ma"
                    },
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Hewu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hewu Li"
                },
                "author": "Hewu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20750v2",
                "updated": "2025-08-01T05:09:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    5,
                    9,
                    21,
                    4,
                    213,
                    0
                ],
                "published": "2024-12-30T06:44:25Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    6,
                    44,
                    25,
                    0,
                    365,
                    0
                ],
                "title": "Enhanced Vision-Language Models for Diverse Sensor Understanding:\n  Cost-Efficient Optimization and Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Vision-Language Models for Diverse Sensor Understanding:\n  Cost-Efficient Optimization and Benchmarking"
                },
                "summary": "Large-scale Vision-Language Models (VLMs) have achieved notable progress in\naligning visual inputs with text. However, their ability to deeply understand\nthe unique physical properties of non-RGB vision sensor images remains limited.\nIn this paper, we revisit and analyze these limitations and introduce a novel,\ncost-efficient paradigm that significantly advances sensor image\nunderstanding-without requiring extensive training data or any modifications to\nthe existing VLM architectures. Specifically, we propose Sensor-Aware\nAttributes Fine-Tuning (SAFT) with the Diverse Negative Attributes (DNA)\noptimization, which leverages minimal sensor-specific data to enable robust\nlearning of non-RGB characteristics and overcome RGB-centric biases inherent in\ncurrent VLMs. In addition, we present VS-TDX-the first comprehensive, public\nbenchmark designed to rigorously evaluate VLMs' sensor-specific understanding\nacross diverse and realistic scenarios. Through extensive experiments on VLMs\nand various sensor modalities, we validate that our method consistently\ndelivers superior performance and generalization under resource-constrained and\narchitecture-invariant settings. Our approach provides a practical advance\ntowards scalable deployment of VLMs in increasingly sensor-diverse real-world\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Vision-Language Models (VLMs) have achieved notable progress in\naligning visual inputs with text. However, their ability to deeply understand\nthe unique physical properties of non-RGB vision sensor images remains limited.\nIn this paper, we revisit and analyze these limitations and introduce a novel,\ncost-efficient paradigm that significantly advances sensor image\nunderstanding-without requiring extensive training data or any modifications to\nthe existing VLM architectures. Specifically, we propose Sensor-Aware\nAttributes Fine-Tuning (SAFT) with the Diverse Negative Attributes (DNA)\noptimization, which leverages minimal sensor-specific data to enable robust\nlearning of non-RGB characteristics and overcome RGB-centric biases inherent in\ncurrent VLMs. In addition, we present VS-TDX-the first comprehensive, public\nbenchmark designed to rigorously evaluate VLMs' sensor-specific understanding\nacross diverse and realistic scenarios. Through extensive experiments on VLMs\nand various sensor modalities, we validate that our method consistently\ndelivers superior performance and generalization under resource-constrained and\narchitecture-invariant settings. Our approach provides a practical advance\ntowards scalable deployment of VLMs in increasingly sensor-diverse real-world\nenvironments."
                },
                "authors": [
                    {
                        "name": "Sangyun Chung"
                    },
                    {
                        "name": "Youngjoon Yu"
                    },
                    {
                        "name": "Se Yeon Kim"
                    },
                    {
                        "name": "Youngchae Chee"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15059v2",
                "updated": "2025-08-01T05:06:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    5,
                    6,
                    11,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-20T17:50:49Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    17,
                    50,
                    49,
                    6,
                    201,
                    0
                ],
                "title": "Rethinking Pan-sharpening: Principled Design, Unified Training, and a\n  Universal Loss Surpass Brute-Force Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Pan-sharpening: Principled Design, Unified Training, and a\n  Universal Loss Surpass Brute-Force Scaling"
                },
                "summary": "The field of pan-sharpening has recently seen a trend towards increasingly\nlarge and complex models, often trained on single, specific satellite datasets.\nThis approach, however, leads to high computational overhead and poor\ngeneralization on full resolution data, a paradigm we challenge in this paper.\nIn response to this issue, we propose PanTiny, a lightweight, single-step\npan-sharpening framework designed for both efficiency and robust performance.\nMore critically, we introduce multiple-in-one training paradigm, where a\nsingle, compact model is trained simultaneously on three distinct satellite\ndatasets (WV2, WV3, and GF2) with different resolution and spectral\ninformation. Our experiments show that this unified training strategy not only\nsimplifies deployment but also significantly boosts generalization on\nfull-resolution data. Further, we introduce a universally powerful composite\nloss function that elevates the performance of almost all of models for\npan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny\nmodel, benefiting from these innovations, achieves a superior\nperformance-to-efficiency balance, outperforming most larger, specialized\nmodels. Through extensive ablation studies, we validate that principled\nengineering in model design, training paradigms, and loss functions can surpass\nbrute-force scaling. Our work advocates for a community-wide shift towards\ncreating efficient, generalizable, and data-conscious models for\npan-sharpening. The code is available at\nhttps://github.com/Zirconium233/PanTiny .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of pan-sharpening has recently seen a trend towards increasingly\nlarge and complex models, often trained on single, specific satellite datasets.\nThis approach, however, leads to high computational overhead and poor\ngeneralization on full resolution data, a paradigm we challenge in this paper.\nIn response to this issue, we propose PanTiny, a lightweight, single-step\npan-sharpening framework designed for both efficiency and robust performance.\nMore critically, we introduce multiple-in-one training paradigm, where a\nsingle, compact model is trained simultaneously on three distinct satellite\ndatasets (WV2, WV3, and GF2) with different resolution and spectral\ninformation. Our experiments show that this unified training strategy not only\nsimplifies deployment but also significantly boosts generalization on\nfull-resolution data. Further, we introduce a universally powerful composite\nloss function that elevates the performance of almost all of models for\npan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny\nmodel, benefiting from these innovations, achieves a superior\nperformance-to-efficiency balance, outperforming most larger, specialized\nmodels. Through extensive ablation studies, we validate that principled\nengineering in model design, training paradigms, and loss functions can surpass\nbrute-force scaling. Our work advocates for a community-wide shift towards\ncreating efficient, generalizable, and data-conscious models for\npan-sharpening. The code is available at\nhttps://github.com/Zirconium233/PanTiny ."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Li Xueheng"
                    },
                    {
                        "name": "Ke Cao"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Fang Jiabin"
                    },
                    {
                        "name": "Yang Qize"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10009v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10009v3",
                "updated": "2025-08-01T04:52:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    4,
                    52,
                    21,
                    4,
                    213,
                    0
                ],
                "published": "2025-03-13T03:40:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    40,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problems with Reasoning LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problems with Reasoning LLM"
                },
                "summary": "With the rise of artificial intelligence (AI), applying large language models\n(LLMs) to mathematical problem-solving has attracted increasing attention. Most\nexisting approaches attempt to improve Operations Research (OR) optimization\nproblem-solving through prompt engineering or fine-tuning strategies for LLMs.\nHowever, these methods are fundamentally constrained by the limited\ncapabilities of non-reasoning LLMs. To overcome these limitations, we propose\nOR-LLM-Agent, an AI agent framework built on reasoning LLMs for automated OR\nproblem solving. The framework decomposes the task into three sequential\nstages: mathematical modeling, code generation, and debugging. Each task is\nhandled by a dedicated sub-agent, which enables more targeted reasoning. We\nalso construct BWOR, an OR dataset for evaluating LLM performance on OR tasks.\nOur analysis shows that in the benchmarks NL4OPT, MAMO, and IndustryOR,\nreasoning LLMs sometimes underperform their non-reasoning counterparts within\nthe same model family. In contrast, BWOR provides a more consistent and\ndiscriminative assessment of model capabilities. Experimental results\ndemonstrate that OR-LLM-Agent utilizing DeepSeek-R1 in its framework\noutperforms advanced methods, including GPT-o3, Gemini 2.5 Pro, DeepSeek-R1,\nand ORLM, by at least 7\\% in accuracy. These results demonstrate the\neffectiveness of task decomposition for OR problem solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of artificial intelligence (AI), applying large language models\n(LLMs) to mathematical problem-solving has attracted increasing attention. Most\nexisting approaches attempt to improve Operations Research (OR) optimization\nproblem-solving through prompt engineering or fine-tuning strategies for LLMs.\nHowever, these methods are fundamentally constrained by the limited\ncapabilities of non-reasoning LLMs. To overcome these limitations, we propose\nOR-LLM-Agent, an AI agent framework built on reasoning LLMs for automated OR\nproblem solving. The framework decomposes the task into three sequential\nstages: mathematical modeling, code generation, and debugging. Each task is\nhandled by a dedicated sub-agent, which enables more targeted reasoning. We\nalso construct BWOR, an OR dataset for evaluating LLM performance on OR tasks.\nOur analysis shows that in the benchmarks NL4OPT, MAMO, and IndustryOR,\nreasoning LLMs sometimes underperform their non-reasoning counterparts within\nthe same model family. In contrast, BWOR provides a more consistent and\ndiscriminative assessment of model capabilities. Experimental results\ndemonstrate that OR-LLM-Agent utilizing DeepSeek-R1 in its framework\noutperforms advanced methods, including GPT-o3, Gemini 2.5 Pro, DeepSeek-R1,\nand ORLM, by at least 7\\% in accuracy. These results demonstrate the\neffectiveness of task decomposition for OR problem solving."
                },
                "authors": [
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Pengcheng Luo"
                    },
                    {
                        "name": "Genke Yang"
                    },
                    {
                        "name": "Boon-Hee Soong"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "8 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10009v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10009v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19573v2",
                "updated": "2025-08-01T04:33:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    4,
                    33,
                    49,
                    4,
                    213,
                    0
                ],
                "published": "2025-02-26T21:33:06Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    21,
                    33,
                    6,
                    2,
                    57,
                    0
                ],
                "title": "Do Large Language Models Know How Much They Know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Know How Much They Know?"
                },
                "summary": "Large Language Models (LLMs) have emerged as highly capable systems and are\nincreasingly being integrated into various uses. However, the rapid pace of\ntheir deployment has outpaced a comprehensive understanding of their internal\nmechanisms and a delineation of their capabilities and limitations. A desired\nattribute of an intelligent system is its ability to recognize the scope of its\nown knowledge. To investigate whether LLMs embody this characteristic, we\ndevelop a benchmark designed to challenge these models to enumerate all\ninformation they possess on specific topics. This benchmark evaluates whether\nthe models recall excessive, insufficient, or the precise amount of\ninformation, thereby indicating their awareness of their own knowledge. Our\nfindings reveal that all tested LLMs, given sufficient scale, demonstrate an\nunderstanding of how much they know about specific topics. While different\narchitectures exhibit varying rates of this capability's emergence, the results\nsuggest that awareness of knowledge may be a generalizable attribute of LLMs.\nFurther research is needed to confirm this potential and fully elucidate the\nunderlying mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as highly capable systems and are\nincreasingly being integrated into various uses. However, the rapid pace of\ntheir deployment has outpaced a comprehensive understanding of their internal\nmechanisms and a delineation of their capabilities and limitations. A desired\nattribute of an intelligent system is its ability to recognize the scope of its\nown knowledge. To investigate whether LLMs embody this characteristic, we\ndevelop a benchmark designed to challenge these models to enumerate all\ninformation they possess on specific topics. This benchmark evaluates whether\nthe models recall excessive, insufficient, or the precise amount of\ninformation, thereby indicating their awareness of their own knowledge. Our\nfindings reveal that all tested LLMs, given sufficient scale, demonstrate an\nunderstanding of how much they know about specific topics. While different\narchitectures exhibit varying rates of this capability's emergence, the results\nsuggest that awareness of knowledge may be a generalizable attribute of LLMs.\nFurther research is needed to confirm this potential and fully elucidate the\nunderlying mechanisms."
                },
                "authors": [
                    {
                        "name": "Gabriele Prato"
                    },
                    {
                        "name": "Jerry Huang"
                    },
                    {
                        "name": "Prasanna Parthasarathi"
                    },
                    {
                        "name": "Shagun Sodhani"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "arxiv_comment": "ublished as a long paper at the 2024 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP). Official version of paper within\n  conference proceedings is available at\n  https://aclanthology.org/2024.emnlp-main.348/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]